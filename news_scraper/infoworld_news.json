[
{"title": "Reconciling political beliefs with career ambitions", "overview": "Data professionals face personal dilemmas as AI contributes to ethical issues such as reduced privacy, algorithmic bias, and advanced military weapons. ", "image_url": null, "url": "https://www.infoworld.com/article/3599208/reconciling-political-beliefs-with-career-ambitions.html", "body": "Today’s political environment is increasingly hostile to data management as a profession. If you’ve made your career in machine learning, data mining, predictive modeling, or related fields, these controversies may have you second-guessing your decision to pursue this line of work. The principal flashpoints center on issues of data privacy, algorithmic bias, and AI weaponization.Prioritizing privacy over data-driven marketingData management professionals face growing scrutiny over privacy violations, surveillance, and other intrusive impacts of the applications they’re responsible for building and managing.Also on InfoWorld: Will work from home be permanent? Many developers like the ideaOne of the most disturbing new themes is the ideological framing of how technology enables “surveillance capitalism.” This term, coined by Harvard Business School professor Shoshana Zuboff, essentially stigmatizes the collection, ownership, processing, and use of customers’ PII (personally identifiable information). This notion regards any enterprise use of customer PII—such as microsegmentation, contextual offer targeting, and cross-channel ad optimization—as a form of monitoring and control.Concerns over data-driven CRM aren’t limited to academics. It's clear from recent congressional hearings that this perspective aligns with popular opinion regarding the practices of Google, Facebook, Amazon, and other 21 century digital businesses. The public is increasingly uneasy about privacy encroachment, as big brands compete to see who can acquire the most comprehensive range of intrusive data about every aspect of our lives, including our inner thoughts, sentiments, and predilections.Consequently, many data professionals are facing a crossroads in their careers. On the one hand, their employers have built successful businesses fueled by predictive targeting, one-to-one personalization and multichannel engagement. On the other hand, data professionals are feeling more squeamish about the depth to which they use customer PII to fuel AI-driven CRM programs, such as predictive personality profiling, next-best-action targeting, or real-time behavioral pricing.None of this is particularly strange, sinister, or shameful. These practices are central to how business is done these days. If you have ideological misgivings about these or other data-driven CRM methodologies, you’ll probably never work in enterprise data management or modern marketing. If you refuse to work on a program simply because it implements these modern customer-engagement methodologies, you won’t get much sympathy from your employers and, in fact, they are likely to show you the door.But that doesn’t mean you have to stand idly by while your employer runs amok with customer data. You can become your company’s foremost data-privacy advocate, for example. If nothing else, you can make sure your firm complies rigorously with the European Union’s General Data Protection Regulation and similar privacy laws elsewhere.Ridding our lives of data-driven algorithmic biasesData has been on the front lines in recent culture wars due to accusations of racial, gender, and other forms of socioeconomic bias perpetrated in whole or in part through algorithms.Algorithmic biases have become a hot-button issue in global society, a trend that has spurred many jurisdictions and organizations to institute a greater degree of algorithmic accountability in AI practices. Data scientists who’ve long been trained to eliminate biases from their work now find their practices under growing scrutiny from government, legal, regulatory, and other circles.Eliminating bias in the data and algorithms that drive AI requires constant vigilance on the part of not only data scientists but up and down the corporate ranks. As Black Lives Matter and similar protests have pointed out, data-driven algorithms can embed serious biases that harm demographic groups (racial, gender, age, religious, ethnic, or national origin) in various real-world contexts.Much of the recent controversy surrounding algorithmic biases has focused on AI-driven facial recognition software. Biases in facial recognition applications are especially worrisome if used to direct predictive policing programs or potential abuse by law enforcement in urban areas with many disadvantaged minority groups.Many AI solution vendors have seen an extensive grassroots effort among their own employees to take a strong stand against police abuses of facial recognition. In June as the Black Lives Matter protests heated up, employees at Amazon Web Services called on the firm to sever its police contracts. More than 250 Microsoft employees published an open letter demanding that the company end its work with police departments.This is an AI application domain in which practitioners will have to take their lumps as biases continue to surface, and associated legal and regulatory penalties follow closely behind. So far there is little consensus on viable frameworks for regulating uses, deployments, and management of facial recognition programsNevertheless, AI practitioners know that the opportunities in facial recognition are too numerous and lucrative to forgo indefinitely. Embedding facial recognition into iPhones and other devices will ensure that this technology is a key tool in everybody’s personal tech portfolio. More businesses are incorporating facial recognition into internal and customer-facing applications for biometric authentication, image/video autotagging, query by image, and other valuable uses. Social distancing has made many people more receptive to facial recognition as a contactless option for strong authentication to many device-level and online services.Indeed, a recent Cap Gemini global survey found that adoption of facial recognition is likely to continue growing among large businesses in every sector and region, even as the pandemic recedes.Bias isn’t an issue that can be fixed once and for all. Where decision-making algorithms are concerned, organizations must always make biases as transparent as possible and attempt to eliminate any that perpetuate unfair societal outcomes. In addition, ongoing auditing of AI biases—not just in facial recognition but in all other socially impactful application domains—must become a standard task in AI devops workflows.­­All of this raises the possibility that more data scientists will have to decide whether to participate in such projects and to what extent. Many may take the limited, near-term option of opting out of contributing to law enforcement applications of facial recognition. As a longer-term sustainable approach, data scientists who aren’t ideologically opposed to facial recognition will need to redouble efforts to eliminate biases that are baked into these models and the facial-image data sets that train them.Opting out of AI-driven weapons developmentData is central to modern warfare. Data-driven algorithms, especially deep learning, give weapons systems the capability of seeing, hearing, sensing, and adjusting real-time strategies far better and faster than most humans.AI is the future of warfare and of defenses against algorithmic weapon systems. Future battles will almost certainly have casualty counts that are staggering and lopsided, especially when one side’s arsenal is almost entirely composed of autonomous weapons systems equipped with phalanxes of 3D cameras, millimeter-wave radar, biochemical detectors, and other ambient sensors.If you’re a career-minded data scientist, you’ll be tempted to lend your talents to military projects, which tend to be the most exciting, cutting-edge, R&D-driven initiatives in AI. The shortage of highly qualified AI professionals practically ensures that if you have what it takes, you’ll fetch a high salary with numerous perks. Also, the amount of VC money flowing into startups in this sector ensures that many data scientists who’ve cut their teeth on AI-centric weapon programs will become quite wealthy and powerful.AI professionals are highly ambivalent about participating in military projects. In a recent survey cited here, U.S. AI specialists reported more favorable than unfavorable attitudes about working with the Department of Defense, though a large plurality are neutral on the topic. Respondents reported being more favorably inclined to accepting DoD grants for basic research than applied research. In the survey, AI professionals’ most cited reason for taking DoD grants was to work on “interesting problems.” “Discomfort with how DoD will use the work” was the most frequently cited downside. Approximately three-quarters of those surveyed had negative attitudes about battlefield applications of AI.AI professionals may think that hitching their career to a commercial cloud provider such as Amazon Web Services, Microsoft, Google, or IBM will help them avoid pressure to work on military projects. That’s just not so. Many tech firms have been applying their innovations to DoD projects for several years. In fact, Big Tech continues to cultivate close ties with the U.S. military, as shown in this recent study.If you think you can limit your contributions to defensive and back-office AI applications in the military—per the “ground rules” that former Google CEO Eric Schmidt proposed a few years ago—you’re in for a rude awakening. No such rules for commercial AI vendors’ military engagement can realistically stop the underlying approaches from being used in weapons systems for offensive purposes. In fact, the likelihood of that possibility is heightened by the fact that many military projects’ underlying AI technologies, including open-source modeling software and unclassified image data, are available freely.In the unlikely scenario that all AI companies walk away from projects with the United States’ and other nations’ military establishments, that would still leave an opportunity for universities and nonprofit research centers to pick up the work. Considering how much money the military is likely to funnel into such contracts, this could easily reverse the brain drain that’s causing the best and brightest AI researchers to leave academia and seek their fortunes in the private sector.None of this means you can’t opt out of participating in the cyber-industrial complex that’s sprung up around militarized AI. If you’re morally opposed to this sort of work, you can spend your entire data career without ever having to compromise your principles. AI specialists can find plenty of humanitarian or other unobjectionable uses for their talents.Alternately, you might consider working on technological countermeasures designed to neutralize an adversary’s AI-powered weaponry. One of the most promising new professional opportunity is in building AI-driven counterdrone defenses. In recent years, militaries all over the world have deployed drones successfully as a fast-strike, low-cost, AI-driven alternative to conventional warfare tactics such as armored vehicles and fighter jets. For example, Azerbaijan used drones successfully in a recent war against Armenia, deploying the miniature unmanned aerial vehicles to destroy tanks and other armored fighting vehicles.Counterdrone defenses are a hot focus of R&D and startup activity. Many such projects use AI to automate detecting drones, pinpointing locations, and predicting likely flight paths of drones. Many use AI to automate classification of approaching drones by model, operator, and threat profile, taking care to minimize false positive and false negatives. They also rely on machine learning to automatically trigger security alerts and activate the mechanisms to physically destroy, disable, distract, or otherwise neutralize weaponized drones.Finding a middle roadIn a politically polarized cultural landscape, data professionals may find it difficult to keep their leanings under wraps. They may also have pangs of conscience that deter them from engaging in projects whose objectives contravene their deep convictions.Before you opt out of some otherwise objectionable datacentric project, consider whether you can contribute to implementing effective controls such as privacy protection mechanisms, debiasing processes, and automated countermeasures that mitigate the more objectionable aspects. That could allow you, on some level, to reconcile your political convictions with your professional ambitions.In the process, you would be doing your part to make the world a better place.", "pub_date": "2020-12-03"},
{"title": "Nvidia pushes into a wider application ecosystem", "overview": "The BlueField DPU architecture and DOCA SDK provide a strategic platform for expanding the company's reach.", "image_url": null, "url": "https://www.infoworld.com/article/3587769/nvidia-pushes-into-a-wider-application-ecosystem.html", "body": "Nvidia is extending its solution footprint far beyond artificial intelligence (AI) and gaming, venturing broadly across the entire computing ecosystem into mobility and the next-generation cloud data center.Nvidia’s ambitions in this regard are clear from its pending acquisition of Arm Technology and from CEO Jensen Huang’s positioning of the company as a “full-stack computing” provider. Demonstrating that he’s putting substantial R&D dollars behind this vision, at the virtual Nvidia GPU Technology Conference this month, Huang announced the rollout of the company’s new BlueField “data processing unit (DPU)” chip architecture.Also on InfoWorld: What is CUDA? Parallel programming for GPUsAccelerating diverse workloads through programmable CPU offloadStrategically, the BlueField DPU builds on two of Nvidia’s boldest recent acquisitions. The new hardware architecture runs on Arm’s CPU architecture. It also incorporates high-speed interconnect technology that Nvidia acquired recently with Mellanox.Marking the company’s evolution beyond a GPU-centric product architecture, Nvidia’s new DPU architecture is a high-performance, multicore SoC (system on chip). BlueField DPUs incorporate software-programmable data-processing engines that can accelerate a wide range of AI, networking, acceleration, virtualization, security, storage, and other enterprise workloads.As the foundation of server-based intelligent network interface controllers, DPUs offload workloads from CPUs while efficiently parsing, processing, and transferring high volumes of data at line speeds. In addition to their CPU-offload acceleration benefits, Nvidia’s DPUs can strengthen data center security because the Arm cores embedded within them provide an added level of isolation between security services and CPU-executed applications.Announced at this latest GTC were the following versions of this new DPU SoC family:: Due to be included in new systems from Nvidia server hardware partners in 2021, this architecture features all capabilities of the Nvidia Mellanox ConnectX-6 Dx SmartNIC. It incorporates programmable Arm cores, supports data transfer rates of 200Gbps, and provides hardware offloads to accelerate key data center tasks. It speeds up security, networking and storage tasks, including isolation, root trust, key management, RDMA/RDMA over Converged ethernet, GPUDirect, elastic block storage, and data compression. It includes a controller for managing high-performance back-end nonvolatile memory express storage, all-flash arrays and hyperconverged systems. A single BlueField-2 DPU can offload data center workloads from as many as 125 CPU cores, thereby freeing up cycles to process other enterprise applications.: Under development and due to become available in 2021, this adds an Nvidia Ampere architecture GPU to BlueField-2 for in-networking computing with CUDA and Nvidia AI. It includes all the key features of BlueField-2 and leverages Nvidia’s third-generation Tensor Cores for real-time AI-driven security analytics. It can identify abnormal traffic indicative of theft of confidential data. It can also encrypt traffic analytics at line rate and introspect traffic to identify malicious activity and automatically trigger security features and automated responses.Nvidia also announced that it will launch next-generation BlueField-3 and BlueField-3X DPUs in 2022, and BlueField-4X in 2023. In the latter generation, Nvidia will integrate the GPU and Arm cores at the silicon level. The company promised that BlueField-4 will boost the DPU’s processing speeds 1000 times beyond BlueField-2X and 600 times beyond BlueField-3X.Building a robust ecosystem around the DPU accelerator architectureAs it evolves its hardware platform into a DPU-centric architecture in support of new enterprise applications, Nvidia is also making sure that it fully integrates its BlueField/DOCA accelerators into the Arm partner ecosystem.Signaling that strategy at GTC, the vendor announced that it will help Arm partners go to market with full-stack solution platforms that consist of GPU-enabled as well as DPU-enabled networking, storage and security technologies. It has engaged Arm partners to create full-stack solutions for high-performance computing, cloud, edge and PC opportunities. Also, it is porting its AI and RTX engines to Arm, so that they address a much larger market than the x86 platforms on which Nvidia has traditionally run.Partners are essential to Nvidia’s plans to support a wider range of enterprise application workloads than just AI on its new DPU product family. Integral to Nvidia’s land-and-expand strategy is DOCA, a new data center infrastructure SoC architecture and software development kit.Currently available to early access partners only, the DOCA SDK enables developers to program applications on BlueField-accelerated data center infrastructure services. Developers can offload CPU workloads to BlueField DPUs. Consequently, this new offering builds out Nvidia’s enterprise developer tools, complementing the CUDA programming model that enables development of GPU-accelerated applications. In addition, the SDK is fully integrated into the Nvidia NGC catalog of containerized software, thereby encouraging third-party application providers to develop, certify, and distribute DPU-accelerated applications.Several leading software vendors (VMware, Red Hat, Canonical, and Check Point Software Technologies) announced plans at GTC to integrate their wares with the new DSP/DOCA acceleration architecture in the coming year. In addition, Nvidia announced that several leading server manufacturers, including Asus, Atos, Dell Technologies, Fujitsu, Gigabyte, H3C, Inspur, Lenovo, Quanta/QCT, and Supermicro, plan to integrate the DPU into their respective products in the same timeframe.Although there was no specific Arm tie-in to Huang’s announcement that Microsoft is adopting Nvidia AI on Azure to bring GPU-accelerated smart experiences to its cloud-based Microsoft Office experience, it would not be surprising if, in coming years, more of the mobile experience on this and other Office apps were accelerated locally by leveraging DPU-offload technology .Enabling Nvidia solutions to lessen their dependency on GPU-centric functionalityNvidia’s product teams are wasting no time to incorporate the DPUs’ CPU-offload acceleration into their solutions. Most notably, Huang announced that the Nvidia EGX AI edge-server platform is evolving to combine the Nvidia Ampere architecture GPU and BlueField-2 DPU on a single PCIe card.Although there was no specific BlueField DPU tie-in to Nvidia Jetson, the company’s Arm-based SoC for AI robotics, one should expect that the DOCA SDK will advance to support development of these applications, which are a hot growth field for Nvidia’s core platforms. It’s also a safe bet that the company will use its new hardware and SDK to accelerate its Omniverse platform for collaborative 3-D content production, its Jarvis platform for conversational AI, and its new Maxine platform for cloud-native, AI-accelerated video streaming.Maintaining momentumNvidia’s new BlueField DPU architecture and DOCA SDK provide a strategic platform for broadening its reach into enterprise, service provider, and consumer opportunities of all types.By enabling hardware-accelerated CPU-offload of diverse workloads, the DPU architecture provides Nvidia with a clear path for converging the new DOCA programming models with its CUDA AI development framework and NGC catalog of containerized cloud solutions. This will enable the company to provide both its own product teams and solution partners with the hardware and software platforms needed to accelerate a full range of application and infrastructure workloads from cloud to edge.As it awaits the eventual approval of its proposed acquisition of Arm Technology, Nvidia will need to prove this new architecture to its existing partner ecosystem. If DPU technology falls short of Nvidia’s aggressive performance promises, that deficiency could sour relations with Arm’s vast array of licensees, all of whom rely heavily on its CPU-based processor architecture and would benefit from more seamless integration with Nvidia’s market-leading AI technology.Clearly Nvidia cannot afford to lose momentum in the cloud-to-edge microprocessor wars just when it has begun to pull away from archrival and CPU-powerhouse Intel.", "pub_date": "2020-10-29"},
{"title": "Weaponizing the cloud to fight addiction", "overview": "We’re seeing a rise in overdose deaths in the U.S. during the pandemic. Perhaps it’s time to leverage cloud technology to address this problem. ", "image_url": null, "url": "https://www.infoworld.com/article/3597988/weaponizing-the-cloud-to-fight-addiction.html", "body": "According to the CDC, opioids were involved in 46,802 overdose deaths in 2018 (69.5 percent of all drug overdose deaths). For those of you living in the United States, this is old news.As the pandemic stretches on, deaths in the U.S. from opioids and other habit-forming drugs such as alcohol, are likely to rise in 2020. We’re at a point where most health organizations are deeply concerned.We could reduce the number of deaths by helping addicts in better ways. The combinations of cloud, artificial intelligence, and IoT (Internet of Things) working together could replace rehab clinics as the preferred way to overcome dangerous or unhealthy addictions.Core to this approach is the notion of habit tracking. Or, better put, monitoring behaviors over a long period of time to determine if you have a problem. Then you find a way to resolve it on your own using technology. Also on InfoWorld: Cloud tech certifications count more than degrees nowA virtual breathalyzer is in development to combat alcoholism, for example. Cloud-connected sensors in devices that we carry, such as smartphones or fitness bands, are able to detect someone’s level of intoxication based on walking patterns. Automobiles could be prevented from starting until the intoxication subsides. These behaviors are turned into patterns that can be spotted with machine learning systems, and the problem diagnosed. From there certain automated processes are triggered, such as not allowing driving, or more importantly, kicking off treatment processes that could prevent at least 10 to 20 percent of addiction-related deaths.These systems would not be possible without cloud computing and related services, such as AI and IoT. Unless the cost is low and the back-end processing powerful, they won’t be effective. Moreover, humans have to carry or wear these devices, so the user experience has to be compelling, as well.This is really a matter of leveraging technology that is becoming a commodity (cloud and IoT) to solve another problem. Most of us know an addict or perhaps are one ourselves. We have the potential to do a lot of good with a very small amount of cost and technology.", "pub_date": "2020-11-20"},
{"title": "On-premises data warehouses are dead", "overview": "The writing's been on the wall for awhile now. Data analytics migration to the cloud is now underway and is making on-premises data warehouses obsolete.   ", "image_url": null, "url": "https://www.infoworld.com/article/3597971/on-premises-data-warehouses-are-dead.html", "body": "Global Market Insights estimates that cloud providers will host the majority of data warehousing loads by 2025. But don’t take their word for it. Gartner estimates that 30 percent of data warehousing workloads now run in the cloud and that this will grow to two-thirds by 2024. Just a few years ago in 2016 the figure was less than 7 percent, also according to Gartner.   None of this should be a surprise. Even the core data warehouse technology providers have seen this trend and are spending the majority of their R&D budgets to build solutions for public cloud providers. Moreover, the public cloud providers themselves have “company killing” products, such as AWS’s RedShift, a columnar database designed to compete with the larger enterprise data warehouse players. What is cloud computing? Everything you need to know nowwhat is IaaS (infrastructure as a service)what is PaaS (platform as a service)what is SaaS (software as a service)What is multicloud? The next step in cloud computingPast impediments to building data warehouses and data marts on public clouds included a perception that security was still an issue on public clouds. Also petabytes of data were difficult to move from on-premises systems, considering that they had to be physically moved with portable storage systems. Finally, in many instances those running data warehouses on-premises could not find analytics tools to leverage locally and did not want to change. The reality is that all these blocks have been removed. Most were removed well before the people building and maintaining data warehouses understood that public clouds were far ahead of most on-premises tools. Today the cloud has better security, performance, cost, and analytics.    The real killer of on-premises data warehouses has been the rise of artificial intelligence on the cloud and the ability to integrate AI with traditional data analytics. AI is not new, but the ability to pay next to nothing for data intelligence, collocated with your data on the public clouds is. AI is a game changer, considering that data warehouses are also a source of training data that could span decades and provide business insights not yet achievable. Another trend that has pushed many on-premises data warehouses to the cloud is the rising need to leverage transactional data directly for analytics. Data warehouses have been famous for just taking snapshots of transactional data and rolling it up into a data warehouse for analytics. This means that the information could be several weeks if not months old. More and more executives are asking for real-time dashboards that consider current data from transactional systems, such as sales order entry.    This means we’ll need to use transitional data using data abstraction layers to emulate analytical databases and bind them with AI systems to make the solution even more compelling. It should come as no surprise that this technology can be found from public cloud providers, either through native services or through their ecosystems and marketplaces. So, on-premises data warehousing is pretty much dead. It’s survived by cloud-based data analytics and database technology that is easily augmented by cheap AI and the ability to deal with data in more innovative ways, such as using transactional data.    The movement to the cloud does a few things. First, it allows enterprises to finally consolidate data on a centrally accessible platform. Second, the data is typically more secure in the cloud. Finally, those who need to leverage the data are no longer restricted to the limitations of on-premises technology. These should be the last three nails in the coffin.  ", "pub_date": "2020-11-24"},
{"title": "Review: DataRobot aces automated machine learning", "overview": "DataRobot’s end-to-end AutoML suite not only speeds up the creation of accurate models, but can combine time series, images, geographic information, tabular data, and text in a single model", "image_url": null, "url": "https://www.infoworld.com/article/3596593/review-datarobot-aces-automated-machine-learning.html", "body": "Data science is nothing if not tedious, in ordinary practice. The initial tedium consists of finding data relevant to the problem you’re trying to model, cleaning it, and finding or constructing a good set of features. The next tedium is a matter of attempting to train every possible machine learning and deep learning model to your data, and picking the best few to tune.Then you need to understand the models well enough to explain them; this is especially important when the model will be helping to make life-altering decisions, and when decisions may be reviewed by regulators. Finally, you need to deploy the best model (usually the one with the best accuracy and acceptable prediction time), monitor it in production, and improve (retrain) the model as the data drifts over time.Read the InfoWorld review: Google Cloud AI lights up machine learningAutoML, i.e. automated machine learning, can speed up these processes dramatically, sometimes from months to hours, and can also lower the human requirements from experienced Ph.D. data scientists to less-skilled data scientists and even business analysts. DataRobot was one of the earliest vendors of AutoML solutions, although they often call it Enterprise AI and typically bundle the software with consulting from a trained data scientist. DataRobot didn’t cover the whole machine learning lifecycle initially, but over the years they have acquired other companies and integrated their products to fill in the gaps.As shown in the listing below, DataRobot has divided the AutoML process into 10 steps. While DataRobot claims to be the only vendor to cover all 10 steps, other vendors might beg to differ, or offer their own services plus one or more third-party services as a “best of breed” system. Competitors to DataRobot include (in alphabetical order) AWS, Google (plus Trifacta for data preparation), H2O.ai, IBM, MathWorks, Microsoft, and SAS.The 10 steps of automated machine learning, according to DataRobot: Data identificationData preparationFeature engineeringAlgorithm diversityAlgorithm selectionTraining and tuningHead-to-head model competitionsHuman-friendly insightsEasy deploymentModel monitoring and managementDataRobot platform overviewAs you can see in the slide below, the DataRobot platform tries to address the needs of a variety of personas, automate the entire machine learning lifecycle, deal with the issues of model explainability and governance, deal with all kinds of data, and deploy pretty much anywhere. It mostly succeeds.DataRobot helps data engineers with its AI Catalog and Paxata data prep. It helps data scientists primarily with its AutoML and automated time series, but also with its more advanced options for models and its Trusted AI. It helps business analysts with its easy-to-use interface. And it helps software developers with its ability to integrate machine learning models with production systems. DevOps and IT benefit from DataRobot MLOps (acquired in 2019 from ParallelM), and risk and compliance officers can benefit from its Trusted AI. Business users and executives benefit from better and faster model building and from data-driven decision making.End-to-end automation speeds up the entire machine learning process and also tends to produce better models. By quickly training many models in parallel and using a large library of models, DataRobot can sometimes find a much better model than skilled data scientists training one model at a time. A quote from an associate professor of information management on one of DataRobot’s web pages essentially says that DataRobot AutoML managed to find a model in one hour(!) that outperformed (by a factor of two!) the best model a skilled grad student was able to train in a few months, because the student had missed a class of algorithms that worked well for the data. Your mileage may vary, of course.In the row marked multimodal in the diagram below, there are five icons. At first they confused me, so I asked what they mean. Essentially, DataRobot has models that can handle time series, images, geographic information, tabular data, and text. The surprising bit is that it can combine all of those data types in a single model.DataRobot offers you a choice of deployment locations. It will run on a Linux server or Linux cluster on-premises, in a cloud VPC, in a hybrid cloud, or in a fully managed cloud. It supports Amazon Web Services, Microsoft Azure, or Google Cloud Platform, as well as Hadoop and Kubernetes.DataRobot platform diagram. Several of the features were added to the platform through acquisitions, including data preparation and MLOps.Paxata data prepDataRobot acquired self-service data preparation company Paxata in December 2019. Paxata is now integrated with DataRobot’s AI Catalog and feels like part of the DataRobot product, although you can still buy it as a standalone product if you wish.Paxata has three functions. First, it allows you to import datasets. Second, it lets you explore, clean, combine, and condition the data. And third, it allows you to publish prepared data as an AnswerSet. Each step you perform in Paxata creates a version, so that you can always continue to work on the data.Also on InfoWorld: MLOps: The rise of machine learning operationsData cleaning in Paxata includes standardizing values, removing duplicates, finding and fixing errors, and more. You can shape your data using tools such as pivot, transpose, group by, and more.The screenshot below shows a real estate dataset that has a dozen Paxata processing steps. It starts with a house price tabular dataset; then it adds exterior and interior images, removes unnecessary columns and bad rows, and adds ZIP code geospatial information. This screenshot is from the House Listings demo.Paxata allows the user to construct AnswerSets from datasets one step at a time. The Paxata tools all have a GUI, although the Compute tool lets the user enter simple formulas or build advanced formulas using columns and functions.DataRobot automated machine learningBasically, DataRobot AutoML works by going through a couple of exploratory data analysis (EDA) phases, identifying informative features, engineering new features (especially from  types), then trying a lot of models with small amounts of data.EDA phase 1 runs on up to 500MB of your dataset and provides summary statistics, as well as checking for outliers, inliers, excess zeroes, and disguised missing values. When you select a target and hit run, DataRobot “searches through millions of possible combinations of algorithms, preprocessing steps, features, transformations, and tuning parameters. It then uses supervised learning algorithms to analyze the data and identify (apparent) predictive relationships.”DataRobot autopilot mode starts with 16% of the data for all appropriate models, 32% of the data for the top 16 models, and 64% of the data for the top eight models. All results are displayed on the leaderboard. Quick mode runs a subset of models on 32% and 64% of the data. Manual mode gives you full control over which models to execute, including specific models from the repository.DataRobot AutoML in action. The models being trained are at the right, along with the percentage of the data being used for training each model.DataRobot time-aware modelingDataRobot can do two kinds of time-aware modeling if you have date/time features in your dataset. You should use out-of-time validation (OTV) when your data is time-relevant but you are not forecasting (instead, you are predicting the target value on each individual row). Use OTV if you have single event data, such as patient intake or loan defaults.You can use time series when you want to forecast multiple future values of the target (for example, predicting sales for each day next week). Use time series to extrapolate future values in a continuous sequence.In general, it has been difficult for machine learning models to outperform traditional statistical models for time series prediction, such as ARIMA. DataRobot’s time series functionality works by encoding time-sensitive components as features that can contribute to ordinary machine learning models. It adds columns to each row for examples of predicting different distances into the future, and columns of lagged features and rolling statistics for predicting that new distance.Values over time graph for time-related data. This helps to determine trends, weekly patterns, and seasonal patterns.DataRobot Visual AIIn April 2020 DataRobot added image processing to its arsenal. Visual AI allows you to build binary and multi-class classification and regression models with images. You can use it to build completely new image-based models or to add images as new features to existing models. Also on InfoWorld: 10 MLops platforms to manage the machine learning lifecycleVisual AI uses pre-trained neural networks, and three new models: Neural Network Visualizer, Image Embeddings, and Activation Maps. As always, DataRobot can combine its models for different field types, so classified images can add accuracy to models that also use numeric, text, and geospatial data. For example, an image of a kitchen that is modern and spacious and has new-looking, high-end appliances might result in a home-pricing model increasing its estimate of the sale price.There is no need to provision GPUs for Visual AI. Unlike the process of training image models from scratch, Visual AI’s pre-trained neural networks work fine on CPUs, and don’t even take very long.This multi-class confusion matrix for image classification shows a fairly clean separation, with most of the predictions true positives or true negatives.The color overlays in these home exterior images from the House Listings demo highlight the features the model factored into its sale price predictions. These factors were combined with other fields, such as square footage and number of bedrooms.DataRobot Trusted AIIt’s easy for an AI model to go off track, and there are numerous examples of what not to do in the literature. Contributing factors include outliers in the training data, training data that isn’t representative of the real distribution, features that are dependent on other features, too many missing feature values, and features that leak the target value into the training.DataRobot has guardrails to detect these conditions. You can fix them in the AutoML phase, or preferably in the data prep phase. Guardrails let you trust the model more, but they are not infallible.Humble AI rules allow DataRobot to detect out of range or uncertain predictions as they happen, as part of the MLOps deployment. For example, a home value of $100 million in Cleveland is unheard-of; a prediction in that range is most likely a mistake. For another example, a predicted probability of 0.5 may indicate uncertainty. There are three ways of responding when humility rules fire: Do nothing but keep track, so that you can later refine the model using more data; override the prediction with a “safe” value; or return an error.Too many machine learning models lack explainability; they are nothing more than black boxes. That’s often especially true of AutoML. DataRobot, however, goes to great lengths to explain its models. The diagram that follows is fairly simple, as neural network models go, but you can see the strategy of processing text and categorical variables in separate branches and then feeding the results into a neural network.Blueprint for an AutoML model. This model processes categorical variables using one-hot encoding and text variables using word-grams, then factors them all into a Keras Slim Residual neural network classifier. You can drill into any box to see the parameters and get a link to the relevant documentation.DataRobot MLOpsOnce you have built a good model you can deploy it as a prediction service. That isn’t the end of the story, however. Over time, conditions change. We can see an example in the graphs below. Based on these results, some of the data that flows into the model — elementary school locations — needs to be updated, and then the model needs to be retrained and redeployed.Looking at the feature drift from MLOps tells you when conditions change that affect the model’s predictions. Here we see that a new elementary school has opened, which typically raises the value of nearby homes.Overall, DataRobot now has an end-to-end AutoML suite that takes you from data gathering through model building to deployment, monitoring, and management. DataRobot has paid attention to the pitfalls in AI model building and provided ways to mitigate many of them. Overall, I rate DataRobot very good, and a worthy competitor to Google, AWS, Microsoft, and H2O.ai. I haven’t reviewed the machine learning offerings from IBM, MathWorks, or SAS recently enough to rate them.Also on InfoWorld: How to choose a cloud machine learning platformI was surprised and impressed to discover that DataRobot can run on CPUs without accelerators and produce models in a few hours, even when building neural network models that include image classification. That may give it a slight edge over the four competitors I mentioned for AutoML, because GPUs and TPUs are not cheap.", "pub_date": "2020-12-02"},
{"title": "Why enterprises are turning from TensorFlow to PyTorch", "overview": "The deep learning framework PyTorch has infiltrated the enterprise thanks to its relative ease of use. Three companies tell us why they chose PyTorch over Google’s renowned TensorFlow framework.", "image_url": null, "url": "https://www.infoworld.com/article/3597904/why-enterprises-are-turning-from-tensorflow-to-pytorch.html", "body": "A subcategory of machine learning, deep learning uses multi-layered neural networks to automate historically difficult machine tasks—such as image recognition, natural language processing (NLP), and machine translation—at scale.TensorFlow, which emerged out of Google in 2015, has been the most popular open source deep learning framework for both research and business. But PyTorch, which emerged out of Facebook in 2016, has quickly caught up, thanks to community-driven improvements in ease of use and deployment for a widening range of use cases.Also on InfoWorld: The best free online PyTorch courses and tutorialsPyTorch is seeing particularly strong adoption in the automotive industry—where it can be applied to pilot autonomous driving systems from the likes of Tesla and Lyft Level 5. The framework also is being used for content classification and recommendation in media companies and to help support robots in industrial applications.Joe Spisak, product lead for artificial intelligence at Facebook AI, told InfoWorld that although he has been pleased by the increase in enterprise adoption of PyTorch, there’s still much work to be done to gain wider industry adoption.“The next wave of adoption will come with enabling lifecycle management, MLOps, and Kubeflow pipelines and the community around that,” he said. “For those early in the journey, the tools are pretty good, using managed services and some open source with something like SageMaker at AWS or Azure ML to get started.”Disney: Identifying animated faces in moviesSince 2012, engineers and data scientists at the media giant Disney have been building what the company calls the Content Genome, a knowledge graph that pulls together content metadata to power machine learning-based search and personalization applications across Disney’s massive content library.“This metadata improves tools that are used by Disney storytellers to produce content; inspire iterative creativity in storytelling; power user experiences through recommendation engines, digital navigation and content discovery; and enable business intelligence,” wrote Disney developers Miquel Àngel Farré, Anthony Accardo, Marc Junyent, Monica Alfaro, and Cesc Guitart in a blog post in July.Before that could happen, Disney had to invest in a vast content annotation project, turning to its data scientists to train an automated tagging pipeline using deep learning models for image recognition to identify huge quantities of images of people, characters, and locations.Disney engineers started out by experimenting with various frameworks, including TensorFlow, but decided to consolidate around PyTorch in 2019. Engineers shifted from a conventional histogram of oriented gradients (HOG) feature descriptor and the popular support vector machines (SVM) model to a version of the object-detection architecture dubbed regions with convolutional neural networks (R-CNN). The latter was more conducive to handling the combinations of live action, animations, and visual effects common in Disney content.“It is difficult to define what is a face in a cartoon, so we shifted to deep learning methods using an object detector and used transfer learning,” Disney Research engineer Monica Alfaro explained to InfoWorld. After just a few thousand faces were processed, the new model was already broadly identifying faces in all three use cases. It went into production in January 2020.“We are using just one model now for the three types of faces and that is great to run for a Marvel movie like Avengers, where it needs to recognize both Iron Man and Tony Stark, or any character wearing a mask,” she said.As the engineers are dealing with such high volumes of video data to train and run the model in parallel, they also wanted to run on expensive, high-performance GPUs when moving into production.The shift from CPUs allowed engineers to re-train and update models faster. It also sped up the distribution of results to various groups across Disney, cutting processing time down from roughly an hour for a feature-length movie, to getting results in between five to 10 minutes today.“The TensorFlow object detector brought memory issues in production and was difficult to update, whereas PyTorch had the same object detector and Faster-RCNN, so we started using PyTorch for everything,” Alfaro said.That switch from one framework to another was surprisingly simple for the engineering team too. “The change [to PyTorch] was easy because it is all built-in, you only plug some functions in and can start quick, so it’s not a steep learning curve,” Alfaro said.When they did meet any issues or bottlenecks, the vibrant PyTorch community was on hand to help.Blue River Technology: Weed-killing robotsBlue River Technology has designed a robot that uses a heady combination of digital wayfinding, integrated cameras, and computer vision to spray weeds with herbicide while leaving crops alone in near real time, helping farmers more efficiently conserve expensive and potentially environmentally damaging herbicides.The Sunnyvale, California-based company caught the eye of heavy equipment maker John Deere in 2017, when it was acquired for $305 million, with the aim to integrate the technology into its agricultural equipment.Blue River researchers experimented with various deep learning frameworks while trying to train computer vision models to recognize the difference between weeds and crops, a massive challenge when you are dealing with cotton plants, which bear an unfortunate resemblance to weeds.Highly-trained agronomists were drafted to conduct manual image labelling tasks and train a convolutional neural network (CNN) using PyTorch “to analyze each frame and produce a pixel-accurate map of where the crops and weeds are,” Chris Padwick, director of computer vision and machine learning at Blue River Technology, wrote in a blog post in August.“Like other companies, we tried Caffe, TensorFlow, and then PyTorch,” Padwick told InfoWorld. “It works pretty much out of the box for us. We have had no bug reports or a blocking bug at all. On distributed compute it really shines and is easier to use than TensorFlow, which for data parallelisms was pretty complicated.”Padwick says the popularity and simplicity of the PyTorch framework gives him an advantage when it comes to ramping up new hires quickly. That being said, Padwick dreams of a world where “people develop in whatever they are comfortable with. Some like Apache MXNet or Darknet or Caffe for research, but in production it has to be in a single language, and PyTorch has everything we need to be successful.”Datarock: Cloud-based image analysis for the mining industryFounded by a group of geoscientists, Australian startup Datarock is applying computer vision technology to the mining industry. More specifically, its deep learning models are helping geologists analyze drill core sample imagery faster than before.Typically, a geologist would pore over these samples centimeter by centimeter to assess mineralogy and structure, while engineers would look for physical features such as faults, fractures, and rock quality. This process is both slow and prone to human error.“A computer can see rocks like an engineer would,” Brenton Crawford, COO of Datarock told InfoWorld. “If you can see it in the image, we can train a model to analyze it as well as a human.”Similar to Blue River, Datarock uses a variant of the RCNN model in production, with researchers turning to data augmentation techniques to gather enough training data in the early stages.“Following the initial discovery period, the team set about combining techniques to create an image processing workflow for drill core imagery. This involved developing a series of deep learning models that could process raw images into a structured format and segment the important geological information,” the researchers wrote in a blog post.Using Datarock’s technology, clients can get results in half an hour, as opposed to the five or six hours it takes to log findings manually. This frees up geologists from the more laborious parts of their job, Crawford said. However, “when we automate things that are more difficult, we do get some pushback, and have to explain they are part of this system to train the models and get that feedback loop turning.”Like many companies training deep learning computer vision models, Datarock started with TensorFlow, but soon shifted to PyTorch.Also on InfoWorld: 5 reasons to choose PyTorch for deep learning“At the start we used TensorFlow and it would crash on us for mysterious reasons,” Duy Tin Truong, machine learning lead at Datarock told InfoWorld. “PyTorch and Detecton2 was released at that time and fitted well with our needs, so after some tests we saw it was easier to debug and work with and occupied less memory, so we converted,” he said.Datarock also reported a 4x improvement in inference performance from TensorFlow to PyTorch and Detectron2 when running the models on GPUs — and 3x on CPUs.Truong cited PyTorch’s growing community, well-designed interface, ease of use, and better debugging as reasons for the switch and noted that although “they are quite different from an interface point of view, if you know TensorFlow, it is quite easy to switch, especially if you know Python.”", "pub_date": "2020-12-02"},
{"title": "Review: Google Cloud AI lights up machine learning", "overview": "Google Cloud AI and Machine Learning Platform is missing some pieces, and much is still in beta, but its scope and quality are second to none. ", "image_url": null, "url": "https://www.infoworld.com/article/3586836/review-google-cloud-ai-lights-up-machine-learning.html", "body": "Google has one of the largest machine learning stacks in the industry, currently centering on its Google Cloud AI and Machine Learning Platform. Google spun out TensorFlow as open source years ago, but TensorFlow is still the most mature and widely cited deep learning framework. Similarly, Google spun out Kubernetes as open source years ago, but it is still the dominant container management system.Google is one of the top sources of tools and infrastructure for developers, data scientists, and machine learning experts, but historically Google AI hasn’t been all that attractive to business analysts who lack serious data science or programming backgrounds. That’s starting to change.Also on InfoWorld: How to choose a cloud machine learning platformThe Google Cloud AI and Machine Learning Platform includes AI building blocks, the AI platform and accelerators, and AI solutions. The AI solutions are fairly new and aimed at business managers rather than data scientists. They may include consulting from Google or its partners.The AI building blocks, which are pre-trained but customizable, can be used without intimate knowledge of programming or data science. Nevertheless, they are often used by skilled data scientists for pragmatic reasons, essentially to get stuff done without extensive model training.The AI platform and accelerators are generally for serious data scientists, and require coding skill, knowledge of data preparation techniques, and lots of training time. I recommend going there only after trying the relevant building blocks.There are still some missing links in Google Cloud’s AI offerings, especially in data preparation. The closest thing Google Cloud has to a data import and conditioning service is the third-party Cloud Dataprep by Trifacta; I tried it a year ago and was underwhelmed. The feature engineering built into Cloud AutoML Tables is promising, however, and it would be useful to have that sort of service available for other scenarios.The seamy underside of AI has to do with ethics and responsibility (or the lack thereof), along with persistent model biases (often because of biased data used for training). Google published its AI Principles in 2018. It’s a work in progress, but it’s a basis for guidance as discussed in a recent blog post on Responsible AI.There is lots of competition in the AI market (over a dozen vendors), and lots of competition in the public cloud market (over half-a-dozen credible vendors). To do the comparisons justice, I’d have to write an article at least five times as long as this one, so as much as I hate leaving them out, I’ll have to omit most product comparisons. For the top obvious comparison, I can summarize: AWS does most of what Google does, and is also very good, but generally charges higher prices.Google Cloud’s AI building blocks don’t require much machine learning expertise, instead building on pre-trained models and automatic training. The AI Platform allows you to train and deploy your own machine learning and deep learning models.Google Cloud AI Building BlocksGoogle Cloud AI Building Blocks are easy-to-use components that you can incorporate into your own applications to add sight, language, conversation, and structured data. Many of the AI building blocks are pre-trained neural networks, but can be customized with transfer learning and neural network search if they don’t serve your needs out of the box. AutoML Tables is a little different, in that it automates the process a data scientist would use to find the best machine learning model for a tabular data set.The Google Cloud AutoML services provide customized deep neural networks for language pair translation, text classification, object detection, image classification, and video object classification and tracking. They require tagged data for training, but don’t require significant knowledge of deep learning, transfer learning, or programming.Google Cloud AutoML customizes Google’s battle-tested, high-accuracy deep neural networks for your tagged data. Rather than starting from scratch when training models from your data, AutoML implements automatic deep transfer learning (meaning that it starts from an existing deep neural network trained on other data) and neural architecture search (meaning that it finds the right combination of extra network layers) for language pair translation and the other services listed above.In each area, Google already has one or more pre-trained services based on deep neural networks and huge sets of labeled data. These may well work for your data unmodified, and you should test that to save yourself time and money. If they don’t do what you need, Google Cloud AutoML helps you to create a model that does, without requiring that you know how to perform transfer learning or how to design neural networks.Transfer learning offers two big advantages over training a neural network from scratch. First, it requires a lot less data for training, since most of the layers of the network are already well trained. Second, it trains a lot faster, since it’s only optimizing the final layers.While the Google Cloud AutoML services used to be presented together as a package, they are now listed with their base pre-trained services. What most other companies call AutoML is performed by Google Cloud AutoML Tables.Read the full review of Google Cloud AutoMLI tested an AutoML Vision custom flower classifier, which I trained in an hour from Google sample images, with a photo of tulips that I took at a nearby art museum.The usual data science process for many regression and classification problems is to create a table of data for training, clean and condition the data, perform feature engineering, and try to train all of the appropriate models on the transformed table, including a step to optimize the best models’ hyperparameters. Google Cloud AutoML Tables can perform this entire process automatically once you manually identify the target field.AutoML Tables automatically searches through Google’s model zoo for structured data to find the best model for your needs, ranging from linear/logistic regression models for simpler data sets to advanced deep, ensemble, and architecture-search methods for larger, more complex ones. It automates feature engineering on a wide range of tabular data primitives — such as numbers, classes, strings, timestamps, and lists — and helps you detect and take care of missing values, outliers, and other common data issues.Its codeless interface guides you through the full end-to-end machine learning lifecycle, making it easy for anyone on your team to build models and reliably incorporate them into broader applications. AutoML Tables provides extensive input data and model behavior explainability features, along with guardrails to prevent common mistakes. AutoML Tables is also available in API and notebook environments.AutoML Tables competes with Driverless AI and several other AutoML implementations and frameworks.Google Cloud AutoML Tables automates the whole pipeline for creating predictive models for tabular data, from feature engineering through deployment.At the Analyze phase of AutoML Tables, you can see the descriptive statistics of all your original features.The free Google Cloud Vision “Try the API” interface allowed me to drag a JPEG onto a web page and see the results. The child was smiling, so the “Joy” tag is correct. The algorithm didn’t quite recognize the paper Pilgrim hat.The Google Cloud Vision API is a pre-trained machine learning service for categorizing images and extracting various features. It can classify images into thousands of pre-trained categories, ranging from generic objects and animals found in the image (such as a cat), to general conditions (for example, dusk), to specific landmarks (Eiffel Tower, Grand Canyon), and identify general properties of the image, such as its dominant colors. It can isolate areas that are faces, then apply geometric (facial orientation and landmarks) and emotional analyses to the faces, although it does not recognize faces as belonging to specific people, except for celebrities (which requires a special usage license). Vision API uses OCR to detect text within images in more than 50 languages and various file types. It can also identify product logos, and detect adult, violent, and medical content.Read the full review of Google Cloud Machine Learning APIsThe Google Cloud Video Intelligence API automatically recognizes more than 20,000 objects, places, and actions in stored and streaming video. It also distinguishes scene changes and extracts rich metadata at the video, shot, or frame level. It additionally performs text detection and extraction using OCR, detects explicit content, automates closed captioning and subtitles, recognizes logos, and detects faces, persons, and poses.Google recommends the Video Intelligence API for extracting metadata to index, organize, and search your video content. It can transcribe videos and generate closed captions, as well as flag and filter inappropriate content, all more cost-effectively than human transcribers. Use cases include content moderation, content recommendations, media archives, and contextual advertisements.The Google Cloud Natural Language API finds entities, sentiment, syntax, and categories. Here we see syntax diagrams for two sentences from a Google press release.Natural language processing (NLP) is a big part of the “secret sauce” that makes input to Google Search and the Google Assistant work well. The Google Cloud Natural Language API exposes that same technology to your programs. It can perform syntax analysis (see the image below), entity extraction, sentiment analysis, and content classification, in 10 languages. You may specify the language if you know it; otherwise, the API will attempt to auto-detect the language. A separate API, currently available for early access on request, specializes in healthcare-related content.Read the full review of Google Cloud Machine Learning APIsThe Google Cloud Translation API can translate over a hundred language pairs, can auto-detect the source language if you don’t specify it, and comes in three flavors: Basic, Advanced, and Media Translation. The Advanced Translation API supports a glossary, batch translation, and the use of custom models. The Basic Translation API is essentially what is used by the consumer Google Translate interface. AutoML Translation allows you to train custom models using transfer learning.The Media Translation API translates content directly from audio (speech), either audio files or streams, in 12 languages, and automatically generates punctuation. There are separate models for video and phone call audio.Read the full review of Google Cloud Machine Learning APIs", "pub_date": "2020-11-09"},
{"title": "Microsoft brings .NET dev to Apache Spark", "overview": ".NET for Apache Spark 1.0 provides high-performance .NET APIs to Apache Spark including Spark SQL, Spark Streaming, and MLlib ", "image_url": null, "url": "https://www.infoworld.com/article/3587595/microsoft-brings-net-dev-to-apache-spark.html", "body": "Microsoft and the .NET Foundation have released version 1.0 of .NET for Apache Spark, an open source package that brings .NET development to the Spark analytics engine for large-scale data processing.Announced October 27, .NET for Apache Spark 1.0 has support for .NET applications targeting .NET Standard 2.0 or later. Users can access Spark DataFrame APIs, write Spark SQL, and create user-defined functions UDFs).Also on InfoWorld: How to choose a cloud machine learning platformThe .NET for Apache Spark framework is available on the .NET Foundation’s GitHub page or from NuGet. Other capabilities of .NET for Apache Spark 1.0 include:An API extension framework to add support for additional Spark libraries including Linux Foundation Delta Lake, Microsoft OSS Hyperspace, ML.NET, and Apache Spark MLlib functionality..NET for Apache Spark programs that are not UDFs show the same speed as Scala and PySpark-based non-UDF applications. If applications include UDFs, .NET for Apache Spark programs are at least as fast as PySpark programs or might be faster..NET for Apache Spark is built into Azure Synapse and Azure HDInsight. It also can be used in other Apache Spark cloud offerings including Azure Databricks.The first public version of the project was announced in April 2019. Driving the development of .NET for Apache Spark was increased demand for an easier way to build big data applications instead of having to learn Scala or Python. The project is operated under the .NET Foundation and has been filed as a Spark Project Improvement Proposal to be considered for inclusion in the Apache Spark project directly.Looking ahead, Microsoft is addressing obstacles including setting up prerequisites and dependencies and finding quality documentation, with examples such as community-contributed “ready-to-run” Docker images and updates to .NET for Apache Spark documentation. Another priority is supporting deployment options including integration with CI/CD devops pipelines and publishing jobs directly from Visual Studio.", "pub_date": "2020-10-29"},
{"title": "Today’s data science roles won’t exist in 10 years", "overview": "AutoML is poised to turn developers into data scientists — and vice versa. Here’s how AutoML will radically change data science for the better. ", "image_url": null, "url": "https://www.infoworld.com/article/3596894/todays-data-science-roles-wont-exist-in-10-years.html", "body": "In the coming decade, the data scientist role as we know it will look very different than it does today. But don’t worry, no one is predicting lost jobs, just  jobs.Data scientists will be fine — according to the Bureau of Labor Statistics, the role is still projected to grow at a higher than average clip through 2029. But advancements in technology will be the impetus for a huge shift in a data scientist’s responsibilities and in the way businesses approach analytics as a whole. And AutoML tools, which help automate the machine learning pipeline from raw data to a usable model, will lead this revolution.Also on InfoWorld: How to choose a cloud machine learning platformIn 10 years, data scientists will have entirely different sets of skills and tools, but their function will remain the same: to serve as confident and competent technology guides that can make sense of complex data to solve business problems.AutoML democratizes data scienceUntil recently, machine learning algorithms and processes were almost exclusively the domain of more traditional data science roles—those with formal education and advanced degrees, or working for large technology corporations. Data scientists have played an invaluable role in every part of the machine learning development spectrum. But in time, their role will become more collaborative and strategic. With tools like AutoML to automate some of their more academic skills, data scientists can focus on guiding organizations toward solutions to business problems via data.In many ways, this is because AutoML democratizes the effort of putting machine learning into practice. Vendors from startups to cloud hyperscalers have launched solutions easy enough for developers to use and experiment on without a large educational or experiential barrier to entry. Similarly, some AutoML applications are intuitive and simple enough that non-technical workers can try their hands at creating solutions to problems in their own departments—creating a “citizen data scientist” of sorts within organizations.In order to explore the possibilities these types of tools unlock for both developers and data scientists, we first have to understand the current state of data science as it relates to machine learning development. It’s easiest to understand when placed on a maturity scale.Smaller organizations and businesses with more traditional roles in charge of digital transformation (i.e.,  classically trained data scientists) typically fall on this end of this scale. Right now, they are the biggest customers for out-of-the-box machine learning applications, which are more geared toward an audience unfamiliar with the intricacies of machine learning.These turnkey applications tend to be easy to implement, and relatively cheap and easy to deploy. For smaller companies with a very specific process to automate or improve, there are likely several viable options on the market. The low barrier to entry makes these applications perfect for data scientists wading into machine learning for the first time. Because some of the applications are so intuitive, they even allow non-technical employees a chance to experiment with automation and advanced data capabilities—potentially introducing a valuable sandbox into an organization.This class of machine learning applications is notoriously inflexible. While they can be easy to implement, they aren’t easily customized. As such, certain levels of accuracy may be impossible for certain applications. Additionally, these applications can be severely limited by their reliance on pretrained models and data.Examples of these applications include Amazon Comprehend, Amazon Lex, and Amazon Forecast from Amazon Web Services and Azure Speech Services and Azure Language Understanding (LUIS) from Microsoft Azure. These tools are often sufficient enough for burgeoning data scientists to take the first steps in machine learning and usher their organizations further down the maturity spectrum.How data analysis, AI, and IoT will shape the post-pandemic ‘new normal’Customizable solutions with AutoMLOrganizations with large yet relatively common data sets—think customer transaction data or marketing email metrics—need more flexibility when using machine learning to solve problems. Enter AutoML. AutoML takes the steps of a manual machine learning workflow (data discovery, exploratory data analysis, hyperparameter tuning, etc.) and condenses them into a configurable stack. AutoML applications allow more experiments to be run on data in a larger space. But the real superpower of AutoML is the accessibility — custom configurations can be built and inputs can be refined relatively easily. What’s more, AutoML isn’t made exclusively with data scientists as an audience. Developers can also easily tinker within the sandbox to bring machine learning elements into their own products or projects.While it comes close, AutoML’s limitations mean accuracy in outputs will be difficult to perfect. Because of this, degree-holding, card carrying data scientists often look down upon applications built with the help of AutoML — even if the result is accurate enough to solve the problem at hand.Examples of these applications include Amazon SageMaker AutoPilot or Google Cloud AutoML. Data scientists a decade from now will undoubtedly need to be familiar with tools like these. Like a developer who is proficient in multiple programming languages, data scientists will need to have proficiency with multiple AutoML environments in order to be considered top talent.“Hand-rolled” and homegrown machine learning solutionsThe largest enterprise-scale businesses and Fortune 500 companies are where most of the advanced and proprietary machine learning applications are currently being developed. Data scientists at these organizations are part of large teams perfecting machine learning algorithms using troves of historical company data, and building these applications from the ground up. Custom applications like these are only possible with considerable resources and talent, which is why the payoff and risks are so great.Like any application built from scratch, custom machine learning is “state-of-the-art” and is built based on a deep understanding of the problem at hand. It’s also more accurate — if only by small margins — than AutoML and out-of-the-box machine learning solutions.Getting a custom machine learning application to reach certain accuracy thresholds can be extremely difficult, and often requires heavy lifting by teams of data scientists. Additionally, custom machine learning options are the most time-consuming and most expensive to develop.An example of a hand-rolled machine learning solution is starting with a blank Jupyter notebook, manually importing data, and then conducting each step from exploratory data analysis through model tuning by hand. This is often achieved by writing custom code using open source machine learning frameworks such as Scikit-learn, TensorFlow, PyTorch, and many others. This approach requires a high degree of both experience and intuition, but can produce results that often outperform both turnkey machine learning services and AutoML.Also on InfoWorld: The best free data science courses during quarantineTools like AutoML will shift data science roles and responsibilities over the next 10 years. AutoML takes the burden of developing machine learning from scratch off of data scientists, and instead puts the possibilities of machine learning technology directly in the hands of other problem solvers. With time freed up to focus on what they know—the data and the inputs themselves — data scientists a decade from now will serve as even more valuable guides for their organizations.Rackspace—newtechforum@infoworld.com", "pub_date": "2020-11-18"},
{"title": "The cloud’s role in elections grows more significant", "overview": "Most campaigns view polling data, logistics, and demographics as fundamental to winning an election. More likely, data science and cloud computing will win the day. ", "image_url": null, "url": "https://www.infoworld.com/article/3587342/the-clouds-role-in-elections-grows-more-significant.html", "body": "A year ago I wrote about the importance of leveraging cloud computing and data science to win elections, pointing to the 2020 campaigns, which come to a conclusion today. The basic assertion was that technology, focusing on the true meaning of data, would count more than traditional robocalls and door-to-door campaigning, since you’re able to understand more about the electorate, and thus target more effectively. The campaigns won’t let you talk to their data nerds, but they all have them. Data science approaches and the use of cloud computing allow the campaigns to come to some hidden conclusions that can boost their candidate's chances.Also on InfoWorld: How dataops improves data, analytics, and machine learningSophisticated calculations and analytics are able to derive patterns in the data that those viewing the data in traditional ways won’t see. For example, data science can find undecided voters who are likely to vote for a particular candidate and come up with a motivation for them to vote. It can tap into opposition to a local bill that really has nothing to do with the federal election, and use that issue in the messaging to get another 10 percent of undecided voters, with more than 80 percent of them voting for your candidate. Compare this precision versus a “spray-and-prey” approach where you blast out messaging and hope it hits a few undecided voters.     A human brain can’t find these types of patterns in the data. You need advanced analytics that use machine learning to find things you did not know existed. The ability to weaponize massive amounts of seemingly innocuous data is something that was nice to have back in 2012, but in 2020 it’s mandatory to put more odds in your favor. Modern campaigning is really about data wars and strategic targeting of voters rather than promoting ideas.In this world, ideas are dynamic; how the campaign messaging is viewed depends on who the campaign wants to view it. You may see a different message than your neighbor does. Deep analytics and AI have discerned that you and your neighbor are motivated by different issues, so you are each given different messages to drive your behaviors.  The result: The campaign with the best data science approaches and the ability to leverage cloud computing as a force multiplier will likely win. I’m not sure this what the Founding Fathers saw coming, but here it is.            ", "pub_date": "2020-11-03"},
{"title": "Apple releases TensorFlow fork with speedups for M1 Macs", "overview": "Apple says the M1-compiled version of TensorFlow delivers several times faster performance on a number of benchmarks, while running existing TensorFlow scripts as-is", "image_url": null, "url": "https://www.infoworld.com/article/3596904/apple-releases-tensorflow-fork-with-speedups-for-m1-macs.html", "body": "Apple has released its own fork of the TensorFlow 2.4 machine learning framework, specifically optimized for its newly released M1 processor.According to Apple, the M1-compiled version of TensorFlow delivers several times faster performance on a number of benchmarks, compared to the same jobs running on an Intel version of the same 2020 edition MacBook Pro.InfoWorld’s 2020 Technology of the Year Award winners: The best software development, cloud computing, data analytics, and machine learning products of the yearThe fork, available as open source, requires MacOS 11.0 or better, and provides accelerations on Macs running the new M1 processor.Existing TensorFlow scripts run as-is with the fork; they do not need to be reworked to take advantage of its performance gains. According to VentureBeat, Apple plans to contribute its changes to the main TensorFlow project, to serve as a basis for other optimizations.Apple’s revamp of TensorFlow is one of the first examples of how M1 Macs are intended to draw developers to the Mac platform. M1 chips in new Macs replace the use of the Intel x86 processor, but can run existing software compiled for the x86 by way of Apple’s Rosetta2 binary translation technology.However, Rosetta2-translated apps do incur a performance hit, with some benchmarks running as slowly as 59% of native speed. For performance-sensitive applications, it makes sense to compile them to run natively on the M1.", "pub_date": "2020-11-18"},
{"title": "Anti-adversarial machine learning defenses start to take root", "overview": "Adversarial attacks are one of the greatest threats to the integrity of the emerging AI-centric economy.", "image_url": null, "url": "https://www.infoworld.com/article/3596596/anti-adversarial-machine-learning-defenses-start-to-take-root.html", "body": "Much of the anti-adversarial research has been on the potential for minute, largely undetectable alterations to images (researchers generally refer to these as “noise perturbations”) that cause AI’s machine learning (ML) algorithms to misidentify or misclassify the images. Adversarial tampering can be extremely subtle and hard to detect, even all the way down to pixel-level subliminals. If an attacker can introduce nearly invisible alterations to image, video, speech, or other data for the purpose of fooling AI-powered classification tools, it will be difficult to trust this otherwise sophisticated technology to do its job effectively.Growing threat to deployed AI appsThis is no idle threat. Eliciting false algorithmic inferences can cause an AI-based app to make incorrect decisions, such as when a self-driving vehicle misreads a traffic sign and then turns the wrong way or, in a worst-case scenario, crashes into a building, vehicle, or pedestrian. Though the research literature focuses on simulated adversarial ML attacks that were conducted in controlled laboratory environments, general knowledge that these attack vectors are available will almost certainly cause terrorists, criminals, or mischievous parties to exploit them.Also on InfoWorld: How to choose a cloud machine learning platformAlthough high-profile adversarial attacks did not appear to impact the ML that powered this year’s U.S. presidential campaign, we cannot deny the potential for these in future electoral cycles. Throughout this pandemic-wracked year, adversarial attacks on ML platforms have continued to intensify in other sectors of our lives.This year, the National Vulnerability Database (part of the U.S. National Institute for Science and Technology) issued its first Common Vulnerabilities and Exposures report  for an ML component in a commercial system. Also, the Software Engineering Institute’s CERT Coordination Center issued its first vuln note flagging the extent to which many operational ML systems are vulnerable to arbitrary misclassification attacks.Late last year, Gartner predicted that during the next two years 30 percent of all cyberattacks on AI apps would use adversarial tactics. Sadly, it would be premature to say that anti-adversarial best practices are taking hold within the AI community. A recent industry survey by Microsoft found that few industry practitioners are taking the threat of adversarial machine learning seriously at this point or using tools that can mitigate the risks of such attacks.Even if it were possible to identify adversarial attacks in progress, targeted organizations would find it challenging to respond to these assaults in all their dizzying diversity. And there’s no saying whether ad-hoc responses to new threats will coalesce into a pre-emptive anti-adversarial AI “hardening” strategy anytime soon.Anti-adversarial ML security methodologies As these attacks surface in greater numbers, AI professionals will clamor for a consensus methodology for detecting and dealing with adversarial risks.An important milestone in adversarial defenses took place recently. Microsoft, MITRE, and 11 other organizations released an Adversarial ML Threat Matrix. This is an open, extensible framework structured like MITRE’s widely adopted ATT&CK framework that helps security analysts classify the most common adversarial tactics that have been used to disrupt and deceive ML systems.Developed in conjunction with Carnegie Mellon and other leading research universities, the framework presents techniques for monitoring an organization’s ML systems to detect whether such attacks are in progress or have already taken place. It lists vulnerabilities and adversary behaviors that are effective against production ML systems. It also provides case studies describing how well-known attacks such as the Microsoft Tay poisoning and the Proofpoint evasion attack can be analyzed using this framework.As discussed in the framework, there are four principal adversarial tactics for compromising ML apps.involves unauthorized recovery of a functionally equivalent ML model by iteratively querying the model with arbitrary inputs. The attacker can infer and generate a high-fidelity offline copy of the model to guide further attacks to the deployed production ML model. occurs when attackers iteratively introduce arbitrary inputs, such as subtle pixel-level changes to images. The changes are practically undetectable to human senses but cause vulnerable ML models to classify the images or other doctored content incorrectly.involves unauthorized recovery of the predictive features that were used to build an ML model. It enables attackers to launch inferences that compromise the private data that was used in training the model. means training data has been contaminated in order to surreptitiously produce specific unauthorized inferences when arbitrary input data is introduced to the poisoned ML model in runtime.Taken individually or combined in diverse ways, these tactics could enable an attacker to surreptitiously “reprogram” an AI app or steal precious intellectual property (data and ML models). All are potential tools for perpetrating fraud, espionage, or sabotage against applications, databases, and other online systems with ML algorithms at their heart.Fruitful anti-adversarial ML tools and tacticsAnti-adversarial tactics must be rooted deeply in the ML development pipeline, leveraging code repositories, CI/CD (continuous integration/continuous delivery), and other devops infrastructure and tools.Grounding their recommendations in devsecops and traditional application security practices, the framework’s authors call for a multipronged anti-adversarial methodology that includes critical countermeasures. would reduce exploitable adversarial vulnerabilities in ML programs and enable other engineers to audit source code. In addition, security-compliance code examples in popular ML frameworks would contribute to the spread of adversarially hardened ML apps. So far TensorFlow is the only ML framework that provides consolidated guidance around traditional software attacks and links to tools for testing against adversarial attacks. The framework’s authors recommend exploring whether containerizing ML apps can help to quarantine uncompromised ML systems from the impact of adversarially impacted ML systems. help detect potential adversarial weaknesses in ML apps as coded or when the apps execute particular code paths. ML tools such as cleverhans, secml, and IBM’s Adversarial Robustness Toolbox support varying degrees of static and dynamic ML code testing. The Adversarial ML Threat Matrix’s publishers call for such tools to be integrated with full-featured ML development toolkits to support fine-grained code assessment before ML apps are committed to the code repository. They also recommend integration of dynamic code-analysis tools for adversarial ML into CI/CD pipelines. This latter recommendation would support automation of adversarial ML testing in production ML apps. support runtime detection of adversarial and other anomalous processes being executed on ML systems. The matrix’s publishers call for ML platforms to use these tools to monitor, at the very least, for attacks listed in the curated repository. This would enable tracing adversarial attacks back to their sources and exporting anomalous event logs to security incident and event management systems. They propose that detection methods be written into a format that facilitates easy sharing among security analysts. They also recommend that the adversarial ML research community register adversarial vulnerabilities in a trackable system like the National Vulnerability Database in order to alert impacted vendors, users, and other stakeholders.A growing knowledgebaseThe new anti-adversarial framework’s authors provide access through their GitHub repo to what they call a “curated repository of attacks.” Every attack documented in this searchable resource has a description of the adversarial technique, the type of advanced persistent threat that has been observed to use the tactic, recommendations for detecting it, and references to publications that provide further insight.As they become aware of new adversarial ML attack vectors, AI and security professionals should register those in this repository. This way the initiative can keep pace with the growing range of threats to the integrity, security, and reliability of deployed ML apps.Going forward, AI application developers and security analysts should also:Assume the possibility of adversarial attacks on all in-production ML applications.Perform adversarial threat assessments prior to writing or deploying vulnerable code.Generate adversarial examples as a standard risk-mitigation activity in the AI training pipeline.Test AI apps against a wide range of adversarial inputs to determine the robustness of their inferences.Reuse adversarial-defense knowledge, such as that provided by the new Adversarial ML Threat Matrix, to improve AI resilience against bogus input examples.Update ongoing adversarial attack defenses throughout the lifecycle of deployed AI models.Ensure data scientists have sophisticated anti-adversarial methodologies to guide them in applying these practices throughout the AI development and operationalization lifecycle.For further information on the new Adversarial ML Threat Matrix, check out the initiative’s GitHub repository, MITRE’s announcement, and the Carnegie Mellon SEI/CERT’s blog post. Other useful resources for security analysts to develop their own anti-adversarial strategies include Microsoft’s taxonomy of ML failure modes, threat modeling guidance specifically for ML systems, and the security development lifecycle bug bar to systematically triage attacks on ML systems.", "pub_date": "2020-11-19"},
{"title": "Oracle open-sources Java machine learning library", "overview": "Tribuo offers tools for building and deploying classification, clustering, and regression models in Java, along with interfaces to TensorFlow, XGBoost, and ONNX", "image_url": null, "url": "https://www.infoworld.com/article/3574935/oracle-open-sources-java-machine-learning-library.html", "body": "Looking to meet enterprise needs in the machine learning space, Oracle is making its Tribuo Java machine learning library available free under an open source license.With Tribuo, Oracle aims to make it easier to build and deploy machine learning models in Java, similar to what already has happened with Python. Released under an Apache 2.0 license and developed by Oracle Labs, Tribuo is accessible from GitHub and Maven Central.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesTribuo provides standard machine learning functionality including algorithms for classification, clustering, anomaly detection, and regression. Tribuo also includes pipelines for loading and transforming data and provides a suite of evaluations for supported prediction tasks. Because Tribuo collects statistics on inputs, Tribuo can describe the range of each input, for example. It also names features, managing feature IDs and output IDs under the hood to avoid ID conflicts and confusion when chaining models, loading data, and featurizing inputs.A Tribuo model knows when it sees a feature for the first time, which is particularly useful when working with natural language processing. Models know what outputs are, with outputs being strongly typed. Developers do not need to wonder if a float is a probability, a regressed value, or a cluster ID. With Tribuo, each of these is a separate type; the model can describe types and ranges it knows about. Use of strongly typed inputs and outputs means Tribuo can track the model construction process, from the point data is loaded through train/test splits or dataset transformations to model training and evaluation. This tracking data is baked into all models and evaluations.The Tribuo provenance system can generate a configuration that rebuilds the training pipeline to reproduce the model or evaluation. Also, a tweaked model can be built on new data or hyperparameters. Thus users always know what a Tribuo model is, where it came from, and how to create it.Oracle sees Tribuo filling a gap in the marketplace for machine learning for enterprise applications. For example, whereas the Google-built TensorFlow library provides core algorithms for deep learning, Tribuo provides several machine learning algorithms, some of which are in TensorFlow and some of which are not, while also providing an interface to TensorFlow, said Oracle’s Adam Pocock, principal member of the Oracle Labs technical staff. And whereas the Apache Spark analytics engine is for large, distributed systems, Tribuo is for smaller computations that can fit on a single machine, Pocock said.Also on InfoWorld: How to choose a cloud machine learning platformIn addition to TensorFlow, Tribuo provides interfaces to XGBoost and the ONNX runtime, allowing models stored in the ONNX format or trained in TensorFlow and XGBoost to be deployed alongside native Tribuo models. Support for the ONNX model format allows deployment in Java of models trained using popular Python libraries such as PyTorch.Tribuo runs on Java 8 or later. Oracle accepts code contributions to Tribuo under the Oracle Contributor Agreement. Tribuo already has been used internally at Oracle in the Fusion Cloud ERP product for intelligent document recognition, for example.", "pub_date": "2020-09-15"},
{"title": "How Nvidia’s Arm acquisition will drive AI to every edge", "overview": "Filling a gap in their offerings, Nvidia hopes to gain money and market share with Arm's extensive reach ", "image_url": null, "url": "https://www.infoworld.com/article/3575396/how-nvidias-arm-acquisition-will-drive-ai-to-every-edge.html", "body": "Nvidia is sitting pretty in AI (artificial intelligence) right now. For the next few years, most AI systems will continue to be trained on Nvidia GPUs and specialized hardware and cloud services that incorporate these processors.However, Nvidia has been frustrated in its attempts to become a dominant provider of AI chips for deployment into smartphones, embedded systems, and other edge devices. To address that strategic gap, Nvidia this past week announced that it is acquiring processor architecture firm Arm Holdings from SoftBank Group and the SoftBank Vision Fund.Also on InfoWorld: What is CUDA? Parallel programming for GPUsOnce the acquisition closes in the expected 18 months, Nvidia will retain Arm’s name, brand identity, management team, and base of operations in Cambridge, United Kingdom. It will also expand Arm’s Cambridge-based research and development facility, while establishing an Nvidia research facility, developer training facilities, and startup incubator at the site. Arm will operate as an Nvidia division.This is truly a landmark deal. Nvidia will almost certainly integrate its GPU technology into the core smartphone, IoT, and embedded chip architectures designed by Arm, thereby driving its AI technologies ubiquitously to edge devices everywhere.What follows are the principal respects in which Nvidia will benefit from acquiring Arm.Bolstering Nvidia’s bottom lineNvidia is picking up Arm for a $40 billion consideration of cash and shares, which makes it far larger than the $7 billion acquisition of Mellanox that closed in April.Arm comes into Nvidia as a significant cash cow from which its new parent will almost certainly fund ambitious new projects and fill-in acquisitions going forward. Nvidia’s acquisition of Arm is expected to be accretive to the acquirer’s bottom line. Once the deal closes, Arm will start contributing its considerable profits to its new parent’s own net income immediately. Nvidia’s income statement will gain millions of dollars in Arm's annual licensing fees and billions of dollars in royalty fees.The transaction, which is subject to the usual closing conditions and regulatory approvals, represents a middling return on SoftBank’s $32 billion outlay when it took Arm private in 2016. Nvidia’s bargain on the deal stems largely from the fact that SoftBank had run into a cash crunch after losing billions of dollars due to the pandemic and bad bets on Uber and WeWork. SoftBank will acquire an ownership stake in Nvidia of no more than 10 percent, effectively making it one of Nvidia’s largest shareholders.Giving Nvidia a new competitive leverNvidia’s Arm acquisition comes at a time when Intel’s next generation of chips has encountered major delays. Nvidia will be able to leverage the Arm acquisition to contend with Intel across a wide range of mobile, edge, embedded, gaming, and IoT end points. Arm provides the basic architectures for the low-power central processor chips within smartphones and tablets from such licensees as Apple, Samsung, and Qualcomm.Through licensee Apple, Arm-based processors will replace Intel processors in the next generation of Mac computers. The deal will give Nvidia a better shot at displacing Imagination Technologies as Apple’s GPU supplier for its iOS devices. And it will enable Nvidia to serve chip makers who are adapting Arm designs to work in servers and PCs, which have long been Intel’s stronghold.Boosting Nvidia’s processor market positionNvidia is rapidly becoming the dominant vendor of processors for cloud-to-edge deployments of AI. Even before the Arm acquisition, Nvidia was racking up record sales, saw its share price double this year, and overtook Intel as the most valuable U.S. semiconductor company. At the same time, Intel continued to stumble in its efforts to bring to market a credible GPU alternative to Nvidia’s flagship offerings.Nvidia’s Arm acquisition will boldly advance its position in the smartphone market, as well as in the markets for embedded, IoT, and other edge devices, which are all segments from which Nvidia has largely been absent. By contrast, Arm boasts a near-monopoly in providing IP (intellectual property) for mobile device chip architectures. Arm currently licenses designs for third-party microprocessors that power approximately 90 percent of the world’s smartphones and in many other types of edge and mobile devices. Arm’s energy-efficient designs have been used to create 160 billion chips that are manufactured and sold by more than 1,000 licensees.Diversifying Nvidia’s solution and technology portfolioNvidia is acquiring a firm with a complementary technology portfolio, business model, and go-to-market approach.Nvidia doesn’t design CPUs, which are the core of Arm’s chip IP. Nvidia doesn’t license IP to semiconductor companies, which is Arm’s principal business model. And Nvidia doesn’t compete in the mobility market, which is where Arm’s primary licensees operate.Also, Nvidia doesn't own any chip fabrication plants, but outsources production of its chip designs to specialized foundries. Arm, by contrast, doesn’t outsource its chip designs at all, but instead licenses its IP to other vendors that fabricate them, either in their own facilities or outsourced.Once the acquisition is finalized, Nvidia plans to build an Arm-powered supercomputer to support AI R&D at Arm’s Cambridge location. Nvidia also plans to expand Arm’s IP licensing portfolio with Nvidia technologies, especially the latter’s market-leading GPUs.The converged firms will be able to address cloud-to-edge opportunities that combine Nvidia’s AI solutions with Arm’s vast array of licensees. Even before this latest deal, SoftBank had driven Arm’s diversification into new opportunities to license its IP into partnerships in the data center, automotive, IoT, and network processing markets. Arm had already announced that it was designing its Pelion software IP for a growing range of low-power, high-performance AI apps running on edge devices.It’s unclear where Arm’s AI investments will land in the converged firm’s strategies and solution portfolio. To support its ambitions in the AI arena, Arm already leverages IP from its recent acquisitions of Stream Technologies and Treasure Data. These technologies support ingestion, storage, and management of data to be used in building and training machine learning (ML) models that can execute transparently across CPUs, GPUs, and neural network processing units. Arm has also been investing in tools that allow ML models to be dynamically updated on edge devices and also to support secure, distributed, cross-node ML computations.It's important to note that Arm’s processor designs also serve as the basis for Amazon Web Services’ Graviton2 processors and for Fujitsu's A64FX processors that are used in the latter’s Fugaku supercomputer. It remains to be seen whether these Arm licensees—whose respective AI solution portfolios directly compete with Nvidia—will adopt any add-on AI core tech.Nevertheless, even if such licensees balk at adopting Nvidia’s AI, we can expect the combined Nvidia-Arm to offer more of its own chips (GPUs, CPUs, etc.) to power AI in “big-core” data center, high-performance computing, and supercomputer deployments. We should also expect Arm to aggressively offer such IP to its vast ecosystem of licensees.Locking down a strategic supplier for NvidiaNvidia, in acquiring Arm, will be securing its future access to Arm’s processor technology, while keeping it out of the hands of competitors. If this deal is approved, many semiconductor companies that would otherwise have simply been Nvidia’s rivals will also become its customers.Nevertheless, Nvidia has announced its intention to continue Arm’s open licensing ecosystem, a pledge that will be absolutely necessary in order to secure the necessary regulatory approvals. Nvidia has pledged to continue Arm’s policy of customer neutrality, licensing IP to companies who may compete with Nvidia in GPU, AI, and other product segments.Extending Nvidia’s market reach and scaleLast but not least, Nvidia is acquiring a vendor with a much larger market reach and scale than its own.For starters, the deal will expand Nvidia’s reach in the development community from the current 2 million to more than 15 million. More significantly, Nvidia shipped about 100 million chips in the past year, while Arm’s more than 1,000 technology partner licensees shipped more than 22 billion (with a “b”) chips last year and more than 180 billion to date.Facing oppositionNvidia’s pledge to continue Arm’s open licensing and customer neutrality will be a key element for running the gauntlet of regulatory challenges that are sure to ensue.China looked long and hard at the recently approved Mellanox acquisition and may prove difficult to placate, especially considering strains in its geopolitical posture vis-à-vis the United States.Nvidia’s assurance that Arm’s site and team will remain intact in England will prove essential in securing that country's approval. However, political forces at work in the United Kingdom might cause the regulatory approvals to be difficult to secure.Seemingly to prevent the matter from being a regulatory distraction during the approval process, Nvidia’s Arm acquisition will not include Arm‘s two IoT Services Group software businesses. SoftBank had previous announced a plan to spin off the two businesses into new SoftBank-owned stand-alone entities. But nearly three weeks ago, Arm said it was halting the spin-off plans.The biggest potential challenge for a combined Nvidia-Arm, even after the deal clears all regulatory hurdles, will be whether Arm licensees in the microprocessor industry will be comfortable sourcing this key technology from a competitor. Arm’s reputation as the “Switzerland of the semiconductor industry” is at stake. Nvidia rivals—such as AMD—may seek out alternative sources if they perceive that the deal gives CEO Jensen Huang’s firm an unfair advantage in the battle to bring AI to edge devices.", "pub_date": "2020-09-17"},
{"title": "10 MLops platforms to manage the machine learning lifecycle", "overview": "Machine learning lifecycle management systems rank and track your experiments over time, and sometimes integrate with deployment and monitoring", "image_url": null, "url": "https://www.infoworld.com/article/3572442/10-mlops-platforms-to-manage-the-machine-learning-lifecycle.html", "body": "For most professional software developers, using application lifecycle management (ALM) is a given. Data scientists, many of whom do not have a software development background, often have  used lifecycle management for their machine learning models. That’s a problem that’s much easier to fix now than it was a few years ago, thanks to the advent of “MLops” environments and frameworks that support machine learning lifecycle management.Also on InfoWorld: How to choose a cloud machine learning platformWhat is machine learning lifecycle management?The easy answer to this question would be that machine learning lifecycle management is the same as ALM, but that would also be wrong. That’s because the lifecycle of a machine learning model is different from the software development lifecycle (SDLC) in a number of ways.To begin with, software developers more or less know what they are trying to build before they write the code. There may be a fixed overall specification (waterfall model) or not (agile development), but at any given moment a software developer is trying to build, test, and debug a feature that can be described. Software developers can also write tests that make sure that the feature behaves as designed.By contrast, a data scientist builds models by doing experiments in which an optimization algorithm tries to find the best set of weights to explain a dataset. There are many kinds of models, and currently the only way to determine which is best is to try them all. There are also several possible criteria for model “goodness,” and no real equivalent to software tests.Unfortunately, some of the best models (deep neural networks, for example) take a long time to train, which is why accelerators such as GPUs, TPUs, and FPGAs have become important to data science. In addition, a great deal of effort often goes into cleaning the data and engineering the best set of features from the original observations, in order to make the models work as well as possible.Keeping track of hundreds of experiments and dozens of feature sets isn’t easy, even when you are using a fixed dataset. In real life, it’s even worse: Data often drifts over time, so the model needs to be tuned periodically.There are several different paradigms for the machine learning lifecycle. Often, they start with ideation, continue with data acquisition and exploratory data analysis, move from there to R&D (those hundreds of experiments) and validation, and finally to deployment and monitoring. Monitoring may periodically send you back to step one to try different models and features or to update your training dataset. In fact, any of the steps in the lifecycle can send you back to an earlier step.Machine learning lifecycle management systems try to rank and keep track of all your experiments over time. In the most useful implementations, the management system also integrates with deployment and monitoring.Machine learning lifecycle management productsWe’ve identified several cloud platforms and frameworks for managing the machine learning lifecycle. These currently include Algorithmia, Amazon SageMaker, Azure Machine Learning, Domino Data Lab, the Google Cloud AI Platform, HPE Ezmeral ML Ops, Metaflow, MLflow, Paperspace, and Seldon.Algorithmia can connect to, deploy, manage, and scale your machine learning portfolio. Depending on which plan you choose, Algorithmia can run on its own cloud, on your premises, on VMware, or on a public cloud. It can maintain models in its own Git repository or on GitHub. It manages model versioning automatically, can implement pipelining, and can run and scale models on-demand (serverless) using CPUs and GPUs. Algorithmia provides a keyworded library of models (see screenshot below) in addition to hosting your models. It does not currently offer much support for model training.Amazon SageMaker is Amazon’s fully managed integrated environment for machine learning and deep learning. It includes a Studio environment that combines Jupyter notebooks with experiment management and tracking (see screenshot below), a model debugger, an “autopilot” for users without machine learning knowledge, batch transforms, a model monitor, and deployment with elastic inference.Azure Machine Learning is a cloud-based environment that you can use to train, deploy, automate, manage, and track machine learning models. It can be used for any kind of machine learning, from classical machine learning to deep learning, and both supervised learning and unsupervised learning.Azure Machine Learning supports writing Python or R code as well as providing a drag-and-drop visual designer and an AutoML option. You can build, train, and track highly accurate machine learning and deep-learning models in an Azure Machine Learning Workspace, whether you train on your local machine or in the Azure cloud.Azure Machine Learning interoperates with popular open source tools, such as PyTorch, TensorFlow, Scikit-learn, Git, and the MLflow platform to manage the machine learning lifecycle. It also has its own open source MLOps environment, shown in the screenshot below.Also on InfoWorld: How dataops improves data, analytics, and machine learningThe Domino Data Science platform automates devops for data science, so you can spend more time doing research and test more ideas faster. Automatic tracking of work enables reproducibility, reusability, and collaboration. Domino lets you use your favorite tools on the infrastructure of your choice (by default, AWS), track experiments, reproduce and compare results (see screenshot below), and find, discuss, and re-use work in one place.The Google Cloud AI Platform includes a variety of functions that support machine learning lifecycle management: an overall dashboard, the AI Hub (see screenshot below), data labeling, notebooks, jobs, workflow orchestration (currently in a pre-release state), and models. Once you have a model you like, you can deploy it to make predictions.The notebooks are integrated with Google Colab, where you can run them for free. The AI Hub includes a number of public resources including Kubeflow pipelines, notebooks, services, TensorFlow modules, VM images, trained models, and technical guides. Public data resources are available for image, text, audio, video, and other types of data.HPE Ezmeral ML Ops offers operational machine learning at enterprise scale using containers. It supports the machine learning lifecycle from sandbox experimentation with machine learning and deep learning frameworks, to model training on containerized distributed clusters, to deploying and tracking models in production. You can run the HPE Ezmeral ML Ops software on-premises on any infrastructure, on multiple public clouds (including AWS, Azure, and GCP), or in a hybrid model.Metaflow is a Python-friendly, code-based workflow system specialized for machine learning lifecycle management. It dispenses with the graphical user interfaces you see in most of the other products listed here, in favor of decorators such as , as shown in the code excerpt below. Metaflow helps you to design your workflow as a directed acyclic graph (DAG), run it at scale, and deploy it to production. It versions and tracks all your experiments and data automatically. Metaflow was recently open-sourced by Netflix and AWS. It can integrate with Amazon SageMaker, Python-based machine learning and deep learning libraries, and big data systems.MLflow is an open source machine learning lifecycle management platform from Databricks, still currently in Alpha. There is also a hosted MLflow service. MLflow has three components, covering tracking, projects, and models.MLflow tracking lets you record (using API calls) and query experiments: code, data, config, and results. It has a web interface (shown in the screenshot below) for queries.MLflow projects provide a format for packaging data science code in a reusable and reproducible way, based primarily on conventions. In addition, the Projects component includes an API and command-line tools for running projects, making it possible to chain together projects into workflows.MLflow models use a standard format for packaging machine learning models that can be used in a variety of downstream tools — for example, real-time serving through a REST API or batch inference on Apache Spark. The format defines a convention that lets you save a model in different “flavors” that can be understood by different downstream tools.Paperspace Gradientº is a suite of tools for exploring data, training neural networks, and building production-grade machine learning pipelines. It has a cloud-hosted web UI for managing your projects, data, users, and account; a CLI for executing jobs from Windows, Mac, or Linux; and an SDK to programmatically interact with the Gradientº platform.Gradientº organizes your machine learning work into projects, which are collections of experiments, jobs, artifacts, and models. Projects can optionally be integrated with a GitHub repo via the GradientCI GitHub app. Gradientº supports Jupyter and JupyterLab notebooks.Also on InfoWorld: Applying devops in data science and machine learningExperiments (see screenshot below) are designed for executing code (such as training a deep neural network) on a CPU and optional GPU without managing any infrastructure. Experiments are used to create and start either a single job or multiple jobs (e.g. for a hyperparameter search or distributed training). Jobs are a made up of a collection of code, data, and a container that are packaged together and remotely executed. Paperspace experiments can generate machine learning models, which can be interpreted and stored in the Gradient Model Repository.Paperspace Core can manage virtual machines with CPUs and optionally GPUs, running in Paperspace’s own cloud or on AWS. Gradientº jobs can run on these VMs.  Seldon Core is an open-source platform for rapidly deploying machine learning models on Kubernetes. Seldon Deploy is an enterprise subscription service that allows you to work in any language or framework, in the cloud or on-prem, to deploy models at scale. Seldon Alibi is an open-source Python library enabling black-box machine learning model inspection and interpretation.Deep learning vs. machine learning: Understand the differencesWhat is machine learning? Intelligence derived from dataWhat is deep learning? Algorithms that mimic the human brainMachine learning algorithms explainedWhat is natural language processing? AI for speech and textAutomated machine learning or AutoML explainedSupervised learning explainedSemi-supervised learning explainedUnsupervised learning explainedReinforcement learning explainedKaggle: Where data scientists learn and competeWhat is CUDA? Parallel processing for GPUsHow to choose a cloud machine learning platformDeeplearning4j: Deep learning and ETL for the JVMReview: Amazon SageMaker plays catch-upTensorFlow 2 review: Easier machine learningReview: Google Cloud AutoML is truly automated machine learningReview: MXNet deep learning shines with GluonPyTorch review: A deep learning framework built for speedReview: Keras sails through deep learning", "pub_date": "2020-09-21"},
{"title": "14 open source tools to make the most of machine learning", "overview": "Tap the predictive power of machine learning with these diverse, easy-to-implement libraries and frameworks", "image_url": null, "url": "https://www.infoworld.com/article/3575420/14-open-source-tools-to-make-the-most-of-machine-learning.html", "body": "Spam filtering, face recognition, recommendation engines — when you have a large data set on which you’d like to perform predictive analysis or pattern recognition, machine learning is the way to go. The proliferation of free open source software has made machine learning easier to implement both on single machines and at scale, and in most popular programming languages. These open source tools include libraries for the likes of Python, R, C++, Java, Scala, Clojure, JavaScript, and Go.Also on InfoWorld: How to choose a cloud machine learning platformApache MahoutApache Mahout provides a way to build environments for hosting machine learning applications that can be scaled quickly and efficiently to meet demand. Mahout works mainly with another well-known Apache project, Spark, and was originally devised to work with Hadoop for the sake of running distributed applications, but has been extended to work with other distributed back ends like Flink and H2O.Mahout uses a domain specific language in Scala. Version 0.14 is a major internal refactor of the project, based on Apache Spark 2.4.3 as its default.ComposeCompose, by Innovation Labs, targets a common issue with machine learning models: labeling raw data, which can be a slow and tedious process, but without which a machine learning model can’t deliver useful results. Compose lets you write in Python a set of labeling functions for your data, so labeling can be done as programmatically as possible. Various transformations and thresholds can be set on your data to make the labeling process easier, such as placing data in bins based on discrete values or quantiles.Core ML ToolsApple’s Core ML framework lets you integrate machine learning models into apps, but uses its own distinct learning model format. The good news is you don’t have to pretrain models in the Core ML format to use them; you can convert models from just about every commonly used machine learning framework into Core ML with Core ML Tools.Core ML Tools runs as a Python package, so it integrates with the wealth of Python machine learning libraries and tools. Models from TensorFlow, PyTorch, Keras, Caffe, ONNX, Scikit-learn, LibSVM, and XGBoost can all be converted. Neural network models can also be optimized for size by using post-training quantization (e.g., to a small bit depth that’s still accurate). CortexCortex provides a convenient way to serve predictions from machine learning models using Python and TensorFlow, PyTorch, Scikit-learn, and other models. Most Cortex packages consist of only a few files — your core Python logic, a cortex.yaml file that describes what models to use and what kinds of compute resources to allocate, and a requirements.txt file to install any needed Python requirements. The whole package is deployed as a Docker container to AWS or another Docker-compatible hosting system. Compute resources are allocated in a way that echoes the definitions used in Kubernetes for same, and you can use GPUs or Amazon Inferentia ASICs to speed serving.Also on InfoWorld: The best free data science courses during quarantineFeaturetoolsFeature engineering, or feature creation, involves taking the data used to train a machine learning model and producing, typically by hand, a transformed and aggregated version of the data that’s more useful for the sake of training the model. Featuretools gives you functions for doing this by way of high-level Python objects built by synthesizing data in dataframes, and can do this for data extracted from one or multiple dataframes. Featuretools also provides common primitives for the synthesis operations (e.g., , to provide time elapsed between instances of time-stamped data), so you don’t have to roll those on your own.GoLearnGoLearn, a machine learning library for Google’s Go language, was created with the twin goals of simplicity and customizability, according to developer Stephen Whitworth. The simplicity lies in the way data is loaded and handled in the library, which is patterned after SciPy and R. The customizability lies in how some of the data structures can be easily extended in an application. Whitworth has also created a Go wrapper for the Vowpal Wabbit library, one of the libraries found in the Shogun toolbox.GradioOne common challenge when building machine learning applications is building a robust and easily customized UI for the model training and prediction-serving mechanisms. Gradio provides tools for creating web-based UIs that allow you to interact with your models in real time. Several included sample projects, such as input interfaces to the Inception V3 image classifier or the MNIST handwriting-recognition model, give you an idea of how you can use Gradio with your own projects.H2OH2O, now in its third major revision, provides a whole platform for in-memory machine learning, from training to serving predictions. H2O’s algorithms are geared for business processes—fraud or trend predictions, for instance—rather than, say, image analysis. H2O can interact in a stand-alone fashion with HDFS stores, on top of YARN, in MapReduce, or directly in an Amazon EC2 instance.Hadoop mavens can use Java to interact with H2O, but the framework also provides bindings for Python, R, and Scala, allowing you to interact with all of the libraries available on those platforms as well. You can also fall back to REST calls as a way to integrate H2O into most any pipeline. OryxOryx, courtesy of the creators of the Cloudera Hadoop distribution, uses Apache Spark and Apache Kafka to run machine learning models on real-time data. Oryx provides a way to build projects that require decisions in the moment, like recommendation engines or live anomaly detection, that are informed by both new and historical data. Version 2.0 is a near-complete redesign of the project, with its components loosely coupled in a lambda architecture. New algorithms, and new abstractions for those algorithms (e.g., for hyperparameter selection), can be added at any time.PyTorch LightningWhen a powerful project becomes popular, it’s often complemented by third-party projects that make it easier to use. PyTorch Lightning provides an organizational wrapper for PyTorch, so that you can focus on the code that matters instead of writing boilerplate for each project.Lightning projects use a class-based structure, so each common step for a PyTorch project is encapsulated in a class method. The training and validation loops are semi-automated, so you only need to provide your logic for each step. It’s also easier to set up the training results in multiple GPUs or different hardware mixes, because the instructions and object references for doing so are centralized. Also on InfoWorld: 5 reasons to choose PyTorch for deep learningScikit-learnPython has become a go-to programming language for math, science, and statistics due to its ease of adoption and the breadth of libraries available for nearly any application. Scikit-learn leverages this breadth by building on top of several existing Python packages—NumPy, SciPy, and Matplotlib—for math and science work. The resulting libraries can be used for interactive “workbench” applications or embedded into other software and reused. The kit is available under a BSD license, so it’s fully open and reusable.ShogunShogun is one of the longest-lived projects in this collection. It was created in 1999 and written in C++, but can be used with Java, Python, C#, Ruby, R, Lua, Octave, and Matlab. The latest major version, 6.0.0, adds native support for Microsoft Windows and the Scala language.Though popular and wide-ranging, Shogun has competition. Another C++-based machine learning library, Mlpack, has been around only since 2011, but professes to be faster and easier to work with (by way of a more integral API set) than competing libraries. Spark MLlibThe machine learning library for Apache Spark and Apache Hadoop, MLlib boasts many common algorithms and useful data types, designed to run at speed and scale. Although Java is the primary language for working in MLlib, Python users can connect MLlib with the NumPy library, Scala users can write code against MLlib, and R users can plug into Spark as of version 1.5. Version 3 of MLlib focuses on using Spark’s DataFrame API (as opposed to the older RDD API), and provides many new classification and evaluation functions.Another project, MLbase, builds on top of MLlib to make it easier to derive results. Rather than write code, users make queries by way of a declarative language à la SQL. Also on InfoWorld: 10 MLops platforms to manage the machine learning lifecycleWekaWeka, created by the Machine Learning Group at the University of Waikato, is billed as “machine learning without programming.” It’s a GUI workbench that empowers data wranglers to assemble machine learning pipelines, train models, and run predictions without having to write code. Weka works directly with R, Apache Spark, and Python, the latter by way of a direct wrapper or through interfaces for common numerical libraries like NumPy, Pandas, SciPy, and Scikit-learn. Weka’s big advantage is that it provides browsable, friendly interfaces for every aspect of your job including package management, preprocessing, classification, and visualization.", "pub_date": "2020-09-23"},
{"title": "Predictions for cloud computing in 2021", "overview": "Public cloud vendors will cement their dominance, but enterprises will look to hybrid clouds, PaaS, open-partner ecosystems, and augmented reality. ", "image_url": null, "url": "https://www.infoworld.com/article/3601731/predictions-for-cloud-computing-in-2021.html", "body": "Cloud computing is the centerpiece of the world’s technical response to the COVID-19 crisis. Indeed, the leading public cloud providers were standout business successes in this most unusual of years. As businesses everywhere managed to keep the lights on by having personnel work from their homes, all of the principal cloud providers substantially grew their revenues and continued to deliver innovations at a blistering pace.With the novel coronavirus pandemic still hammering the global economy, here are my predictions for the enterprise cloud computing market in 2021.Also on InfoWorld: The state of cloud computing in 2020Cloud computing will be central to the postpandemic new normalIn the sad and fraught year now coming to a close, cloud services were a godsend for keeping the economy and our lives from grinding to a halt.In 2021 and beyond, everybody will continue to rely thoroughly on clouds (as well as on streaming, remote collaboration, smart sensors, and other cloud-reliant digital technologies) to emerge from a pandemic that is still grinding down on us remorselessly. Enterprise technology professionals will adjust their cloud strategies with one eye on COVID-19 trends and the other on their digital transformation initiatives. The tech vendors who stand to gain the most are those such as Amazon, Google, and Microsoft that provide full, cloud-to-edge ecosystems that enable seamless new normal lifestyles.Public clouds will grow even more dominantIn the past year, public clouds rode the pandemic to faster growth. According to IDC, enterprise cloud spending, both public and private, increased 34.4 percent from a year previous, while non-cloud IT spending declined by eight percent.In 2021 leading public cloud platforms—especially Amazon Web Services, Microsoft Azure, and Google Cloud Platform—will cement their dominance in the cloud market and expand their sway across many sectors of the global economy. AWS will retain its leading market share, though Microsoft, Google, and Alibaba will continue to close the gap. Revenue growth will remain explosive through mid-decade, according to Deloitte’s projections, never dipping below 30 percent annually. Global cloud spending will grow seven times faster than overall IT spending through this period. IDC forecasts that worldwide spending on public cloud services and infrastructure will nearly double, to around $500 billion, by 2023.Enterprises will mitigate cloud lock-in through hybrid and multicloud strategiesIn the past year, public clouds’ deepening dominance compelled traditional enterprise computing companies to set their strategic focus on hybrid and multiclouds. In 2021, enterprises will grow more uneasy with their reliance on the top tier providers. IT professionals will seek out hybrid and multicloud tools to reduce their risks of being locked in to specific providers. This is already a mainstream tactic considering that, per Flexera estimates, 93 percent of enterprises have a multicloud strategy and 87 percent have a hybrid cloud strategy.Going forward the growing maturity of hybrid/multicloud offerings from AWS, Microsoft, and Google will tempt enterprise cloud managers into increasing their spending with these providers. At the same time, private-cloud stalwarts IBM, Hewlett Packard Enterprise, Cisco, Dell EMC, VMware, and others will continue to beef up their hybrid/multicloud integrations with the dominant public cloud services in order to defend their enterprise IT market shares.Nevertheless this space is now a war of attrition. Hybrid-cloud appliances such as AWS Outposts and Google Anthos won’t significantly boost those vendors’ shares in their core public cloud market segments.Platform as a service will grow in public cloud revenue shareIn this year’s rapid shift to work-from-home arrangements, SaaS providers such as Oracle, SAP, and Salesforce provided an essential platform for continue business as usual in spite of the disruptions.As remote work remains a mainstream approach, SaaS providers of all sorts will be poised for runaway growth. In 2021 Gartner projects SaaS will remain the largest cloud market segment by revenues, growing to $117.7 billion by year end. However, PaaS-based application services will grow even faster, driven by enterprise customers’ increasing emphasis on cloud-native, containerized, and serverless cloud platforms.One of the biggest PaaS growth segments in 2021 will be multicloud serverless offerings from Vendia, Microsoft, Red Hat, and others. These solutions and associated low-code platforms will be essential ingredients in more enterprises’ application modernization, digital transformation, and business continuity strategies.Intelligent edge will become the principal cloud on-rampIn the year gone by, the rapid shift of most economic sectors to remote work triggered a boom in mobile devices, AI (artificial intelligence)-powered automation, autonomous robotics, and industrial IoT (Internet of Things) platforms.In 2021 public cloud providers will shift an increasing share of their workloads to intelligent-edge platforms to deliver the low latencies required by these applications. By this time next year, approximately 90 percent of industrial enterprises will use edge computing, according to Frost & Sullivan projections. As 5G is rolled out worldwide during the coming years, demand for cloud-to-edge applications will skyrocket. More of these workloads will involve edge-based, AI-driven processing of smart sensor data and Tiny ML (machine learning) workloads.In the coming year, IBM/Red Hat, Hewlett Packard Enterprise, and Microsoft will invest deeply in the strategic 5G/edge/AI platforms that they launched in 2020. Nvidia will leverage its Arm acquisition to deepen its portfolio of solutions to automate delivery of AI apps to cloud-to-edge and hybrid-cloud environments. This trend will also drive demand for software-defined wide area networks to the benefit of VMware, Cisco, Juniper, Arista, and other vendors in this segment.Under regulatory siege, Big Tech will emphasize open partner ecosystemsIn the year now ending, Amazon, Microsoft, Google, and other Big Tech companies found themselves fending off lawsuits, regulatory actions, and legislative pressure to be broken up. In 2021, these and other dominant cloud providers will slow down strategic acquisitions, which they’ve been accused of using to stop rivals in their tracks, while ramping up and boasting of their open partner ecosystems. Increasingly the big cloud providers will position themselves as back-end fulfillment agents within partner-led enterprise deals. They will downplay efforts to use their multicloud and hybrid cloud solutions as a lever to migrate enterprise workloads from legacy, on-premises platforms.Augmented reality will help train the employees needed to support cloud growth.Here is one last cloud-computing prediction, pertaining more to cloud professionals’ working lives than to the market for cloud services. As enterprises retool their cloud management processes, they will adopt a growing range of augmented reality tools to deliver training and guidance to cloud technical staff working from home.Like all businesses, cloud companies and their customers will need to ensure that their personnel are safe, healthy, and productive in distanced work environments. Innovative approaches such as augmented reality are the only way that Amazon can hope to deliver on its recently announced plan to train 29 million people worldwide to work in cloud computing. Even in normal times, Amazon would find it impractical to train this many people in person.Beyond keeping people safe, the cloud computing industry’s biggest challenge in 2021 will be to find, train, and equip enough skilled people to support customers’ digital transformation initiatives.", "pub_date": "2020-12-17"},
{"title": "What’s new in Kubernetes 1.20", "overview": "The latest release of the container orchestration system deprecates the Docker runtime in favor of its own runtime interface", "image_url": null, "url": "https://www.infoworld.com/article/3229359/whats-new-in-kubernetes.html", "body": "KubernetesThe Docker runtime is being deprecated. However, this doesn’t mean Docker images or Dockerfiles don’t work in Kubernetes anymore. It just means Kubernetes will now use its own Container Runtime Interface (CRI) product to execute containers instead of the Docker runtime. For most users this will have no significant impact—e.g., any existing Docker images will work fine. But some issues might result when dealing with runtime resource limits, logging configurations, or how GPUs and other special hardware interact with the runtime (something to note for those using Kubernetes for machine learning). The previous link provides details on how to migrate workloads, if needed, and what issues to be aware of.Volume snapshot operations are now stable. This allows volume snapshots—images of the state of a storage volume—to be used in production. Kubernetes applications that depend on highly specific state, such as images of database files, will be easier to build and maintain with this feature active.Kubectl Debug is now in beta, allowing common debug workflows to be conducted from within the kubectl command-line environment. API Priority and Fairness (APF) is now enabled by default, although still in beta. Incoming requests to kube-apiserver can be sorted by priority levels, so that the administrator can specify which requests should be satisfied most immediately.Process PID Limiting is now in general availability. This feature ensures that pods cannot exhaust the number of process IDs available on a Linux host, or interfere with other pods by using up too many processes.Kubernetes 1.17, released in December 2019, introduced the following key new features and revisions: Volume snapshots, introduced in alpha in Kubernetes 1.12, are now promoted to beta. This feature allows a volume in a cluster to be snapshotted at a given moment in time. Snapshots can be used to provision a new volume with data from the snapshot, or to roll back an existing volume to an earlier snapshotted version. Volume snapshots make it possible to perform elaborate data-versioned or code-versioning operations inside a cluster that weren’t previously possible.More of the “in-tree” (included by default) storage plug-ins are now being moved to the Container Storage Interface (CSI) infrastructure. This means less direct dependencies on those drivers for the core version of Kubernetes. However, a cluster has to be explicitly updated to support migrating the in-tree storage plug-ins, but a successful migration shouldn’t have any ill effects for a cluster.The cloud provider labels feature, originally introduced in beta back in Kubernetes 1.2, is now generally available. Nodes and volumes are labeled based on the cloud provider where the Kubernetes cluster runs, as a way to describe to the rest of Kubernetes how those nodes and volumes should be handled (e.g., by the scheduler). If you are using the earlier beta versions of the labels yourself, you should upgrade them to their new counterparts to avoid problems.Where to download KubernetesYou can download the Kubernetes source code from the releases page of its official GitHub repository. Kubernetes is also available by way of the upgrade process provided by the numerous vendors that supply Kubernetes distributions.In this 90-second video, learn about Kubernetes, the open-source system for automating containerized applications, from one of the technology’s inventors, Joe Beda, founder and CTO at Heptio.What’s new in Kubernetes 1.16Custom resource definitions (CRDs), the long-recommended mechanism for extending Kubernetes functionality introduced in Kubernetes 1.7, are now officially a generally available feature. CRDs have already been widely used by third parties. With the move to GA, many optional-but-recommended behaviors are now required by default to keep the APIs stable.Many changes have been made to how volumes are handled. Chief among them is moving the volume resizing API, found in the Container Storage Interface (CSI), to beta.Kubeadm now has alpha support for joining Windows worker nodes to an existing cluster. The long-term goal here is to make Windows and Linux nodes both first-class citizens in a cluster, instead of having only a partial set of behaviors for Windows.CSI plug-in support is now available in alpha for Windows nodes, so those systems can start using the same range of storage plug-ins as Linux nodes.A new feature, Endpoint Slices, allows for greater scaling of clusters and more flexibility in handling network addresses. Endpoint Slices are now available as an alpha test feature.The way metrics are handled continues a major overhaul with Kubernetes 1.16. Some metrics are being renamed or deprecated to bring them more in line with Prometheus. The plan is to remove all deprecated metrics by Kubernetes 1.17.Finally, Kubernetes 1.16 removes a number of deprecated API versions. What’s new in Kubernetes 1.15More features (currently in alpha and beta) for Custom Resource Definitions, or CRDs. CRDs in Kubernetes are the foundation of its extensibility technology, allowing Kubernetes instances to be customized without falling out of conformance with upstream Kubernetes standards. The new features include the ability to convert CRDs between versions (something long available for native resources), OpenAPI publishing for CRDs, default values for fields in OpenAPI-validated schemas for CRDs, and more.Native high availability (HA) in Kubernetes is now in beta. Setting up a cluster for HA still requires planning and forethought, but the long-term goal is to make HA possible without any third-party software.More plug-ins that manage volumes have been migrated to use the Container Storage Interface (CSI), a consistent way to manage storage for hosted containers. Among the new features introduced in alpha for CSI are volume cloning, so that new persistent volumes can be based on an existing one.Certificate management now automatically rotates certificates before expiration.A new framework for plug-ins that perform scheduling operations has entered alpha.What’s new in Kubernetes 1.14Microsoft Windows Server 2019 is now officially supported as a platform for running both Kubernetes worker nodes and container scheduling. This means entire Kubernetes clusters can run on Windows exclusively, rather than having a mix of Windows and Linux systems.The plugin mechanism for Kubectl, the default Kubernetes command-line tool, is now a stable feature, letting developers implement their own Kubectl subcommands as standalone binaries.Persistent local volumes are now a stable feature. This lets locally attached storage be used by Kubernetes for persistent volumes. Aside from offering better performance than using network-attached storage, it also makes it easier (and potentially cheaper) to stand up a cluster.Process ID limiting for Linux hosts is now a beta feature. This prevents any one pod from using up too many process IDs and thus causing resource exhaustion on the host.What’s new in Kubernetes 1.13\n\n\n\n\nCoreDNS\nWhat’s new in Kubernetes 1.12Released in late September 2018, Kubernetes 1.12 brings to general availability the Kubelet TLS Bootstrap. The Kubelet TLS Bootstrap allows a Kubelet, or the primary agent that runs on every Kubernetes node, to join a TLS-secured cluster automatically, by requesting a TLS client certificate through an API. By automating this process, Kubernetes allows clusters to be configured with higher security by default.Also new in Kubernetes 1.12 is support for Microsoft Azure’s virtual machine scale sets (VMSS), a way to set up a group of VMs that automatically ramp up or down on schedule or to meet demand. Kubernetes’s cluster-autoscaling feature now works with VMSS.Other new features in Kubernetes 1.12:Snapshot and restore functionality for volumes (alpha).Custom metrics for pod autoscaling (beta). This allows custom status conditions or other metrics to be used when scaling a pod—for instance, if resources that are specific to a given deployment of Kubernetes need to be tracked as part of the application’s management strategy.Vertical pod scaling (beta), which allows a pod’s resource limits to be varied across its lifetime, as a way to better manage pods that have a high cost associated with disposing of them. This is a long-standing item on many wish lists for Kubernetes, because it allows for strategies to deal with pods whose behaviors aren’t easy to manage under the current scheduling strategy.What’s new in Kubernetes 1.11Released in early July 2018, Kubernetes 1.11 adds IPVS, or IP Virtual Server, to provides high-performance cluster load balancing using an in-kernel technology that’s less complex than the  system normally used for such things. Eventually, Kubernetes will use IPVS as the default load balancer, but for now it’s opt-in.Custom resource definitions, billed as a way to make custom configuration changes to Kubernetes without breaking its standardizations, may now be versioned to allow for graceful transitions from one set of custom resources to another over time. Also new are ways to define “status” and “scale” subresources, which can integrate with monitoring and high-availability frameworks in a cluster.Other major changes include:CoreDNS, introduced in 1.10, is now available as a cluster DNS add-on, and is used by default in the  administration tool.Kubelet configuration changes can now be rolled out across a live cluster without first taking the cluster down.The Container Storage Interface (CSI) now supports raw block volumes, interoperates with the kubelet plugin registration system, and can pass secrets more readily to CSI plugins.There are many changes to storage, including online resizing of persistent volumes, the ability to specify a maximum volume count for a node, and better support for protecting storage objects from being removed when in use.What’s new in Kubernetes 1.10The March 2018 Kubernetes 1.10 production release includes the beta release of the Container Storage Interface (alpha as of Kubernetes 1.9) that promotes an easier way to add volume plug-ins to Kubernetes, something that previously required recompilng the Kubernetes binary. The Kubectl CLI, used to perform common maintenance and administrative tasks in Kubernetes, can now accept binary plug-ins that perform authentication against third-party services such as cloud providers and Active Directory.“Non-shared storage,” or the ability to mount local storage volumes as persistent Kubernetes volumes, is now also beta. The APIs for persistent volumes now have additional checks to make sure persistent volumes that are in use aren’t deleted. The native DNS provider in Kubernetes can now be swapped with CoreDNS, a CNCF-managed DNS project with a modular architecture, although the swap can only be accomplished when a Kubernetes cluster is first set up.The Kubernetes project is now also moving to an automated issue life-cycle management project, to ensure stale issues don’t stay open for too long.What’s new in Kubernetes 1.9Kubernetes 1.9 was released in December 2017.Promoted to beta in Kubernetes 1.8 and now in production release in Kubernetes 1.9, the Apps Workloads API provides ways to define workloads based on their behaviors, such as long-running apps that need persistent state.Version 1 of the Apps Workloads API brings four APIs to general availability:", "pub_date": "2020-12-09"},
{"title": "What to expect from AWS re:Invent 2020", "overview": "The world’s biggest cloud vendor has shifted its annual event online and spread it across three weeks. Here’s our guide to what to expect and how to navigate the event from your home office.", "image_url": null, "url": "https://www.infoworld.com/article/3598018/what-to-expect-from-aws-reinvent-2020.html", "body": "Cloud computing giant Amazon Web Services (AWS) has its biggest event of the year next week, with AWS re:Invent running online-only and free of charge for the first time, starting November 30 and closing December 18.This year the event will not be spread across various hotels on the Las Vegas strip, but rather across a three-week period online. This brings its own logistical challenges.Also on InfoWorld: Why AWS leads in the cloudThe event will kick off with a “Late Night with AWS” session on Monday night, followed by CEO Andy Jassy’s typical three hour keynote on Tuesday, December 1. This will be followed by the Thursday partner keynote. CTO Werner Vogels will give his technical keynote during the third week, on Tuesday, December 15. The other keynotes will focus on technical areas like machine learning and infrastructure.Aside from the keynotes there are various “leadership” sessions, breakouts, lounges, and “ask the expert” sessions across 50 content tracks and multiple language options. Except for the Late Night sessions there will be no content on Mondays or Fridays.Industry-specific newsIt’s safe to expect Jassy and his senior leadership team to continue its recent efforts to make AWS more accessible to business decision makers, while keeping its core developer audience excited and engaged.“AWS has missed a direct line of communication to the C-suite, not just IT leaders or the developer community, but a business conversation. How does AWS help a senior leader of a business react, respond, or transform? This is the time for AWS to be doing that, as technology and business have never been so entwined,” Nick McQuire, vice president for enterprise research at CCS Insight told InfoWorld.McQuire sees this manifesting around a set of announcements focused on specific industries, such as telecoms, media, and industry. AWS announced a major deal with Verizon at last year’s re:Invent but will look to maintain momentum as 5G networks start to come online.This industry-specific work will span the whole portfolio at AWS but specifically its efforts around the edge and hybrid cloud, where Outposts and Local Zones could see some updates as they see momentum across those key industries. We also wouldn’t be surprised to see more vertical-specific machine learning products, like the anti-fraud detection for financial services and various medical applications which were announced last year.AWS product announcementsIn terms of major product announcements, AWS is expected to bolster its multicloud management toolset, enabling customers to manage their workloads on rival clouds, as well as those with AWS and on-premises. An October report from The Information proposes that such a product is due to be announced at re:Invent. AWS did not deny the validity of the claims when asked by InfoWorld.AWS has long tried to convince the market that enterprises should shift all of their workloads to the public cloud—ideally theirs—but has since enabled more hybrid cloud options to run cloud-like workloads in on-prem or private clouds. Now it could be set to soften its stance further in face of market pressures.Whatever is announced should help AWS compete with products like Google Cloud’s Anthos multicloud management platform, or Microsoft’s Azure Arc, or IBM’s suite of options via its newly acquired Red Hat assets.In an interesting piece of timing, Google Cloud CEO Thomas Kurian recently published a blog post expounding the values of an “open cloud approach”—one that “ensures operational and technical consistency across public clouds or private data centers and effective management of infrastructure, applications, and data across the organization.”McQuire at CCS Insight is less convinced, however. “I am not sure if multicloud will necessarily be the headline,” he said.Instead, he expects AWS to build on its hybrid cloud options like AWS Outposts and Local Zones, as well as continuing to build out its managed container and Kubernetes services. That is “unless they can showcase a capability which clearly exceeds the competition,” he added.Attendees should also expect announcements around the Amazon’s work with custom silicon, building on last year’s announcement of an extended partnership with Arm to design the Graviton2 and a new EC2 Inf1 instance that features AWS’s own Inferentia chips, which are optimized for machine learning workloads and now run most of the company’s Alexa workloads.Also on InfoWorld: How to make the most of the AWS free tierAWS expert lounges and learningAWS re:Invent will be particularly difficult to navigate this year, even without having to keep track of the shuttle schedule. The session catalog is difficult to parse and there is no clear agenda to follow.For those worried about missing out on all that networking they normally get to do, AWS is setting up plenty of virtual lounges. These lounges will also have AWS technical experts on hand to break down technical presentations and answer questions.It wouldn’t be AWS re:Invent without some training and certifications, with AWS running its usual range of “Jams” and “GameDays” for hands-on learning.", "pub_date": "2020-11-24"},
{"title": "Steeltoe: Simplify building .NET cloud microservices", "overview": "Open source .NET tools help build and deploy distributed applications to Spring Cloud and Kubernetes.", "image_url": null, "url": "https://www.infoworld.com/article/3598923/steeltoe-simplify-building-net-cloud-microservices.html", "body": "The .NET Foundation is the home for more than .NET. It’s the open source hub for languages and frameworks to help you build on top of the various .NET runtimes and compilers, with contributions from companies and individuals around the world.One of the more popular tools came out of Pivotal (now part of VMware again). Intended to help developers build better .NET microservices, Steeltoe serves as a bridge between .NET and Pivotal’s Spring Cloud and Cloud Foundry platforms, as well as to Kubernetes and other containers, with a set of libraries that speed up application development. Now that it’s open, it’s receiving code contributions from across the .NET community, including teams at Microsoft, and building on work done in Netflix’s open-source libraries. You can find its repository on GitHub.Also on InfoWorld: What are microservices? Your next software architectureMicroservices with SteeltoeBy building on familiar ways of working, Steeltoe can also be used as a springboard into .NET development from Java and other enterprise languages. You can take work you’ve done in Java on Spring Cloud and port it to .NET, or use Steeltoe connectors to mix different technologies so existing applications can be enhanced with .NET microservices. Project managers will find this approach helpful, as they can mix and match available resources as necessary without having to worry about compatibility issues.Steeltoe can create a cloud microservice using its initialization tool, adding providers for different cloud services and automatically configuring code for deployment. Configuration can be easily stored outside your code, so you can keep tokens and other important authentication details in secure services such as Hashicorp’s Vault without exposing them in code repositories.Its biggest advantages are its libraries, which prepackage useful cloud design patterns ready for use in your code. These include support for service discovery, with Eureka and Consul clients, along with distributed tracing to help debug code. Other key elements support messaging by working with network file shares and provide connectors to the cloud services you’re going to use. Cloud-native microservices need to be stateless, easy to compose, and well-defined, and Steeltoe’s libraries help build code that supports cloud-native design patterns without changing how you write your .NET Core applications.The latest release is Steeltoe 3.0. This moves support away from the .NET Framework to .NET Core, ready for .NET 5 and future .NET Core-based releases. It’s added support for other distributed application platforms, such as Kubernetes, and for messaging-based architectures. That has meant changes in package naming, so code will need some refactoring when you upgrade to the new releases.Using circuit breaker patternsOne useful Steeltoe feature is support for Circuit Breaker patterns using tools based on Netflix’s Hystrix. The original Hystrix is in maintenance mode, but the model it uses is still useful for making microservices fault tolerant, adding resilience to applications by providing a way to quickly lock down failures and prevent cascades.Circuit breakers are an important tool in distributed architectures where you have limited observability into remote or third-party services. That makes them useful where services are built and managed by different devops teams, and where you may be consuming a service that becomes unreliable when too many applications use it. A circuit breaker monitors all calls to a service from your code. If too many calls fail, it falls back to alternatives and raises an alert. This can trigger a restart or simply wait until the remote service is available again.Steeltoe makes it easy to add a circuit breaker wrapper around any service call. This runs in its own thread pool to help manage load on the target service. If a thread isn’t available because there are too many calls from your code or the circuit breaker is waiting while the remote service isn’t responding, the circuit breaker can call its fallback. Similarly calls can have a timeout, with all actions logged and displayed in their own dashboard so you can see how services are operating. These logs are a useful diagnostic, and you can use tools like the ELK stack or Azure’s Monitor to make them part of your devops processes.Deploying to Kubernetes with Project TyeIf you want to make .NET part of a wider cloud-native environment, Steeltoe works well with Microsoft’s Project Tye, bringing .NET microservices to Kubernetes (in particular Azure Kubernetes Service). Here you build services using Steeltoe libraries before adding them to containers using Project Tye. This automates the process of creating .NET containers and Kubernetes manifests, as well as finding other services and handling container dependencies. It’s a useful tool, installed using the .NET command line tools. Once installed it’s easy to go from app scaffolding to running code, with a dashboard to monitor running services and view logs. Multiple services can be added to a single .NET solution, which can then be run using a single tye call.More complex configurations are managed using Project Tye’s own YAML manifests, with application and per-service configurations. The tye.yaml file will help manage dependencies, working with Docker files and container registries to pull in services and deploy them as necessary. Some cases require preconfiguring and deploying external services, for example databases or caches.Building services with Steeltoe and bundling with Project Tye is a quick way to get started with cloud-native .NET development. Although Project Tye remains an experiment, it will be interesting to see if Microsoft continues working on it now that .NET 5 has shipped, as its planned integration with tools like Dapr would make it a useful set of extensions to any .NET development environment. From my point of view, integrating it into the .NET command line environment would make a lot of sense, doing for .NET much of what tools like Draft do for the wider Kubernetes ecosystem.A cloud-agnostic approach to distributed applications in .NETBuilding and running distributed applications shouldn’t be hard; they’re key to working in and across multiple clouds. By supporting Spring Cloud and Kubernetes, Steeltoe offers an agnostic approach to multicloud development. You can build .NET code using familiar tools and techniques with the Steeltoe libraries, before using its configuration tools to support delivering your code to your target platform.The result is code that can be quickly switched from one cloud to another, and from your data center to a managed platform. Modern .NET Core-based code is designed for microservices, and Steeltoe helps provide many of the features you’d otherwise need to write yourself. It’s well worth exploring the various projects hosted by the .NET Foundation; you’re likely to find something that helps you write better code more quickly.", "pub_date": "2020-12-01"},
{"title": "Tune cloud app performance and cost efficiency", "overview": "Think your cloud apps are fully optimized for performance and cost? Guess again. These three tricks can cut thousands of dollars off cloud service bills and reduce latency. ", "image_url": null, "url": "https://www.infoworld.com/article/3599334/tune-cloud-app-performance-and-cost-efficiency.html", "body": "The elastic capabilities of public cloud computing are both good and bad. We can provision all the resources and cloud architects to solve performance issues. But tossing money at the situation willy-nilly doesn’t fix the root causes of the problems, which are typically traceable to bad design or bad coding.However some basic tricks of the cloud application development and design trade should allow you to at least reduce the number of performance issues (and by extension, trim monthly cloud bills as well). Here are three of the top ways to improve performance and cost efficiency and not break the bank:Also on InfoWorld: Which multicloud architecture will win out? Although many report larger cloud bills when running serverless, that has not been my experience. Indeed, if you’re unwilling or not budgeted to go into the application code and database schema to fine-tune performance issues, this approach leaves the resource provisioning to the cloud providers (using automation). They know how to optimize their resources better than you do.The idea is that the serverless system won’t over- or underprovision, based on the real-time resource requirements of the application, as determined by the serverless system. Optimization of cloud resources will be the responsibility of the serverless platform itself; many will find that unoptimized applications and databases will better perform if refactored for serverless.If you’re thinking that this is just technological duct tape, you’re right. This assumes that you can’t or won’t redesign, recode, and redeploy the applications, but you can port to a serverless platform. The cost savings should pay for moving to serverless in the first month, but you should gather metrics before and after to make sure. It’s a fundamental architectural principle to store data as close to the application using it as possible. I’m still seeing a number of databases stored in different regions than the applications, databases stored on different clouds, even on-premises databases with applications running in the public clouds. The result is more latency, poorer performance, more outages, and higher compute and network traffic bills.Many of these design flaws were driven by concerns about security and compliance. Even if your data needs to stay in country, or even on-premises, make sure that the application instances that use that data exist close to it. You’ll be happier with the cloud bill, and your users will be much happier with the performance and resiliency. This is the most laborious and thus the most expensive of all three tricks, although the level of effort required has fallen in the past several years.Code scanners, application profilers, and applications testing platforms can reveal a great deal of information about how applications can be refactored to increase performance by as much as three-fold. Of course this means recoding the applications in places, including testing and redeployment, which makes this approach the most expensive. But it’s also the most effective. See how that works?Performance and optimization issues are like back pain—we’re all going to encounter it at one time in our lives. When you do, any of these tricks will be a good place to find relief.", "pub_date": "2020-12-01"},
{"title": "AWS re:Invent: AWS gets more hybrid, but not multicloud", "overview": "The cloud giant announced several new options for customers to ease deployment and management of container-based and serverless applications, but stopped short of easing compatibility with cloud rivals. ", "image_url": null, "url": "https://www.infoworld.com/article/3599101/aws-reinvent-aws-gets-more-hybrid-but-not-multicloud.html", "body": "Amazon Web Services made a set of announcements during the first day of its AWS re:Invent conference this week aimed at helping customers ease the deployment and management of container-based and serverless applications both on premises and in the AWS cloud, but stopped short of explicitly making it easier to run alongside rival clouds.In this respect there were three major announcements from AWS CEO Andy Jassy’s virtual re:Invent keynote on Tuesday, December 1. The first two, Amazon EKS Anywhere and Amazon ECS Anywhere, are aimed at helping customers run containerized workloads seamlessly on premises and in the cloud.Also on InfoWorld: Why AWS leads in the cloudAmazon Elastic Kubernetes Service (EKS) is a managed Kubernetes service that uses the popular open source container orchestrator. Elastic Container Service (ECS) is a more proprietary, AWS-centric option for running containers.Jassy acknowledged that customers often use different flavors of these managed container services for different workloads and in different teams depending on their skill sets and unique requirements.With the Anywhere options, AWS is looking to make it easier to run EKS and ECS both on premises and in the cloud, while alleviating common management headaches by allowing developers to use the same APIs and cluster configurations for both types of workloads.Amazon's EKS Distro (EKS-D) is also being open sourced, allowing developers to maintain consistent Kubernetes deployments across environments, including bare metal and VMs. \"We’ve learned that customers want a consistent experience on-premises and in the cloud for migration purposes or to enable hybrid cloud setups,\" a blog post by Michael Hausenblas and Micah Hausler from AWS said.The third announcement in this space was the public preview of AWS Proton, a new service that allows developer teams to manage AWS infrastructure provisioning and code deployments for both serverless and container-based applications using a set of templates.These centrally managed templates will define and configure everything from cloud resources to the CI/CD pipeline for testing and deployment, with observability on top. Developers can choose from a set of Proton templates for simple deployment, with monitoring and alerts built-in. Proton also identifies downstream dependencies to alert the relevant teams of changes, upgrade requirements, and rollbacks. Proton will support on-premises workloads through EKS Anywhere and ECS Anywhere as they come online for customers.Hybrid, not multicloudTowards the end of his keynote Jassy reiterated his view that most companies will eventually run predominantly in the cloud, but it will take time to get there. Hence the need for hybrid capabilities—such as AWS Outposts, EKS and ECS Anywhere, and AWS Direct Connect—as a necessary on-ramp for enterprise customers.“We think of hybrid infrastructure as including the cloud alongside other edge nodes, including on-premises data centers. Customers want the same APIs, control plane, tools, and hardware they are used to using in AWS regions. Effectively they want us to distribute AWS to these various edge nodes,” Jassy said.Many enterprise customers want to run different workloads with multiple cloud providers depending on their specific needs. Further, many of these customers want to avoid becoming too dependent on any one cloud. For example, 37% of respondents to the IDG Cloud Computing Survey this year cited the desire to avoid vendor lock-in as one of their primary goals.Ahead of the event it was rumored that AWS would go further in launching a broader multicloud management option which would allow customers to manage Kubernetes workloads running on rival Google Cloud Platform and Microsoft Azure cloud infrastructure, much like Google Cloud is trying to do with Anthos and Microsoft with Azure Arc, or IBM’s suite of options via its newly acquired Red Hat assets.This didn’t happen on day one of re:Invent.“With the notable exception of fully embracing multicloud services, AWS is gradually becoming more flexible in supporting a wider range of customer requirements,” Nick McQuire, senior vice president at CCS Insight said after the keynote.That doesn’t mean EKS couldn’t be used to traverse various environments. As InfoWorld contributor and AWS employee Matt Asay tweeted, “Could you run EKS Distro on another cloud? Yep.”“Customers will be able to run EKS Anywhere on any customer-managed infrastructure. We are still working on the specific infrastructure types that we will support at GA,” Deepak Singh, AWS VP of compute services, told InfoWorld.Other major announcementsOver the three hours of Jassy’s keynote there were many other announcements, including those around databases, which also focused on customers’ desires for portability. AWS Glue Elastic Views was announced as a means for simple data replication across various data stores, while the open source Babelfish for Aurora PostgreSQL offers a way to run SQL Server applications on Aurora PostgreSQL.The machine learning platform Amazon SageMaker was enhanced with a new automated data wrangler feature and a feature store to make it easier to store and reuse features. Amazon SageMaker Pipelines was announced as a CI/CD solution for machine learning pipelines.Also on InfoWorld: How to make the most of the AWS free tierJassy also spent a lot of time focusing on the vendor’s continued work around more price performant custom silicon. He first reflected on the success of its Graviton2 and Inferentia chips announced last year, the latter of which are optimized for machine learning workloads and now run most of the company’s Alexa workloads.This year focused on price performance for machine learning workloads, with the announcement of Habana Gaudi-based EC2 instances, offering more economically viable processors for machine learning workloads and AWS Trainium as a cost-effective option for machine learning model training.Day one down, just 20 left to go.", "pub_date": "2020-12-01"},
{"title": "Oracle adds analytics to MySQL in the cloud", "overview": "Oracle MySQL Database Service with the MySQL Analytics Engine combines OLTP and OLAP in the same MySQL database in the Oracle Cloud", "image_url": null, "url": "https://www.infoworld.com/article/3599668/oracle-adds-analytics-to-mysql-in-the-cloud.html", "body": "Oracle is combining OLTP (online transaction processing) and OLAP (online analytical processing) services in its open source MySQL database in the Oracle Cloud, via Oracle MySQL Database Service with the MySQL Analytics Engine.With this Oracle Cloud service being introduced December 2, users gain the ability to run sophisticated analytics against their operational MySQL database. The MySQL Analytics Engine is an in-memory analytics accelerator that can scale to thousands of Oracle Cloud Infrastructure (OCI) cores.Also on InfoWorld: The best open source software of 2020Positioned to compete with the Amazon Redshift cloud data warehouse, the Oracle MySQL Database Service with the MySQL Analytics Engine saves users from having to integrate with a separate analytics database. Database administrators and developers gain a unified platform for OLTP and OLAP workloads. OLAP has not been a specialty of MySQL in the past.Capabilities of the MySQL Analytics Engine include:In-memory, hybrid columnar processingInter-node and intra-node parallelism optimized for Oracle Cloud InfrastructureDistributed query processing algorithmsTo add the analytics capabilities, the MySQL optimizer was enhanced to support analytics while the parser itself was unaltered. Hence, the existing MySQL syntax continues to work as is. With the analytics not requiring maintenance or development of any indexes, data updates made by users are propogated to the analytics engine and available to be queried immediately.Supporting large volumes of data and complex queries, MySQL Database Service with MySQL Analytics Engine has use cases such as fraud detection or decision support. The signup for the service can be found at oracle.com. ", "pub_date": "2020-12-02"},
{"title": "Salesforce targets public clouds with Hyperforce", "overview": "The SaaS CRM vendor is taking the first steps toward moving customers out of its own data centers and into those of its public cloud partners.", "image_url": null, "url": "https://www.infoworld.com/article/3599854/salesforce-targets-public-clouds-with-hyperforce.html", "body": "Salesforce has re-architected its underlying infrastructure to make all of its CRM solutions run in the public cloud as part of a program it calls Hyperforce.Hyperforce may not have been Salesforce’s headline announcement during this year’s Dreamforce—the CRM giant’s big annual conference—but for IT professionals tasked with managing CRM platforms, it is an important one.Also on InfoWorld: Why AWS leads in the cloudSalesforce has long had a strong relationship with infrastructure-as-a-service provider Amazon Web Services (AWS), and has also built relationships with Microsoft Azure, Alibaba Cloud, and Google Cloud Platform over the past few years.For customers who opt to switch to Hyperforce, Salesforce will shift your Sales Cloud, Service Cloud, Marketing Cloud, Commerce Cloud, Industries, and other CRM products, complete with existing customizations and security controls, to the public cloud according to specific data residency needs and other, as yet unclear factors.“Currently, Salesforce determines which cloud providers and specific services are used in each location,” a Salesforce spokesperson told InfoWorld.In short, Salesforce is looking to make it easier for customers to move their CRM to the public cloud, where they get access to greater flexibility and resiliency, as well as fine-grained data residency controls to comply with any local compliance or regulatory needs. Hyperforce is also backward compatible, so developers will not need to rearchitect any custom apps to run on Hyperforce.Hyperforce’s security architecture plugs into existing identity controls and it provides encryption at rest and in transit. Security compliance certifications will transfer over as standard.Salesforce COO Bret Taylor announced the new platform at Dreamforce, hailing it as the biggest change the SaaS company had made since its founding in 1999. Taylor also touted “partnerships with all the amazing public cloud vendors to enable you to work in every region you do business.”According to Salesforce’s 2020 annual report, the company currently provides “the majority of our services to our customers from infrastructure designed and operated by us but secured within third-party data center facilities. In combination with these third-party data center facilities, we also run our services on cloud computing platform partners who offer Infrastructure-as-a-Service, including servers, storage, databases, and networking.”In this context, Hyperforce looks like the first step toward more of the latter, reducing the vendor’s reliance on its own hardware investments.“Hyperforce is a long-term project. Salesforce will continue to maintain existing infrastructure and data centers for an extended period of time,” a spokesperson told InfoWorld.Jason Wong, VP and analyst on the App Design and Development team at Gartner sees the Hyperforce announcement as “the culmination of many years of architecture and infrastructure modernization that Salesforce was doing to enhance its underlying platform that powers its core CRM SaaS products,” he told InfoWorld via email.Also on InfoWorld: Amazon, Google, and Microsoft take their clouds to the edge“This allows Salesforce to reduce reliance on running certain products on its own data centers and potentially its infrastructure costs over time,” Wong added. “As Salesforce’s growth comes more from international markets, the flexibility to support regional cloud deployments and multicloud coverage across regions is an important part of Hyperforce.”Hyperforce is live today in India and Germany, with availability coming to 10 more countries next year. Customers who decide to move to Hyperforce currently will not pay more than their existing set up.", "pub_date": "2020-12-03"},
{"title": "Public clouds hit a wall in innovation", "overview": "As virtual conferences become the new normal, it’s easier to see that cloud is in a creative stall. ", "image_url": null, "url": "https://www.infoworld.com/article/3600129/public-clouds-hit-a-wall-in-innovation.html", "body": "As it opens this week, AWS re:Invent is not taking place in Vegas but is virtual and free. Virtual events are a silver lining of the pandemic because they keep me off airplanes and eliminate seven miles of walking each day at the bigger public cloud conferences. Maybe I’m getting lazy in my old age, but the time that virtual events save seems to be more productive.Not to pick on AWS, but when we look at the announced innovations at public cloud events during the past year, few were game changers. Yes, most vendors will continue to move toward the intelligent edge, providing more points of presence, and they will continue to exploit artificial intelligence. However, these are mostly evolutionary steps rather than revolutionary ideas.Also on InfoWorld: Cloud tech certifications count more than degrees nowIt doesn’t matter if we’re talking about moving from containers to serverless containers or from relational databases to purpose-built cloud-based databases or from outdated to next-generation cloud security. We’re at a point with public clouds where incremental improvements to existing cloud services will provide much more value to cloud users than taking the technology forward by leaps and bounds.As a technology CTO many times over, I’ve watched all technologies go through this kind of stall as the need to grow the business replaces the need to create net-new innovations. As a business, it’s often the right thing to do at a particular point in time. A more risk-averse posture can generate incremental growth and remove the chance of failure that naturally comes at the cutting edge.So, what happened to the revolution? It’s clear that public cloud providers have not pursued all ideas and innovations that could occur in public cloud computing. The large hyperscale companies still have a broad, ambitious vision for their futures. Enterprise IT professionals who consume cloud technology still want forward-thinking technologies, such as when cloud storage first came on the scene. At the same time, they also respond positively to service improvements and net-new tactical cloud services.The lack of innovations, while understandable, means that we’re no longer pushing technology to its limits or experimenting with an open mind as to what’s possible. I’m sure many inventive projects are taking place behind the scenes—perhaps some of them game changers—but what’s been announced so far in 2020 does not rise to that standard.", "pub_date": "2020-12-04"},
{"title": "Microsoft rolls out data governance service on Azure", "overview": "Azure Purview automates the discovery and cataloging of data on-premises, in the cloud, and in SaaS applications.", "image_url": null, "url": "https://www.infoworld.com/article/3600228/microsoft-azure-rolls-out-data-governance-service.html", "body": "Microsoft on December 3 introduced a preview of a cloud-based data governance service, called Azure Purview, to help organizations understand and manage their data.Free until January 1, Azure Purview automates the discovery and cataloging of data on-premises, in the cloud, and in SaaS applications. Information needed to govern data and assess compliance risks is gathered.Also on InfoWorld: When hybrid multicloud has technical advantagesMicrosoft said the service enables the creation of a holistic, up-to-date map of the company’s entire data landscape. Billed as the next generation of the data catalog, Azure Purview purports to reduce costs on multiple fronts, including reducing efforts to discover and classify data while eliminating hidden and explicit costs of maintaining homegrown systems and Excel-based solutions.Capabilities of Azure Purview include:Automated data discovery.Sensitive data classification, with data labeled across SQL Server, Azure, Microsoft 365, and Power BI.End-to-end data lineage.Finding of valuable, trustworthy data by data consumers.Integration of data systems using Apache Atlas APIs.Semantic search and data discovery using business or technical terms.Creation of a map of data across hybrid sources, with metadata management provided.Extraction of metadata, lineage, and classifications from existing data stores.AI-based data classifiers help identify data exposures and out-of-compliance data by looking for personally identifiable information and sensitive data.In February, Azure Purview’s data discovery, classification, and mapping capabilities will be available for data managed by other storage providers. Azure Purview was announced concurrently with the general availability of Azure Synaptics Analytics, an analytics service leveraging data integration, data warehousing, and enterprise data analytics.", "pub_date": "2020-12-04"},
{"title": "Has serverless computing run out of gas?", "overview": "While some industry observers see signs that serverless computing has stalled, others believe the paradigm is just getting started. ", "image_url": null, "url": "https://www.infoworld.com/article/3598049/has-serverless-computing-run-out-of-gas.html", "body": "Serverless computing offers the promise of applications executed as event-driven compute cycles, whereby your code runs only when triggered by a request. Subscribers to this paradigm can save money by paying only for compute time actually used instead of maintaining a virtual or physical server. Users also are spared from having to manage infrastructure.  AWS Lambda, Microsoft Azure Functions, Google Cloud Functions, and other serverless compute offerings have attracted a lot of attention in recent years. But the serverless revolution may be showing signs of stalling, with some shops not exactly falling all over themselves to adopt the paradigm.Also on InfoWorld: AWS vs. Azure vs. Google Cloud: Which free tier is best?In a report on cloud adoption published this spring, O’Reilly Media found that, among participants surveyed who had not adopted serverless, most—about 62 percent—had no plans to so so. “It certainly feels like serverless has lost its steam, based on the conversations we’re having,” said Mike Loukides, vice president of content strategy at O’Reilly Media. Loukides noted that serverless has fallen short of growth expectations. “Some of that is due to those technical issues that won’t go away,” he said. “Designing systems that can tolerate lots of latency is a big architectural challenge. But it’s pretty clear that a fair number are making it work.” Whether they have dealt with the architectural issues, ignored them, or have a use case where it doesn’t matter is an interesting question, Loukides said.Peder Ulander, product market lead for enterprise and developer audience at AWS, which offers the AWS Lambda serverless platform, disputes the notion of any kind of lull in the serverless realm. “We’ve been very happy with regard to the growth of our serverless business and seeing it becoming more and more strategic,” Ulander said. He added that serverless reduces overall costs and improves developer productivity. Hundreds of thousands of AWS customers are using AWS Lambda.Inquiries about serverless have increased this year at Gartner, analyst Arun Chandrasekaran said. There is a class of applications that serverless is extremely good at, particularly event-driven architectures, with serverless adding a substantial advantage, he noted. Highly ephemeral applications that live for a few minutes or a few seconds are also suitable for serverless.  But Chandrasekaran cautioned that new technologies like serverless often go through a hype cycle before customers realize what the technology actually can do. “In many cases, customers may be trying to use the technology beyond what it’s meant for,” he said.Also on InfoWorld: PaaS, CaaS, or FaaS? How to chooseAndrew Davidson, vice president of cloud products at MongoDB, concurred that serverless was going through a hype cycle but stressed it was still in its early days. “There’s plenty of people who haven’t even started using serverless,” Davidson said. Serverless has worked for some situations but not for others, he said. “It can be used to augment specific, targeted use cases, providing ancillary application services,” he said. MongoDB has a serverless service native to its MongoDB Atlas database service to extend the value of the database.Gartner’s Chandrasekaran believes challenges in the serverless ecosystem today include application debugging and testing, monitoring, and security. More maturity and tools are needed for this, he said. Serverless ecosystem vendors need to provide the same tooling in edge environments as in public cloud environments. AWS’s Ulander, meanwhile, expects that as serverless advances, there will be more integrations around containers or Kubernetes.", "pub_date": "2020-12-08"},
{"title": "Exploring Azure Spatial Analysis containers", "overview": "Use containers and Azure IoT Hub to bring Cognitive Services machine learning to your own servers on the edge.", "image_url": null, "url": "https://www.infoworld.com/article/3600193/exploring-azure-spatial-analysis-containers.html", "body": "Azure’s Cognitive Services are a quick and easy way to add machine learning to many different types of applications. Available as REST APIs, they can be quickly hooked into your code using simple asynchronous calls from their own dedicated SDKs and libraries. It doesn’t matter what language or platform you’re building on, as long as your code can deliver HTTP calls and parse JSON documents.Not all applications have the luxury of a low-latency connection to Azure. That’s why Microsoft is rolling out an increasing number of its Cognitive Services as containers, for use on appropriate hardware that may only have intermittent connectivity. That often requires using systems with a relatively high-end GPU, as the underlying neural nets used by the ML inferencing models require a lot of compute. Even so, with devices like Intel’s NUC9 hardware with an Nvidia Tesla-series GPU, that can be very small indeed.Also on InfoWorld: Kubernetes meets the real world: 3 success storiesPackaging Azure Cognitive ServicesAt the heart of the Cognitive Services suite are Microsoft’s computer vision models. They manage everything from object recognition and image analysis to object detection and tagging to character and handwriting recognition. They’re useful tools that can form the basis of complex applications and feed into either serverless Azure Functions or into no-code Power Apps.Microsoft has taken some of its computer vision modules and packaged them in containers for use on low-latency edge hardware or where regulations require data to be held inside your own data center. That means you can use the OCR container to capture data from pharmacies securely or use the spatial analysis container to deliver a secure and safe work environment.With businesses struggling to manage social distancing and safe working conditions during the current pandemic, tools like the Cognitive Services spatial analysis are especially important. With existing camera networks or relatively low-cost devices, such as the Azure Kinect Camera, you can build systems that can identify people and show if they are working safely: keeping away from dangerous equipment, maintaining a safe separation, or being in a well-ventilated space. All you need is an RTSP (real time streaming protocol) stream from each camera you’re using and an Azure subscription.Setting up spatial analysisGetting started with the spatial analysis container is easy enough. It’s intended for use with Azure IoT Edge, which manages container deployment, and requires a server with at least one Nvidia Tesla GPU. Microsoft recommends its own Azure Stack Edge hardware, as this now offers a T4 GPU option. Using Azure Stack Edge reduces your capital expenditure, as the hardware and software is managed from Azure and billed through an Azure subscription. For test and development, a desktop is good enough; the recommended hardware is a fairly hefty workstation-class PC with 32GB of RAM and two Tesla T4 GPUs with 16GB of GPU RAM.Any system running the container needs to have Docker with Nvidia GPU support, Nvidia’s CUDA tool and multiprocess support, and the Azure IoT Edge runtime, all on Ubuntu 18.04. Your cameras need to deliver an H.264 encoded stream over RTSP at 15fps and 1080p.The spatial analytics container is currently in preview, so you do need approval from the Cognitive Services team before you access it in Microsoft’s container registry. The approval form will ask what type of application you’re building and how it’s intended to be used. Once you get approval you can install Azure IoT Hub, either directly on an Azure Stack Edge device via the Azure CLI or after installing the Azure IoT Edge on your Linux PC. Once that’s in place, you can connect it to an Azure-hosted IoT Hub using the connect strings you can generate from the Azure command line. This will install the container on your edge hardware.It’s a lot easier to get things going on Azure Stack Edge hardware. Here the container runs in a managed Kubernetes cluster, set up as part of the Azure Stack Edge compute tools from the Azure Portal. This will host your IoT Hub Edge compute, hosting the Azure IoT Edge runtime. Again the container is deployed using an Azure CLI command, installing it as a module that can be managed from your IoT Hub instance. You will be able to see the container’s status from the Azure portal.Once deployed and installed, you can start to use the spatial analysis container in your code, connecting to it using the Azure Event Hub SDK. Local code can work directly with the default end point; if you want to use data elsewhere in your network you will need to route messages to another end point or to Azure blob storage.Using spatial analysis to detect peopleOnce the spatial analysis container is running, it’s time to start exploring its features. These take in an RTSP stream and deliver JSON documents. There are several different operations: counting the people in a zone, spotting when people cross a designated line or edge of a polygon, and tracking violations of distance rules. All the operations can be carried out on either individual frames or on live video. Like most IoT Hub services, the spatial analytics container outputs are delivered as events.You will need to configure each operation before using it, giving it a URL for the camera RTSP feed, an indication of whether you’re using live or recorded data, and a JSON definition of the line or area you want to analyze. More than one zone can be set up for each camera field of view. By setting thresholds for counts, you can define the confidence level of the model before triggering an event. Once you have the count data, you can trigger an alert if, for example, too many people are in a space. This can help businesses manage space in accordance with regulations, changing thresholds as regulations change. Lines are treated the same way as zones, triggering counts when the line is crossed.Microsoft demonstrated Azure Cognitive Services to manage site safety at its BUILD event back in 2018. It’s taken a while for these tools to arrive in preview, but it’s easy to see how you can combine spatial analytics and other computer vision services to continuously monitor and protect workspaces. A robot plasma cutter in a dynamic safety zone can quickly be shut it down if a worker enters its operation area. Or maybe you’re watching for workers on high catwalks after a frost, where you want them to avoid slipping.You can provide a set of operations that are quick to configure and easy to manage using Event Hubs and Event Grids to route events to the appropriate application. Now you’re able to build applications that can work with multiple cameras and mix local and cloud operations. You could mix object recognition with line crossing; for example, to detect if someone is too close to a gas pump while smoking a cigarette.Using spatial analysis outputsOutputs are JSON documents with both event and source information, ready for further processing and logging. It’s sensible to keep logs of detected incidents in order to refine the thresholds for detection. If a model is too sensitive, you can ramp up the detection threshold to reduce false positives. Logging events can help with any regulatory issues for compliance with safety rules or privacy requirements.By only providing count or event data, Microsoft falls clearly on the side of privacy with its spatial analysis tools. There’s no identification; just an alert that too many people are in a space or someone has crossed a line. It’s a sensible compromise that lets you build essential safety tools while ensuring that no personal information moves from your network to Microsoft’s.Taking an event-driven approach simplifies building applications. You can use Event Grid to route messages where necessary, drive serverless Functions, or trigger low- and no-code process automations. Using Event Grid as a publish and subscribe backbone increases the number of applications that can work with spatial analysis events, distributing them as needed without having to build your own messaging architecture.", "pub_date": "2020-12-08"},
{"title": "What are deepfakes? AI that deceives", "overview": "Deepfakes extend the idea of video compositing with deep learning to make someone appear to say or do something they didn’t really say or do", "image_url": null, "url": "https://www.infoworld.com/article/3574949/what-are-deepfakes-ai-that-deceives.html", "body": "Deepfakes are media — often video but sometimes audio — that were created, altered, or synthesized with the aid of deep learning to attempt to deceive some viewers or listeners into believing a false event or false message.The original example of a deepfake (by reddit user /u/deepfake) swapped the face of an actress onto the body of a porn performer in a video – which was, of course, completely unethical, although not initially illegal. Other deepfakes have changed what famous people were saying, or the language they were speaking.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesDeepfakes extend the idea of video (or movie) compositing, which has been done for decades. Significant video skills, time, and equipment go into video compositing; video deepfakes require much less skill, time (assuming you have GPUs), and equipment, although they are often unconvincing to careful observers.How to create deepfakesOriginally, deepfakes relied on autoencoders, a type of unsupervised neural network, and many still do. Some people have refined that technique using GANs (generative adversarial networks). Other machine learning methods have also been used for deepfakes, sometimes in combination with non-machine learning methods, with varying results.Essentially, autoencoders for deepfake faces in images run a two-step process. Step one is to use a neural network to extract a face from a source image and encode that into a set of features and possibly a mask, typically using several 2D convolution layers, a couple of dense layers, and a softmax layer. Step two is to use another neural network to decode the features, upscale the generated face, rotate and scale the face as needed, and apply the upscaled face to another image.Training an autoencoder for deepfake face generation requires a lot of images of the source and target faces from multiple points of view and in varied lighting conditions. Without a GPU, training can take weeks. With GPUs, it goes a lot faster.Generative adversarial networks can refine the results of autoencoders, for example, by pitting two neural networks against each other. The generative network tries to create examples that have the same statistics as the original, while the discriminative network tries to detect deviations from the original data distribution.Training GANs is a time-consuming iterative technique that greatly increases the cost in compute time over autoencoders. Currently, GANs are more appropriate for generating realistic single image frames of imaginary people (e.g. StyleGAN) than for creating deepfake videos. That could change as deep learning hardware becomes faster.How to detect deepfakesEarly in 2020, a consortium from AWS, Facebook, Microsoft, the Partnership on AI’s Media Integrity Steering Committee, and academics built the Deepfake Detection Challenge (DFDC), which ran on Kaggle for four months.The contest included two well-documented prototype solutions: an introduction, and a starter kit. The winning solution, by Selim Seferbekov, also has a fairly good writeup.The details of the solutions will make your eyes cross if you’re not into deep neural networks and image processing. Essentially, the winning solution did frame-by-frame face detection and extracted SSIM (Structural Similarity) index masks. The software extracted the detected faces plus a 30 percent margin, and used EfficientNet B7 pretrained on ImageNet for encoding (classification). The solution is now open source.Sadly, even the winning solution could only catch about two-thirds of the deepfakes in the DFDC test database.Deepfake creation and detection applicationsOne of the best open source video deepfake creation applications is currently Faceswap, which builds on the original deepfake algorithm. It took Ars Technica writer Tim Lee two weeks, using Faceswap, to create a deepfake that swapped the face of Lieutenant Commander Data (Brent Spiner) from  into a video of Mark Zuckerberg testifying before Congress. As is typical for deepfakes, the result doesn’t pass the sniff test for anyone with significant graphics sophistication. So, the state of the art for deepfakes still isn’t very good, with rare exceptions that depend more on the skill of the “artist” than the technology.That’s somewhat comforting, given that the winning DFDC detection solution isn’t very good, either. Meanwhile, Microsoft has announced, but has not released as of this writing, Microsoft Video Authenticator. Microsoft says that Video Authenticator can analyze a still photo or video to provide a percentage chance, or confidence score, that the media is artificially manipulated.Video Authenticator was tested against the DFDC dataset; Microsoft hasn’t yet reported how much better it is than Seferbekov’s winning Kaggle solution. It would be typical for an AI contest sponsor to build on and improve on the winning solutions from the contest.Facebook is also promising a deepfake detector, but plans to keep the source code closed. One problem with open-sourcing deepfake detectors such as Seferbekov’s is that deepfake generation developers can use the detector as the discriminator in a GAN to guarantee that the fake will pass that detector, eventually fueling an AI arms race between deepfake generators and deepfake detectors.On the audio front, Descript Overdub and Adobe’s demonstrated but as-yet-unreleased VoCo can make text-to-speech close to realistic. You train Overdub for about 10 minutes to create a synthetic version of your own voice; once trained, you can edit your voiceovers as text.A related technology is Google WaveNet. WaveNet-synthesized voices are more realistic than standard text-to-speech voices, although not quite at the level of natural voices, according to Google’s own testing. You’ve heard WaveNet voices if you have used voice output from Google Assistant, Google Search, or Google Translate recently.Deepfakes and non-consensual pornographyAs I mentioned earlier, the original deepfake swapped the face of an actress onto the body of a porn performer in a video. Reddit has since banned the /r/deepfake sub-Reddit that hosted that and other pornographic deepfakes, since most of the content was non-consensual pornography, which is now illegal, at least in some jurisdictions.Another sub-Reddit for -pornographic deepfakes still exists at /r/SFWdeepfakes. While the denizens of that sub-Reddit claim they’re doing good work, you’ll have to judge for yourself whether, say, seeing Joe Biden’s face badly faked into Rod Serling’s body has any value — and whether any of the deepfakes there pass the sniff test for credibility. In my opinion, some come close to selling themselves as real; most can charitably be described as crude.Banning /r/deepfake does not, of course, eliminate non-consensual pornography, which may have multiple motivations, including revenge porn, which is itself a crime in the US. Other sites that have banned non-consensual deepfakes include Gfycat, Twitter, Discord, Google, and Pornhub, and finally (after much foot-dragging) Facebook and Instagram.In California, individuals targeted by sexually explicit deepfake content made without their consent have a cause of action against the content’s creator. Also in California, the distribution of malicious deepfake audio or visual media targeting a candidate running for public office within 60 days of their election is prohibited. China requires that deepfakes be clearly labeled as such.Deepfakes in politicsMany other jurisdictions  laws against political deepfakes. That can be troubling, especially when high-quality deepfakes of political figures make it into wide distribution. Would a deepfake of Nancy Pelosi be worse than the conventionally slowed-down video of Pelosi manipulated to make it sound like she was slurring her words? It could be, if produced well. For example, see this video from CNN, which concentrates on deepfakes relevant to the 2020 presidential campaign.Deepfakes as excuses“It’s a deepfake” is also a possible excuse for politicians whose real, embarrassing videos have leaked out. That recently happened (or allegedly happened) in Malaysia when a gay sex tape was dismissed as a deepfake by the Minister of Economic Affairs, even though the other man shown in the tape swore it was real.On the flip side, the distribution of a probable amateur deepfake of the ailing President Ali Bongo of Gabon was a contributing factor to a subsequent military coup against Bongo. The deepfake video tipped off the military that something was wrong, even more than Bongo’s extended absence from the media.Also on InfoWorld: The best free data science courses during quarantineMore deepfake examplesA recent deepfake video of , the 1999 Smash Mouth classic, is an example of manipulating video (in this case, a mashup from popular movies) to fake lip synching. The creator, YouTube user ontyj, notes he “Got carried away testing out wav2lip and now this exists...” It’s amusing, although not convincing. Nevertheless, it demonstrates how much better faking lip motion has gotten. A few years ago, unnatural lip motion was usually a dead giveaway of a faked video.It could be worse. Have a look at this deepfake video of President Obama as the target and Jordan Peele as the driver. Now imagine that it didn’t include any context revealing it as fake, and included an incendiary call to action.Are you terrified yet?Deep learning vs. machine learning: Understand the differencesWhat is machine learning? Intelligence derived from dataWhat is deep learning? Algorithms that mimic the human brainMachine learning algorithms explainedAutomated machine learning or AutoML explainedSupervised learning explainedSemi-supervised learning explainedUnsupervised learning explainedReinforcement learning explainedWhat is computer vision? AI for images and videoWhat is face recognition? AI for Big BrotherWhat is natural language processing? AI for speech and textKaggle: Where data scientists learn and competeWhat is CUDA? Parallel processing for GPUs", "pub_date": "2020-09-15"},
{"title": "How to choose a data analytics platform", "overview": "A brief guide to the analytics lifecycle, the expanding array of tools and technologies, and selecting the right data platform for your needs", "image_url": null, "url": "https://www.infoworld.com/article/3564537/how-to-choose-a-data-analytics-platform.html", "body": "Whether you have responsibilities in software development, devops, systems, clouds, test automation, site reliability, leading scrum teams, infosec, or other information technology areas, you’ll have increasing opportunities and requirements to work with data, analytics, and machine learning.AnalyticsHow to choose a data analytics platform (InfoWorld)6 best practices for business data visualization (Computerworld)Healthcare analytics: 4 success stories (CIO)SD-WAN and analytics: A marriage made for the new normal (Network World)How to protect algorithms as intellectual property (CSO)Your exposure to analytics may come through IT data, such as developing metrics and insights from agile, devops, or website metrics. There’s no better way to learn the basic skills and tools around data, analytics, and machine learning than to apply them to data that you know and that you can mine for insights to drive actions.Things get a little bit more complex once you branch out of the world of IT data and provide services to data scientist teams, citizen data scientists, and other business analysts performing data visualizations, analytics, and machine learning.How data analysis, AI, and IoT will shape the post-pandemic ‘new normal’First, data has to be loaded and cleansed. Then, depending on the volume, variety, and velocity of the data, you’re likely to encounter multiple back-end databases and cloud data technologies. Lastly, over the last several years, what used to be a choice between business intelligence and data visualization tools has ballooned into a complex matrix of full-lifecycle analytics and machine learning platforms.The importance of analytics and machine learning increases IT’s responsibilities in several areas. For example:IT often provides services around all the data integrations, back-end databases, and analytics platforms.Devops teams often deploy and scale the data infrastructure to enable experimenting on machine learning models and then support production data processing.Network operations teams establish secure connections between SaaS analytics tools, multiclouds, and data centers.IT service management teams respond to data and analytics service requests and incidents.Infosec oversees data security governance and implementations.Developers integrate analytics and machine learning models into applications.Given the explosion of analytics, cloud data platforms, and machine learning capabilities, here is a primer to better understand the analytics lifecycle, from data integration and cleaning, to dataops and modelops, to the databases, data platforms, and analytics offerings themselves.Analytics begins with data integration and data cleaningBefore analysts, citizen data scientists, or data science teams can perform analytics, the required data sources must be accessible to them in their data visualization and analytics platforms.To start, there may be business requirements to integrate data from multiple enterprise systems, extract data from SaaS applications, or stream data from IoT sensors and other real-time data sources.These are all the steps to collect, load, and integrate data for analytics and machine learning. Depending on the complexity of the data and data quality issues, there are opportunities to get involved in dataops, data cataloging, master data management, and other data governance initiatives.We all know the phrase, “garbage in, garbage out.” Analysts must be concerned about the quality of their data, and data scientists must be concerned about biases in their machine learning models. Also, the timeliness of integrating new data is critical for businesses looking to become more real-time data-driven. For these reasons, the pipelines that load and process data are critically important in analytics and machine learning.Databases and data platforms for all types of data management challengesLoading and processing data is a necessary first step, but then things get more complicated when selecting optimal databases. Today’s choices include enterprise data warehouses, data lakes, big data processing platforms, and specialized NoSQL, graph, key-value, document, and columnar databases. To support large-scale data warehousing and analytics, there are platforms like Snowflake, Redshift, BigQuery, Vertica, and Greenplum. Lastly, there are the big data platforms, including Spark and Hadoop.Large enterprises are likely to have multiple data repositories and to use cloud data platforms like Cloudera Data Platform or MapR Data Platform, or data orchestration platforms like InfoWorks DataFoundy, to make all of those repositories accessible for analytics.The major public clouds, including AWS, GCP, and Azure, all have data management platforms and services to sift through. For example, Azure Synapse Analytics is Microsoft’s SQL data warehouse in the cloud, while Azure Cosmos DB provides interfaces to many NoSQL data stores, including Cassandra (columnar data), MongoDB (key-value and document data), and Gremlin (graph data).Data lakes are popular loading docks to centralize unstructured data for quick analysis, and one can pick from Azure Data Lake, Amazon S3, or Google Cloud Storage to serve that purpose. For processing big data, the AWS, GCP, and Azure clouds all have Spark and Hadoop offerings as well.Analytics platforms target machine learning and collaborationWith data loaded, cleansed, and stored, data scientists and analysts can begin performing analytics and machine learning. Organizations have many options depending on the types of analytics, the skills of the analytics team performing the work, and the structure of the underlying data.Analytics can be performed in self-service data visualization tools such as Tableau and Microsoft Power BI. Both of these tools target citizen data scientists and expose visualizations, calculations, and basic analytics. These tools support basic data integration and data restructuring, but more complex data wrangling often happens before the analytics steps. Tableau Data Prep and Azure Data Factory are the companion tools to help integrate and transform data.Analytics teams that want to automate more than just data integration and prep can look to platforms like Alteryx Analytics Process Automation. This end-to-end, collaborative platform connects developers, analysts, citizen data scientists, and data scientists with workflow automation and self-service data processing, analytics, and machine learning processing capabilities.Alan Jacobson, chief analytics and data officer at Alteryx, explains, “The emergence of analytic process automation (APA) as a category underscores a new expectation for every worker in an organization to be a data worker. IT developers are no exception, and the extensibility of the Alteryx APA Platform is especially useful for these knowledge workers.”There are several tools and platforms targeting data scientists that aim to make them more productive with technologies like Python and R while simplifying many of the operational and infrastructure steps. For example, Databricks is a data science operational platform that enables deploying algorithms to Apache Spark and TensorFlow, while self-managing the computing clusters on the AWS or Azure cloud. Now some platforms like SAS Viya combine data preparation, analytics, forecasting, machine learning, text analytics, and machine learning model management into a single modelops platform. SAS is operationalizing analytics and targets data scientists, business analysts, developers, and executives with an end-to-end collaborative platform.David Duling, director of decision management research and development at SAS, says, “We see modelops as the practice of creating a repeatable, auditable pipeline of operations for deploying all analytics, including AI and ML models, into operational systems. As part of modelops, we can use modern devops practices for code management, testing, and monitoring. This helps improve the frequency and reliability of model deployment, which in turn enhances the agility of business processes built on these models.​”Dataiku is another platform that strives to bring data prep, analytics, and machine learning to growing data science teams and their collaborators. Dataiku has a visual programming model to enable collaboration and code notebooks for more advanced SQL and Python developers.Other analytics and machine learning platforms from leading enterprise software vendors aim to bring analytics capabilities to data center and cloud data sources. For example, Oracle Analytics Cloud and SAP Analytics Cloud both aim to centralize intelligence and automate insights to enable end-to-end decisions.Choosing a data analytics platformSelecting data integration, warehousing, and analytics tools used to be more straightforward before the rise of big data, machine learning, and data governance. Today, there’s a blending of terminology, platform capabilities, operational requirements, governance needs, and targeted user personas that make selecting platforms more complex, especially since many vendors support multiple usage paradigms. Businesses differ in analytics requirements and needs but should seek new platforms from the vantage point of what is already in place. For example:Companies that have had success with citizen data science programs and that already have data visualization tools in place may want to extend this program with analytics process automation or data prep technologies.Enterprises that want a toolchain that enables data scientists working in different parts of the business may consider end-to-end analytics platforms with modelops capabilities.Organizations with multiple, disparate back-end data platforms may benefit from cloud data platforms to catalog and centrally manage them.Companies standardizing all or most data capabilities on a single public cloud vendor ought to investigate the data integration, data management, and data analytics platforms offered.With analytics and machine learning becoming an important core competency, technologists should consider deepening their understanding of the available platforms and their capabilities. The power and value of analytics platforms will only increase, as will their influence throughout the enterprise. ", "pub_date": "2020-07-13"},
{"title": "Deeplearning4j: Deep learning and ETL for the JVM", "overview": "Aimed at integrating models with Java applications, Deeplearning4j offers a stack of components for building JVM-based applications that incorporate AI", "image_url": null, "url": "https://www.infoworld.com/article/3567055/deeplearning4j-deep-learning-and-etl-for-the-jvm.html", "body": "Eclipse Deeplearning4j is an open source, distributed, deep learning library for the JVM. Deeplearning4j is written in Java and is compatible with any JVM language, such as Scala, Clojure, or Kotlin. The underlying computations are written in C, C++, and Cuda. Keras will serve as the Python API. Integrated with Hadoop and Apache Spark, Deeplearning4j brings AI to business environments for use on distributed GPUs and CPUs.Deeplearning4j is actually a stack of projects intended to support all the needs of a JVM-based deep learning application. Beyond Deeplearning4j itself (the high-level API), it includes ND4J (general-purpose linear algebra,), SameDiff (graph-based automatic differentiation), DataVec (ETL), Arbiter (hyperparameter search), and the C++ LibND4J (underpins all of the above). LibND4J in turns calls on standard libraries for CPU and GPU support, such as OpenBLAS, OneDNN (MKL-DNN), cuDNN, and cuBLAS.Also on InfoWorld: JDK 15: The new features in Java 15The goal of Eclipse Deeplearning4j is to provide a core set of components for building  that incorporate AI. AI products within an enterprise often have a wider scope than just machine learning. The overall goal of the distribution is to provide smart defaults for building deep learning applications.Deeplearning4j competes, at some level, with every other deep learning framework. The most comparable project in scope is TensorFlow, which is the leading end-to-end deep learning framework for production. TensorFlow currently has interfaces for  Python, C++, and Java (experimental), and a separate implementation for JavaScript. TensorFlow uses two ways of training: graph-based and immediate mode (eager execution). Deeplearning4j currently only supports graph-based execution.PyTorch, probably the leading deep learning framework for research, only supports immediate mode; it has interfaces for Python, C++, and Java. H2O Sparkling Water integrates the H2O open source, distributed in-memory machine learning platform with Spark. H2O has interfaces for Java and Scala, Python, R, and H2O Flow notebooks.Commercial support for Deeplearning4j can be purchased from Konduit, which also supports many of the developers working on the project.How Deeplearning4j worksDeeplearning4j treats the tasks of loading data and training algorithms as separate processes. You load and transform the data using the DataVec library, and train models using tensors and the ND4J library.You ingest data through a  interface, and walk through the data using a . You can choose a  class to use as a preprocessor for your . Use the  for image data, the  if you have a uniform range along all dimensions of your input data, and  for most other cases. If necessary, you can implement a custom  class. objects are containers for the features and labels of your data, and keep the values in several instances of : one for the features of your examples, one for the labels, and two additional ones for masking, if you are using time series data. In the case of the features, the  is a tensor of the size . Typically you’ll divide the data into mini-batches for training; the number of examples in an  is small enough to fit in memory but large enough to get a good gradient.If you look at the Deeplearning4j code for defining models, such as the Java example below, you’ll see that it’s a very high-level API, similar to Keras. In fact, the planned Python interface to Deeplearning4j will use Keras; right now, if you have a Keras model, you can import it into Deeplearning4j.The  class is the simplest network configuration API available in Eclipse Deeplearning4j; for DAG structures, use the  instead. Note that the optimization algorithm (SGD in this example) is specified separately from the updater (Nesterov in this example). This very simple neural network has one dense layer with a  activation function and one output layer with  loss and a  activation function, and is solved by back propagation. More complex networks may also have , , , and others of the two dozen supported layer types and sixteen layer space types.The simplest way to train the model is to call the  method on the model configuration with your  as an argument. You can also reset the iterator and call the  method for as many epochs as you need, or use an .To test model performance, use an  class to see how well the trained model fits your test data, which should not be the same as the training data.Deeplearning4j provides a listener facility help you monitor your network’s performance visually, which will be called after each mini-batch is processed. One of most often used listeners is .Installing and testing Deeplearning4jAt the moment, the easiest way to try out Deeplearning4j is by using the official quick start. It requires a relatively recent version of Java, an installation of Maven, a working Git, and a copy of IntelliJ IDEA (preferred) or Eclipse. There are also a few user-contributed quick starts. Start by cloning the eclipse/deeplearning4j-examples repo to your own machine with Git or GitHub Desktop. Then install the projects with Maven from the dl4j-examples folder.Once the installation is complete, open the dl4j-examples/ directory with IntelliJ IDEA and try running some of the examples.The README under dl4j-examples lists all the examples and briefly describes them. By the way, you can use the IntelliJ IDEA preferences to install a new version of the JDK and apply it to the project.The well-known Iris dataset has just 150 samples and is generally easy to model, although a few of the irises are often misclassified. The model used here is a three-layer dense neural network.Running the Iris classifier shown in the previous figure yields a fairly good fit: accuracy, precision, recall, and F1 score are all ~98%. Note in the confusion matrix that only one of the test cases was misclassified.The Linear Classifier demo runs in a few seconds and generates probability plots for the training and test datasets. The data was generated specifically to be linearly separable into two classes.A multi-layer perceptron (MLP) classification model for the MNIST hand-written digit dataset yields accuracy, precision, recall, and F1 score all ~97% after about 14K iterations. That isn’t as good or as fast as the results of convolutional neural networks (such as LeNet) on this dataset.", "pub_date": "2020-07-20"},
{"title": "Is the carbon footprint of AI too big?", "overview": "Artificial intelligence obviously requires electricity and computers to work its practical magic. But don’t ignore the downstream environmental benefits", "image_url": null, "url": "https://www.infoworld.com/article/3568680/is-the-carbon-footprint-of-ai-too-big.html", "body": "It’s no surprise that AI has a carbon footprint, which refers to the amount of greenhouse gases (carbon dioxide and methane, primarily) that producing and consuming AI releases into the atmosphere. In fact, training AI models requires so much computing power, some researchers have argued that the environmental costs outweigh the benefits. However, I believe they’ve not only underestimated the benefits of AI, but also overlooked the many ways that model training is becoming more efficient. Greenhouse gases are what economists refer to as an “externality” — a cost borne inadvertently by society at large, such as through the adverse impact of global warming, but inflicted on us all by private participants who have little incentive to refrain from the offending activity. Typically, public utilities emit these gases when they burn fossil fuels in order to generate electricity that powers the data centers, server farms, and other computing platforms upon which AI runs.How data analysis, AI, and IoT will shape the post-pandemic ‘new normal’Consider the downstream carbon offsets realized by AI appsDuring the past few years, AI has been unfairly stigmatized as a major contributor to global warming, owing to what some observers regard as its inordinate consumption of energy in the process of model training.Unfortunately, many AI industry observers contribute to this stigma by using an imbalanced formula for calculating AI’s overall carbon footprint. For example, MIT Technology Review published an article a year ago in which University of Massachusetts researchers reported that the energy needed to train a single machine learning model could emit carbon dioxide at nearly five times the lifetime emissions of the average American car.This manner of calculating AI’s carbon footprint does the technology a huge disservice. At the risk of sounding pretentious, this discussion suggests Oscar Wilde’s remark about a cynic being someone who “knows the price of everything and the value of nothing.” I’m not taking issue with the UMass researchers’ finding on the carbon cost of AI training, or with the need to calculate and reduce that cost for this and other human activities. I am curious why the researchers didn’t also discuss the value that AI provides downstream, often indirectly, in reducing human-generated greenhouse gases from the environment.If an AI model delivers a steady stream of genuinely actionable inferences over an application’s life, it should generate beneficial, real-world outcomes. In other words, many AI apps ensure that people and systems take optimal actions in myriad application scenarios. Many of these AI-driven benefits may be carbon-offsetting, such as reducing the need for people to get in their cars, take business trips, occupy expensive office space, and otherwise engage in activities that consume fossil fuels.Perhaps a quick “traveling salesman” thought experiment is in order. Let’s say that a manufacturing company has a national sales force of six people, and each has a company-provided car. If the company implements a new AI-based sales force automation system that enables one of those individuals to do the work of the entire team—such as through improved lead prospecting and route optimization—that organization could conceivably dismiss the other five individuals, scrap their company cars, and close their respective branch offices.So, in one fell swoop, the five-car carbon footprint of the AI model at the heart of the sales force automation app would be entirely offset (and then some) by eliminating the greenhouse gases of exactly five cars, as well as the electricity savings from closing those offices and associated equipment.We might quibble over the feasibility of this particular example, but we must admit that it’s entirely plausible. This thought experiment highlights the fact that AI’s productivity, efficiency, and acceleration benefits often produce downstream efficiencies in energy utilization.I’m not going to argue that every AI application—or even most of them—has a substantial downstream impact on reducing carbon emissions. But I do take issue with observers, such as an AI expert quoted in this recent Wall Street Journal article, who trivialize the productivity impact from AI with statements such as: “If people could see the true cost of these systems, I think we’d have a lot of harder questions about whether that convenience [of AI-based digital assistants, for example] is worth the planetary cost.”Sentiments such as these obscure the fact that (to use digital assistants as an example) many real-world AI use cases deliver “convenience” in the form of data-driven recommendations that help people buy the right product, take the optimal route to their destination, follow the best practice in managing their finances, and so on. Many of these actionable recommendations may have an impact—large or small—on the energy that people use in their homes, offices, cars, and elsewhere.Upstream AI training may drive greater downstream carbon offsetsMany AI apps have the potential to generate downstream carbon offsets that counterbalance the emissions associated with electricity needed to train the underlying models. If AI lets us do more work with only a fraction of the office space, meetings, and travel, the technology will be contributing mightily to the battle against global warming.Consequently, achieving carbon-neutrality in AI apps may very well depend on intensively training the underlying models to be more effective at their assigned tasks. Equivalent to a capital investment, a well-trained AI model may be amortized over time through deployment into future applications.Bear in mind that even as AI developers seek to improve their models’ accuracy, training is not necessarily the resource hog we’ve been led to believe. Consider the following trends that are reducing the carbon footprint of this and other AI pipeline workloads:AI server farms powered by renewable energy sources instead of fossil fuelsMore energy-efficient chipsets, servers, and cloud providers in AI platformsLess time and data needed to train AI modelsGreater adoption of pretrained AI models in real-world appsComparisons of the energy efficiency of different models within the AI devops pipelineDevelopment of AI on “once-for-all” neural networks that can be trained to run with maximum efficiency on many different kinds of processorsAs these trends converge during the next several years, we’re likely to see dramatic drops in the carbon footprint associated with AI training. As that trend intensifies, AI pipelines will become the most environmentally sustainable platforms in the IT universe.", "pub_date": "2020-08-06"},
{"title": "How to choose a cloud machine learning platform", "overview": "12 capabilities every cloud machine learning platform should provide to support the complete machine learning lifecycle", "image_url": null, "url": "https://www.infoworld.com/article/3568889/how-to-choose-a-cloud-machine-learning-platform.html", "body": "In order to create effective machine learning and deep learning models, you need copious amounts of data, a way to clean the data and perform feature engineering on it, and a way to train models on your data in a reasonable amount of time. Then you need a way to deploy your models, monitor them for drift over time, and retrain them as needed.You can do all of that on-premises if you have invested in compute resources and accelerators such as GPUs, but you may find that if your resources are adequate, they are also idle much of the time. On the other hand, it can sometimes be more cost-effective to run the entire pipeline in the cloud, using large amounts of compute resources and accelerators as needed, and then releasing them.AI and machine learning5 machine learning success stories: An inside look (CIO)AI at work: Your next co-worker could be an algorithm (Computerworld)How secure are your AI and machine learning projects? (CSO)How to choose a cloud machine learning platform (InfoWorld)How AI can create self-driving data centers (Network World)The major cloud providers — and a number of minor clouds too — have put significant effort into building out their machine learning platforms to support the complete machine learning lifecycle, from planning a project to maintaining a model in production. How do you determine which of these clouds will meet your needs? Here are 12 capabilities every end-to-end machine learning platform should provide. Be close to your dataIf you have the large amounts of data needed to build precise models, you don’t want to ship it halfway around the world. The issue here isn’t distance, however, it’s time: Data transmission speed is ultimately limited by the speed of light, even on a perfect network with infinite bandwidth. Long distances mean latency. The ideal case for very large data sets is to build the model where the data already resides, so that no mass data transmission is needed. Several databases support that to a limited extent.The next best case is for the data to be on the same high-speed network as the model-building software, which typically means within the same data center. Even moving the data from one data center to another within a cloud availability zone can introduce a significant delay if you have terabytes (TB) or more. You can mitigate this by doing incremental updates.The worst case would be if you have to move big data long distances over paths with constrained bandwidth and high latency. The trans-Pacific cables going to Australia are particularly egregious in this respect.Support an ETL or ELT pipelineETL (export, transform, and load) and ELT (export, load, and transform) are two data pipeline configurations that are common in the database world. Machine learning and deep learning amplify the need for these, especially the transform portion. ELT gives you more flexibility when your transformations need to change, as the load phase is usually the most time-consuming for big data.In general, data in the wild is noisy. That needs to be filtered. Additionally, data in the wild has varying ranges: One variable might have a maximum in the millions, while another might have a range of -0.1 to -0.001. For machine learning, variables must be transformed to standardized ranges to keep the ones with large ranges from dominating the model. Exactly which standardized range depends on the algorithm used for the model.Support an online environment for model buildingThe conventional wisdom used to be that you should import your data to your desktop for model building. The sheer quantity of data needed to build good machine learning and deep learning models changes the picture: You can download a small sample of data to your desktop for exploratory data analysis and model building, but for production models you need to have access to the full data.Web-based development environments such as Jupyter Notebooks, JupyterLab, and Apache Zeppelin are well suited for model building. If your data is in the same cloud as the notebook environment, you can bring the analysis to the data, minimizing the time-consuming movement of data.Support scale-up and scale-out trainingThe compute and memory requirements of notebooks are generally minimal, except for training models. It helps a lot if a notebook can spawn training jobs that run on multiple large virtual machines or containers. It also helps a lot if the training can access accelerators such as GPUs, TPUs, and FPGAs; these can turn days of training into hours.Support AutoML and automatic feature engineeringNot everyone is good at picking machine learning models, selecting features (the variables that are used by the model), and engineering new features from the raw observations. Even if you’re good at those tasks, they are time-consuming and can be automated to a large extent.AutoML systems often try many models to see which result in the best objective function values, for example the minimum squared error for regression problems. The best AutoML systems can also perform feature engineering, and use their resources effectively to pursue the best possible models with the best possible sets of features.Support the best machine learning and deep learning frameworksMost data scientists have favorite frameworks and programming languages for machine learning and deep learning. For those who prefer Python, Scikit-learn is often a favorite for machine learning, while TensorFlow, PyTorch, Keras, and MXNet are often top picks for deep learning. In Scala, Spark MLlib tends to be preferred for machine learning. In R, there are many native machine learning packages, and a good interface to Python. In Java, H2O.ai rates highly, as do Java-ML and Deep Java Library.The cloud machine learning and deep learning platforms tend to have their own collection of algorithms, and they often support external frameworks in at least one language or as containers with specific entry points. In some cases you can integrate your own algorithms and statistical methods with the platform’s AutoML facilities, which is quite convenient.Some cloud platforms also offer their own tuned versions of major deep learning frameworks. For example, AWS has an optimized version of TensorFlow that it claims can achieve nearly-linear scalability for deep neural network training.Offer pre-trained models and support transfer learningNot everyone wants to spend the time and compute resources to train their own models — nor should they, when pre-trained models are available. For example, the ImageNet dataset is huge, and training a state-of-the-art deep neural network against it can take weeks, so it makes sense to use a pre-trained model for it when you can.On the other hand, pre-trained models may not always identify the objects you care about. Transfer learning can help you customize the last few layers of the neural network for your specific data set without the time and expense of training the full network.Offer tuned AI servicesThe major cloud platforms offer robust, tuned AI services for many applications, not just image identification. Example include language translation, speech to text, text to speech, forecasting, and recommendations.These services have already been trained and tested on more data than is usually available to businesses. They are also already deployed on service endpoints with enough computational resources, including accelerators, to ensure good response times under worldwide load.Manage your experimentsThe only way to find the best model for your data set is to try everything, whether manually or using AutoML. That leaves another problem: Managing your experiments.A good cloud machine learning platform will have a way that you can see and compare the objective function values of each experiment for both the training sets and the test data, as well as the size of the model and the confusion matrix. Being able to graph all of that is a definite plus.Support model deployment for predictionOnce you have a way of picking the best experiment given your criteria, you also need an easy way to deploy the model. If you deploy multiple models for the same purpose, you’ll also need a way to apportion traffic among them for a/b testing.Monitor prediction performanceUnfortunately, the world tends to change, and data changes with it. That means you can’t deploy a model and forget it. Instead, you need to monitor the data submitted for predictions over time. When the data starts changing significantly from the baseline of your original training data set, you’ll need to retrain your model.Also on InfoWorld: Applying devops in data science and machine learningControl costsFinally, you need ways to control the costs incurred by your models. Deploying models for production inference often accounts for 90% of the cost of deep learning, while the training accounts for only 10% of the cost.The best way to control prediction costs depends on your load and the complexity of your model. If you have a high load, you might be able to use an accelerator to avoid adding more virtual machine instances. If you have a variable load, you might be able to dynamically change your size or number of instances or containers as the load goes up or down. And if you have a low or occasional load, you might be able to use a very small instance with a partial accelerator to handle the predictions.", "pub_date": "2020-08-10"},
{"title": "MLops: The rise of machine learning operations", "overview": "Once machine learning models make it to production, they still need updates and monitoring for drift. A team to manage ML operations makes good business sense ", "image_url": null, "url": "https://www.infoworld.com/article/3570716/mlops-the-rise-of-machine-learning-operations.html", "body": "As hard as it is for data scientists to tag data and develop accurate machine learning models, managing models in production can be even more daunting. Recognizing model drift, retraining models with updating data sets, improving performance, and maintaining the underlying technology platforms are all important data science practices. Without these disciplines, models can produce erroneous results that significantly impact business.Developing production-ready models is no easy feat. According to one machine learning study, 55 percent of companies had not deployed models into production, and 40 percent or more require more than 30 days to deploy one model. Success brings new challenges, and 41 percent of respondents acknowledge the difficulty of versioning machine learning models and reproducibility.How data analysis, AI, and IoT will shape the post-pandemic ‘new normal’The lesson here is that new obstacles emerge once machine learning models are deployed to production and used in business processes.Model management and operations were once challenges for the more advanced data science teams. Now tasks include monitoring production machine learning models for drift, automating the retraining of models, alerting when the drift is significant, and recognizing when models require upgrades. As more organizations invest in machine learning, there is a greater need to build awareness around model management and operations.The good news is platforms and libraries such as open source MLFlow and DVC, and commercial tools from Alteryx, Databricks, Dataiku, SAS, DataRobot, ModelOp, and others are making model management and operations easier for data science teams. The public cloud providers are also sharing practices such as implementing MLops with Azure Machine Learning.There are several similarities between model management and devops. Many refer to model management and operations as MLops and define it as the culture, practices, and technologies required to develop and maintain machine learning models.Understanding model management and operationsTo better understand model management and operations, consider the union of software development practices with scientific methods.As a software developer, you know that completing the version of an application and deploying it to production isn’t trivial. But an even greater challenge begins once the application reaches production. End-users expect regular enhancements, and the underlying infrastructure, platforms, and libraries require patching and maintenance.Now let’s shift to the scientific world where questions lead to multiple hypotheses and repetitive experimentation. You learned in science class to maintain a log of these experiments and track the journey of tweaking different variables from one experiment to the next. Experimentation leads to improved results, and documenting the journey helps convince peers that you’ve explored all the variables and that results are reproducible.Data scientists experimenting with machine learning models must incorporate disciplines from both software development and scientific research. Machine learning models are software code developed in languages such as Python and R, constructed with TensorFlow, PyTorch, or other machine learning libraries, run on platforms such as Apache Spark, and deployed to cloud infrastructure. The development and support of machine learning models require significant experimentation and optimization, and data scientists must prove the accuracy of their models.Like software development, machine learning models need ongoing maintenance and enhancements. Some of that comes from maintaining the code, libraries, platforms, and infrastructure, but data scientists must also be concerned about model drift. In simple terms, model drift occurs as new data becomes available, and the predictions, clusters, segmentations, and recommendations provided by machine learning models deviate from expected outcomes.Successful model management starts with developing optimal modelsI spoke with Alan Jacobson, chief data and analytics officer at Alteryx, about how organizations succeed and scale machine learning model development. “To simplify model development, the first challenge for most data scientists is ensuring strong problem formulation. Many complex business problems can be solved with very simple analytics, but this first requires structuring the problem in a way that data and analytics can help answer the question. Even when complex models are leveraged, the most difficult part of the process is typically structuring the data and ensuring the right inputs are being used are at the right quality levels.”I agree with Jacobson. Too many data and technology implementations start with poor or no problem statements and with inadequate time, tools, and subject matter expertise to ensure adequate data quality. Organizations must first start with asking smart questions about big data, investing in dataops, and then using agile methodologies in data science to iterate toward solutions.Monitoring machine learning models for model driftGetting a precise problem definition is critical for ongoing management and monitoring of models in production. Jacobson went on to explain, “Monitoring models is an important process, but doing it right takes a strong understanding of the goals and potential adverse effects that warrant watching. While most discuss monitoring model performance and change over time, what’s more important and challenging in this space is the analysis of unintended consequences.”One easy way to understand model drift and unintended consequences is to consider the impact of COVID-19 on machine learning models developed with training data from before the pandemic. Machine learning models based on human behaviors, natural language processing, consumer demand models, or fraud patterns have all been affected by changing behaviors during the pandemic that are messing with AI models.Technology providers are releasing new MLops capabilities as more organizations are getting value and maturing their data science programs. For example, SAS introduced a feature contribution index that helps data scientists evaluate models without a target variable. Cloudera recently announced an ML Monitoring Service that captures technical performance metrics and tracking model predictions.MLops also addresses automation and collaborationIn between developing a machine learning model and monitoring it in production are additional tools, processes, collaborations, and capabilities that enable data science practices to scale. Some of the automation and infrastructure practices are analogous to devops and include infrastructure as code and CI/CD (continuous integration/continuous deployment) for machine learning models. Others include developer capabilities such as versioning models with their underlying training data and searching the model repository.Also on InfoWorld: Applying devops in data science and machine learningThe more interesting aspects of MLops bring scientific methodology and collaboration to data science teams. For example, DataRobot enables a champion-challenger model that can run multiple experimental models in parallel to challenge the production version’s accuracy. SAS wants to help data scientists improve speed to markets and data quality. Alteryx recently introduced Analytics Hub to help collaboration and sharing between data science teams.All this shows that managing and scaling machine learning requires a lot more discipline and practice than simply asking a data scientist to code and test a random forest, k-means, or convolutional neural network in Python.", "pub_date": "2020-08-13"},
{"title": "Cloudops tool integration is more important than the tools themselves ", "overview": "Focus on the features and functions of cloud operations and monitoring tools and you could miss the much larger advantage", "image_url": null, "url": "https://www.infoworld.com/article/3570440/cloudops-tool-integration-is-more-important-than-the-tools-themselves.html", "body": "It’s 3:00 on a Tuesday, and your AIops tool messages that the corporate network is reaching a saturation point. It seems that one of the virtual cloud servers is spinning off a massive number of packets, hijacked by a rogue piece of software placed by a hacker the night before.You wish that the security operations tool would have picked up on this, but it was the general-purpose management and monitoring tool that saw the network traffic spiking out of threshold and sounded the alarm that drew attention to the breach. The offending server is quickly taken down; all is right with the world again. However, this could have gone much better.Also on InfoWorld: Application monitoring: What devops can do betterWhat’s missing is direct integration between the AIops tool and the security tool. Although they have different missions, they need each other. The security tool needs visibility into the behavior of all applications and infrastructure, considering that behaviors that are out of line with normal operations can often be tracked to security issues, such as DDoS attacks.  At the same time, the cloudops tool could play some role in automatically defending the cloud-based systems, such as attempting a restart or taking other corrective action so the issue does not result in an outage. The recovery could be reported back to the security tool, which would take further action, such as blocking the IP address that is the source of the DDoS attack.This example describes security and ops tools working together, but there is much value in other tool integration as well. Configuration management, testing, special-purpose monitoring such as edge computing and IoT, data governance, etc., can all benefit from working together to create common automation between tools. The smarter cloud management and monitoring players, especially those selling AIops tools, have largely gotten the tool integration religion. They are able to work and play well with other cloud tools to move towards a 1+1=3 kind of value driver. It’s the number one thing I look for these days, beyond the feature and function of each tool, but it’s still not high on the radar of most of the enterprises selecting cloud tools for the first time.  The reality is no magical tool does it all. They all have limited missions, which is a good thing, considering that they can be good at a few things versus being bad at many things. The strategy is to select the best-of-breed tools for each function (security, performance management, network management, application monitoring, data governance) and have those tools provide a common integration layer where events and information can be shared in a peer-to-peer method.  If this seems much more complex than just picking a single-purpose tool for a single purpose and not having to think about how they work together, you’re right. But, if this stuff were easy, it would not bring the value of cloud computing that enterprises have been promised.", "pub_date": "2020-08-18"},
{"title": "What is computer vision? AI for images and video", "overview": "Computer vision systems are not only good enough to be useful, but in some cases more accurate than human vision", "image_url": null, "url": "https://www.infoworld.com/article/3572553/what-is-computer-vision-ai-for-images-and-video.html", "body": "Computer vision identifies and often locates objects in digital images and videos. Since living organisms process images with their visual cortex, many researchers have taken the architecture of the mammalian visual cortex as a model for neural networks designed to perform image recognition. The biological research goes back to the 1950s.The progress in computer vision over the last 20 years has been absolutely remarkable. While not yet perfect, some computer vision systems achieve 99% accuracy, and others run decently on mobile devices.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesThe breakthrough in the neural network field for vision was Yann LeCun’s 1998 LeNet-5, a seven-level  for recognition of handwritten digits digitized in 32x32 pixel images. To analyze higher-resolution images, the LeNet-5 network would need to be expanded to more neurons and more layers.Today’s best image classification models can identify diverse catalogs of objects at HD resolution in color. In addition to pure deep neural networks (DNNs), people sometimes use hybrid vision models, which combine deep learning with classical machine-learning algorithms that perform specific sub-tasks.Other vision problems besides basic image classification have been solved with deep learning, including image classification with localization, object detection, object segmentation, image style transfer, image colorization, image reconstruction, image super-resolution, and image synthesis.How does computer vision work?Computer vision algorithms usually rely on convolutional neural networks, or CNNs. CNNs typically use convolutional, pooling, ReLU, fully connected, and loss layers to simulate a visual cortex.The convolutional layer basically takes the integrals of many small overlapping regions. The pooling layer performs a form of non-linear down-sampling. ReLU layers apply the non-saturating activation function .In a fully connected layer, the neurons have connections to all activations in the previous layer. A loss layer computes how the network training penalizes the deviation between the predicted and true labels, using a Softmax or cross-entropy loss for classification.Computer vision training datasetsThere are many public image datasets that are useful for training vision models. The simplest, and one of the oldest, is MNIST, which contains 70,000 handwritten digits in 10 classes, 60K for training and 10K for testing. MNIST is an easy dataset to model, even using a laptop with no acceleration hardware. CIFAR-10 and Fashion-MNIST are similar 10-class datasets. SVHN (street view house numbers) is a set of 600K images of real-world house numbers extracted from Google Street View.COCO is a larger-scale dataset for object detection, segmentation, and captioning, with 330K images in 80 object categories. ImageNet contains about 1.5 million images with bounding boxes and labels, illustrating about 100K phrases from WordNet. Open Images contains about nine million URLs to images, with about 5K labels.Google, Azure, and AWS all have their own vision models trained against very large image databases. You can use these as is, or run transfer learning to adapt these models to your own image datasets. You can also perform transfer learning using models based on ImageNet and Open Images. The advantages of transfer learning over building a model from scratch are that it is much faster (hours rather than weeks) and that it gives you a more accurate model. You’ll still need 1,000 images per label for the best results, although you can sometimes get away with as few as 10 images per label.Computer vision applicationsWhile computer vision isn’t perfect, it’s often good enough to be practical. A good example is vision in self-driving automobiles.Waymo, formerly the Google self-driving car project, claims tests on seven million miles of public roads and the ability to navigate safely in daily traffic. There has been at least one accident involving a Waymo van; the software was not believed to be at fault, according to police.Also on InfoWorld: The best free data science courses during quarantineTesla has three models of self-driving car. In 2018 a Tesla SUV in self-driving mode was involved in a fatal accident. The report on the accident said that the driver (who was killed) had his hands off the steering wheel despite multiple warnings from the console, and that neither the driver nor the software tried to brake to avoid hitting the concrete barrier. The software has since been upgraded to require rather than suggest that the driver’s hands be on the steering wheel.Amazon Go stores are checkout-free self-service retail stores where the in-store computer vision system detects when shoppers pick up or return stock items; shoppers are identified by and charged through an Android or iPhone app. When the Amazon Go software misses an item, the shopper can keep it for free; when the software falsely registers an item taken, the shopper can flag the item and get a refund for that charge.In healthcare, there are vision applications for classifying certain features in pathology slides, chest x-rays, and other medical imaging systems. A few of these have demonstrated value when compared to skilled human practitioners, some enough for regulatory approval. There’s also a real-time system for estimating patient blood loss in an operating or delivery room.There are useful vision applications for agriculture (agricultural robots, crop and soil monitoring, and predictive analytics), banking (fraud detection, document authentication, and remote deposits), and industrial monitoring (remote wells, site security, and work activity).There are also applications of computer vision that are controversial or even deprecated. One is face recognition, which when used by government can be an invasion of privacy, and which often has a training bias that tends to misidentify non-white faces. Another is deepfake generation, which is more than a little creepy when used for pornography or the creation of hoaxes and other fraudulent images.Computer vision frameworks and modelsMost deep learning frameworks have substantial support for computer vision, including Python-based frameworks TensorFlow (the leading choice for production), PyTorch (the leading choice for academic research), and MXNet (Amazon’s framework of choice). OpenCV is a specialized library for computer vision that leans toward real-time vision applications and takes advantage of MMX and SSE instructions when they are available; it also has support for acceleration using CUDA, OpenCL, OpenGL, and Vulkan.Amazon Rekognition is an image and video analysis service that can identify objects, people, text, scenes, and activities, including facial analysis and custom labels. The Google Cloud Vision API is a pretrained image analysis service that can detect objects and faces, read printed and handwritten text, and build metadata into your image catalog. Google AutoML Vision allows you to train custom image models. Both Amazon Rekognition Custom Labels and Google AutoML Vision perform transfer learning.The Microsoft Computer Vision API can identify objects from a catalog of 10,000, with labels in 25 languages. It also returns bounding boxes for identified objects. The Azure Face API does face detection that perceives faces and attributes in an image, person identification that matches an individual in your private repository of up to one million people, and perceived emotion recognition. The Face API can run in the cloud or on the edge in containers.IBM Watson Visual Recognition can classify images from a pre-trained model, allow you to train custom image models with transfer learning, perform object detection with object counting, and train for visual inspection. Watson Visual Recognition can run in the cloud, or on iOS devices using Core ML.The data analysis package Matlab can perform image recognition using machine learning and deep learning. It has an optional Computer Vision Toolbox and can integrate with OpenCV.Computer vision models have come a long way since LeNet-5, and they are mostly CNNs. Examples include AlexNet (2012), VGG16/OxfordNet (2014), GoogLeNet/InceptionV1 (2014), Resnet50 (2015), InceptionV3 (2016), and MobileNet (2017-2018). The MobileNet family of vision neural networks was designed with mobile devices in mind.The Apple Vision framework performs face and face landmark detection, text detection, barcode recognition, image registration, and general feature tracking. Vision also allows the use of custom Core ML models for tasks like classification or object detection. It runs on iOS and macOS. The Google ML Kit SDK has similar capabilities, and runs on Android and iOS devices. ML Kit additionally supports natural language APIs.As we’ve seen, computer vision systems have become good enough to be useful, and in some cases more accurate than human vision. Using transfer learning, customization of vision models has become practical for mere mortals: computer vision is no longer the exclusive domain of Ph.D.-level researchers.Deep learning vs. machine learning: Understand the differencesWhat is machine learning? Intelligence derived from dataWhat is deep learning? Algorithms that mimic the human brainMachine learning algorithms explainedWhat is natural language processing? AI for speech and textAutomated machine learning or AutoML explainedSupervised learning explainedSemi-supervised learning explainedUnsupervised learning explainedReinforcement learning explainedKaggle: Where data scientists learn and competeWhat is CUDA? Parallel processing for GPUsHow to choose a cloud machine learning platformDeeplearning4j: Deep learning and ETL for the JVMReview: Amazon SageMaker plays catch-upTensorFlow 2 review: Easier machine learningReview: Google Cloud AutoML is truly automated machine learningReview: MXNet deep learning shines with GluonPyTorch review: A deep learning framework built for speedReview: Keras sails through deep learning", "pub_date": "2020-08-27"},
{"title": "MLflow is now a Linux Foundation project", "overview": "Databricks framework for managing machine learning projects will go to an open governance model", "image_url": null, "url": "https://www.infoworld.com/article/3563344/mlflow-is-now-a-linux-foundation-project.html", "body": "Databricks, the company behind the commercial development of Apache Spark, is placing its machine learning lifecycle project MLflow under the stewardship of the Linux Foundation.MLflow provides a programmatic way to deal with all the pieces of a machine learning project through all its phases — construction, training, fine-tuning, deployment, management, and revision. It tracks and manages the the datasets, model instances, model parameters, and algorithms used in machine learning projects, so they can be versioned, stored in a central repository, and repackaged easily for reuse by other data scientists.InfoWorld review: Nvidia RAPIDS brings Python analytics to the GPUMLflow’s source is already available under the Apache 2.0 license, so this isn’t about open sourcing a previously proprietary project. Instead, it’s about giving the project “a vendor neutral home with an open governance model,” according to Databricks’s press release.Projects for managing entire machine learning pipelines have taken shape over the past couple of years, providing single overarching tools for governing what is typically a sprawling and complex process involving multiple moving parts. Among them is a Google project, Tensorflow Extended, but better known is its descendent project Kubeflow, which uses Kubernetes to manage machine learning pipelines.MLflow differs from Kubeflow in several key ways. For one, it doesn’t require Kubernetes as a component; it runs on local machines by way of simple Python scripts, or in Databricks’s hosted environment. And while Kubeflow focuses on TensorFlow and PyTorch as its learning systems, MLflow is agnostic — it can work with models from those frameworks and many others. ", "pub_date": "2020-06-25"},
{"title": "Kaggle: Where data scientists learn and compete", "overview": "By hosting datasets, notebooks, and competitions, Kaggle helps data scientists discover how to build better machine learning models", "image_url": null, "url": "https://www.infoworld.com/article/3564164/kaggle-where-data-scientists-learn-and-compete.html", "body": "Data science is typically more of an art than a science, despite the name. You start with dirty data and an old statistical predictive model and try to do better with machine learning. Nobody checks your work or tries to improve it: If your new model fits better than the old one, you adopt it and move on to the next problem. When the data starts drifting and the model stops working, you update the model from the new dataset.Doing data science in Kaggle is quite different. Kaggle is an online machine learning environment and community. It has standard datasets that hundreds or thousands of individuals or teams try to model, and there’s a leaderboard for each competition. Many contests offer cash prizes and status points, and people can refine their models until the contest closes, to improve their scores and climb the ladder. Tiny percentages often make the difference between winners and runners-up.Also on InfoWorld: The best free data science courses during quarantineKaggle is something that professional data scientists can play with in their spare time, and aspiring data scientists can use to learn how to build good machine learning models.What is Kaggle?Looked at more comprehensively, Kaggle is an online community for data scientists that offers machine learning competitions, datasets, notebooks, access to training accelerators, and education. Anthony Goldbloom (CEO) and Ben Hamner (CTO) founded Kaggle in 2010, and Google acquired the company in 2017.Kaggle competitions have improved the state of the machine learning art in several areas. One is mapping dark matter; another is HIV/AIDS research. Looking at the winners of Kaggle competitions, you’ll see lots of XGBoost models, some Random Forest models, and a few deep neural networks.Kaggle competitionsThere are five categories of Kaggle competition: Getting Started, Playground, Featured, Research, and Recruitment.Getting Started competitions are semi-permanent, and are meant to be used by new users just getting their foot in the door in the field of machine learning. They offer no prizes or points, but have ample tutorials. Getting Started competitions have two-month rolling leaderboards.Playground competitions are one step above Getting Started in difficulty. Prizes range from kudos to small cash prizes.Featured competitions are full-scale machine learning challenges that pose difficult prediction problems, generally with a commercial purpose. Featured competitions attract some of the most formidable experts and teams, and offer prize pools that can be as high as a million dollars. That might sound discouraging, but even if you don’t win one of these, you’ll learn from trying and from reading other people’s solutions, especially the high-ranked solutions.Research competitions involve problems that are more experimental than featured competition problems. They do not usually offer prizes or points due to their experimental nature.In Recruitment competitions, individuals compete to build machine learning models for corporation-curated challenges. At the competition’s close, interested participants can upload their resume for consideration by the host. The prize is (potentially) a job interview at the company or organization hosting the competition.There are several formats for competitions. In a standard Kaggle competition, users can access the complete datasets at the beginning of the competition, download the data, build models on the data locally or in Kaggle Notebooks (see below), generate a prediction file, then upload the predictions as a submission on Kaggle. Most competitions on Kaggle follow this format, but there are alternatives. A few competitions are divided into stages. Some are code competitions that must be submitted from within a Kaggle Notebook.Kaggle datasetsKaggle hosts over 35 thousand datasets. These are in a variety of publication formats, including comma-separated values (CSV) for tabular data, JSON for tree-like data, SQLite databases, ZIP and 7z archives (often used for image datasets), and BigQuery Datasets, which are multi-terabyte SQL datasets hosted on Google’s servers.InfoWorld review: Nvidia RAPIDS brings Python analytics to the GPUThere are several ways of finding Kaggle datasets. On the Kaggle home page you will find a listing of “hot” datasets and datasets uploaded by people you follow. On the Kaggle datasets page you will find a dataset list (initially ordered by “hottest” but with other ordering options) and a search filter. You can also use tags and tag pages to locate datasets, for example https://www.kaggle.com/tags/crime.You can create public and private datasets on Kaggle from your local machine, URLs, GitHub repositories, and Kaggle Notebook outputs. You can set a dataset created from a URL or GitHub repository to update periodically.At the moment, Kaggle has quite a few COVID-19 datasets, challenges, and notebooks. There have already been several community contributions to the effort to understand this disease and the virus that causes it.Kaggle NotebooksKaggle supports three types of notebook: scripts, RMarkdown scripts, and Jupyter Notebooks. Scripts are files that execute everything as code sequentially. You can write notebooks in R or Python. R coders and people submitting code for competitions often use scripts; Python coders and people doing exploratory data analysis tend to prefer Jupyter Notebooks.Notebooks of any stripe can optionally have free GPU (Nvidia Tesla P100) or TPU accelerators and may use Google Cloud Platform services, but there are quotas that apply, for example 30 hours of GPU and 30 hours of TPUs per week. Basically, don’t use a GPU or a TPU in a notebook unless you need to accelerate deep learning training. Using Google Cloud Platform services may incur charges to your Google Cloud Platform account if you exceed free tier allowances.You can add Kaggle datasets to Kaggle notebooks at any time. You can also add Competition datasets, but only if you accept the rules of the competition. If you wish, you can chain notebooks by adding the output of one notebook to the data of another notebook.Notebooks run in kernels, which are essentially Docker containers. You can save versions of your notebooks as you develop them.You can search for notebooks with a site keyword query and a filter on notebooks, or by browsing the Kaggle homepage. You can also use the Notebook listing; like datasets, the order of notebooks in the list is by “hotness” by default. Reading public notebooks is a good way to learn how people do data science.You can collaborate with others on a notebook multiple ways, depending on whether the notebook is public or private. If it is public, you can grant editing privileges to specific users (everyone can view). If it is private, you can grant viewing or editing privileges.Kaggle public APIIn addition to building and running interactive notebooks, you can interact with Kaggle using the Kaggle command line from your local machine, which calls the Kaggle public API. You can install the Kaggle CLI using the Python 3 installer , and authenticate your machine by downloading an API token from the Kaggle site.The Kaggle CLI and API can interact with competitions, datasets, and notebooks (kernels). The API is open source and is hosted on GitHub at https://github.com/Kaggle/kaggle-api. The README file there provides the full documentation for the command-line tool.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesKaggle community and educationKaggle hosts community discussion forums and micro-courses. Forum topics include Kaggle itself, getting started, feedback, Q&A, datasets, and micro-courses. Micro-courses cover skills relevant to data scientists in a few hours each: Python, machine learning, data visualization, Pandas, feature engineering, deep learning, SQL, geospatial analysis, and so on.All in all, Kaggle is very useful for learning data science and for competing with others on data science challenges. It’s also very useful as a repository for standard public datasets. It’s not, however, a replacement for paid cloud data science services or for doing your own analysis.", "pub_date": "2020-06-30"},
{"title": "The unwavering optimism of Tim O'Reilly", "overview": null, "image_url": null, "url": "https://www.infoworld.com/article/3564824/the-unwavering-optimism-of-tim-oreilly.html", "body": "For better or worse, Tim O’Reilly has become known as something of an oracle for the technology industry in his forty-year career as a technical publisher, author and venture capitalist, credited with coining terms like Open Source and Web 2.0.Today, O'Reilly finds himself in the interesting position of being both a techno-optimist – for instance, about how artificial intelligence could augment human workers and help solve existential problems like climate change – while also being a fierce critic of the new power centres technology has created, particularly in Silicon Valley.\"I totally think that there is a massive opportunity for us to augment humans to do things, we need the machines,\" O'Reilly told InfoWorld last week, from his home in Oakland, California.With the world facing a rapidly ageing population, and the pressing need to prevent climate catastrophe, \"we'll be lucky if the AI and the robots arrive in time, quite honestly,\" he says.\"There are such enormous challenges facing our society. Inequity and inequality is a huge part of it. But for me, one of the really big ones is climate change,\" he says. \"We have to solve this problem or we're all toast. We're going to need every bit of ingenuity to do that. I think it will become the focus of innovation.\"That change in focus could also lead to an enormous raft of new jobs, he argues – provided the planet shifts away from fossil fuels, and what he describes as the \"Ponzi scheme\" of startup valuations.O’Reilly stops short of pushing for the sweeping radicalism of \"a new socialism\", but he insists that \"we have to design this system for human flourishing.”But what does that look like? How do we reskill the workforce to focus on this new class of problems, while ensuring the spoils are spread evenly, and not concentrated in the hands of big tech companies? Or entrepreneurs like Elon Musk, whom O'Reilly admires.Short of telling people to \"learn to code\", O'Reilly sees a new set of literacies being required if the workforce of the future is to take advantage of the oncoming \"augmentation\" that intelligent systems could enable.\"I think the golden age of the last couple of decades where you can become a programmer and you'll get a job... is sort of over,\" O'Reilly says. \"Programming is now more like being able to read and write. You just have to be able to do it to be able to get the most out of the tools and the environments that you're presented with, whatever they are.\"\"Every working scientist today is a programmer,\" he adds. \"Programming can make a journalist more successful, programming can make a marketer more successful, programming can make a salesperson more successful, programming can make an HR person more successful. Having technical literacy is on the same level as being good at reading, writing, and speaking.\"O'Reilly isn't blind to the trade-offs that society has made for the convenience that certain technologies bring. How does he maintain such a sunny disposition when it comes to the potential of technology in the face of growing inequality, the erosion of privacy, and the disinformation crisis that Silicon Valley has wrought?\"It's quite clear that we're now really aware of the enormous risks of these technologies, the risks for abuse,\" he says, adding that he doesn't believe government should be singled out to solve all of these issues.Although O'Reilly recognises that Congress recently announcing that it will legislate to regulate facial recognition technology is a step in the right direction, he notes that it's not nearly comprehensive enough to truly mitigate the risks. \"We're not really getting to the root of our engagement with the question of what is the governance structure for technologies that are really changing our society,\" he says. Complex problems require complex solutions. Take the recent exodus of advertising revenue from Facebook, where brands such as Unilever and Ben and Jerry's have pulled their marketing dollars from the social network over its policies surrounding hate speech.O'Reilly argues that Facebook is only doing what it is designed to do and has been thus far rewarded by the market for doing: attract as many eyeballs as possible and sell ads against that attention using algorithms.\"If you understand how algorithmic systems work, you realise they are curatorial systems, they represent choices,\" O'Reilly says. \"We need to have a completely different conversation about it. So too with facial recognition, it's on a continuum with all kinds of other technologies that take away people's privacy. On that continuum are things that people like and embrace and want, and things that they don't want.\"There is no silver bullet to solve these issues, but there are some steps that could be taken to realign the priorities of technology companies with those of society at large.\"Until we build ethical principles more broadly into our company governance – which things like the B Corp movement have tried to do – we have to take this as a comprehensive problem, with comprehensive solutions,\" O'Reilly says.As a long-time exponent of the power of open source, where does this community fit in to O'Reilly's vision for technology to help solve society's biggest problems?\"Open source is really challenged in this world, it's not going to be the same thing that it was in the PC era,\" he says.Tracing open source back to its roots, there have always been a plethora of opinions around what open source truly means, from the Free Software Foundation's definition, to the computer scientists at UC Berkley, or the MIT X Window System, which O'Reilly is most closely aligned with.The central idea here is that all code should be openly available to be modified and copied, with the overall aim being to push forward the state of the art.\"If you look at where open source is really thriving it is in areas like science, where there's not that desire to make a lot of money off of this, they just want other people to be able to use this and benefit from it,\" he says.\"That's why, for example, very early on in the open source discussion, I was saying data is going to be the new source of lock-in, we shouldn't be so focused on source code,\" he adds. \"If we had focused a lot more on issues of what it means when somebody controls the data, when somebody controls the algorithms which shape what data people see? That's where the open source discussion needs to be now.\"", "pub_date": "2020-07-03"},
{"title": "Learn PyTorch: The best free online courses and tutorials", "overview": "Look no further than these excellent free resources to master the development of deep learning models using PyTorch", "image_url": null, "url": "https://www.infoworld.com/article/3563527/learn-pytorch-the-best-free-online-courses-and-tutorials.html", "body": "Deep learning continues to be one of the hottest fields in computing, and while Google’s TensorFlow remains the most popular framework in absolute numbers, Facebook’s PyTorch has quickly earned a reputation for being easier to grasp and use.PyTorch has taken the world of deep learning research by storm, outstripping TensorFlow as the implementation framework of choice in submitted papers for AI conferences in the past two years. With recent improvements for producing optimized models and deploying them to production, PyTorch is definitely a framework ready for use in industry as well as R&D labs.Also on InfoWorld: 5 reasons to choose PyTorch for deep learningBut how to get started? You’ll find plenty of books and paid resources available for learning PyTorch, of course. But there are also plenty of resources on the Internet that will help you get to grips with the framework — for absolutely nothing. Plus, some of the free resources are of even higher quality than what you can pay for. Let’s take a look at what is on offer.PyTorch.org tutorialsPerhaps the most obvious place to start is the PyTorch website itself. Along with the usual resources such as an API reference, the website includes more digestible works such as a 60-minute video and text blitz through PyTorch via setting up an image classification model. There are guides for both the standard and the more esoteric features of the framework, and when a new major capability is added, such as quantization or pruning of models, you’ll normally get a quick tutorial on how to implement them in your own applications.On the downside, the code in the various tutorials tends to vary quite a lot, and sometimes standard steps will be missed or passed over in order to show off the feature that the tutorial is concentrating on rather than producing idiomatic PyTorch code. In fairness, the tutorial code has definitely improved over the past couple of years, but you do sometimes have to be a little careful. For this reason, I wouldn’t recommend using the PyTorch website as your primary resource for learning. Nevertheless, it’s a useful resource to have on hand — and the best place to learn how to use the latest new features.Udacity’s and edX’s PyTorch deep learning coursesI’m bundling Udacity’s Introduction to Deep Learning with PyTorch and edX’s Deep Learning with Python and PyTorch together here as they have similar structures, cover a lot of the same ground, and appear to suffer from the same issues. They both have a traditional series of lectures that build up from the foundations of deep learning, introducing you to concept after concept, then tackling more complex scenarios such as image and text classification by the end of the course. This is a completely fine way to go about teaching deep learning, but it does mean that you’ll be sinking some considerable time into the lessons before you get to do anything exciting with PyTorch, unlike, say, what happens with the Fast.ai course.Both the Udacity and edX courses do appear to suffer from being a little out of date in terms of content and PyTorch itself. You won’t learn anything about generative adversarial networks (GANs) or Transformer-based networks in either course, and the Udacity course is based on PyTorch 0.4. This isn’t necessarily a problem, but we’re currently at PyTorch 1.5, so you may find yourself running into deprecation warnings when trying to replicate code on the latest version. If you’re choosing between these two courses, I would give Udacity a slight edge over edX due to the Facebook stamp of approval.Fast.ai’s Practical Deep Learning for CodersSince its beginnings 2016, fast.ai has been the gold standard for free deep learning education. Every year, it has released a new iteration of its two-part course, iterating on the previous incarnation and pushing things forward a little every time. While the first year was based on Keras and TensorFlow, fast.ai switched to PyTorch from year two and hasn’t really looked back (though it has cast a few glances at Swift for TensorFlow).Fast.ai has a somewhat unique approach to teaching deep learning. Other courses devote many of the early lectures and material laying the foundations before you even consider building even the tiniest neural network. Fast.ai is, well, faster. By the end of the first lesson, you’ll have built a state-of-the-art image classifier. This has led to some criticism that the Fast.ai course leans too heavily on “magic” rather than teaching you the basics, but the following lectures do give you a good grounding in what is happening under the covers.And yet, I’d be a little hesitant to recommend Fast.ai as your sole resource for learning PyTorch. Because Fast.ai uses a library on top of the framework rather than pure PyTorch, you tend to learn PyTorch indirectly rather than explicitly. That’s not to say it’s a bad approach; the Part Two Lessons of the 2019 course include an astonishing set of lectures that builds a somewhat-simplified version of PyTorch from scratch, solving bugs in actual PyTorch along the way. (This set of lectures, I think, puts paid to any notion that Fast.ai is too magical, for what it’s worth.) That said, you might want to use Fast.ai in conjunction with another course in order to understand what Fast.ai’s library is doing for you versus standard PyTorch. EPFL’s Deep Learning (EE-559)Next up, how about a course from an actual university? EE-559, taught by François Fleuret at the École Polytechnique Fédérale de Lausanne, in Switzerland, is a traditional university course, with slides, exercises, and video clips. While it begins with the basics, it does ramp up beyond what’s on offer with the Udacity and edX courses by taking in GANs, adversarial samples, and closes out with Attention mechanisms and Transformer models. It also has the advantage of being current with recent PyTorch releases, so you should be confident that you’re learning techniques and code that are not using deprecated features of the framework.Other PyTorch learning resourcesThere are a few more resources that are very useful but perhaps not core to learning PyTorch itself. First, there’s PyTorch Lightning, which some describe as PyTorch’s equivalent to Keras. While I wouldn’t go that far, as PyTorch Lightning is not a complete high-level API for PyTorch, it is a great way of producing organized PyTorch code. Further, it provides implementations of standard boilerplate (for details like training, testing, validation, and taking care of distributed GPU/CPU setups) that you would otherwise end up re-writing for most of your PyTorch work.The documentation on the project’s website includes some good tutorials to get you started. In particular, there’s a wonderful video that shows off the process of converting a normal PyTorch project to PyTorch Lightning. The video really shows off the flexibility and ease-of-use that PyTorch Lightning provides, so definitely have a look at that once you’ve mastered the basics.Second, there’s Huggingface’s Transformers library, which has become the  standard for Transformer-based models over the past 18 months. If you want to do anything approaching state-of-the-art with deep learning and text processing, Transformers is a wonderful place to start. Containing implementations for BERT, GPT-2, and a brace of other Transformer models (with more being added seemingly on a weekly basis), it is an amazing resource. Happily, it also includes a selection of Google Colab notebooks that will get you up and running with the library swiftly.And third, I can’t write his article without mentioning Yannic Kilcher’s explainer videos. These are not PyTorch specific at all, but they are a great way to keep track of current papers and research trends, with clear explanations and discussion. You probably won’t need to watch these when you start learning PyTorch, but by the time you’ve gone through some of the coursework mentioned here, you’ll be wanting to know what else is out there, and Kilcher’s videos point the way.Learning PyTorch deep learningIf you’re looking to learn PyTorch, I think your best bet is to work through both the Fast.ai course and one of the more traditional courses at the same time. (My pick for the companion course would be EE-559, since it stays current with PyTorch.) As a bonus, there’s a Fast.ai book coming out in August that will be one of the best introductory texts for deep learning.Based on the new FastAI2 library (which among other things has a multi-tiered API structure for easier integration with standard PyTorch), the Fast.ai book is likely to be essential for getting started in the field really quickly. And while I recommend buying a physical copy, you can read it all for free in notebook form on GitHub. Dive into the book, and you’ll be telling dogs from cats in no time at all!", "pub_date": "2020-07-06"},
{"title": "Social engineering hacks weaken cybersecurity during the pandemic", "overview": "Disinformation, malware, and an array of cyberattacks are rising as fast as case counts", "image_url": null, "url": "https://www.infoworld.com/article/3565197/social-engineering-hacks-weaken-cybersecurity-during-the-pandemic.html", "body": "Cybersecurity inevitably suffers when scares infect the populace. The COVID-19 outbreak appears to be the most acute global crisis since the Second World War.Every aspect of the COVID-19 crisis has been exploited by opportunistic hackers, terrorists, and other criminals. In addition to capitalizing on rampant fear, uncertainty, and doubt, attackers are targeting a fresh new honeypot of federal aid, in the form of payouts from unemployment checks, stimulus checks, and the Paycheck Protection Program.DevSecOps: How to bring security into agile development and CI/CDSocial engineering cyberhacks prey on pandemic anxietiesPervasive social engineering attacks are hindering the world’s coordinated response to the COVID-19 emergency. As noted in this recent press report, cyberattacks have spiked during the first half of 2020. The FBI noted that as of May 28, it had received nearly the same number of complaints for this calendar year as for all of 2019.Preying on social engineering factors, cyberattackers exploit the following facets of society’s collective response to the pandemic:: A swelling number of malicious COVID-19 websites and emails claim to offer useful information on the coronavirus and how to protect oneself. It’s no surprise that thousands of COVID-19 scam and malware sites are being created daily. Many spread false narratives about the COVID-19 outbreak’s progression and impact while stirring anxiety, selling bogus treatments and cures, price gouging for face masks and other needed supplies, and otherwise taking advantage of nervous people’s gullibility.: DDoS attacks have bombarded websites people depend on for their quarantined existence. In addition, hackers are targeting DDoS attacks at the enterprise VPN ports and protocols used for remote access, thereby crippling employees’ ability to get their work done from the coronavirus-free comfort of home. Hackers may initiate thousands of SSL connections to an SSL VPN and then leave them hanging, exhausting memory and thereby preventing legitimate users from using the service.: Phishing attacks have increased. They are frequently cloaked in emails that include pandemic maps or other content related to the coronavirus. In addition, social media is being used as a broadcast platform for predatory and deceptive content, while the companies that run those communities attempt to nip it in the bud. Social engineering tactics in phishing and spam campaigns trick people into disclosing passwords and other sensitive personal and financial information.: People working from home for the first time are acutely exposed to cybersecurity intrusions. Many remote workers may fail to use prudent cybersecurity practices. These lapses often include not securing their passwords effectively, opting not to use multifactor authentication, or neglecting the need for a virtual private network. Corporate IT staff may themselves be working from home, lacking the resources needed to monitor and secure a huge remote workforce’s access to corporate IT assets effectively. In addition, there has been a spurt of voice phishing attacks where callers pretend to be from workplace technical support and thereby convince employees to disclose passwords or to enter authentication information into malicious websites.: More COVID-19-related ransomware attacks via email exploit people and organizations’ increasingly desperate straits due to job losses and the general recession. Some attacks involve hacking enterprise routers to direct users to bogus COVID-19 websites that trick people into downloading malware onto their computers. An uptick in text message phishing perpetrates such scams or dupes targets into loading malicious content onto mobile devices.: Cyberattacks on public-sector healthcare coordinating bodies have ramped up. The U.S. Department of Health and Human Services was recently the target of a cyberattack apparently designed to undermine the country’s response to the coronavirus pandemic. In addition, a state-sponsored hacking group attempted, albeit unsuccessfully, to breach IT systems at the World Health Organization. The FBI has detected cybersecurity attacks against the healthcare industry since the start of the outbreak, such as email fraud campaigns designed to solicit donations for nonexistent healthcare-related organizations and bogus contact-tracing apps that download malware onto a user’s device.Social distancing deepens cybersecurity vulnerabilitiesSocial distancing has become the critical response for flattening the curve of COVID-19. As in-person encounters become less frequent, we’ll have to rely on each person to ensure that they don’t fall victim to these tactics in their myriad virtual and online interactions. That will place more of a burden on the IT infrastructure—and personnel—to guide everybody in the new normal of vigilance against these risks.Exacerbating it all is the fact that many IT professionals have been thrown off balance by their own need to work from home while supporting a vastly expanded home-based workforce. The increasing demand for social distancing, lockdowns, and shutdowns has made it difficult for many IT vendors, including big cloud service providers, to keep the lights on in their facilities. As users find it harder to receive 24x7 support for cybersecurity issues that pop up during the COVID-19 emergency, the attacks on their computers, data, and other online assets will grow.Robotics, postperimeter, and AI are key cyberdefenses against social engineering tacticsIf there’s any hope to reduce society’s exposure to pandemic-stoked social engineering hacks, it comes in the form of AI-driven robotics. To the extent that we can automate more of the tasks in our lives, we’ll reduce the need for human decisions and our vulnerability to cyberscams. Fortunately, the COVID-19 crisis has brought robotic systems to the front lines in every conceivable scenario: in industry, commerce, and the consumer worlds, including (especially) in the back-end data centers that are the beating hearts of the modern economy.Postperimeter security will be another key defense against social engineering hacks in the postpandemic economy. It ensures that users access cloud apps only from managed devices and secure apps. Enterprise IT can block users from falling prey to social engineering tactics, such as requests to connect their mobile devices to unsupported or risky cloud services. In this way, postperimeter security gives people who work from home access to many resources beyond the enterprise perimeter while also giving corporate IT fine-grained control over what, when, and how they do this.Artificial intelligence (AI) will play a pivotal role in postpandemic defenses against social engineering hacks. Automated systems can’t have hard-and-fast rules for detecting the zillion potential cybersecurity attack vectors. But they can use AI’s embedded machine learning models for high-powered pattern recognition, detecting suspicious behavior, and activating effective countermeasures in real time. For example, AI-based defenses can proactively isolate or quarantine threatening components or traffic after determining that a website is navigating to malicious domains or opening malicious files, or after sensing that installed software is engaging in microbehaviors that are characteristic of ransomware attacks.However, AI-based defenses are no panacea, especially when monitoring social engineering attacks that have complex signatures and evolve rapidly. AI-based defenses detect and block abnormal behavioral patterns involving endpoints, or in the network, or in how users interact with devices, applications, and systems. If the AI-learned attack vector is too broad, it’s at risk of blocking an excessive number of legitimate user behaviors as cybersecurity attacks. If the pattern is too narrow, the cybersecurity program risks permitting a wide range of actual attacks to proceed unchecked.Moving forwardThese and other cyberdefenses will crystallize into a new normal for enterprises in the postpandemic era. It’s likely that many people will continue to work from home or, at the very least, switch back and forth between home and traditional offices in their normal routines. As the global community stays on high alert for signs of new pandemics—or recurrence of the present one—safeguards will need to ensure that these anxieties don’t expose enterprise IT assets to social engineering tactics perpetrated by hackers, terrorists, and other criminals.", "pub_date": "2020-07-09"},
{"title": "Algorithmia unveils MLops for teams", "overview": "Algorithmia Teams delivers machine learning operations and management for small groups as a pay-as-you-go cloud service", "image_url": null, "url": "https://www.infoworld.com/article/3572325/algorithmia-unveils-mlops-for-teams.html", "body": "Machine learning tools vendor Algorithmia has unveiled a version of its machine learning operations and management platform for small groups within organizations, called Algorithmia Teams.The cloud-hosted offering, for quickly delivering machine learning models into production, is intended to provide the performance of an enterprise-grade MLOps platform with the cost flexibility of a cloud-based service. A pay-as-you-go consumption model is leveraged to limit short-term financial exposure while enabling users to scale use of the platform as needed. Introduced August 25, Teams is intended to address a situation in which some companies have been constrained by the tools needed to manage AI/ML efforts as well as costs.There are Basic and Professional versions of Teams, with both offering secure workspaces for collaboration and connectors to external cloud data sources such as AWS, Microsoft Azure, and Google Cloud Platform. The Professional edition includes support, a dedicated Slack channel, and the option to purchase a custom language pack or data connector development. The Professional version costs $299 per month plus pay-as-you-go CPU/GPU consumption. The Basic version is pay-as-you-go CPU/GPU consumption with no monthly fee.Algorithmia, which was launched in 2014, maintains the infrastructure for Teams. You can sign up for Algorithmia Teams at the Algorithmia website.", "pub_date": "2020-08-27"},
{"title": "How to use TensorFlow in your browser", "overview": "Take advantage of TensorFlow.js to develop and train machine learning models in JavaScript and deploy them in a browser or on Node.js", "image_url": null, "url": "https://www.infoworld.com/article/3570444/how-to-use-tensorflow-in-your-browser.html", "body": "While you can train simple neural networks with relatively small amounts of training data with TensorFlow, for deep neural networks with large training datasets you really need to use CUDA-capable Nvidia GPUs, or Google TPUs, or FPGAs for acceleration. The alternative has, until recently, been to train on clusters of CPUs for weeks.One of the innovations introduced with TensorFlow 2.0 is a JavaScript implementation, TensorFlow.js. I wouldn’t have expected that to improve training or inference speed, but it does, given its support for all GPUs (not just CUDA-capable GPUs) via the WebGL API.What is TensorFlow.js?TensorFlow.js is a library for developing and training machine learning models in JavaScript, and deploying them in a browser or on Node.js. You can use existing models, convert Python TensorFlow models, use transfer learning to retrain existing models with your own data, and develop models from scratch.TensorFlow.js back endsTensorFlow.js supports multiple back ends for execution, although only one can be active at a time. The TensorFlow.js Node.js environment supports using an installed build of Python/C TensorFlow as a back end, which may in turn use the machine’s available hardware acceleration, for example CUDA. There is also a JavaScript-based back end for Node.js, but its capabilities are limited.In the browser, TensorFlow.js has several back ends with different characteristics. The WebGL back end provides GPU support using WebGL textures for storage and WebGL shaders for execution, and can be up to 100x faster than the plain CPU back end. WebGL does not require CUDA, so it can take advantage of whatever GPU is present.The WebAssembly (WASM) TensorFlow.js back end for the browser uses the XNNPACK library for optimized CPU implementation of neural network operators. The WASM back end is generally much faster (10x to 30x) than the JavaScript CPU back end, but is usually slower than the WebGL back end except for very small models. Your mileage may vary, so test both the WASM and WebGL back ends for your own models on your own hardware.TensorFlow.js models and layersTensorFlow.js supports two APIs for building neural network models. One is the Layers API, which is essentially the same as the Keras API in TensorFlow 2. The other is the Core API, which is essentially direct manipulation of tensors.Like Keras, the TensorFlow.js Layers API has two ways to create a model: sequential and functional. The sequential API is a linear stack of layers, implemented with a layer list (as shown below) or with the  method:The functional API uses the  API and can create arbitrary DAG (directed acyclic graph) networks:The Core API can accomplish the same goals, with different code, and less of an intuitive tie to layers. The model below may look like basic tensor operations, but it creates the same network as the two previous formulations. Note the use of  and , which are both neural network operations, in the  function below.Pre-built TensorFlow.js modelsThere are over a dozen pre-built TensorFlow.js models documented, available in the repository, and hosted on NPM (for use in Node.js) and unpkg (for use in a browser). You can use these models as supplied or for transfer learning. With a little work, you can also use them as building blocks for other models.Several of these models use a device's camera in real time, for example handpose:Handpose can detect palms and track hand-skeleton fingers. The list below is a convenient index into most of the prepackaged TensorFlow.js models.Image classificationObject detectionBody segmentationPose estimationText toxicity detectionUniversal sentence encoderSpeech command recognitionKNN classifierSimple face detectionSemantic segmentationFace landmark detectionHand pose detectionNatural language question answeringWhat is ml5.js?ml5.js is an open source, friendly, high-level interface to TensorFlow.js developed primarily at NYU. ml5.js provides immediate access in the browser to pre-trained models for detecting human poses, generating text, styling an image with another, composing music, pitch detection, common English language word relationships, and much more. While TensorFlow.js is aimed primarily at data scientists and developers, ml5.js aims to support broader public understanding of machine learning and foster deeper engagement with ethical computing, responsible data collection, and accessibility and diversity of people and perspectives in technology and the arts.Most of the examples in ml5.js depend on TensorFlow.js models. They have been packaged as web pages that you can run as is, or edit, for example to use different images.PoseNet can perform real-time pose estimation in the browser, from images or a video feed. Demo: Iris classification with TensorFlow.jsThe famous Iris discrimination dataset, originated by R.A. Fisher in 1936 to illustrate linear discriminant analysis, is still used as a test case for statistical and machine learning classification methods. It uses four features, the length and width of the flower sepals and petals, to classify three species of Iris, with 50 samples of each species. (Fisher’s original paper was published in the , which says more about science in 1936 than it does about the data or the statistics.)If you perform cluster analysis on this data, two of the species will share one cluster, with the third (I. Setosa) in a separate cluster. On the other hand, principal component analysis can separate all three species fairly well.The TensorFlow.js sample fits the Iris data with two fully-connected (dense) neural network layers, as shown in the code extract below.As you can see in the screenshot below, this model does a decent job of classifying the three species. If you play around with the parameters, however, you’ll discover that some confusion between two of the species (the ones in the same cluster) reappears if you iterate for more than 40 epochs.Model training for the Iris dataset using a two-dense-layer neural network model.Converting Python TensorFlow models to JavaScriptPart of the TensorFlow.js repository contains a converter for saved TensorFlow and Keras models. It supports three formats: SavedModel (the default for TensorFlow), HDF5 (the default for Keras), and TensorFlow Hub. You can use the converter for saved models from the standard repositories, models you’ve trained yourself, and models you’ve found elsewhere.There are actually two steps to the conversion. The first step is to convert the existing model to model.json and binary weight files. The second step is to use an API to load the model into TensorFlow.js, either  for converted TensorFlow and TensorFlow Hub models, or  for converted Keras models.Using transfer learningTensorFlow.js supports transfer learning in essentially the same way as TensorFlow. The documentation supplies examples for customizing MobileNet for your own images and customizing a model for speech command recognition for your own sound classes. Essentially, what you are doing in each of these codelabs is adding a small custom classifier on top of the trained model, and training that.Overall, TensorFlow.js can do almost anything that TensorFlow can do. However, given that the target environments for TensorFlow.js (garden variety GPUs for gaming) typically have less in the way of GPU memory than the big Nvidia server GPUs typically used for TensorFlow deep learning training, you may have to reduce the size of your model to make it run in a browser. The conversion utility does some of this for you, but you may have to take out layers manually and reduce the batch sizes for your training.", "pub_date": "2020-09-02"},
{"title": "Black Friday retailers must be effective, ready, and resilient", "overview": "It’s a matter of deploying the right personalization, search, and offers to maximize conversion rate while avoiding boom fizzle pop. ", "image_url": null, "url": "https://www.infoworld.com/article/3587780/black-friday-retailers-must-be-effective-ready-and-resilient.html", "body": "An unprecedented Black Friday is coming. Will consumers run masklessly back into stores and clean out the shelves? Only if you have a sale on Clorox toilet wand refills. Will they swarm your app or website? Maybe. As one data scientist put it, “All predictions are wrong. Some are lucky.”Also on InfoWorld: Black developers tell how the US tech industry could do betterSome retailers will likely continue to see diminished demand even on Black Friday—such as those specializing in men’s slacks. Most retailers have already seen their traffic move online. For this Black Friday, they need to be effective, ready, resilient, and cost-effective.Effectiveness is the conversion rate of visitors to buyers. Ready is measured by throughput and latency, even when lots of traffic comes at once. Resilient is handling whatever 2020 throws at it. Finally, in the cloud era, cost-effectiveness is eating the world. (I felt that eye roll.)Effective front pageThe front page needs to sell to be effective. Any website analytics will tell you the most visited page is the homepage. Analytics will also tell you the homepage has the highest bounce rate. For the front page to be useful, its content needs to be informed by context. are usually the easiest to convert. They did not come to your site through a pay-per-click ad, a search, or a referral. They came directly to your site because they know you. When they arrive at the homepage, it should not be just any promotion or set of items. It should be offers tailored specifically for that customer., the homepage should also be customized. AI, machine learning, or just plain old counting should calculate what other items were bought by people who visited that offer. If the offer is brand new (a cold start), use data from existing customers who clicked on that item., the page should also not be static. As the day wears on, the content should rotate based on what people are buying or searching. Additionally, the front page should make offers to these first-time shoppers to encourage them to identify themselves.Effective searchThe straight keyword search is dead. As a repeat customer, if I type “shoes,” the search should know that I do not mean heels. My customer profile and my past searches should inform my search results. I should have a way out of this personalization by being more specific, but if I type “shoes” I should see running shoes and flip flops, based on my past purchases.While the site may not know a first-time shopper’s preferences, the context should inform future searches. Meaning if I type “skis” and then “goggles” and then “boots” I should see snow boots and ski boots and not cowboy boots.There are plentiful tools for personalizing search and homepages. Some commerce platforms such as BigCommerce or Shopify have much of this built-in. Third-party vendors such as Coveo and Lucidworks offer solutions for this sort of signal capture and machine learning personalization. Effective offersJust for you, we are offering 20% off on de-icing equipment. Seriously, I have had vendors offer me de-icing equipment for all of those frozen winters we get here in Savannah, GA. Context and personalization make offers compelling. If I searched on something, clicked on it, and started to leave the site, the smart play is to offer me something similar or a discount on what I was viewing.If I have searched several related items such as a glass carboy, tubing, malt, and hops, as a result, perhaps I should be offered a bundle such as a homebrew starter kit or the most popular homebrew item.No matter what, offers should be personal (Who am I? What do I like?) and contextual (What else did I search on? Am I leaving, searching for more things, buying something?). Many tools do this kind of personalized offer, including the tools built into commerce suites. There are other third-party offerings such as those by Justuno or Privy.Ready and resilientIn 2020, retailers should use cloud computing, have an elastic architecture, and live on the edge. Most retailers have already moved their core infrastructure to the cloud. Ideally, this should either be a commerce platform or a combination of SaaS and cloud-native solutions. Content delivery networks (CDNs) like Akamai, Cloudflare, or Netlify have now existed for decades. Images, static content, and other resources can be easily cached at the edge of the network to reduce the load on your web server. Even five years ago, CDNs were the luxury of kings or at least the Fortune 500. Today, they are affordable for the rest of us, and there is no good reason not to use them for a modern website.The next step is to ensure the site’s search and database architecture are elastic and resilient. While most ecommerce platforms include search, these search tools may not scale when all personalization tools are enabled. It may be necessary to investigate other solutions. Those solutions should replicate and have some way to scale and to fail gracefully. In short, they should have a modern cluster architecture. There are still many ecommerce sites that use decades-old search technology, especially Endeca. These tools were not designed for modern traffic or the cloud.As for databases, retail is often “overserved.” A NoSQL database that can relax consistency will meet the needs of most retail use cases. Meaning you do not need to pay for a distributed transaction unless the specific use case requires it. Almost any of the NoSQL databases can replicate to multiple data centers and provide sufficient high availability.My recommendation is to that allows for quickly capturing low-latency signal data, retrieving product data, and running real-time analytical queries. There are cloud service provider offerings such as Microsoft’s CosmosDB and third-party vendor products such as Couchbase. Barring that, you could cobble a few different databases together, but that involves paying more for ETL and dealing with greater complexity.Cost-effectiveEven for retailers with more traffic, margin is strained, so no one wants to pay for capacity they are not using. The term “autoscaling” is overused and stretched out. Ask any vendor (be they database, search, or otherwise) and they will tell you that they can scale up. But what happens when you want to  the number of instances you are using? Can that happen without an outage or excessive performance degradation? A lot of technology goes into scaling up. A lot more goes into scaling down.The questions to ask are, “Can I easily remove nodes?” and “How long does it typically take to rebalance for my type of data and load?” You should also ask, “Can I turn off components that I am not using?” and “What is the idle cost of the software just running?”Boom fizzle popIt is 2020. Assume that anything can happen. Things seem to be going boom, fizzle, or pop every day. The stakes are higher this year. They require investing more imagination into “What could go wrong?” Multicloud is not just a buzzword. For global retailers, it is a necessity. There are geographies where a preferred cloud service provider does not reach or where there is only one data center. A global retailer needs alternatives. Nor can you assume your provider will never fail. While cloud providers fail rarely compared to the self-hosted data centers of old, and failures of multiple availability zones are rare, it does happen. Having it happen on Black Friday in 2020 could be catastrophic for some retailers. What if your whizbang personalization component or third-party service chokes? It is one thing if the homepage becomes more generic or does not show an offer on exit. It is a bigger problem if visitors see a 404 inside an iframe or get an hourglass or spinning disc. Also on InfoWorld: Beyond NoSQL: The case for distributed SQLKeep it togetherThe great news about 2020 is that it is almost over. Staying up and converting customers this Black Friday is not fundamentally different than in years past. If anything, there are more options. It is just a matter of deploying the right personalization, search, and offers to maximize conversion rate while ensuring an adequate cloud-native architecture that can handle multiple failures while scaling to meet demand.", "pub_date": "2020-11-04"},
{"title": "The move to dynamic distributed cloud architectures ", "overview": "Now that we’re moving things out to the edge, an emerging approach blows the door off the cloud-only and edge-only models.", "image_url": null, "url": "https://www.infoworld.com/article/3587772/the-move-to-dynamic-distributed-cloud-architectures.html", "body": "Edge computing is getting a great deal of attention now, and for good reason. Cloud architecture requires that some processing be placed closest to the point of data consumption. Think computing systems in your car, industrial robots, and now full-blown connected mini clouds such as Microsoft’s Stack and AWS’s Outpost, certainly all examples of edge computing.The architectural approach to edge computing—and IoT (Internet of Things), for that matter—is the creation of edge computing replicants in the public clouds. You can think of these as clones of what exists on the edge computing device or platform, allowing you to sync changes and manage configurations on “the edge” centrally.The trouble with this model is that it’s static. The processing and data is tightly coupled to the public cloud or an edge platform. There is typically no movement of those processes and data stores, albeit data is transmitted and received. This is a classic distributed architecture.Also on InfoWorld: Amazon, Google, and Microsoft take their clouds to the edgeThe trouble with the classic approach is that sometimes the processing and I/O load requirements expand to 10 times the normal load. Edge devices are typically underpowered, considering that their mission is fairly well defined, and edge applications are created to match the amount of resources on the edge device or platform. However, as edge devices become more popular, we’re going to need to expand the load on these devices or they will more frequently hit an upward limit that they can’t handle.The answer is the dynamic migration of processing and data storage from an edge device to the public cloud. Considering that a replicant is already on the public cloud provider, that should be less of a problem. You will need to start syncing the data as well as the application and configuration, so at any movement one can take over for the other (active/active).The idea here is to keep things as simple as you can. Assuming that the edge device does not have the processing power needed for a specific use case, the processing shifts from the edge to the cloud. Here the amount of CPU and storage resources are almost unlimited, and processing should be able to scale, afterwards returning the processing to the edge device, with up-to-date, synced data.Some ask the logical question about just keeping the processing and the data on the cloud and not bothering with an edge device. Edge is an architectural pattern that is still needed, with processing and data storage placed closest to the point of origin. Dynamic distributed leverages centralized processing as needed, dynamically. It provides the architectural advantage of allowing scalability without the loss of needed edge functionality.A little something to add to your bag of cloud architecture tricks.", "pub_date": "2020-10-30"},
{"title": "How to make the most of the Google Cloud free tier", "overview": "10 tips for stretching Google’s free services to the limit and pinning your cloud meter at zero.", "image_url": null, "url": "https://www.infoworld.com/article/3585633/how-to-make-the-most-of-the-google-cloud-free-tier.html", "body": "The cloud computing industry loves to give away free samples and Google is no different from Amazon or Microsoft in this respect. The companies know that if you give the customers a free taste, they’ll come back when it’s time for a meal.Google offers two types of free. New customers get $300 to spend on any of the machines or services spread out among the 24 “cloud regions,” 73 “zones,” and 144 “network edge locations.” The money works pretty much everywhere in the Google cloud from raw compute power to any of several dozen different products like databases or map services.Also on InfoWorld: How to make the most of the AWS free tierBut even when that free money runs out, the free gifts continue. There are 24 different products that offer continuous free samples that are billed as “always free.” Even if you’ve been a customer for years, you can still experiment. Of course Google adds the caveat that the word “always” in this generous promise is “subject to change.” But until that day comes, the BigQuery database will answer one terabyte of queries each month and AutoML Translation will turn 500,000 characters from one language to another.Some developers use the free tier for what it’s intended to be: an opportunity to explore without begging their boss and their boss’s boss for a budget. Others work on a side hustle or a website for the neighborhood kids. When the load is small, it’s easy to innovate without dealing with a monthly bill.Some developers take this to the extreme. They try to stay in the free tier as long as possible. Perhaps it’s because they want to brag about their insanely low burn rate. Maybe it’s just a form of modern machismo. Maybe they’re low on cash.In any case, working this free angle as long as possible generally leads to lean and efficient web applications that do as much as possible with as little as possible. When the day comes that they leave the free tier, the monthly bills will stay small as the project scales, something that warms the heart of every CFO.Here are a few of the secrets for squeezing every last drop of goodness from Google’s free offering. Maybe you’re cheap. Maybe you’re just waiting to tell your boss until the awesomeness is completely realized. Maybe you’re just having fun and this is a goof. Whatever the case, there are many ways to save. Store only what’s necessaryThe free databases like Firestore and Cloud Storage are completely flexible tools that squirrel away key-value documents and objects respectively. Google Cloud’s always-free tier lets you store your first 1GB and 10GB in each product respectively. But the more details your app keeps, the faster the free gigabytes will run out. So quit saving information unless you absolutely need it. This means no obsessive collection of data just in case you need it for debugging later. No extra timestamps, no large cache full of data you’re keeping just to be ready.Compression is your friendThere are dozens of good pieces of code for adding a layer of compression to your clients. Instead of storing fat blocks of JSON, the client code can run the data through an algorithm like LZW or Gzip before sending it over the wire to your server instances, which store it without unpacking it. That means faster responses, fewer bandwidth issues, and less impact on your free monthly data storage quota. Be a bit careful because some very small data packets can get bigger when the overhead from compression is included.Go serverlessGoogle is more generous with their intermittent compute services that are billed per request. Cloud Run will boot up and run a stateless container that answers two million requests each month for free. Cloud Functions will fire up your function in response to another two million requests. That’s more than 100,000 different operations each day on average. So quit waiting and start writing your code to the serverless model.Note: Some architects will cringe at the idea of using two completely different services. It may save money but it will double the complexity of the application and that means it will be harder to maintain. That is a real danger, but often you can more or less duplicate the function-as-a-service structure of Cloud Functions inside your own container, making it possible to consolidate your code later if you plan for it.Also on InfoWorld: 13 ways Google Cloud beats AWSUse the App EngineGoogle’s App Engine remains one of the best ways to spin up a web application without fussing over all of the details of how to deploy or scale it. Almost everything is automated so it will deploy new instances if the load grows. The App Engine comes with 28 “instance hours” for each day—meaning that your basic app will run free for 24 hours per day and can even scale up for four hours if there’s a burst of demand.Consolidate service callsThere is some freedom to add extras if you’re careful. The limits on serverless invocations are on the number of individual requests not on the complexity. You can pack more action and more results into each exchange by bundling all of the data operations into one bigger packet. So you can offer silly gimmicks like stock quotes, but only if you slip the extra few bytes into the absolutely essential packets. Just keep in mind that Google counts the memory used and the compute time. Your functions can’t exceed 400,000 GB-seconds memory and 200,000 GHz-seconds of compute time.Use local storageThe modern web API offers a number of good places to store information. There’s the perfectly good, old-fashioned cookie that’s limited to four kilobytes. The Web Storage API is a document-based key-value system that will cache at least five megabytes of data and some browsers will keep 10 megabytes. The IndexedDB offers a richer set of features like database cursors and indices that will speed plowing through the data which is often stored without limits.The more data you store locally on your user’s machine, the less you need to use your precious server-side storage. This can also mean faster responses and much less bandwidth devoted to carrying endless copies of the data back to your server. There will be problems, though, when users switch devices because the data probably won’t be in sync. Just make sure the important details are consistent.Find the hidden bargainsGoogle maintains a helpful page that summarizes all of the “always free” products, but if you poke around you’ll find plenty of free services that don’t even make the list. Google Maps, for instance, offers “$200 free monthly usage.” Google Docs and a few of the other APIs are always free.Use G SuiteMany of the G Suite products including Docs, Sheets, and Drive are billed separately and the users either get them free with their GMail account or their business pays for them as a suite. Instead of creating an app with built-in reporting, just write the data to a spreadsheet and share that. The spreadsheets are powerful enough to include graphs and plots like any dashboard. If you build a web app, you’ll need to burn your compute and data quotas to handle the interactive requests. But if you just create a Google Doc for your report, you’re dumping most of the work on Google’s machine.Strip away gimmicksSome features of modern web applications are pretty superfluous. Does your bank application need stock quotes? Do you need to include local time or temperature? Do you need to embed the latest tweets or Instagram photos? No. Get rid of all of these extras because each one means another call to your server machines and that eats away at your free limits. The product design team may dream big, but you can tell them, “No!”Be careful with new optionsSome of the cooler tools for building artificial intelligence services for your stack offer good limits for experimenting. The AutoML Video service will let you train your machine learning model on video feeds for 40 hours each month, before charges kick in. The service for tabular data will grind your rows and rows of information on a node free for six hours. This gives you enough rope to experiment or build basic models, but watch out. It would be dangerous to automate the process so every user could trigger a big machine learning job.Keep costs in perspectiveIt’s easy to take this game to the extreme and turn your application’s architecture into a Rube Goldberg device just to save a bit more cash. It’s important to remember that the jump from free tier to paying customer is often a pretty tiny step in Google Cloud. While there are many free services on the Internet that jump from free to thousands of dollars with one click, Google’s services generally aren’t priced like that.Also on InfoWorld: Amazon, Google, and Microsoft take their clouds to the edgeAfter churning through two million free invocations of Cloud Functions, the next one is a whopping $0.0000004. That’s only 40 cents per million. If you dig around your sock drawer, you should be able to cover a few extra million with little trouble.The price schedule is generous enough that you’re not going to have a heart attack when you step out of the free zone. If your application needs a few extra million this or that, you’ll probably be able to cover it. The important lesson is that keeping the computational load low will translate to smaller bills and faster responses.", "pub_date": "2020-11-02"},
{"title": "Azure Cosmos DB goes serverless", "overview": "Microsoft adds new options to its cloud-scale database.", "image_url": null, "url": "https://www.infoworld.com/article/3588095/azure-cosmos-db-goes-serverless.html", "body": "Azure’s Cosmos DB is one of the platform’s foundations, powering many of its key services. Designed from the ground up as a distributed database, it implements a set of different consistency models allowing you to trade off between performance and latency for your applications. Then there are its different models for working with data, from familiar NoSQL and SQL APIs, to support for Mongo DB’s API, to the Gremlin graph database query engine.There’s enough in Cosmos DB to support most common cloud development scenarios, giving you a consistent data platform that can share data on a global scale. Microsoft often describes it as a “planetary-scale database,” a fitting description.Also on InfoWorld: 13 ways Microsoft Azure beats AWSThe serverless alternative to provisioned throughputFor all the benefits, Cosmos DB does have some downsides; not least its cost. Although there is a relatively limited free option, running it at scale can be expensive, and you need to take that into account when building applications around it. Budgeting for Cosmos DB request units is a complex process that’s hard to get right the first time, especially when you factor in scaling, either manually or automatically.Microsoft has run a preview of a serverless option for Cosmos DB for a while now, based on its core SQL API. It’s an interesting alternative to the traditionally provisioned option. It only charges you when it runs a request and suspends your instance when there’s nothing happening. There will be additional latency in database operations, as your instance needs to spin up when it’s been suspended. Of course there’s a charge for storage, but that’s the same with any Azure database. The initial trial has now been expanded to all of the Cosmos DB APIs, with general availability not too far in the future.Adding a serverless option to Cosmos DB makes a lot of sense for many types of workloads where you’re getting requests in small numbers and in batches. For a small workload with an irregular pattern of operations, a consumption-based pricing model makes a lot of sense—and can save a considerable amount of money over the long term as there is no commitment to provisioned throughput.Costs are low: You pay $0.282 per serverless request unit, for as many as a million RUs in a billing cycle. If you need a more reliable server you can set up an availability zone, though this increases costs by 1.25x. That’s still a reasonable deal, and what you lose in predictability, you gain in lower costs. Storage costs remain the same for both manual and automatic provisioned throughput.Getting started with serverless Cosmos DBJumping in is easy enough. Like a standard Cosmos DB account, you’ll need to provision it to a subscription and add your serverless instance to a resource group. Next choose the API you plan to use for queries, and when asked to choose a capacity mode, pick serverless rather than provisioned throughput. Finally link it to a region, remembering that you can only use serverless in a single Azure region; there is no option for geo-redundancy. You won’t be able to use it with the free tier, either.Once your serverless instance is running you can use its APIs to load data and make queries. Like a standard instance of Cosmos DB, you can build JavaScript functions and triggers that run inside the database, as well as use its many different APIs to manage queries.Serverless Cosmos DB should move out of preview soon, and is adding support for all its APIs, even for its recent Cassandra API. As it’s a public preview, you can set it up and explore its operation straight from the Azure Portal. While in preview there’s no support for ARM or other infrastructure as code deployment tools, though there should be once the service is generally available. You can’t automate configuration and deployment, so you won’t be able to use it as part of a CI/CD (continuous integration/continuous delivery) pipeline for now, as deployments will need to be manual.Building code with serverless Cosmos DBOne place you should get a lot of value from serverless Cosmos DB is in parallel with Azure Functions. The two serverless environments work well together and are ideal for bursty, low-volume, event-driven applications. Serverless Cosmos DB can quickly ramp up from zero to 5,000 request units a second, so if you’re writing code that uses Functions to track error conditions or other alerts, it’s an option for quickly gathering and storing data.Microsoft recommends using it as part of a development environment where you’re capturing data about the requests your full-scale application needs. As provisioning requests units is something of a black art, a serverless implementation running with all your in-database code is a useful development tool. You can set up an operational environment, run your tests, capture the number of requests used, and then use that data to provision throughput for a production deployment.Understanding the serverless limitationsThere are limitations to using a serverless Cosmos DB account. Perhaps the most important is that you don’t get access to multiregion deployments, as serverless accounts only run across a single region. It’s a limitation that makes sense: Multiregion Cosmos DB implementations need multiple instances running at the same time for inter-region replication and consistency. If serverless instances only run when they’re processing requests, then there’s no guarantee another region will be online to handle replication. As a result, there are changes to the Cosmos DB service-level objective for serverless instances, with writes expected to be 30ms or less, and reads 10ms or less.The other key limitation is a maximum 5,000 request units per second. Again, that should be sufficient for most simple or development implementations, but it does require you to keep an eye on your applications and be ready to switch to a provisioned Cosmos DB instance if you regularly go over your limits. At the same time, each serverless container can only store 50GB of data and indexes. Microsoft provides tools in the Azure Portal to help monitor operations, as well as in Azure Monitor.Adding a serverless option to Cosmos DB answers many questions about cost. For low-usage scenarios where you don’t need global coverage, it should be your first choice. Shift to using a provisioned throughput instance only when you’re able to understand your application’s request pattern and can budget accordingly.", "pub_date": "2020-11-03"},
{"title": "Using OPA for cloud-native app authorization", "overview": "How companies like Netflix, Pinterest, Yelp, Chef, and Atlassian use OPA for ‘who-and what-can-do-what’ application policy", "image_url": null, "url": "https://www.infoworld.com/article/3586149/using-opa-for-cloud-native-app-authorization.html", "body": "In the cloud-native space, microservice architectures and containers are reshaping the way that enterprises build and deploy applications. They function, in a word,  than traditional monolithic applications.Microservices are far more distributed and dynamic than their traditional counterparts. A single application might have tens or hundreds of microservices, leading potentially to thousands of separate OS-level processes, each with its own API, deployed across multiple data centers all over the world, and spun up and down dynamically. These architectural differences from monolithic applications cause challenges for developers, operations, and security alike.OPA: A general-purpose policy engine for cloud-nativeUsing OPA to safeguard KubernetesAmong the biggest security challenges is authorization: controlling what users and machines can and can’t do within the application. In the pre-cloud era, authorization was solved at the gateway level—at the point at which end-users interacted with the application’s API—and often the downstream requests were assumed to be authorized.For a microservice application running in the cloud, gateway-level authorization is far less secure. If an attacker compromises a single microservice component, they can run the APIs of all the microservices that make up that application, which gives them complete control over it and all of the data it protects. So while internal APIs and microservice architecture enable better time-to-market for the application, they also pose a substantial security risk and need to be protected by deploying authentication and authorization, just like for public APIs.Changing development strategyNot only are the architectural changes brought about by microservices substantial, but the way people develop microservice-based applications is changing radically too. Traditionally, companies had a few highly coordinated development teams who built the application in tandem, and who released software only after intense scrutiny. Today, numerous dev teams work independently on discrete, individual application components, and they release code whenever they see fit. No team is responsible for “the whole app.” The benefit of having numerous teams is that they can all work independently, and therefore deliver software more efficiently overall.However, having numerous teams also means that every team needs to maintain and enforce authorization policies for each of the services they are responsible for. The real challenge is that security, compliance, and operational teams need to understand these authorization policies; they’re responsible for them. And if you ask 10 teams to implement authorization for their service, you’ll end up with 10 different hardcoded implementations in different programming languages, with different enforcement guarantees and different ways of logging unauthorized access attempts.All of this wreaks havoc on the outside teams that need to modify, or even just review, these authorization policies and understand how they are being enforced. Unifying authorization for the applicationA growing number of organizations (e.g. Netflix, Pinterest, Yelp, Chef, Atlassian) are implementing authorization by providing a unified authorization framework for all development teams. That framework provides a single language or GUI for writing policy, a single way of distributing those policies to the services that need them, a single architecture for enforcing those policies, and a single system for logging decisions.A unified framework for authorization often means faster time-to-market, since developers have less code to write. More importantly, a unified framework guarantees that policies are decoupled from the software systems they govern. There are numerous benefits to this fact: Security and compliance teams can review the policies separately from their implementation; policies enforced in multiple places only need to be written once; policies are enforced consistently across services; decisions are logged consistently, enabling easier SIEM analysis; and policies can be updated dynamically to respond to security events.In the end, because authorization is a topic under the purview of security, operations, compliance, and developers, a unified framework provides a natural point at which all of those different teams can collaborate.Using OPA for application authorizationFor many developers, operations, security, and compliance teams, Open Policy Agent (OPA) has become a primary tool for implementing consistent, secure, and scalable authorization across an organization and across the cloud-native ecosystem. OPA is a domain-agnostic, general-purpose policy engine—a building block for creating and implementing consistent and flexible policies. Developers can plug OPA into different layers of the stack for numerous rules-based use cases, from safeguarding Kubernetes, to managing policy consistently across CICD pipelines and public clouds, to implementing in-app authorization.For this latter use case (today’s topic), OPA is growing quickly in popularity, because it lets companies write policies that are understandable across many different dev teams and translatable across each of a company’s applications and their components. While there are numerous aspects of authorization where OPA applies, here we will focus on three of the most popular: service-level authorization, end user authorization, and policy-suite authorization. OPA for service-level authorizationOne popular OPA use case is service-level authorization, or the “what-can-do-what” policies that determine what applications and their components can do and how they talk to each other—if microservice A can talk to microservice B, for instance. When you have dozens or hundreds of microservices components, it’s essential that you have clear, consistent, and scalable rules about which services can call which APIs. From a security perspective, it’s also critical to have policies that limit the ability of attackers to laterally traverse those APIs.In fact, this was an early OPA use case for Netflix, which provides a textbook example. From an operational perspective, security engineers at Netflix needed to ensure that developers could tightly control which applications and services, from the vast back-end Netflix ecosystem, could call the REST API endpoint on their own service. Without such rules, Netflix services might not scale correctly to maintain high availability, as traffic pathways could become heavily congested.Because speed was a critical requirement of the overall system, service-level authorization decisions needed to happen at a sub-millisecond timescale. Netflix also needed to ensure that a potential attacker couldn’t compromise one service and traverse to others, and they needed a way to hot patch  authorization policy across microservices to stop an attack in its tracks.Because OPA provides a consistent policy framework and decouples authorization decisions from the app itself, it provided Netflix with a way to not only let dev teams control service-level authorization for their own apps, but also to update and federate policies across their environment in real time.OPA for end-user authorizationAnother way that app developers use OPA is for end-user authorization. These policies make sure that users (living, breathing human beings) have the right levels of access in the application and can take only certain actions depending on the user’s permissions and dynamic context. For some applications the end-user is a customer, and for others the end-user is an employee. Sometimes the policies are extensions of the service-level authorization policies described earlier, but allow developers or operators to run microservice APIs if they are on-call. This style of policy makes decisions based on the user’s identity, the corporate groups the user is a member of, and possibly other information loaded dynamically into OPA.Other times OPA is used to implement authorization for the application itself. For example, in a banking application, the policy might stop someone from withdrawing $100 from an account they don’t own. In this case, OPA is a building block for developers who must either build this functionality themselves or use a policy that already exists.Using OPA lets developers future-proof their authorization implementation. While in the early days, authorization requirements were often limited, they usually changed over time. OPA’s flexibility ensures you can start with a simple (say role-based) model and grow to a more sophisticated (say attribute-based) model as the requirements evolve. OPA for product-suite authorizationA third popular way to use OPA is to unify authorization across a suite of different products, sometimes known as entitlements services in sectors like banking. Enterprises often have numerous products deployed across the organization, and they need a unified way of implementing authorization across this product suite. Typically, this need comes from one of two places. Either the company is moving their legacy products into the cloud and the on-premises authorization system can’t come with, or the organization wants to improve the user experience across the product suite.Also on InfoWorld: The best open source software of 2020In the latter case, when different products owned by the same organization have different ways of performing authentication and authorization, the user experience can be poor, because it gives the impression that many companies are delivering these products, or that the organization lacks cohesion and organization. In both of these cases, OPA provides companies with a unified way to implement authorization or entitlements across all of their cloud-native products.Regardless of how or where enterprises are building applications in cloud-native environments, there are numerous authorization problems they must overcome—from end-user authorization, to managing microservices APIs, to deploying consistent authorization policies across the organization. OPA, as an open-source tool, is emerging as a clear standard for helping organizations solve these problems—helping to reduce complexity and development time while increasing the scalability and security of development.Open Policy AgentStyra—newtechforum@infoworld.com", "pub_date": "2020-11-04"},
{"title": "IBM adds code risk analyzer to cloud-based CI/CD", "overview": "IBM Cloud Continuous Delivery’s Code Risk Analyzer scans Python, Node.js, and Java source code in Git repositories for security and legal risks ", "image_url": null, "url": "https://www.infoworld.com/article/3588100/ibm-adds-code-risk-analyzer-to-cloud-based-cicd.html", "body": "Looking to bring security and compliance analytics to devops, IBM has added its Code Risk Analyzer capability to its IBM Cloud Continuous Delivery service.Code Risk Analyzer is described by IBM as a security measure that can be configured to run at the start of a developer’s code pipeline, analyzing and reviewing Git repositories to discover issues with open source code. The goal is to help application teams recognize cybersecurity threats, prioritize application security problems, and resolve security issues. IBM Cloud Continuous Delivery helps provision toolchains, automate tests and builds, and control software quality with analytics.DevSecOps: How to bring security into agile development and CI/CDIBM said that as cloud-native development practices such as microservices and containers change security and compliance processes, it is no longer feasible for centralized operations teams to manage application security and compliance. Developers need cloud-native capabilities such as Code Risk Analyzer to embed into existing workflows. Code Risk Analyzer helps developers ensure security and compliance in routine workflows.In developing Code Risk Analyzer, IBM surveyed source artifacts used by IT organizations in building and deploying applications and in provisioning and configuring Kubernetes infrastructure and cloud services. Existing cloud solutions provide limited security controls across the source code spectrum including vulnerability scanning of application manifests. Thus it is necessary to design a solution that encompasses security and compliance assessment across artifacts.Code Risk Analyzer scans Git-based source code repositories for Python, Node.js, and Java code and performs vulnerability checks, license management checks, and CIS (Center for Internet Security) compliance checks on deployment configurations and generating a “bill of materials” for all dependencies and their sources. Terraform files used to provision cloud services such as Cloud Object Store are scanned to find any security misconfigurations. IBM sought to anchor security controls in standards such as NIST or CIS and to flatten the learning curve while introducing users to new security practices. Developers are shielded from having to understand security definitions and policies, with actionable feedback provided.", "pub_date": "2020-11-04"},
{"title": "A short guide to AWS Savings Plans", "overview": "The potential for cost cutting is real, but you need to understand the nuances of AWS Savings Plans to reap the biggest savings. ", "image_url": null, "url": "https://www.infoworld.com/article/3587346/a-short-guide-to-aws-savings-plans.html", "body": "Cloud buying is getting renewed attention as enterprises seek added cost savings after migrating systems to the cloud in response to COVID-19. It’s time for corporate cloud buyers to learn more about the Amazon Web Services (AWS) Savings Plans, launched in November 2019, which provide cost savings of up to 72% for cloud buyers.A recent survey of more than 50 IT executives on Pulse.qa, a social research platform, showed that only 16% of IT executives believe that AWS Savings Plans are saving them money on their cloud spending. While the potential for cost cutting is real, you need to understand the nuances of this offering to ensure you’re reaping the biggest savings.Also on InfoWorld: How to make the most of the AWS free tierHow AWS Savings Plans workAWS Savings Plans involve a one-year to three-year commitment to a consistent amount of usage measured in dollars per hour. For example, if you commit to $100 of compute usage per hour, you will get the Savings Plans prices on that usage up to $100, and AWS will charge you on-demand rates for any usage beyond the commitment. AWS offers two Savings Plans:The Compute Savings Plan is the most flexible of the two plans. Compute pricing on this plan are up to 66% off on-demand rates.The EC2 Instance Savings Plan provides savings up to 72% off on-demand rates. As a customer, you must commit to a specific instance family in your chosen AWS region. This plan applies automatically to usage regardless of size, operating system, and tenancy within the specified family in the region. You can’t increase a Savings Plan budget in the middle of a period of performance. Instead, you’ll need to provision a new and separate Savings Plan.If you’re running a new AWS account, you may encounter some limitations to your Savings Plans. If this happens to you, it’s best to work with your system integrator or AWS account representative to resolve any restrictions you might be facing.One advantage of AWS Savings Plan is being able to apply savings to on-demand instances, instead of only reserved instances. Compute Savings Plans can apply to instances across different AWS regions. You can also apply Savings Plans to Fargate (instance-based PaaS for containers) and Lambda.AWS Savings Plans don’t apply to Relational Database Service (RDS), Redshift, or ElastiCache (in-memory database and cache). Savings Plans also don’t cover any services you provision from the AWS Marketplace.Visualizing the spend is vital to both you as the end customer and your systems integrator. Your AWS utilization and coverage reports offer the ability to understand AWS Savings Plans cost and usage better. These reports show an aggregate Savings Plan utilization trending over time. You can also use these reports to view each of your Savings Plans with their respective utilization, costs, and savings in a tabular format.Navigating AWS Savings PlansAs a cloud buyer who wants to take full advantage of AWS Savings Plans, you need to “shift left” with cloud spending management. While your organization may not have a cloud economist on staff (or even contract), you can use your cloud management platform to track your cloud spending over the past three to six months to get a historical perspective. That historical data should feed your decision-making process to determine if Savings Plans save you money. The data should serve you well in any service provider negotiations.You can get started with AWS Savings Plans from your AWS dashboard by using the AWS Cost Explorer or by using the API/CLI. If you’re buying cloud services through a systems integrator, then it’s best to inquire about their use of Savings Plans.Understanding your cloud workloads is key to making good use of AWS Savings Plans, according to Venkat Ramasamy, COO for FileCloud, an enterprise file sharing, sync, backup, and remote access solution provider. Ramasamy stresses knowing if you’re running a variable or static workload. Unless you’re building and operating SaaS applications, chances are you’re running static loads, so AWS Savings Plans would make sense for you and could save you a lot of money.Scott Dix, senior solutions delivery manager for Cloudtamer.io, a cloud management platform provider, suggests using a cloud roadmap to guide your decision-making process about AWS Savings Plans. With a cloud roadmap in place, you can model your savings from reserved and on-demand instance spending from the early phases of your cloud projects—not after your first cloud billing surprise.AWS Savings Plans and your system integratorThe AWS Savings Plan, when used by a system integrator, has the potential to create more profit margin when a customer orders on-demand EC2 Instances. The aforementioned Pulse.qa survey of IT executives showed that 64% negotiated hard with their system integrators for the best discounts on their cloud spending. Negotiating is essential because Savings Plans can deliver around 25% savings overall for the SI.System integrators run into risks when they don’t put themselves in their customers’ shoes, according to Amir Shariff, VP of product and marketing at Opsani, a cloud optimization solutions provider. He said that SIs need to be an agent for their customer when managing AWS Savings Plans. “Use the golden rule and guide them through bringing their cloud costs down through shifting their cloud resources,” Shariff advises.Shariff emphasizes the need for the SI to know about the customer’s business so they can best advise them on how to lower their cloud spending. You should expect as much from your system integrator. Also on InfoWorld: Which multicloud architecture will win out?AWS Savings Plans and youDix from Cloudtamer.io recommends erring on the side of caution when committing to reserved instances and AWS Savings Plans. “What ends up happening is folks jump in and think the savings are incredible, so let’s go to the maximum and pay for the long term,” he explains. Proceed slowly, keeping a close eye on your usage and savings. Beyond the commitment that Dix mentioned, treat AWS Savings Plans as a cloud economics exercise. Take a systematic approach. Do the historical analysis and make the process adjustments and reporting changes necessary to give you and your stakeholders the full confidence that AWS Savings Plans are working to deliver significant savings on your AWS bill. ", "pub_date": "2020-11-05"},
{"title": "What will cloud security look like in 3 years?", "overview": "Cloud security has been better than on-premises security for several years now. Increased automation and interoperability will cement its position as a best practice.", "image_url": null, "url": "https://www.infoworld.com/article/3591236/what-will-cloud-security-look-like-in-3-years.html", "body": "Gartner states through 2020, public IaaS workloads will suffer at least 60 percent fewer security incidents than workloads in traditional data centers. When I pointed this out several years ago, many scoffed at the claim.Both the hyperscalers and third-party security providers are spending about 70 to 80 percent of their R&D budgets on supporting public clouds. It should be no surprise that the quality and functionality of most cloud security technologies will be superior to traditional on-premises systems.Also on InfoWorld: When hybrid multicloud has technical advantagesWhat do we have coming down the line in terms of cloud security? Here is what I think the landscape will look like in three years, maybe sooner. Some security systems automate existing processes today, but in five years this will be taken to the next level. We’ll have uber-dynamic interactions with potential threats, backed up by a machine learning system, using intercloud and intracloud orchestration of many different resources to find and stop attacks.This moves cloud security from a passive state to an active one. We’re no longer waiting to get attacked; we can detect when an attack is imminent and automatically challenge the attacker with automated defenses before the first penetration attempt. In some cases, we’ll have the ability to launch automated counterattacks. As we move to a multicloud world, we’re finding that using native security systems for each public cloud is way too laborious and causes complexity and confusion that can lead to breaches.  As I’ve stated before, multicloud is really not about cloud. It’s about the technology that exists between the clouds. Technology that has access to native interfaces, but logically runs above all public clouds. This means that you can orchestrate services to put up a unified defense as well as share knowledgebases as to how to best defend against specific kinds of attacks.  You will also need visibility into all major applications, databases, and storage systems within all public clouds; for instance, being able to see a CPU saturation that should be checked as a possible attack.     You may think of a Terminator-like scenario where the machines turn on us, but the reality is that humans are the weakest link in the security chain. Gartner states that through 2025, 99 percent of cloud security failures will be the customer’s fault. In my world, it’s more like 99.999 percent.  No matter if it’s misconfigurations that leave doors open or plain mistakes because of lack of training, the more we factor humans out of the cloud security equation, the more secure we’ll be.    This goes back to the “automate everything” approach that most security systems will use to provide cloud security within three years. If you’re worried about your job, don’t be. Somebody has to set up these automations and continuously improve them over time.   The bottom line is that security will improve, and the cloud will become the safest place to be. As long as the R&D dollars pour into cloud-based security, this is a foregone conclusion.  ", "pub_date": "2020-11-06"},
{"title": "Technology disenfranchisement is now a thing", "overview": "With the cloud taking over R&D dollars and IT budgets, traditional systems are being left behind.   ", "image_url": null, "url": "https://www.infoworld.com/article/3596317/technology-disenfranchisement-is-now-a-thing.html", "body": "We’ve heard of technical debt: the act of leveraging solutions to speed up deployment with an understanding that the solution is not optimal but will have to be fixed later. However, there may be a new category of technical costs that we’ve yet to understand but will very soon.I’m speaking of technical disenfranchisement. Simply put, this is the concept of traditional technology (legacy technology) not having the power to hold onto R&D dollars as everything moves towards more modern platforms (public cloud computing).Also on InfoWorld: AWS vs. Azure vs. Google Cloud: Which free tier is best?Back in 2018 this was such an obvious trend that it generated a blog post around the forced march to cloud computing. It introduced the idea that according to the R&D spending data, the public cloud had become the favored platform. Most of the funding had begun to funnel into cloud-native and third-party technologies that support public clouds. This included security, monitoring and management, governance, and application development—really the core things that keep systems running and outages rare.Trouble is coming for a great deal of enterprises running legacy technology. If all of the technology spending is going towards cloud-based systems, what’s left over for traditional platforms? Those supporting more traditional systems will find themselves disenfranchised relative to the power they wielded just a few years ago.  There will be some significant disadvantages for companies that continue to use disenfranchised technology, including:Lack of updates and fixes to support ongoing systems maintenance. This can be as innocuous as missing a few performance enhancements, to as serious as missing security patches that could prevent a major breach.Little power to influence the product roadmap. If 80 percent of the R&D dollars are spent elsewhere, you won’t get much of a say as to what the product should be, short term or long term.Reduced ROI. Despite the fact that support and enhancement have diminished or will diminish, the license or subscription fees still remain. You’re not likely to receive any discounts for decreased value delivered and increased risk. What can be done? Not much, other than understand how the market is shifting and plan accordingly. Those most at risk are enterprises that support applications and datasets that are contraindicated for public clouds. They may have to stay on-premises or within an MSP and must deal with technical disenfranchisement.This is nothing new, really. We’ve all owned platforms that were discontinued or perhaps purchased and then neglected. Change forces enterprise IT to look for new digs—hopefully ones that won’t lose market share and interest anytime soon.", "pub_date": "2020-11-10"},
{"title": "Microsoft adds a new Linux: CBL-Mariner", "overview": "Azure’s container infrastructure Linux host gets a public outing on GitHub.", "image_url": null, "url": "https://www.infoworld.com/article/3596347/microsoft-adds-a-new-linux-cbl-mariner.html", "body": "Think of Microsoft and Linux, and you’re likely to think about its work building an optimized Linux kernel for the Windows Subsystem for Linux (WSL). Pushed out through Windows update, Microsoft supports all the WSL2 Linux distributions, including Ubuntu and SUSE.But WSL2’s kernel isn’t Microsoft’s only Linux offering. We’ve looked at some of the others here in the past, including the secure Linux for Azure Sphere. Others include the SONiC networking distribution designed for use with Open Compute Project hardware and used by many public clouds and major online services, and the hosts for Azure ONE (Open Network Emulator) used to validate new networking implementations for Azure.Also on InfoWorld: What is Jamstack? The static website revolution upending web developmentMicrosoft’s Linux Systems GroupWith an ever-growing number of Microsoft Linux kernels and distributions, there’s now an official Linux Systems Group that handles much of the company’s Linux work. This includes an Azure-tuned kernel available as patches for many common Linux distributions, optimizing them for use with Microsoft’s Hyper-V hypervisor, and a set of tools to help deliver policy-based enforcement of system integrity, making distributions more secure and helping manage updates and patches across large estates of Linux servers and virtual machines.The team recently released a new Linux distribution: CBL-Mariner. Although the release is public, much of its use isn’t, as it is part of the Azure infrastructure, used for its edge network services and as part of its cloud infrastructure. The result is a low-overhead, tightly focused distribution that’s less about what’s in it, and much more about what runs on it.Introducing CBL-Mariner: Microsoft’s Linux container hostInvesting in a lightweight Linux such as CBL-Mariner makes a lot of sense, considering Microsoft’s investments in container-based technologies. Cloud economics require hosts to use as few resources as possible, allowing services such as Azure to get a high utilization. At the same time, Kubernetes containers need as little overhead as possible, allowing as many nodes per pod as possible, and allowing new nodes to be launched as quickly as feasible.The same is true of edge hardware, especially the next generation of edge nodes intended for use with 5G networks. Here, like the public cloud, workloads are what’s most important, shifting them and data closer to users. Microsoft uses its growing estate of edge hardware as part of the Azure Content Delivery Network outside its main Azure data centers, caching content from Azure Web apps and from hosted video and file servers, with the aim of reducing latency where possible. The Azure CDN is a key component of its Jamstack-based Azure Static Websites service, hosting pages and JavaScript once published from GitHub.In the past Red Hat’s CoreOS used to be the preferred host of Linux containers, but its recent deprecation means that it’s no longer supported. Anyone using it has had to find an alternative. Microsoft offers the Flatcar Linux CoreOS-fork for Azure users as part of a partnership with developers Kinvolk, but having its own distribution for its own services ensures that it can update and manage its host and container instances on its own schedule. Development in public is available for anyone who wants to make and use their own builds or who wants to contribute new features and optimizations, for example adding support for new networking features.Running CBL-Mariner and containersOut the box, CBL-Mariner only has the basic packages needed to support and run containers, taking a similar approach to CoreOS. At heart, Linux containers are isolated user space. Keeping shared resources to a minimum reduces the security exposure of the host OS by making sure that application containers can’t take dependencies on it. If you’re using CBL-Mariner in your own containers, ensure that you’ve tested any public Docker images before deploying, as they may not contain the appropriate packages. You may need to have your own base images in place as part of your application dockerfiles.CBL-Mariner uses familiar Linux tools to add packages and manage security updates, offering updates either as RPM packages or as complete images that can be deployed as needed. Using RPM allows you to add your own packages to a base CBL-Mariner image to support additional features and services as needed.Getting started with CBL-Mariner can be as simple as firing up an Azure service. But if you want hands-on experience or want to contribute to the project, all the source code is currently on GitHub, along with instructions for building your own installations. Prerequisites for a build on Ubuntu 18.04 include the Go language, the QEMU (Quick EMUlator) utilities, as well as rpm.Build your own installation using the GitHub repositoryYou have several different options for building from the source. Start by checking out the source from GitHub, making a local clone of the project repository. Various branches are available, but for a first build you should choose the current stable branch. From here you can build the Go tools for the project before downloading the sources.For quick builds you have two options, both of which use prebuilt packages and assemble a distribution from them. The first, for bare-metal installs, creates an ISO file ready for install. The second, for using CBL-Mariner as a container host, builds a ready-to-use VHDX file with a virtual machine for use with Hyper-V. An alternative option builds a container image that can be used as a source for your Mariner-based dockerfiles, giving you everything you need to build and run compatible containers with your applications.If you prefer to build from source, the option is available, although builds will be considerably slower than using precompiled packages. However, this will allow you to target alternative CPUs, for example building a version that works with the new generation of ARM-based edge hardware similar to that being used for AWS’s Graviton instances. You can bootstrap the entire build toolchain to ensure that you have control over the whole build process. The full build process can even be used to build supported packages, with the core files listed in a JSON configuration file.Once built, you can start to configure CBL-Mariner’s features. Out the box, these include an iptables-based firewall, support for signed updates, and a hardened kernel. Optional features can be set up at the same time, with tools to improve process isolation and encrypt local storage, important features for a container host in a multitenant environment where you need to protect local data.The result is an effective replacement for CoreOS, and one I’d like to see made available to Azure users as well as to Microsoft’s own teams. CBL-Mariner may not have the maturity of other container-focused Linuxes, but it’s certainly got enough support behind it to make it a credible tool for use in hybrid cloud and edge network architectures, where you’re running code on your own edge servers and in Microsoft’s cloud. If Microsoft doesn’t make it an option, at least you can build it yourself.", "pub_date": "2020-11-10"},
{"title": "The ‘distributed cloud’ isn't emerging — it’s already here", "overview": "Assigning data storage and processing to specific cloud regions is a good move, but still benefits from centralized control and risk awareness.   ", "image_url": null, "url": "https://www.infoworld.com/article/3596562/the-distributed-cloud-isnt-emergingits-already-here.html", "body": "According to Gartner, “Distributed cloud is the distribution of public cloud services to different physical locations, while the operation, governance and evolution of the services remain the responsibility of the public cloud provider.” Analysts go on to explain that the distributed cloud provides a flexible agile environment for applications and data that require low-latency, data cost reduction, and data residency.This idea is not new; I’ve used it to remove latency and/or comply with data sovereignty laws from time to time. At its essence, the advantage is for end-users to have cloud computing resources closer to the physical location where the business activities happen, thus reducing latency.Also on InfoWorld: When hybrid multicloud has technical advantagesOf course, analyst groups love to assign buzzwords to emerging patterns, and I’m no different (although more people listen to a large analyst firm than to me). However, lately we seem to be naming things that are…well…already a thing. Distributed cloud falls in that category.First, public clouds have been distributed for some time, allowing developers and architects to assign specific processing and data storage to a single geographical region or regions. Second, leveraging regions to tune processing and data storage for security, performance, and compliance has been a best practice for years.Let’s look at the merits of the distributed cloud and see if this should be an option for your cloud-based workloads. Moving processing and data out to different regions is actually pretty easy. We’ve all assigned regions when provisioning storage or compute resources. However, you do have to think about coordinating these resources, including synchronizing data and distributing processing. Fortunately, we’ve done this for single distributed public clouds and multicloud also. The solutions are pretty well understood, and the technology works effectively. Multicloud is the way that most will experience distributed clouds. Not only will you be distributing processing and data across a single cloud provider, it could be as many as five providers, with processing and data existing on multicloud deployments. This will increase the number of regions you’re deployed on, multiplied by the same number of cloud brands that you’re using. I don’t mean to pop the distributed cloud balloon. It’s a sound architectural option, but you need to understand when to use it and why.", "pub_date": "2020-11-13"},
{"title": "Why AWS leads in the cloud", "overview": "The offshoot of Amazon’s online bookstore has led the public cloud market for a decade. How did it get there? Will its dominance continue? ", "image_url": null, "url": "https://www.infoworld.com/article/3596813/why-aws-leads-in-the-cloud.html", "body": "The rumors of Amazon Web Services’ fall from the pinnacle were premature. In the push to democratize cloud computing services, AWS had the jump on everyone from the beginning, ever since it was spun out of the mega retailer Amazon in 2002 and launched the flagship S3 storage and EC2 compute products in 2006. It still does. AWS quickly grew into a company that fundamentally transformed the IT industry and carved out a market-leading position, and has maintained that lead — most recently pegged by Synergy Research at almost double the market share of its nearest rival Microsoft Azure, with 33 percent of the market to Microsoft’s 18 percent.Also on InfoWorld: 14 ways AWS beats Microsoft Azure and Google CloudMarket tracker data from IDC for the second half of 2019 also puts AWS in a clear lead, with 13.2 percent of the public cloud services market, narrowly ahead of Microsoft with 11.7 percent.As with any business, Amazon’s cloud success comes down to a confluence of factors: good timing, solid technology, and a parent company with deep enough pockets to make aggressive capital investments early on.There are other, unique factors that have led to the success of AWS, however, including a relentless customer focus, a ruthless competitive streak, and continued commitment to “dogfooding,” or eating your own dog food — a perhaps unfortunate turn of phrase that has proliferated through the tech industry since the late eighties.Dogfooding refers to a company making a bet on its own technology — in Amazon’s case by making it publicly available as a product or service. This is what Amazon did with S3 and EC2 in 2006, and it’s what Amazon has been doing with almost all of its AWS product launches since.We asked the experts how AWS has been able to dominate the public cloud market to date, and, with worldwide adoption of cloud services due to continue climbing, according to the 2020 IDG Cloud Computing Survey, whether AWS can stay on top of the pile for years to come.First-mover advantageThere is no escaping the fact that Amazon’s jump on the competition has put them in the ascendancy from day one, giving them a six-year head start over its nearest competitor, Microsoft Azure.These years didn’t just help position AWS as the dominant cloud computing service provider in people’s minds, it also furnished the company with years of feedback to crunch through and better serve its customer base of software developers, engineers, and architects.“They invented the market space, there wasn’t the concept of public cloud like this before,” Dave Bartoletti, vice president and principal analyst at Forrester said. “We have been renting computing services for 30 or 40 years. Really what AWS did was establish in a corporate environment for a developer or IT person to go to an external service and start a server with a credit card and do computing somewhere else.”As Bartoletti notes, AWS wasn’t just first to market, it also had the deep pockets of its parent company, allowing it to blow anyone else out of the water. “They outspent their rivals,” he bluntly assessed.That being said, not all first-movers lead their market as definitively as AWS is — just ask the founders of Netscape.“Early movers don’t always have an advantage,” Deepak Mohan, research director for cloud infrastructure services at IDC, said, noting that AWS was especially rigorous in creating and bringing products to market. “Being a high-quality company and delivering a high-quality product and being responsive to customer needs all play equally important parts.” Also on InfoWorld: 13 ways Microsoft Azure beats AWSA special relationshipMohan points to Amazon’s superior ability to “eat its own dog food” as a key driver towards its success, as the cloud division had to address significant technology challenges faced by the huge ramping up of scale Amazon was seeing in the aftermath of the dotcom bubble bursting.“You have to consider the relationship between AWS and Amazon the e-commerce company,” said Ed Anderson, distinguished VP analyst at Gartner — which has AWS as its clear leader in its latest Magic Quadrant for Cloud Infrastructure and Platform Services.Just as customers of Google Cloud today want to “run like Google,” early AWS customers wanted to leverage the technology that had enabled Amazon to grow into an e-commerce giant so quickly.“A hallmark of AWS has been how technical and capable it has been,” Anderson notes. “And being really oriented around that ‘builder’ audience of developers, implementers, and architects,” he adds. “As a consequence, the sales team is very technical and capable in having those conversations, which means the experience customers have is really smooth.”Customer obsessionIt is that attention to customer needs that has long been a hallmark of the AWS value proposition, even if they don’t always get it right.As Amazon founder and CEO Jeff Bezos wrote in a 2016 letter to shareholders: “Customers are always beautifully, wonderfully dissatisfied, even when they report being happy and business is great. Even when they don’t yet know it, customers want something better, and your desire to delight customers will drive you to invent on their behalf.”It is this attention to what customers want — and don’t yet know what they want, to paraphrase Steve Jobs, by way of Henry Ford — which has been codified in Amazon’s leadership principles.“Leaders start with the customer and work backwards. They work vigorously to earn and keep customer trust. Although leaders pay attention to competitors, they obsess over customers,” Amazon’s leadership principles state.“That is a value I see exhibited over and over at AWS,” Anderson at Gartner observes. “This attention to customer requirements and the needs of builders and developers and architects, that has prioritized the features they built and is tightly aligned.”“They are incredibly customer focused and everything they build is driven by the customer,” Bartoletti at Forrester adds.” To maintain that as their large pool of customers continues growing gives them the advantage of knowing what their customers want.”Take the 2019 release of the hybrid cloud product AWS Outposts as an example. Instead of squaring neatly with Amazon’s public cloud-centric view of the world, Outposts met the customer’s needs in a different sphere — their on-prem data centers.Also on InfoWorld: 13 ways Google Cloud beats AWSEverything services-firstA key move made by Bezos in the early days of commercial cloud computing was formalizing the way AWS would build and expose products to its customers.Referencing an early-2000s internal email mandate from Bezos, former Amazon and Google engineer Steve Yegee paraphrased in his Google Platforms Rant, from 2011, that: “All teams will henceforth expose their data and functionality through service interfaces. Teams must communicate with each other through these interfaces.” Lastly, “Anyone who doesn’t do this will be fired,” Yegge added.With this mandate, Bezos spurred the creation of an enormous service-oriented architecture, with business logic and data accessible  through application programming interfaces (APIs). “From the time Bezos issued his edict through the time I left [in 2005], Amazon had transformed culturally into a company that thinks about everything in a services-first fashion. It is now fundamental to how they approach all designs, including internal designs for stuff that might never see the light of day externally,” Yegge wrote.The enormous service-oriented architecture had effectively transformed an infrastructure for selling books into an extensible, programmable computing platform. The online bookstore had become a cloud. The everything store for enterprise buildersAll of this has led to an unrivalled breadth and maturity of services available to AWS customers.And while Amazon had the jump on the competition, it hasn’t rested on its laurels, regularly pioneering new services in the public cloud, such as the cloud-based data warehouse Redshift, the high-performance relational database service Aurora, and the event-based serverless computing platform Lambda, after developing the latter service for its AI-driven virtual assistant Alexa.“Yes, Google Cloud and Microsoft have ‘closed the gap,’ but AWS is still more capable on breadth of offerings and the maturity of those individual services,” Anderson at Gartner says. “I would say when it comes to market perception, most customers feel Azure and AWS are effectively on par and Google slightly behind. In terms of pure capability, though, AWS is a more mature architecture and set of capabilities, and the breadth is wider.”At the AWS re:Invent conference in December of 2019, AWS said it had 175 services, with a wealth of options and flavors across compute, storage, database, analytics, networking, mobile, developer tools, management tools, IoT, security, and enterprise applications.“Without doubt the market leader, AWS often wins on developer functionality, due to the breadth of its services as a result of its first-mover advantage,” Nick McQuire, vice president of enterprise research at CCS Insight says. “AWS has also done a good job at translating its scale into economic benefits for customers, although there are times where cloud can be cost prohibitive.”This broad set of capabilities can also be seen as a negative for some, with the service catalog representing a dizzying maze of services and options, but this level of choice has also proved a great resource for engineers.Bartoletti at Forrester, who has called AWS the cloud “everything store” for enterprise builders, points to a key difference in approach. “AWS can have three to four different database services, and they don’t care which one you use, as long as you use it at Amazon,” he notes. “Traditionally vendors would have had to pick one and run with it. That makes AWS tough to compete with.”Also on InfoWorld: 6 ways Alibaba Cloud challenges AWS, Azure, and GCPThe next phase for cloud computingThe age of AWS dominance shows no sign of slowing, but the competition is fierce.“Microsoft has been able to close the gap by being open source focused and commercializing that in their cloud as fast as AWS,” Bartoletti says. “Google is working hard not to over rotate on the bleeding edge and focus on helping enterprises migrate workloads to the cloud.”Breadth and maturity of services, underpinned by strong engineering chops and relentless customer focus, look to keep AWS ahead of the curve for some time. Now, the company’s ability to simplify the adoption of new technology for enterprise customers through managed services will be the litmus test for the next wave of cloud computing adoption. It will also determine how AWS will fare against the ongoing fierce competition from Microsoft Azure and Google Cloud.“I think it is far from given that AWS will always dominate the cloud market,” Mohan at IDC says. At the same time, he acknowledges that the competitors have a lot of catching up to do.“Google is still quite a ways behind, and Microsoft, while a force, has certain advantages in the enterprise market,” Mohan says. “It is conceivable that companies will get closer, but I don’t expect any substantial changes in the next few years... There is a leap in capacity and scale that is yet to be built. All of this gives [AWS] a clearly dominant position for now.”As Warren Buffet said, “Never bet against America.” And when it comes to the public cloud market, we’ve learned it would be just as foolish to bet against Amazon.", "pub_date": "2020-11-16"},
{"title": "Figuring out programming for the cloud", "overview": "A new model of declarative programming languages has emerged for building infrastructure as code, promising more simplicity and safety than imperative languages. ", "image_url": null, "url": "https://www.infoworld.com/article/3597054/figuring-out-programming-for-the-cloud.html", "body": "In the last dozen years or so, we’ve witnessed a dramatic shift from general purpose databases (Oracle, SQL Server, etc.) to purpose-built databases (360 of them and counting). Now programming languages seem to be heading in the same direction. As developers move to API-driven, highly elastic infrastructure (where resources may live for days instead of years), they’re building infrastructure as code (IaC). But  to build IaC remains an open question. For a variety of reasons, the obvious place to start was with imperative languages like C or JavaScript, telling a program  to achieve a desired end state.Also on InfoWorld: Tim O’Reilly says the golden age of the programmer is overBut a new model of declarative languages — like HCL from HashiCorp or Polar from Oso — has emerged, where the developer tells the program  the desired end state is, not overly worrying about how it gets to that state. Of the two approaches, declarative programming might well be the better option, resulting in leaner, safer code. Let’s look at why this is.The why of declarative programmingWhen I asked Sam Scott, co-founder and CTO of Oso, whether we really needed another programming language, his answer was “yes.” The longer version: “With imperative languages there is often a mismatch between the language and its purpose; that is, these languages were designed for people to build apps and scripts from the ground up, as opposed to defining configurations, policies, etc.”Introducing new, declarative languages like Oso’s Polar may actually save us from language proliferation, Scott went on: “It’s not just about introducing another language for solving a specific set of problems. Rather, it’s creating something to avoid people from having to invent their own language time and time again to do some form of embedded logic.”Take, for example, the JSON code written for AppSync authorization:To get to balance between data and logic, the developer has written inline, comment-style permissions in Apache Velocity Template Language. This is just one example of the gymnastics developers go through to try to express authorization (“authZ”) logic using a combination of static configuration and templating.But, really, this isn’t about whether we should use imperative programming, VMware’s Jared Rosoff stressed in an interview. Instead, “It’s a question of ‘We don’t currently use a programming language to express authorization rules, but maybe we should…’.” After all, we use data, not a programming language, to express authZ rules. This is fine, Rosoff notes, when your authZ rules are pretty simple. You basically have a lookup table that you consult when a request comes in to decide if it’s authorized or not. Easy, peasy.Imperative may not fit IaCBut the AppSync example above shows how the data-oriented, imperative approach gets complicated quickly. “What started as a relatively simple JSON document evolved into something with branching and conditionals and variables,” Rosoff continued. In taking this approach, the developer is trying to recreate language-like semantics within the syntax of static data, resulting in a “crappy programming language that has terrible syntax, no libraries, no debuggers, etc.” It’s not ideal.It’s also not concise, says Scott. “It’s hard to encode your authorization logic in traditional languages,” he notes. “Doing it in a special-purpose, declarative language like Polar is more expressive and more concise.”As Scott further explained, there are a lot of problem domains where if a developer were to try to encode a process in Java or Python it would require thousands of lines of code, which is a problem in and of itself, but becomes more problematic in that it obscures the actual business logic. By contrast, a declarative language can accomplish the same job but in tens of lines of code instead.So instead of forklifting configuration syntax onto imperative languages, says Rosoff, we should be using little programs to handle authZ and related functions. There are many programming problems today where we use data to configure a system but it might make more sense to use a program, according to Rosoff. To name a few:AuthorizationDeployment templates (e.g. Terraform, Ansible, etc.). “Each of these tries to create language-like constructs within the confines of a data language with terrible results.” Pulumi offers an interesting approach to this.Workflows. Temporal is an interesting example of taking what would have previously been done in a diagrammatic workflow language and bringing it into a more general purpose programming language format.Just enough freedom, with simplicity and safetyThe trick, says Rosoff, is to give the programmer enough of a language to express the authorization rule, but not so much freedom that they can break the entire application if they have a bug. How does one determine which language to use? Rosoff offers three decision criteria:Does the language allow me to express the complete breadth of programs I need to write? (In the case of authorization, does it let me express all of my authZ rules?)Is the language concise? (Is it fewer lines of code and easier to read and understand than the YAML equivalent?)Is the language safe? (Does it stop the programmer from introducing defects, even intentionally?)We still have a ways to go to make declarative languages the easy and obvious answer to infrastructure-as-code programming. One reason developers turn to imperative languages is that they have huge ecosystems built up around them with documentation, tooling, and more. Thus it’s easier to start with imperative languages, even if they’re not ideal for expressing authorization configurations in IaC.We also still have work to do to make the declarative languages themselves approachable for newbies. This is one reason Polar, for example, tries to borrow imperative syntax.Given the benefits of a declarative approach, however, it may simply be a matter of time until they become standard for IaC.", "pub_date": "2020-11-16"},
{"title": "The ‘intelligent edge’ isn’t always the smartest choice ", "overview": "Just because you can, doesn’t mean you should. Complexity, latency, and network outages may give you pause.", "image_url": null, "url": "https://www.infoworld.com/article/3597388/the-intelligent-edge-isnt-always-the-smartest-choice.html", "body": "The notion of the intelligent edge has been around for a few years. It refers to placing processing out on edge devices to avoid sending data all the way back to the centralized server, typically existing on public clouds.While not always needed, the intelligent edge is able to leverage machine learning technology at the edge, moving knowledge building away from centralized processing and storage. Applications vary, from factory robotics to automobiles to on-premises edge systems residing in traditional data centers. It’s good in any situation where it makes sense to do the processing as close to the data source as you can get.Also on InfoWorld: Amazon, Google, and Microsoft take their clouds to the edgeWe’ve wrestled with this type of architectural problem for many years. With any distributed system, including cloud computing, you have to consider the trade-off of process and storage placement on different physical or virtual devices. The intelligent edge is no different. Keep in mind that your edge devices to centralized systems are always many-to-one. Managing a centralized systems is fairly simplistic considering that it’s in one virtual location. When you have to manage hundreds or thousands of intelligent edge devices, including configuration management, security, and governance, it becomes an operational nightmare. I’m finding companies that pushed processing and data storage out to the edge often pull them back to the centralized servers just due to management complexity. We depend on networks to keep us connected with edge computers, in many cases mobile and thus connected via cellular networks. You’ll probably have to deal with disconnected situations more often than you like, and you must figure out a way to ensure that these outages and performance issues don’t kill your overall system, both edge and centralized.If you don’t, you’ll find that data does not sync and processing is not managed properly. You may get to a point where the systems become unreliable and untrusted. Try explaining to a commercial pilot that the in-flight engine diagnostics on the intelligent edge failed due to a network problem. The resulting flameout won’t go over well on the flight deck.Of course, not all edge limitations are that profound. Typically you’re making architectural mistakes that won’t be discovered until the system begins to scale. By that time, too much has been committed to the intelligent edge architecture, and fixes require a systemic change. Try telling your boss that. It’s won’t go over well there, either. Make sure you consider the trade-offs.", "pub_date": "2020-11-17"},
{"title": "Public clouds and big tech target low-code capabilities", "overview": "Big players are joining the low-code game, offering platforms to help developers and amateurs create apps for everything imaginable.", "image_url": null, "url": "https://www.infoworld.com/article/3598071/public-clouds-and-big-tech-target-low-code-capabilities.html", "body": "I’ve been using low-code and no-code platforms for almost two decades to build internal workflow applications and rapidly develop customer-facing experiences. I always had development teams working on Java, .NET, or PHP applications built on top of SQL and NoSQL datastores, but the business demand for applications far exceeded what we could develop. Low-code and no-code platforms provided an alternative option when the business requirements matched the platform’s capabilities.I recently shared seven low-code platforms developers should know and what IT leaders can learn from low-code platform CTOs. Many of these platforms have been around longer than a decade, and some support tens of thousands of business applications. Over time these platforms have improved capabilities, developer experiences, hosting options, enterprise security, devops tools, application integrations, and other competencies that enable rapid development and easy maintainenance of functionally rich applications.Also on InfoWorld: 7 low-code platforms developers should knowSo when the public clouds and big tech companies got more interested in low-code platforms, I was skeptical. First, the public clouds target development and engineering teams that want to code applications, automate CI/CD (continuous integration/continuous deployment) pipelines, and instantiate infrastructure as code. Developing products for citizen developers and others who want to develop with low-code requires different experiences, tools, and functionality.Second, the stand-alone low-code platforms have evolved through multiple computing paradigms; some go back to client-server days. I had my doubts that newcomers would offer matching capabilities and the strategies and motivations to re-engineer to remain relevant.I found a mix of different developer experiences and advanced capabilities from the public cloud and big tech companies. In some cases, the low-code platforms are woefully behind stand-alone platforms. In other cases, they are demonstrating how low-code can enable machine learning, chatbots, voice interfaces, spatial search, and more.Power Apps leads with apps, integration, and machine learningMicrosoft Power Apps and Power Automate (formally Microsoft Flow) went general availability in October 2016, and new versions are released weekly. In that time, Power Apps has evolved into a very rich low-code application development environment to build forms and workflows that connect to multiple data sources. Power Automate now has more than 400 connectors that developers can use to move data in and out of applications and a separate robotic process automation skill.What sets Power Apps apart is AI Builder, a set of low-code capabilities enabling developers to connect data to natural language processing, image processing, and machine learning. You can train models to categorize and identify entities in text, extract information from forms, identify objects in images, or run predictive models. Some examples include training models to pull invoice data from PDF forms or process photos to update product inventory automatically.The newest offering, Power Virtual Agents, a low-code chatbot platform, went general availability at the end of 2019. The combination of building a low-code app with an embedded chatbot and integrating workflow with other SaaS solutions can be very powerful. Some example tutorials include chatbots that help users submit project ideas, answer questions during the COVID crisis, or perform simple functions such as getting today’s weather.Apple’s, Google’s, and Oracle’s ongoing investment in low-code capabilitiesApple and Google have low-code platforms under independently managed subsidiaries. Apple spun off FileMaker back in 1986 and in 2019 they rebranded the company back to its original name, Claris. The low-code platform is now Claris FileMaker, and they added Claris Connect, a platform for integrations, by acquiring Stamplay in 2019.FileMaker is its 19 major version, and Claris recently launched new add-ons, including calendars, Kanban boards, and photo galleries. FileMaker can now integrate with machine learning models and have Siri answer questions in FileMaker applications. The platform has recently been used by a hospital to track and report patient progress and care and by schools to overcome COVID-19 back-to-school challenges.Meanwhile, Google bought its way into the low-code scene by acquiring AppSheet in early 2020. I spoke with Amit Zavery, VP/GM and head of platform at Google Cloud, about AppSheet’s growth and emerging capabilities. “Over the last nine months, we also started to see a lot of end-user applications built on top of AppSheet—initially started with the idea that you can build pretty much a departmental app, but people want to automate approval processes, embed video conferencing, inspect documents, and connect to APIs.”AppSheet has many of the options you find in other low-code platforms, including integrations, forms, and views, but its advanced capabilities demonstrate Google’s plans. For example, the integration of G-Suite and AppSheet can be used to develop intelligent workflows, while developers can enable connection to their APIs using Apigee as a data source.Some interesting AppSheet applications include a med student’s app that organizes related medical conditions and concepts, a housing association’s app that helps manage back-end operations, and an industrial services company’s app that improve fieldwork efficiency. There are also sample applications to help reduce workplace risk during COVID-19 and others developed for specific industries, including nonprofits and manufacturing.Oracle is a longtime proponent of low-code development. I reviewed Oracle Application Express (APEX), which has a history dating back to an acquisition made in 2006. APEX has more than 50,000 customers, and developers are generating more than 3,000 applications daily. APEX enables no-code data-driven applications. As Michael Hichwa, SVP Software Development at Oracle states, “APEX uses the power of SQL in a business context.”One standout feature uses the Oracle database to manage location data exposed through an APEX-developed application. An example is this application to optimize grocery delivery routes during COVID. The APEX.world community site has more than 30 applications related to COVID-19.Amazon, Salesforce, SAP, IBM, and Alibaba increase low-code capabilitiesThe low-code story doesn’t end there, as other public clouds and tech giants add low-code features and platforms.Amazon launched Honeycode in June 2020, a ”very lightweight tool” designed for citizen developers to build Web and mobile applications.Salesforce has a long history of providing developer tools, and its latest additions include Salesforce Lightning and the incorporation of MuleSoft API integration capabilities.SAP partnered with low-code platform provider Mendix to extend S/4HANA functionality and integrate with machine learning, artificial intelligence, IoT (Internet of Things), and other technologies.IBM has several low-code offerings on the IBM Cloud, including a partnership with Mendix and the June 2020 release of IBM Cloud Pak for Automation with low-code integration of machine learning for operational decisions.Alibaba launched Yida Plus in 2019, a low-code development platform used in retail, hospitality, manufacturing, health care, energy, and education.Software and application development are strategic for organizations investing in customer experiences and employee productivity and wanting to become data driven. The tasks of automating, integrating, and developing experiences is getting easier as more public cloud and technology companies enable low-code and citizen development capabilities.", "pub_date": "2020-11-23"},
{"title": "AI is now a C-suite imperative", "overview": "Survey from Appen finds that C-level involvement in AI initiatives has jumped significantly ", "image_url": null, "url": "https://www.infoworld.com/article/3563885/ai-is-now-a-c-suite-imperative.html", "body": "Executive involvement in enterprise artificial intelligence (AI) initiatives is growing rapidly and more emphasis is being placed on high-quality training data. Both C-suite ownership of AI and budgets over $500K nearly doubled in 2020 due to the COVID-19 pandemic serving as a catalyst for accelerated AI initiatives. A key lesson learned from the pandemic is that businesses need to be ready for anything that requires a high level of business agility. It’s Darwinism at its finest as businesses that can adapt to market trends faster than their competition can become market leaders and maintain that position. Those that can’t do this will fade into obscurity with many going away.How data analysis, AI, and IoT will shape the post-pandemic ‘new normal’But how do business leaders even know what decisions to make? There is a massive amount of data to be analyzed and people can’t process the information fast enough to find those key insights that drive business change. Machines can work at infinitely higher speed and the pressure on the C-suite has never been higher. Now the execs are turning to AI to help them make the best decisions in as short a time as possible.These findings come from Appen Limited’s annual State of AI and Machine Learning Report, which surveyed 374 business and technology decision-makers between April and May of this year. The report measured the state of AI for enterprises with more than 1,000 full-time employees and commercial organizations with fewer than 1,000 full-time employees.C-suite involvement in AI takes a massive jumpThe report uncovered an important change. The C-suite is now more engaged in AI initiatives than ever before, with a whopping 71 percent of organizations reporting executive involvement. In comparison, only 39 percent of executives owned AI initiatives in 2019. CTOs made up 42 percent of the 71 percent of C-suite AI ownership, which partially explains why AI budgets are increasing in 2020. COVID-19 may be temporary but don’t expect AI to be. I believe it will be the biggest driver of business change since the rise of the Internet.Executives see AI as invaluable to their business success. This is true for companies of all sizes across different industries. For 27 percent of survey respondents, enterprise AI budgets have exceeded $1M, while 10 percent said their AI budget is more than $5M. These numbers are expected to continue rising steeply as businesses adopt AI on a global scale.C-level interest brings a focus on risk and ethicsWith increased C-suite visibility, businesses are focusing more on risk management, governance, and ethics as key aspects of rolling out AI initiatives globally or to their full user base. As companies start using AI to supplement human capabilities, responsible use of AI must be part of the process to ensure fairness, privacy, transparency, and security and avoid inappropriate uses of data.  Although businesses believe responsible AI is important to their success, only 25 percent view unbiased AI as mission-critical. Half of the respondents either don’t see it as a major issue or are just starting to think about it. The findings indicate businesses must take a more proactive approach toward responsible AI. Simply having accurate data or algorithms isn’t enough. AI governance with clear ethical standards is also necessary.Data management gets in the way of quality AIAnother big challenge for businesses is data management. Three out of four companies surveyed by Appen said they update their AI models at least quarterly. For 40 percent of the respondents, lack of data or data management are the leading roadblocks to effectively utilizing AI in the enterprise.Most respondents (93 percent) expressed the need for high-quality training data, as businesses continue to deal with more data types and complex data compared to previous years. The report revealed many businesses are still behind on AI adoption, especially when it comes to training data. For this reason, company leaders are going beyond in-house resources and turning to third-party providers to help carry out AI deployments.Data management and quality comes up in almost every AI discussion I have with business leaders. Companies have massive amounts of data — more than ever before and it continues to grow exponentially but much of the data resides in silos and is in a myriad of different formats. In data sciences, there’s an axiom that states “good data leads to good insights.” The reverse holds true too, as bad data will lead to bad insights and partial data will lead to partial insights. Getting a handle on data and data quality needs to be as important as the AI initiatives themselves.With AI, cloud is the wayIn 2020, four times as many business and technology decision-makers reported using cloud machine learning providers, including Microsoft Azure (49 percent), Google Cloud (36 percent), IBM Watson (31 percent), AWS (25 percent), and Salesforce Einstein (17 percent). Each of these providers saw double-digit adoption of cloud machine learning tools in 2020 versus 2019, attributing the surge to businesses looking for solutions that can scale as their AI initiatives grow in complexity.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesDespite dealing with a global pandemic, the majority (70 percent) of businesses surveyed during those months did not expect COVID-19 to negatively impact AI strategies. In fact, nearly half of businesses have fast-tracked their AI strategies, with 20 percent reporting significant acceleration. Only 9 percent of businesses anticipated substantial delays.Increased C-suite investment in enterprise AI is an indication that businesses are choosing to spend money on key initiatives even in a time of crisis. Clearly, these companies view AI-driven agility as the key to both short term survival and long-term leadership. With the right tools and strategies in place, businesses can access clean, high-quality, ethical data to successfully implement AI across the enterprise.", "pub_date": "2020-06-25"},
{"title": "How AIOps improves application monitoring", "overview": "Devops and site reliability engineers are vital to keep applications functioning. AIOps boosts effectiveness another notch", "image_url": null, "url": "https://www.infoworld.com/article/3541308/how-aiops-improves-application-monitoring.html", "body": "IT operation teams use many tools to monitor, diagnose, and resolve system and application performance issues. In a recent survey of 1,300 IT professionals on the future of monitoring and AIOps, 42 percent report using more than 10 monitoring tools; 19 percent use more than 25 tools.That’s a lot of technology just to keep the lights on and provide the data required to monitor, alert, research, and resolve application incidents.Also on InfoWorld: Applying devops in data science and machine learningMonitoring tools are not one size fits all, especially for organizations running mission-critical applications in multicloud environments. As organizations invest in mobile apps, microservices, dataops, and data science programs, new monitoring tools are being added to provide domain-specific monitoring capabilities.AIOps platforms aim to simplify this landscape of monitoring tools. AIOps helps organizations that require high application service levels better manage the complexity of their monitoring tools and IT operational workflows. As the name suggests, AIOps brings machine learning and automation capabilities to the IT operations domain. These technologies aim to resolve incidents faster, identify operational trends that impact performance, and simplify the procedures required to resolve issues.AIOps is an emerging platform. In the survey, 42 percent of respondents either had never heard of AIOps or had thought that applying machine learning to operations was “not a thing.” Only 4 percent are using an AIOps tool in production today. Although AIOps is an emerging platform, there’s a solid business case for many organizations to consider it.AIOps is driven by business need and operational complexityMore businesses today rely on applications to serve customers and run operations. That drives higher requirements and expectations on the reliability, performance, and security of the applications.It also fuels demand for application development teams to build new applications and enhance them more frequently. The job responsibility of maintaining application service levels has also broadened during the past decade.Once upon a time, organizations staffed the NOC (network operations center) as the front line of defense. If you ever walked into a NOC, you would likely see dozens of computer monitors with warning lights and trend visuals to help the staff pinpoint issues—ideally before an end-user experienced one and opened tickets.Business and IT leaders began changing this model by introducing devops practices and site reliability engineers. Devops changes the IT department’s culture by establishing a collective responsibility to enable frequent deployments and better support customer and employee needs. Tools and practices such as CI/CD (continuous integration and continuous delivery) and IaC (infrastructure as code) are part of what enables more frequent deployments.  But devops practices also require a shared operational responsibility ensuring that applications are reliable, perform well, and are secure. That means more people in the IT organization need access to all the different monitoring tools.Many IT organizations also hire SREs (site reliability engineers) to connect development and operations. SREs take a software engineering approach to system administration topics. In another survey that targeted SREs, they indicate that incident response is a massive part of their job: 49 percent claim to respond to at least one incident every week.Maturing devops practices and hiring site reliability engineers is how a growing number of IT organizations are facing increasing operational challenges. But just expecting them to make sense of the dozens of monitoring tools being used is a recipe for poor performance.AIOps platform capabilities and technical architectureHow can AIOps improve the status quo? AIOps platforms typically have the following architecture components and capabilities:A central data platform for aggregating raw logs and data from different monitoring tools.Out-of-the-box integrations with the most common log formats, monitoring tools, IT service management tools, agile development tools, and other collaboration platforms.Machine learning capabilities to help identify patterns in the aggregated data.Consoles, dashboards, and analytics to help IT operations see and manage multiple systems from a central interface.Automation capabilities that enable IT to communicate status, route issues, and autorespond to common problems.What differentiates AIOps from other IT operational platforms is the ability to aggregate data easily, leverage machine learning to find problems, and use automation as a tool to resolve them. AIOps doesn’t replace the existing monitoring tools. It integrates with them so that more people in the IT department have improved visibility to problems without the complexity of learning and using multiple monitoring tools.Similarly, AIOps platforms typically don’t replace existing IT service management, workflow, agile, and other communication tools. Instead, they are a central platform to interface with them while alerting and resolving an incident.Monitoring mission-critical applications without AIOpsImagine your e-commerce application experiences slow performance when users try to complete a purchase. The first indicator that starts to send out alerts is the shopping cart abandonment rate.The e-commerce leader quickly opens a ticket about the issue in Cherwell’s mobile interface, but the IT team has already been alerted to the problem. As more users try to make purchases, the underlying Web servers hang and database connections stay open. Alerts from DataDog report these issues, and Splunk reports Java exceptions in the e-commerce application’s log files.Now imagine the NOC responding to this issue. Where should they start, given the number of alerts going off at the same time? The SREs called in to assist must also investigate the different alerts from different tools. Meanwhile, the e-commerce leader is upset because no one responded to her ticket!AIOps helps IT address issues faster and with less stressHere’s how AIOps platforms can potentially address this issue faster and more effectively.First, AIOps sees that multiple alerts are going off, including application alerts. It automatically alerts the SREs, and when one responds, it automatically updates Cherwell that the incident has been answered by an SRE. No one had to manually update any system to send out these communications.Second, the alerts from Cherwell, the e-commerce platform, Splunk, and DataDog are all aggregated and time sequenced. The SRE immediately knows which alert came before the others triggered. That’s incredibly useful because the SRE can quickly see that the Web server hanging and the pooling database connections all started after the Java application exceptions.The AIOps platform’s machine learning capabilities are fairly sophisticated, so in addition to reporting on alerts, it also highlights other outlier operating conditions. In this case, the e-commerce application has many slow outbound connections to a single IP address. There are no alerts or exceptions on this issue, but its timing precedes any of the other alerts.It doesn’t take the SRE much longer to figure out that this is a connection to a third-party service that validates the city, state, and ZIP code of the buyer. This service is clearly having performance issues that are rippling through the entire application.With a root cause identified, the SRE adds a high-severity defect to the e-commerce development team’s Jira backlog, alerting them to the problem. A high-severity issue flags the agile development team to disrupt their sprint and address it. It’s a quick fix to circumvent the impacting service, and it’s easy to test and deploy the change through their Jenkins CI/CD pipeline.The AIOps platform tracks this defect, the deployment, and the drop in all the alerts and keeps the e-commerce leader updated on the progress. Even though the SRE is monitoring the situation, the AIOps platform closes the issue automatically when all the monitors return to normal.Implementing this scenario isn’t trivial, but neither is it science fiction with AIOps platforms.", "pub_date": "2020-04-30"},
{"title": "How to move data science into production", "overview": "With new Integrated Deployment extensions, data scientists can capture entire KNIME workflows for automatic deployment to production or reuse", "image_url": null, "url": "https://www.infoworld.com/article/3541230/how-to-move-data-science-into-production.html", "body": "Deploying data science into production is still a big challenge. Not only does the deployed data science need to be updated frequently but available data sources and types change rapidly, as do the methods available for their analysis. This continuous growth of possibilities makes it very limiting to rely on carefully designed and agreed-upon standards or work solely within the framework of proprietary tools.KNIME has always focused on delivering an open platform, integrating the latest data science developments by either adding our own extensions or providing wrappers around new data sources and tools. This allows data scientists to access and combine all available data repositories and apply their preferred tools, unlimited by a specific software supplier’s preferences. When using KNIME workflows for production, access to the same data sources and algorithms has always been available, of course. Just like many other tools, however, transitioning from data science creation to data science production involved some intermediate steps.Also on InfoWorld: Artificial intelligence today: What’s hype and what’s realIn this post, we are describing a recent addition to the KNIME workflow engine that allows the parts needed for production to be captured directly within the data science creation workflow, making deployment fully automatic while still allowing every module to be used that is available during data science creation.Why is deploying data science in production so hard?At first glance, putting data science in production seems trivial: Just run it on the production server or chosen device! But on closer examination, it becomes clear that what was built during data science creation is not what is being put into production.I like to compare this to the chef of a Michelin star restaurant who designs recipes in his experimental kitchen. The path to the perfect recipe involves experimenting with new ingredients and optimizing parameters: quantities, cooking times, etc. Only when satisfied, are the final results — the list of ingredients, quantities, procedure to prepare the dish — put into writing as a recipe. This recipe is what is moved “into production,” i.e., made available to the millions of cooks at home that bought the book.This is very similar to coming up with a solution to a data science problem. During data science creation, different data sources are investigated; that data is blended, aggregated, and transformed; then various models (or even combinations of models) with many possible parameter settings are tried out and optimized. What we put into production is not all of that experimentation and parameter/model optimization — but the combination of chosen data transformations together with the final best (set of) learned models.This still sounds easy, but this is where the gap is usually biggest. Most tools allow only a subset of possible models to be exported; many even ignore the preprocessing completely. All too often what is exported is not even ready to use but is only a model representation or a library that needs to be consumed or wrapped into yet another tool before it can be put into production. As a result, the data scientists or model operations team needs to add the selected data blending and transformations manually, bundle this with the model library, and wrap all of that into another application so it can be put into production as a ready-to-consume service or application. Lots of details get lost in translation.For our Michelin chef above, this manual translation is not a huge issue. She only creates or updates recipes every other year and can spend a day translating the results of her experimentation into a recipe that works in a typical kitchen at home. For our data science team, this is a much bigger problem: They want to be able to update models, deploy new tools, and use new data sources whenever needed, which could easily be on a daily or even hourly basis. Adding manual steps in between not only slows this process to a crawl but also adds many additional sources of error.The diagram below shows how data science creation and productionization intertwine. This is inspired by the classic CRISP-DM cycle but puts stronger emphasis on the continuous nature of data science deployment and the requirement for constant monitoring, automatic updating, and feedback from the business side for continuous improvements and optimizations. It also distinguishes more clearly between the two different activities: creating data science and putting the resulting data science process into production.Often, when people talk about “end-to-end data science,” they really only refer to the cycle on the left: an integrated approach covering everything from data ingestion, transforming, and modeling to writing out some sort of a model (with the caveats described above). Actually consuming the model already requires other environments, and when it comes to continued monitoring and updating of the model, the tool landscape becomes even more fragmented. Maintenance and optimization are, in many cases, very infrequent and heavily manual tasks as well. On a side note: We avoid the term “model ops” purposely here because the data science production process (the part that’s moved into “operations”) consists of much more than just a model.Removing the gap between data science creation and data science productionIntegrated deployment removes the gap between data science creation and data science production by enabling the data scientist to model both creation as well as production within the same environment by capturing the parts of the process that are needed for deployment. As a result, whenever changes are made in data science creation, these changes are automatically reflected in the deployed extract as well. This is conceptually simple but surprisingly difficult in reality.If the data science environment is a programming or scripting language, then you have to be painfully detailed about creating suitable subroutines for every aspect of the overall process that could be useful for deployment — also making sure that the required parameters are properly passed between the two code bases. In effect, you have to write two programs at the same time, ensuring that all dependencies between the two are always observed. It is easy to miss a little piece of data transformation or a parameter that is needed to properly apply the model.Using a visual data science environment can make this more intuitive. The new Integrated Deployment node extensions from KNIME allow those pieces of the workflow that will also be needed in deployment to be framed or captured. The reason this is so simple is that those pieces are naturally a part of the creation workflow. This is because first, the exact same transformation pieces are needed during model training, and second, evaluation of the models is needed during fine tuning. The following image shows a very simple example of what this looks like in practice:The purple boxes capture the parts of the data science creation process that are also needed for deployment. Instead of having to copy them or having to go through an explicit “export model” step, now we simply add Capture-Start/Capture-End nodes to frame the relevant pieces and use the Workflow-Combiner to put the pieces together. The resulting, automatically created workflow is shown below:The Workflow-Writer nodes come in different shapes that are useful for all possible ways of deployment. They do just what their name implies: write out the workflow for someone else to use as a starting point. But more powerful is the ability to use Workflow-Deploy nodes that automatically upload the resulting workflow as a REST service or as an analytical application to KNIME Server or deploy it as a container — all possible by using the appropriate Workflow-Deploy node.Many data science solutions promise end-to-end data science, complete model-ops, and other flavors of “complete deployment.” Below is a checklist that covers typical limitations.Can you mix and match technologies (R, Python, Spark, TensorFlow, cloud, on-prem), or are you limited to a particular technology/environment only?Can you use the same set of tools during creation as well as the deployment setup, or does one of the two only cover a subset of the other?Can you deploy automatically into a service (e.g., REST), an application, or a scheduled job, or is the deployment only a library/model that needs to be embedded elsewhere?Is the deployment fully automatic, or are (manual) intermediate steps required?Can you roll back automatically to previous versions of both the data science creation process and the models in production?Can you run both creation as well as production processes years later with guaranteed backward compatibility of all results?Can a revised data science process be deployed in less than one minute?The purpose of this article is not to describe the technical aspects in great detail. Still, it is important to point out that this capture and deploy mechanism works for all nodes in KNIME — nodes that provide access to native data transformation and modeling techniques as well as nodes that wrap other libraries such as TensorFlow, R, Python, Weka, Spark, and all of the other third-party extensions provided by KNIME, the community, or the partner network.With the new Integrated Deployment extensions, KNIME workflows turn into a complete data science creation and productionization environment. Data scientists building workflows to experiment with built-in or wrapped techniques can capture the workflow for direct deployment within that same workflow. For the first time, this enables instantaneous deployment of the complete data science process directly from the environment used to create that process.TwitterLinkedInKNIME blog—newtechforum@infoworld.com", "pub_date": "2020-04-30"},
{"title": "Spell machine learning platform goes on-prem", "overview": "An end-to-end machine learning platform designed for ease of use, Spell now offers incarnations for both public cloud and data center deployment", "image_url": null, "url": "https://www.infoworld.com/article/3541757/spell-machine-learning-platform-goes-on-prem.html", "body": "Spell, an end-to-end platform for machine learning and deep learning—covering data prep, training, deployment, and management—has announced Spell for Private Machines, a new version of its system that can be deployed on your own hardware as well as on cloud resources.Spell was founded by  Piantino, former director of engineering at Facebook and founder of Facebook’s AI Research group. Spell allows teams to create reproducible machine learning systems that incorporate familiar tools such as Jupyter notebooks and that leverage cloud-hosted GPU compute instances.Also on InfoWorld: The best free data science courses during quarantineSpell emphasizes ease of use. For example, hyperparameter optimization for an experiment is a high-level, one-command function. Nor must users do much to configure the infrastructure; Spell detects what hardware is available and orchestrates to suit. Spell also organizes experiment assets, so both experiments and their data can be versioned and check-pointed as part of the development process.Spell originally ran only in the cloud; there’s been no “behind-the-firewall” deployment until now. Spell For Private Machines allows developers to run the platform on their own hardware. Both on-prem and cloud resources can be mixed and matched as needed. For instance, a prototype version of a project could be created on local hardware, then scaled out to an AWS instance for production deployment.Much of Spell’s workflow is already designed to feel as if it runs locally, and to complement existing workflows. Python tools for Spell work can be set up with , for example. And because the Spell runtime uses containers, multiple versions of an experiment with different hyperparameter turnings can be run side by side. ", "pub_date": "2020-05-06"},
{"title": "6 Python libraries for parallel processing", "overview": "Want to distribute that heavy Python workload across multiple CPUs or a compute cluster? These frameworks can make it happen", "image_url": null, "url": "https://www.infoworld.com/article/3542595/6-python-libraries-for-parallel-processing.html", "body": "Python is long on convenience and programmer-friendliness, but it isn’t the fastest programming language around. Some of its speed limitations are due to its default implementation, cPython, being single-threaded. That is, cPython doesn’t use more than one hardware thread at a time.And while you can use the  module built into Python to speed things up,  only gives you , not . It’s good for running multiple tasks that aren’t CPU-dependent, but does nothing to speed up multiple tasks that each require a full CPU. Also on InfoWorld: The best free data science courses during quarantinePython does include a native way to run a Python workload across multiple CPUs. The  module spins up multiple copies of the Python interpreter, each on a separate core, and provides primitives for splitting tasks across cores. But sometimes even  isn’t enough.Sometimes the job calls for distributing work not only across , but also across . That’s where these six Python libraries and frameworks come in. All six of the Python toolkits below allow you to take an existing Python application and spread the work across multiple cores, multiple machines, or both.RayDeveloped by a team of researchers at the University of California, Berkeley, Ray underpins a number of distributed machine learning libraries. But Ray isn’t limited to machine learning tasks alone, even if that was its original use case. Any Python tasks can be broken up and distributed across systems with Ray.Ray’s syntax is minimal, so you don’t need to rework existing apps extensively to parallelize them. The  decorator distributes that function across any available nodes in a Ray cluster, with optionally specified parameters for how many CPUs or GPUs to use. The results of each distributed function are returned as Python objects, so they’re easy to manage and store, and the amount of copying across or within nodes is kept to a minimum. This last feature comes in handy when dealing with NumPy arrays, for instance.Ray even includes its own built-in cluster manager, which can automatically spin up nodes as needed on local hardware or popular cloud computing platforms.DaskFrom the outside, Dask looks a lot like Ray. It, too, is a library for distributed parallel computing in Python, with its own task scheduling system, awareness of Python data frameworks like NumPy, and the ability to scale from one machine to many.Dask works in two basic ways. The first is by way of parallelized data structures — essentially, Dask’s own versions of NumPy arrays, lists, or Pandas DataFrames. Swap in the Dask versions of those constructions for their defaults, and Dask will automatically spread their execution across your cluster. This typically involves little more than changing the name of an import, but may sometimes require rewriting to work completely.The second way is through Dask’s low-level parallelization mechanisms, including function decorators, that parcel out jobs across nodes and return results synchronously (“immediate” mode) or asynchronously (“lazy”). Both modes can be mixed as needed, too.One key difference between Dask and Ray is the scheduling mechanism. Dask uses a centralized scheduler that handles all tasks for a cluster. Ray is decentralized, meaning each machine runs its own scheduler, so any issues with a scheduled task are handled at the level of the individual machine, not the whole cluster.Also on InfoWorld: 8 great Python libraries for natural language processingDask also offers an advanced and still experimental feature called “actors.” An actor is an object that points to a job on another Dask node. This way, a job that requires a lot of local state can run in-place and be called remotely by other nodes, so the state for the job doesn’t have to be replicated. Ray lacks anything like Dask’s actor model to support more sophisticated job distribution.DispyDispy lets you distribute whole Python programs or just individual functions across a cluster of machines for parallel execution. It uses platform-native mechanisms for network communication to keep things fast and efficient, so Linux, MacOS, and Windows machines work equally well.Dispy syntax somewhat resembles  in that you explicitly create a cluster (where  would have you create a process pool), submit work to the cluster, then retrieve the results. A little more work may be required to modify jobs to work with Dispy, but you also gain precise control over how those jobs are dispatched and returned. For instance, you can return provisional or partially completed results, transfer files as part of the job distribution process, and use SSL encryption when transferring data.Pandaral·lelPandaral·lel, as the name implies, is a way to parallelize Pandas jobs across multiple nodes. The downside is that Pandaral·lel works  with Pandas. But if Pandas is what you’re using, and all you need is a way to accelerate Pandas jobs across multiple cores on a single computer, Pandaral·lel is laser-focused on the task.Note that while Pandaral·lel does run on Windows, it will run only from Python sessions launched in the Windows Subsystem for Linux. MacOS and Linux users can run Pandaral·lel as-is. IpyparallelIpyparallel is another tightly focused multiprocessing and task-distribution system, specifically for parallelizing the execution of Jupyter notebook code across a cluster. Projects and teams already working in Jupyter can start using Ipyparallel immediately.Ipyparallel supports many approaches to parallelizing code. On the simple end, there’s , which applies any function to a sequence and splits the work evenly across available nodes. For more complex work, you can decorate specific functions to always run remotely or in parallel.Jupyter notebooks support “magic commands” for actions only possible in a notebook environment. Ipyparallel adds a few magic commands of its own. For example, you can prefix any Python statement with  to automatically parallelize it.JoblibJoblib has two major goals: run jobs in parallel and don’t recompute results if nothing has changed. These efficiencies make Joblib well-suited for scientific computing, where reproducible results are sacrosanct. Joblib’s documentation provides plenty of examples for how to use all its features.Joblib syntax for parallelizing work is simple enough—it amounts to a decorator that can be used to split jobs across processors, or to cache results. Parallel jobs can use threads or processes.Also on InfoWorld: 8 signs you’re doing Python rightJoblib includes a transparent disk cache for Python objects created by compute jobs. This cache not only helps Joblib avoid repeating work, as noted above, but can also be used to suspend and resume long-running jobs, or pick up where a job left off after a crash. The cache is also intelligently optimized for large objects like NumPy arrays. Regions of data can be shared in-memory between processes on the same system by using .One thing Joblib does not offer is a way to distribute jobs across multiple separate computers. In theory it’s possible to use Joblib’s pipeline to do this, but it’s probably easier to use another framework that supports it natively. What is Python? Powerful, intuitive programmingWhat is PyPy? Faster Python without painWhat is Cython? Python at the speed of CCython tutorial: How to speed up PythonHow to install Python the smart wayThe best new features in Python 3.8Better Python project management with PoetryVirtualenv and venv: Python virtual environments explainedPython virtualenv and venv do’s and don’tsPython threading and subprocesses explainedHow to use the Python debuggerHow to use timeit to profile Python codeHow to use cProfile to profile Python codeGet started with async in PythonHow to use asyncio in PythonHow to convert Python to JavaScript (and back again)Python 2 EOL: How to survive the end of Python 212 Pythons for every programming need24 Python libraries for every Python developer7 sweet Python IDEs you might have missed3 major Python shortcomings—and their solutions13 Python web frameworks compared4 Python test frameworks to crush your bugs6 great new Python features you don’t want to miss5 Python distributions for mastering machine learning8 great Python libraries for natural language processing", "pub_date": "2020-05-13"},
{"title": "How data analysis, AI, and IoT will shape the post-pandemic ‘new normal’", "overview": "Only a common public health infrastructure of AI, cloud computing, streaming, and the Internet of Things can marshal our data against pandemics", "image_url": null, "url": "https://www.infoworld.com/article/3543770/how-data-analysis-ai-and-iot-will-shape-the-post-pandemic-new-normal.html", "body": "Pandemics are shocks to communities throughout the world. Each community’s response emerges from the countless changes that individuals make in their daily lives to protect themselves while trying to maintain a semblance of normality.Grassroots responses often emerge first in such crises, but they may not be the most effective approach for slowing the contagion’s spread. From a technological standpoint, solutions invariably involve various blends of remote collaboration, contactless transactions, and replacement of manual processes with automated, robotic, and other human-free processes.Also on InfoWorld: Artificial intelligence today: What’s hype and what’s realWhen a contagion is raging, grassroots responses can be counterproductive if everybody’s operating at cross-purposes. Lack of central coordination can confuse the situation for everybody, stoking a panic-driven infodemic that social media can exacerbate, drowning out guidance from public health officials and other reliable sources. To ensure effective orchestration of community-wide responses to a contagion, there is no substitute for authoritative data analytics to drive effective responses at all levels of society.Going forward, we can expect to see more data-driven, top-down orchestration of pandemic preparedness and remediation among public, private, and nonprofit organizations. China’s experience is instructive in this regard. Though the outbreak’s inception in Wuhan was less than half a year ago, the country has responded rapidly with a top-down, nationwide approach to manage the crisis. Chinese authorities are orchestrating vast resources to save lives, control the spread of infection, and guide individuals for testing, treatment, and quarantining.In contrast, the United States and other nations seem to be responding to the emergency in a chaotic, bottom-up fashion. The key elements in China’s response are impressive. Leveraging sophisticated data analytics and other digital tools, it has responded to COVID-19 through:: Self-service screening tools have reduced nonessential hospital visits and caregiver workloads in China while mitigating the risks of cross-infection. Within the country, Tencent, Alibaba, and vertical online healthcare platforms now offer remote medical services to the public. People consult with doctors online, conduct self-assessments, and decide whether to go to a hospital for further medical checks or remain at home.: China’s digital platforms allow volunteer teams of community residents to assist in disinfection and deliver supplies aided by digital community management and communication tools. Citizens receive a health QR code that lets them submit information regarding travel to major epidemic outbreak regions. It enables compilation of data on close contacts with infected people and other relevant matters, and it enables an assessment on a three-color scale that indicates a person’s recent virus-related health history.: Digital technologies have allowed China’s healthcare professionals to apply their talents to a large number of COVID-19 cases over long distances. China’s 5G networks have allowed many Wuhan hospitals to connect with counterparts in Beijing, who provide real-time consultation based on transmission of ultra-high-definition medical images.: China has used these same technologies, along with the Internet of Things, to rapidly orchestrate an entire manufacturing, logistics, and healthcare supply chain. This has enabled the country to coordinate thousands of domestic firms to build and equip hospitals for testing and treatment of COVID-19 patients. The country has been able to rapidly scale up the production of masks, protective clothing, and disinfectants.: China has implemented a differentiated, location-specific response to limiting COVID-19 transmission. It uses big data analytics and artificial intelligence to estimate the probability that a particular neighborhood or individual was exposed to COVID-19. It matches the locations of smartphones to known locations of infected individuals or groups. It uses this information plus travel data to target government-mandated virus testing to high-risk individuals.Likewise, neighboring Taiwan has put together a comprehensive program of technology-driven tactics for controlling COVID-19’s spread in the country. Distilling from those East Asian nations’ experiences, I’m proposing a new normal for every country; it involves the following data-driven pillars of top-down response to raging pandemics:: In the new normal, most nations will institute counter-contagion nerve centers to coordinate societal responses to these threats. These operations will consolidate information from national health, immigration, customs, telecommunications, and travel databases. They will apply sophisticated AI to identify cases, generate real-tim;e alerts, and coordinate medical interventions. They will use real-time streaming apps to actively surveil and screen all ports into the country. These tools will provide the intelligence necessary to authoritatively close borders, certify which potential entrants are in good health, and quarantine or turn away people and animals that might be spreading contagion.: Painfully, the human race is learning from the COVID-19 crisis how to manage the quarantines, lockdowns, and closures that will be necessary in all future pandemics. In the future, public health authorities will use AI-driven predictive tools to rapidly plan and orchestrate these decisions in order to slow outbreaks while keeping the economy from crashing and minimizing community disruptions. Community contagion dashboards will give every person data-driven intelligence on how to adjust daily routines, based on conditions that currently impact their immediate environment. These and other sources of AI-driven intelligence will allow organizations to calculate the risk of having employees work in shared offices or from home, and to adjust their strategies day to day based on infection risk factors. When considering whether to bring employees back to the office while the pandemic is still going on, organizations base their decisions on data-driven analytics, as well as on site surveys informed by facility-embedded biosensors.: Going forward, we’re likely to see governments mandate AI-driven solutions for keeping people out of range of those who might be spreading infection. Proximity sensors will become ubiquitous in the aftermath of the current pandemic. Embedded in smartphones and wearables, they will feed personal digital assistants with real-time ambient intelligence on crowd conditions. Already, computer vision applications use AI to automate surveillance of people in public places, workplaces, stores, and elsewhere. Real-time AI tools will tell people if they’re standing too close to each other. Wait-time metering and crowd-limiting applications will become a standard feature of many public and private facilities. And we’ll see greater uptake of machine vision applications that can detect and optionally send “keep ‘em separated” alerts when someone moves too close to someone else.: The COVID-19 emergency is accelerating the Internet of Things’ sensor revolution, and there’s little doubt that much of this will be built in to public infrastructure everywhere. The new normal is likely to proliferate biosensors for detecting viral pathogens in the air, water, soil, surfaces, and human and animal tissues. More commonly these biosensors will be wearables, connected socially to detect the potential spread of infections from person to person and to monitor a disease’s progression among large groups of people, such as hospital patients and nursing home residents. AI-driven service robots will interrogate passersby in public places to see if they show signs of the virus. To detect symptoms even before people realize they’re infected, automated environment sensing will use multimodal AI to monitor the environment (pairing facial recognition with temperature scanning and listening to audio of people coughing). Infrared thermal imaging will enable active surveillance and screening for infected and carrier persons at borders, airports, and elsewhere in each country. We can expect governments to mandate embedded contact-tracing apps on every mobile phone.Putting this vision in place in the post-pandemic world will require a pervasive infrastructure of AI, cloud computing, streaming, and the Internet of Things. Managed as a common public health infrastructure, these capabilities will allow humanity to close ranks against the dread diseases that threaten us all.", "pub_date": "2020-05-14"},
{"title": "Apache Spark 3.0 adds Nvidia GPU support for machine learning", "overview": "The next major release of the in-memory data processing framework will support GPU-accelerated functions courtesy of Nvidia RAPIDS", "image_url": null, "url": "https://www.infoworld.com/article/3543319/apache-spark-30-adds-nvidia-gpu-support-for-machine-learning.html", "body": "Apache Spark, the in-memory big data processing framework, will become fully GPU accelerated in its soon-to-be-released 3.0 incarnation. Best of all, today’s Spark applications can take advantage of the GPU acceleration without modification; existing Spark APIs all work as-is.The GPU acceleration components, provided by Nvidia, are designed to complement all phases of Spark applications including ETL operations, machine learning training, and inference serving.InfoWorld review: Nvidia RAPIDS brings Python analytics to the GPUNvidia’s Spark contributions draw on the RAPIDS suite of GPU-accelerated data science libraries. Many of RAPIDS’ internal data structures, like dataframes, complement Spark’s own, but getting Spark to use RAPIDS natively has taken nearly four years of work.Spark 3.0 speedups don’t come solely from GPU acceleration. Spark 3.0 also reaps performance gains by minimizing data movement to and from GPUs. When data does need to be moved across a cluster, the Unified Communication X framework shuttles it directly from one block of GPU memory to another with minimal overhead.Also on InfoWorld: The best free data science courses during quarantineAccording to Nvidia, a preview release of Spark 3.0 running on the Databricks platform yielded a seven-fold performance improvement when using GPU acceleration, though details about the workload and its dataset were not available. No firm date has been given for general availability of Spark 3.0. You can download preview releases from the Apache Spark project website.", "pub_date": "2020-05-14"},
{"title": "Julia vs. Python: Which is best for data science?", "overview": "Python has turned into a data science and machine learning mainstay, while Julia was built from the ground up to do the job", "image_url": null, "url": "https://www.infoworld.com/article/3241107/julia-vs-python-which-is-best-for-data-science.html", "body": "Among the many use cases Python covers, data analytics has become perhaps the biggest and most significant. The Python ecosystem is loaded with libraries, tools, and applications that make the work of scientific computing and data analysis fast and convenient.But for the developers behind the Julia language — aimed specifically at “scientific computing, machine learning, data mining, large-scale linear algebra, distributed and parallel computing”—Python isn’t fast or convenient . Julia aims to give scientists and data analysts not only fast and convenient development, but also blazing execution speed. What is the Julia language?Created in 2009 by a four-person team and unveiled to the public in 2012, Julia is meant to address the shortcomings in Python and other languages and applications used for scientific computing and data processing. “We are greedy,” they wrote. They wanted more: We want a language that’s open source, with a liberal license. We want the speed of C with the dynamism of Ruby. We want a language that’s homoiconic, with true macros like Lisp, but with obvious, familiar mathematical notation like Matlab. We want something as usable for general programming as Python, as easy for statistics as R, as natural for string processing as Perl, as powerful for linear algebra as Matlab, as good at gluing programs together as the shell. Something that is dirt simple to learn, yet keeps the most serious hackers happy. We want it interactive and we want it compiled.(Did we mention it should be as fast as C?)Here are some of the ways Julia implements those aspirations: For faster runtime performance, Julia is just-in-time (JIT) compiled using the LLVM compiler framework. At its best, Julia can approach or match the speed of C. Julia includes a REPL (read-eval-print loop), or interactive command line, similar to what Python offers. Quick one-off scripts and commands can be punched right in. Julia’s syntax is similar to Python’s—terse, but also expressive and powerful. You can specify types for variables, like “unsigned 32-bit integer.” But you can also create hierarchies of types to allow general cases for handling variables of specific types—for instance, to write a function that accepts integers without specifying the length or signing of the integer. You can even do without typing entirely if it isn’t needed in a particular context.Julia can interface directly with external libraries written in C and Fortran. It’s also possible to interface with Python code by way of the PyCall library, and even share data between Python and Julia. Julia programs can generate other Julia programs, and even modify their own code, in a way that is reminiscent of languages like Lisp. Julia 1.1 introduced a debugging suite, which executes code in a local REPL and allows you to step through the results, inspect variables, and add breakpoints in code. You can even perform fine-grained tasks like stepping through a function generated by code.Perfect for IT, Python simplifies many kinds of work, from system automation to working in cutting-edge fields like machine learning.Julia vs. Python: Julia language advantagesJulia was designed from the start for scientific and numerical computation. Thus it’s no surprise that Julia has many features advantageous for such use cases: Julia’s JIT compilation and type declarations mean it can routinely beat “pure,” unoptimized Python by orders of magnitude. Python can be  faster by way of external libraries, third-party JIT compilers (PyPy), and optimizations with tools like Cython, but Julia is designed to be faster right out of the gate. A major target audience for Julia is users of scientific computing languages and environments like Matlab, R, Mathematica, and Octave. Julia’s syntax for math operations looks more like the way math formulas are written outside of the computing world, making it easier for non-programmers to pick up on. Like Python, Julia doesn’t burden the user with the details of allocating and freeing memory, and it provides some measure of manual control over garbage collection. The idea is that if you switch to Julia, you don’t lose one of Python’s common conveniences. Math and scientific computing thrive when you can make use of the full resources available on a given machine, especially multiple cores. Both Python and Julia can run operations in parallel. However, Python’s methods for parallelizing operations often require data to be serialized and deserialized between threads or nodes, while Julia’s parallelization is more refined. Further, Julia’s parallelization syntax is less top-heavy than Python’s, lowering the threshold to its use. Flux is a machine learning library for Julia that has many existing model patterns for common use cases. Since it's written entirely in Julia, it can be modified as needed by the user, and it uses Julia's native just-in-time compilation to optimize projects from inside out. Julia vs. Python: Python advantagesAlthough Julia is purpose-built for data science, whereas Python has more or less evolved into the role, Python offers some compelling advantages to the data scientist. Some of the reasons “general purpose” Python may be the better choice for data science work:In most languages, Python and C included, the first element of an array is accessed with a zero—e.g.,  in Python for the first character in a string. Julia uses 1 for the first element in an array. This isn’t an arbitrary decision; many other math and science applications, like Mathematica, use 1-indexing, and Julia is intended to appeal to that audience. It’s possible to support zero-indexing in Julia with an experimental feature, but 1-indexing by default may stand in the way of adoption by a more general-use audience with ingrained programming habits. Python programs may be slower than Julia programs, but the Python runtime itself is more lightweight, and it generally takes less time for Python programs to start and deliver first results. Also, while JIT compilation speeds up execution time for Julia programs, it comes at the cost of slower startup. Much work has been done to make Julia start faster, but Python still has the edge here. The Julia language is young. Julia has been under development only since 2009, and has undergone a fair amount of feature churn along the way. By contrast, Python has been around for almost 30 years. The breadth and usefulness of Python’s culture of third-party packages remains one of the language’s biggest attractions. Again, Julia’s relative newness means the culture of software around it is still small. Some of that is offset by the ability to use existing C and Python libraries, but Julia needs libraries of its own to thrive. Libraries like Flux and Knet make Julia useful for machine learning and deep learning, but the vast majority of that work is still done with TensorFlow or PyTorch. A language is nothing without a large, devoted, and active community around it. The community around Julia is enthusiastic and growing, but it is still only a fraction of the size of the Python community. Python’s huge community is a huge advantage.  Aside from gaining improvements to the Python interpreter (including improvements to multi-core and parallel processing), Python has become easier to speed up. The mypyc project translates type-annotated Python into native C, far less clunkily than Cython. It typically yields four-fold performance improvements, and often much more for pure mathematical operations.", "pub_date": "2020-05-27"},
{"title": "Building a hybrid SQL Server infrastructure", "overview": "Pairing your on-prem SQL Server with a cloud-based instance for high availability has its challenges, but they can be overcome. Here’s how.", "image_url": null, "url": "https://www.infoworld.com/article/3584425/building-a-hybrid-sql-server-infrastructure.html", "body": "The geographic distribution of cloud data centers makes it easy to configure SQL Server instances for high availability (HA) and/or disaster recovery (DR), but what good is that to you if you’re already invested in a well-tuned SQL Server system that’s firing away on all cylinders on-premises? The idea of ensuring higher levels of availability or easily adding a disaster recovery site may sound very attractive, but the thought of moving  into the cloud to achieve those ends may give rise to cold sweats and nightmares.But here’s the thing: The cloud is not an all-or-nothing proposition. You  mix on-prem and cloud infrastructures to create a  infrastructure. Some parts of your solution will be on-prem, while other parts will be in the cloud. Ensuring that your hybrid infrastructure can deliver the availability support that you want, though, requires an awareness of both the differences between your on-prem and cloud configuration options as well as an understanding of how to bridge those differences.Also on InfoWorld: Amazon, Google, and Microsoft take their clouds to the edgeFeet on the ground, head in the cloudsLet’s assume you want to continue to use your on-prem SQL Server configuration as your primary SQL system. It ain’t broke, as they say, so why move it? But if you want to protect your organization from operational downtime or infrastructure loss that might occur in the event of a catastrophe—a hurricane, for example, or a major earthquake that compromises your on-prem infrastructure—you can put another instance of your SQL Server configuration in the cloud, where it is unlikely to be affected by the local disaster. If a catastrophe takes your on-prem infrastructure offline, the infrastructure in the cloud can take over.One way to look at the cloud, in this scenario, is as if it were a remote data center in which you have a standby server, ready to take over in a moment’s notice. There are two fundamental ways to do this. You could configure a multi-node Always On Availability Group (AG), a feature of SQL Server itself, or you could use Windows Server Failover Clustering (WSFC) to configure a multi-node SQL Server failover cluster instance (FCI).While the former approach is specific to SQL Server and the latter is generic to Windows Server, both approaches enable you to deploy SQL Server on cluster nodes running in geographically separate locations and both orchestrate failover from a primary node (on premises, in this scenario) to a secondary node (in the cloud) in the event of a catastrophe.Hybrid cluster configuration challengesThe challenges you’ll encounter if you view your hybrid architecture this way are twofold. First, you need to ensure that the cloud-based instance of SQL Server can access the data that your on-prem instance has been using, and how you do that depends on the approach you take.If you’re using an AG, the AG can be configured to replicate your user-defined SQL databases from the primary (on-prem) instance of SQL Server to your secondary (cloud-based) instance of SQL Server. However, if you’re using the  AG feature of SQL Server Standard Edition you are limited to replicating from one node to a second (and you are limited to one database per AG). If your on-prem architecture already has two nodes configured as a Basic AG, your cloud-based instance of SQL Server would represent a  node, which Basic AGs do not support. To replicate your on-prem data to that cloud-based instance you’ll have to upgrade your on-prem and cloud-based instances of SQL Server to the Enterprise Edition (2012 or later). That’s a costly upgrade, but that’s the only way you can use AG to support replication of user-defined databases among more than two SQL Server instances.If you’re using WSFC to create a hybrid cluster, you’ll need to think about storage in a different way. Traditionally, Windows Server failover clusters share data on a storage area network (SAN), which all the individual cluster nodes can access. No matter which node in the cluster is running the active instance of SQL Server, it can interact with the databases residing on the SAN. However, there’s no option to share a SAN in the cloud. You  configure a hybrid cluster that binds on-prem and cloud-based infrastructure, but the cloud-based VMs cannot interact with a shared on-prem SAN. Nor can a SAN be put in the cloud to be shared among cloud-based compute nodes. Every failover cluster node in the cloud must be configured with attached storage of its own, and you’ll need to deploy a mechanism to replicate the data from your on-prem storage to the storage attached to each VM running in the cloud.The solution to this challenge is to build a SANless failover cluster using SIOS DataKeeper. SIOS DataKeeper performs block-level replication of all the data on your on-prem storage to the local storage attached to your cloud-based VM. If disaster strikes your on-prem infrastructure and the WSFC fails SQL Server over to the cloud-based cluster node, that cloud-based node can access its own copy of your SQL Server databases and can fill in for your on-prem infrastructure for as long as you need it to.One other advantage afforded by the SANless failover cluster approach is that there is no limit on the number of databases you can replicate. Where you would need to upgrade to SQL Server Enterprise Edition to replicate your user databases to a third node in the cloud, the SANless clustering approach works with both the SQL Server Standard and Enterprise editions. While SQL Server Standard Edition is limited to two nodes in the cluster, DataKeeper allows you to replicate to a third node in the cloud with a manual recovery process. With Enterprise Edition the third node in the cloud can simply be part of the same cluster.Inter-node communications in a hybrid environmentIn addition to addressing the challenge of  to synchronize SQL Server data between your on-prem and cloud-based infrastructures, you need to address the challenge of synchronizing your data securely and expeditiously. Since one of the purposes of a DR infrastructure is to shield you from loss arising from a local disaster, you should plan to locate your DR infrastructure in a cloud data center that’s unlikely to be affected by any localized disaster that would affect your on-prem infrastructure. Such distances are going to require you to use asynchronous replication settings whether you are using an AG approach or a SANless clustering approach, as the distances between the data centers are going to be too great to support synchronous replication.Still, to ensure that your on-prem and cloud-based instances are as close to synchronized as possible, you want to connect them using the fastest network channel you can afford to acquire. AWS, Azure, and Google all offer direct cloud connections as premium options (AWS Direct Connect, Azure Express Route, and GCP Cloud Interconnect). These options provide you with secure, high-speed ways to bypass the public Internet when communicating with your cloud-based infrastructure, but they add further cost to your configuration. If you opt not to use one of these direct connections, you’ll need to configure a VPN channel to secure your connection to the remote infrastructure. Hence your ability to synchronize your data will be gated by the speed of your Internet gateway and the overhead of the VPN you’re using.Note, though, that no matter the speed of your network connection, if you’re using asynchronous replication your on-prem and cloud-based instances may rarely be in perfect harmony. On a busy day, your cloud-based SQL databases may always be slightly behind your active on-prem database. That is not a shortcoming of a hybrid architecture; it’s just the nature of asynchronous replication between any distant points.As a consequence, if there’s a catastrophic incident that prompts an unanticipated failover to your cloud infrastructure, your cloud-based instance of SQL Server may come online without some of the transactions and database updates that had already been committed in the former primary instance. If at some point you can recover the data from the storage associated with the on-prem instance of SQL Server, you may be able to recover and restore those transactions. If not, they may be lost.Hybrid configuration considerations beyond SQL ServerThe failover management components built into AGs and WSFC can ensure a SQL Server failover that is as speedy and as seamless as possible in any disaster scenario. They can orchestrate interactions and updates with ancillary infrastructure components such as load balancers and DNS servers, enabling you to have your cloud instance of SQL Server fully operational, and with minimal data loss, in moments.However, when you’re running SQL Server on-premises and configuring a hybrid architecture for DR, there are additional operational elements you should take into consideration. How you configure elements such as application or terminal servers that may ultimately interact with SQL Server is beyond the scope of this article, but we’ll leave you with a checklist of things to consider when envisioning a hybrid architecture designed for DR.Different disasters have different scopes. An isolated fire in the data center could take your on-prem SQL Server infrastructure offline but leave your application servers and user workstations untouched. An electrical grid issue could take your entire data center offline, making both SQL Server and your application server infrastructure inaccessible, while user workstations in another location remain unaffected. A hurricane could devastate the region, bringing down your SQL Server, applications server, and the workstation infrastructure. These are just examples, for each environment is unique, but each of these scenarios calls for a distinct DR response.: SQL Server alone goes offline. Data center, application servers, and workstation infrastructure are unaffected.DR response:Failover SQL Server to the cloud infrastructure as outlined.Redirect client connections from applications servers to cloud-based SQL infrastructure.: The entire data center goes offline, rendering SQL Server and your application servers inaccessible. User workstations are unaffected.DR response:Failover SQL Server to the cloud infrastructure as outlined.Recover applications servers in the cloud infrastructure using tools such as Azure Site Recovery, AWS CloudEndure, or a third-party product such as Veeam.Redirect client connections to the cloud infrastructure.: The entire data center  the user workstation infrastructure go offline.DR response:Failover SQL Server to the cloud infrastructure as outlined.Recover application servers in the cloud infrastructure using tools such as Azure Site Recovery, AWS CloudEndure, or a third-party product such as Veeam.Redirect client connections to the cloud infrastructure, as in DR scenario 2.Enable users to work from home using remote desktop services running on cloud-based infrastructure.In the end, if your on-prem infrastructure is working for you, run with it. You can take advantage of specific benefits afforded by the cloud—like its ability to provide a DR safety net far more easily than you could build one yourself—without having to move everything into the cloud. Just build that safety net properly and be sure you test it regularly to be sure it can catch you if your on-prem infrastructure takes an unanticipated fall.SIOS Technology", "pub_date": "2020-10-28"},
{"title": "No, you don’t have to run like Google", "overview": "Just because Google, Amazon, or Facebook does it doesn’t mean you should. Here are four ‘best practices’ of the hyperscalers you have permission to ignore.", "image_url": null, "url": "https://www.infoworld.com/article/3585176/no-you-dont-have-to-run-like-google.html", "body": "Years ago, Google struggled with how to pitch its cloud offerings. Back in 2017 I suggested that the company should help mainstream enterprises to “run like Google,” but in a conversation with a senior Google Cloud product executive, he suggested that the company shied away from this approach. The concern? That maybe mainstream enterprises didn’t share Google’s needs, or maybe Google would simply intimidate them.For the mere mortals that run IT within such mainstream enterprises (read: almost everyone), fear not. It turns out there are many things that Google might do that make no sense for your own IT needs.Also on InfoWorld: The best open source software of 2020Just ask Colm MacCárthaigh, AWS engineer and one of the authors of the Apache HTTP Server, who asked for “examples of technical things that don’t make sense for everyone just because Amazon, Google, Microsoft, Facebook” do them. The answers—excessive uptime guarantees, site reliablity engineering, microservices, and monorepos among the highlights—are instructive.Excessive uptime guarantees“Five or five-plus nines availability guarantees,” says Pete Ehlke. “Outside of medicine and 911 call centers, I can’t think of anything shy of FAANG [Facebook, Amazon, Apple, Netflix, and Google] scale that actually needs five nines, and the ROI pretty much never works out.”I remember this one well from the variety of startups for which I worked, as well as when I was at Adobe (whose service-level commitments tend not to be five nines, but are arguably higher than necessary). Are you going to be OK if the multi-player game goes down? Yep. What about Office 365 for a few minutes, or even hours? Yes and yes.Site reliability engineeringA bit of a spin on devops (though it predates the devops movement), SRE (named in multiple replies to MacCárthaigh) came out of Google in 2003, and was designed to infuse engineering with an operational focus. A few core principles guide SRE:Embrace riskUtilize service level objectives (SLOs)Eliminate toilMonitor distributed systemsLeverage automation and embrace simplicityOr, as Ben Traynor, who developed Google’s SRE practice, describes it:SRE is fundamentally doing work that has historically been done by an operations team, but using engineers with software expertise and banking on the fact that these engineers are inherently both predisposed to, and have the ability to, substitute automation for human labor. In general, an SRE team is responsible for availability, latency, performance, efficiency, change management, monitoring, emergency response, and capacity planning.SREs spend much of their time on automation, with the ultimate goal being to automate away their job. They spend considerable time on “operations/on-call duties and developing systems and software that help increase site reliability and performance,” says Silvia Pressard.This sounds important, and even more so if you equate “site reliability” with “business availability.” But do most companies really need their developers to become operational experts? SRE might be critical at Google or Amazon, but it’s arguably a heavy lift for most enterprises, tasking developers with too much of an operational load for them to manage it successfully.Microservices architectureAs commentator “Buzzy” tells it, “Definitely microservices. The number of 20-staff-in-total companies I’ve had to talk down from that ledge….” Nor is he the only one to call out microservices as a needless complication for most enterprises. Many of the replies to MacCárthaigh’s tweet mentioned microservices.As Martin Fowler has argued, “While [microservices] is a useful architecture—many, indeed most, situations would do better with a monolith.” Wait, what? Aren’t monoliths an evil relic of the past? Of course it’s not that simple. As I’ve written,The great promise of microservices is freedom. Freedom to break up an application into distinct services that are independently deployable. Freedom to build these disparate services with different teams using their preferred programming language, tooling, database, etc. In short, freedom for development teams to get stuff done with minimal bureaucracy.But for many applications, that freedom comes at unnecessary costs, as Fowler highlights:Distribution: Distributed systems are harder to program, since remote calls are slow and are always at risk of failure.Eventual consistency: Maintaining strong consistency is extremely difficult for a distributed system, which means everyone has to manage eventual consistency.Operational complexity: You need a mature operations team to manage lots of services, which are being redeployed regularly.This last point is underlined by Sam Newman: “For a small team, a microservice architecture can be hard to justify, as there is work required just to handle the deployment and management of the microservices themselves.”It’s not to say that a microservices approach is always wrong. No, it’s simply a suggestion that we shouldn’t default to a more complicated (but scalable) approach simply because the hyperscalers use it (generally because scale is so critical).A monorepo to rule them allWhether microservices or monolithic in nature, you probably shouldn’t store your code in a “monorepo.” This was a common response to MacCárthaigh’s request. Monorepos store all of a company’s code in a single version control system (VCS), to seize on the (supposed) benefits of reducing duplication of code and increasing collaboration between teams.That’s the theory.The practice, however, is very different. “It quickly becomes unreasonable for a single developer to have the entire repository on their machine, or to search through it using tools like grep,” says Matt Klein, an engineer who has built some of the most sophisticated systems at Amazon, Twitter, and now Lyft. “Given that a developer will only access small portions of the codebase at a time, is there any real difference between checking out a portion of the tree via a VCS or checking out multiple repositories? .”Klein continues:In terms of collaboration and code sharing, at scale, developers are exposed to subsections of code through higher layer tooling. Whether the code is in a monorepo or polyrepo is irrelevant; the problem being solved is the same, and .You be youOf course, some companies may benefit from monorepos or five-nines availability or microservices or SRE. They might also benefit from rolling their own framework, building their own infrastructure, or any of the other things that commentators on MacCárthaigh deride. The point is that just because Google, Facebook, Amazon, or another hyperscaler does it, doesn’t mean  should. When in doubt, doubt. Start with the individual needs of your company and figure out the right approach to building and managing software according to who you are, not who you wish you were.", "pub_date": "2020-10-12"},
{"title": "A foolproof way to understand cloud optimization ", "overview": "Once your cloud architecture works, it’s time to optimize it for efficiency and cost. An audit will reveal how much value it adds to the business. ", "image_url": null, "url": "https://www.infoworld.com/article/3584387/a-foolproof-way-to-understand-cloud-optimization.html", "body": "We’ve discussed the notion of cloud architecture optimization here in the past. Now it’s time to understand how it’s measured. You really have no way to prove your architecture isn’t optimized unless you do an audit, which includes a review of the solution’s approach and any attached costs.In the past, the people who built and deployed cloud solutions were reluctant to have their choices questioned. These days, because we want the most value from the cloud solutions, many have changed their minds about questions and oversight—or more often, company leadership changed their minds for them. Many of the projects I take on these days focus on review and improvement audits rather than on build and migrate deployments.Also on InfoWorld: Which multicloud architecture will win out?Once everything works in cloud architectures, you can deploy and operationalize. Just because it works does not mean that it’s optimized. If you look at the differences between your architecture and one that’s optimized, you could have a “working” solution that costs you millions of dollars each week.To present this visually, see the figure below. Note that positions 1 and 37 are the least optimized. They cost more money and are the most inefficient.When we look at each side, note that cloud solutions can be under-utilized or overutilized, such as with containers and serverless computing. Those on the left of the chart might not have included enough containers, whereas those scoring on the right have used containers too much. The point of optimization is to be in position 19, where we use the right number of containers to make the most of costs and solution efficiency.Of course, you can leverage this metric for any holistic architecture or all configured technology. You could even apply it to microarchitectures, such as a few applications moving to serverless or containers, for instance. Note that including polynomial views of the data for smoothing creates a curve that’s more likely to reflect typical real-life behavior. In the real world, this data never actually runs in a straight line.What does a cloud architecture audit mean? If you’re just starting your cloud journey, it’s a way to test and track the optimization of your proposed solutions. You want to get as close to the middle of the chart as you can.Also on InfoWorld: When hybrid multicloud has technical advantagesIf you’re mid-journey, which most of us are, an audit is a way to evaluate solutions already in place as well as solutions that are currently under development. The hard part will be to allow others to evaluate your decisions. In many instances, they will second-guess the initial architecture using a forced ranking process which will put your solution at some point between 1 and 37 on this chart. Hopefully you land in the middle, but it’s unlikely.It’s not a sign of weakness to have others check your work. People who call for architecture audits on themselves should be praised. The alternative is to spend millions of unnecessary dollars or even do irreparable damage to the business. The stakes are just too high to let egos get in the way. Let’s be smart.", "pub_date": "2020-10-14"},
{"title": "Getting started with Azure SQL Edge", "overview": "Microsoft scaled down its flagship database, squeezing it into 500MB and running it on edge hardware.", "image_url": null, "url": "https://www.infoworld.com/article/3585795/getting-started-with-azure-sql-edge.html", "body": "The cloud is becoming increasingly distributed, with container technologies allowing easy deployment of what had been cloud functionality to devices at the edge of the network. What began with function-as-a-service runtimes has now graduated to supporting PaaS tools and technologies, stretching the cloud platform from public hyperscale data centers down to low-cost devices running on your network.Microsoft has been talking about “the intelligent cloud and the intelligent edge” for a long time now, with a focus on finding ways around the bandwidth crunch between data sources and data processing in cloud architectures. There’s a certain level of self-interest in this approach, with hyperscale data centers close to cheap power but a long way from metropolitan population densities and relatively cheap bandwidth. If power is cheap and data is expensive, then why not process it close to the source?Also on InfoWorld: Amazon, Google, and Microsoft take their clouds to the edgeSQL Server everywhereAt the heart of much of Microsoft’s cloud platform is its SQL Server database, now grown into a family of databases that work across public and private clouds; at desktop, data center, and hyperscale; and on both Windows and Linux. That family of databases has recently been joined by a new member: Azure SQL Edge. Designed for IoT (Internet of Things) and edge gateway scenarios on Intel and ARM hardware, it’s a small, containerized database that can operate both while connected and disconnected, providing a place to preprocess and stream data that can be developed remotely and delivered and managed from Azure.Like the rest of the SQL Server family, Azure SQL Edge is a relational database with nonrelational capabilities. You can use it for traditional table-based storage, or instead work with JSON documents, with graph content, or with time-series data. That combination ensures you can use it for many different roles, but with a focus on IoT applications. It’s built on the SQL Server engine so there’s no need to learn new skills; the same techniques and programming tools you use on larger SQL Server instances work just as well on Azure SQL Edge, with T-SQL queries and functions developed elsewhere fully supported.Azure SQL Edge deployment optionsAzure SQL Edge can be deployed in two different ways. If you’re using Azure IoT Edge to deploy and manage IoT applications, you can find it on the Azure Marketplace and deploy it directly into your instances from the Azure Portal. If you’re using it on unmanaged systems, then it’s available in a container in Docker Hub and can be downloaded and run as a stand-alone container or used as part of a Kubernetes container orchestration. This last option is particularly interesting if you’re considering working with one of the edge-focused Kubernetes implementations, such as Canonical’s MicroK8s or Rancher’s K3s.It’s important to remember that Azure SQL Edge is a variant of the Linux SQL Server release, and its container’s base operating system is Ubuntu 18.04. To get support, any host will need to run either 18.04 or 20.04 LTS releases of Ubuntu (so you can build and test applications on any Windows 10 PC running Windows Subsystem for Linux). You can use other Linux hosts or even the Windows version of Docker, but they won’t be tested and may not have all the services and applications needed to run your database.Once a container instance is up and running, you can connect to it using the built-in command line tools (if you’re not running on ARM64, which doesn’t currently support sqlcmd). From here you can create tables and test queries, much like any other SQL Server database. In practice, you’re more likely to work with it remotely, using your choice of management and development tools. You need to ensure that appropriate container ports are open as part of the initial configuration process to use external tools.Much of Azure SQL Edge is designed to support traditional database operations; there’s additional support for basic stream analytics to work with real-time data from devices. You can use this to build applications that filter data before uploading to a central processing application in Azure, or as a feed into machine-learning applications that work with models developed on Azure or development PCs and exported as ONNX (Open Neural Network Exchange).Designing database applications for the edgeYou don’t need new tools to manage or develop for Azure SQL Edge. It’s designed to work with existing SQL Server development tools, so you can continue to work as usual when designing databases and building triggers or stored procedures. If you’re developing against a live instance on lab hardware, you might want to take advantage of both the remote development and SQL Server extensions for Visual Studio Code. Being able to build, test, and debug applications in a single environment will help keep development time to a minimum.As the container is only 500MB, it’s possible to run Azure SQL Edge on smaller devices such as the 8GB Raspberry Pi 4, especially after recent firmware updates have allowed it to be booted off SSD rather than SD cards. SSD storage makes devices like the Pi faster and more reliable; relatively slow and cheap SD card storage is not designed to handle the write cycles of a modern database.Azure SQL Edge may be a variant of SQL Server, but you shouldn’t expect to be able to bring all your existing applications to the edge, at least not without some modifications. Azure SQL Edge is designed to run only a subset of the core SQL Server database services, so you won’t have access to other parts of the platform, like Analysis or Reporting Services or most of its Machine Learning tools. In practice that shouldn’t be too much of a problem, as those features work best either on interactive systems or with the horsepower necessary for building and training models with large data sets. Using ONNX for inferencing on the edge allows you to deliver pretrained models, ready to filter data or flag exceptions for further analysis.Although Azure SQL Edge can operate with a direct connection to Azure SQL instances in the cloud, it’s designed to function as a stand-alone database on disconnected, low-power systems. With that in mind, you should build applications that don’t rely on connected features such as stretch databases or memory-intensive tools like OLTP, as they’re not part of the Azure SQL Edge build.Also on InfoWorld: When hybrid multicloud has technical advantagesYou get what you pay forLicensing is relatively simple. There are two editions: one for production, one for development. The development SKU supports as many as four cores and 32GB of memory; the production version supports as many as eight cores and 64GB of memory. Those are relatively high specifications for edge hardware, but they’re what you’d expect from hub devices at the network edge. In practice, this hardware would collate and process data from multiple edge devices. Pricing starts at $10 per device per month, with reductions for one-year and three-year reserved instances that can bring it down to $60 per device per year.Even though the price for Azure SQL Edge is relatively low, you do need to compare it with open source alternatives that can interoperate with cloud databases. It has an advantage if you’re already building SQL Server applications as part of an IoT strategy, if you need to take some of their functionality to the edge in order to manage bandwidth costs, or if you need to operate in a partially connected mode, leaving edge hardware operating autonomously for much of the time.", "pub_date": "2020-10-14"},
{"title": "3 unexpected predictions for cloud computing next year", "overview": "It's clear that cloud computing will continue to grow, but here are three less-obvious aspects to keep on your radar.", "image_url": null, "url": "https://www.infoworld.com/article/3586191/3-unexpected-predictions-for-cloud-computing-next-year.html", "body": "It’s that time of the year when PR folk promote their clients as somebody who can make predictions for 2021 that all should regard. I’m often taken back by the obvious nature of the assertions, such as “cloud computing will continue to grow” or “there will be an expanded need for cloud security.” Okay, that’s not at all helpful.Also on InfoWorld: When hybrid multicloud has technical advantagesThe reality is that although we can see much of the future based on obvious past and current patterns, other portions of the cloud computing market are much tougher to forecast. Enterprises won’t see some trends until it’s too late. Here are three to at least put on your radar: Today we’ve not seen a demand for the three major cloud brands to work and play well together at deeper levels. This is largely because they operate in their own self-interest, and building bridges between public clouds is bad for business.  With more than 90 percent of enterprises using multicloud, there is a need for intercloud orchestration. The capability to bind resources together in a larger process that spans public cloud providers is vital. Invoking application and database APIs that span clouds in sequence can solve a specific business problem; for example, inventory reorder points based on a common process between two systems that exist in different clouds.       Emerging technology has attempted to fill this gap, such as cloud management platforms and cloud service brokers. However, they have fallen short. They only provide resource management between cloud brands, typically not addressing the larger intercloud resource and process binding.   This a gap that innovative startups are moving to fill. Moreover, if the public cloud providers want to truly protect their market share, they may want to address this problem as well.   Self-healing is a feature where a tool can take automated corrective action to restore systems to operation. However, you have to build these behaviors yourself, including automations, or wait as the tool learns over time. We’ve all seen the growth of AIops, and the future is that these behaviors will come prebuilt with pre-existing knowledge that can operate distributed or centralized. This means that from day one you can automate most of the issues that cloud and non-cloud systems will need to deal with, and knowledge will build and be shared over time. No matter if it’s operations model changes, skills gaps, or flattening organization structures, enterprises need to renew their focus on the people aspect of cloud computing.   Skills, organizational structures, and processes need to change around the use of cloud computing. Unfortunately, most organizations view these changes as “opening Pandora’s box,” as one client put it years ago. It becomes a can that leadership kicks down the road. Truthfully, you won’t get much out of cloud computing without the courage to push these changes through leadership. The alternative is good technology and bad technology usage, and that means failure.   I suspect more unexpected things will happen next year than expected ones. Watch this space for a discussion of those emerging issues.", "pub_date": "2020-10-16"},
{"title": "How to make the most of the AWS free tier", "overview": "10 tips for stretching Amazon’s free services to the limit and keeping your cloud bill near zero. ", "image_url": null, "url": "https://www.infoworld.com/article/3585757/how-to-make-the-most-of-the-aws-free-tier.html", "body": "Free is a powerful incentive. When I taught a course on web frameworks at the local college, we designed the assignments to ensure that all of the experiments could be done quickly with Amazon Web Services’s collection of free machines. Each student created, built out, and stood up more than a dozen different servers and they didn’t add a penny to their student debt.This is a good example of why Amazon and the other cloud services offer hundreds of different ways to try out their products. New products are born, tested, poked, and prodded for only the cost of the developer’s time. If the code makes it big and starts generating enough revenue, the developers can grow into paying customers. If it doesn’t and they don’t, at least the developers will become comfortable with the tools and probably turn to Amazon for the next project.Also on InfoWorld: Amazon, Google, and Microsoft take their clouds to the edgeThe free tier is not just for ramen-eating students. Sometimes asking the boss for a budget line, no matter how small, means triggering a series of questions and meetings that demand explanations. A number of good developers test their plans on free machines because it’s much more impressive to present a running prototype than a slide deck with some mockups.Amazon offers three different kinds of free services. Some are short-term samples, allowing you to evaluate a new service for a month or so. They’re meant to get teams to explore new products. Others are like a generous welcome wagon for new developers who sign up for an AWS account. They can start exploring without the worry of a bill because they last a full year after you create your new account.The most generous are the “always free” offerings that keep going and going. Some developers make it a point to build their products to live in the free tier as long as possible. It’s a bit of a game because development resources aren’t too expensive at first. They may be saving a few dollars. But this focus on the bottom line can produce good applications that are cleanly engineered to use a minimum of AWS’s resources. When they scale, the bills will scale a bit more slowly. Here are 10 suggestions for how to play the AWS stack and generate the smallest bills using the freest of services.Waste not want notMost of the AWS services in the free tier come with a limit, usually enforced each month. Some of these seem impossibly large like AWS Lambda’s grant of one million function calls. After you’re finished paying homage to Dr. Evil from the Austin Powers movies by echoing his pronunciation of “million,” you can start budgeting your use of these function calls to the most important jobs. Even the generous limits can be exhausted. A million can come pretty soon if you’re not careful.Go staticThe options for computation in the free tier are pretty limited and so it pays to reduce the server-side computation as much as possible. Static site generators like Jekyl or Gatsby turn the data in your dynamic website into HTML, JavaScript, and CSS files that sit out in a static web server. Perhaps you’ll move them to a CDN like Amazon’s CloudFront. Perhaps you’ll serve them directly from Amazon S3. Perhaps you’ll even park them in the corner of another server around your office. The point is to save computational resources that would be generating your web pages dynamically so you can stay within the free tier.Go serverlessAWS Lambda is the only Amazon compute option that remains free after one year. It’s also arguably the best option for a service that will scale smoothly to handle thousands, millions, or billions of requests. Choosing Lambda from the beginning sets your application up for success in the future.Go NoSQLAmazon also encourages us to use their DynamoDB by including 20GB of storage space that’s always free. DynamoDB may not offer the same clever indexing and normalization options that relational database lovers have embraced over the years, but NoSQL remains a smart and flexible architectural choice that’s especially forgiving for evolving prototypes and pivoting startups.Also on InfoWorld: 6 ways Alibaba Cloud challenges AWS, Azure, and GCPCombine AJAX callsSometimes you’re going to need to make your site interactive. The best approach is to bundle the calls to your web services into as few transactions as possible. The Amazon API Gateway free tier, for instance, includes one million API calls and one million HTTP calls. Bundling all of your data into one call makes these limits last longer than dutifully invoking the calls immediately. The simplest way to accomplish this is to cut back on storing documents or form data for the user. Yes, this can make the service a bit less robust and crash resistant, but that’s the cost of doing things for free.Empower the clientWhile cookies and their lesser known cousins like the local Web Storage API have a reputation for helping big business track people, they also offer the opportunity for users to control their privacy by storing their local data. It also makes it easier to build a free tier web application by offloading the cost of storing client data on the client’s own machine. The users’ machines store the data so you don’t have to!More privacy and less central costs. It would be a perfect solution if it weren’t for the total catastrophe that follows a lost phone, a crashed local disk, or any of a million other failures. It’s best to use this for casual data, not mission-critical information.Avoid gimmicksSome websites have added flashy interactive features like autocomplete. These may be fun and they may generate attention, but each of these features usually requires another request to the cloud and that eats into your limit. Avoiding unnecessary moving parts is the simplest way to save compute resources.Run your own databaseThe Amazon-managed relational database services like MySQL or PostgreSQL are great tools for starting up and maintaining a database to hold your app’s information, but the free tier only offers you one of them and it’s only for the first 12 months. There’s nothing stopping you from running your own database on one of the free EC2 instances that are also available for the first 12 months. Yes, you’ll need to install them and configure them yourself, but it will double your database options.Log carefullyAll of the free storage at AWS comes with limits. Good developers create good log files to debug problems and catch failures, but most log files are never used. Staying within the limits for storage is simpler if you clean out your logs frequently. Some just throw away the data and some download it to their desktop disk.Use non-cloud resourcesIt’s not exactly a fair answer to say that you can get more out of the free tier by running your own server back on your desk. Still, some judicious use of non-AWS services can really stretch the work being done on the cloud. Database backups, for instance, could move to your desktop, which might have several terabytes of empty space waiting for some of the random detritus. And you’re probably going to want to back up your projects outside of the cloud anyway. Any service or data that doesn’t need the immediate response and constant uptime of the cloud is fair game.Recognize the limitsThe free tier is an excellent way to explore AWS and it’s fun to strip away all of the extraneous features to try to generate bills for $0.00, but at the end of the day AWS is a business and the free tier is a well-designed marketing tool not a public charity. Some people openly create new accounts with new email addresses to continue to restart the 12 month clock. This may work with disposable projects but not with those that have started attracting users that will be disrupted when you switch accounts.Also on InfoWorld: 14 ways AWS beats Microsoft Azure and Google CloudWhen your creations have found an audience, it’s time to start finding a way to pay the bills. The good news is that all of the lessons you’ve learned from living in the free tier will keep your bills much lower. The API Gateway, for instance, charges just $1 for a million invocations. If you’ve been successfully running in the free tier, then your bills won’t be more than a few dollars a month.That should hold until everything goes insanely viral and your outrageous good fortune makes the AWS bill the least of your worries.", "pub_date": "2020-10-19"},
{"title": "How to manage cloud migration remotely", "overview": "These days, everyone on the cloud migration team is dispersed all over the city or country. This presents some challenges, but I see more opportunities. ", "image_url": null, "url": "https://www.infoworld.com/article/3586299/how-to-manage-cloud-migration-remotely.html", "body": "It’s Monday morning standup, and 22 faces spread across your screen on yet another Zoom call. The focus is on the day’s tasks, as well as looking at problems that need solving.  Unique to this experience is that just seven months ago, all of these faces were present in the same conference room, before COVID-19 sent them home to work remotely. Also on InfoWorld: PaaS, CaaS, or FaaS? How to chooseOne of the more common question I get lately is how to manage a remote migration team and do so without allowing schedules to slip? It’s really not as hard as it sounds; indeed many aspects are easier. Let me offer some advice to those who are tasked with managing remote teams and those who are on remote teams.   No matter if it’s Slack, Teams, Yammer, or something else, there needs to be a mechanism for instantaneous communications and real-time collaboration on tasks. Text messages won’t do here. Communication tools should be integrated with migration tools, capable of launching migrated application testing from the collaboration tool and returning results directly to the team.  Of course, you can go overboard with tools and use too many. Complexity leads to confusion, which leads to mistakes. You’ll need to find a balance.  One of the benefits of everyone sitting in the same building is that everyone is protected by the same security mechanisms: firewalls, local encryption, and even security management tools.  Working remotely means the same security is spread across networks and firewalls out of your direct control. Managing a remote migration team requires a focus on security management and putting technology in place that can manage heterogenous networks(including a variety of home networks) as well as—or better than—when everyone was within the enterprise.  Gone are the days when leaders walked around looking over everyone’s shoulders, sometimes to watch the screen change suddenly.  Those who try to continue such control over remote workers won’t find any positive outcomes from this leadership style. Indeed, turnover will likely increase, and more so when employees realize that they don’t have to carry the dreaded cardboard box out of the building, since they are not allowed in the building.  Those leading migrations, remote or not, need to get over the whole control thing. Focus on expected productivity. Agree on what’s expected and how employees will be measured. Productivity will likely increase, considering that staffers don’t feel watched, and also are no longer spending hours commuting each week.These aren’t exactly secrets, but I’m seeing these mistakes kill migration efforts. With a bit of forethought, you’ll find you can actually move faster.  ", "pub_date": "2020-10-20"},
{"title": "MongoDB Atlas unveils multicloud cluster support", "overview": "MongoDB Atlas database-as-a-service now allows distributed MongoDB databases to span the Amazon, Google, and Microsoft clouds", "image_url": null, "url": "https://www.infoworld.com/article/3586628/mongodb-atlas-unveils-multicloud-cluster-support.html", "body": "NoSQL database vendor MongoDB will enable users to deploy a distributed MongoDB database across AWS, Google Cloud, and Microsoft Azure via a multicloud clusters capability being added to the company’s MongoDB Atlas cloud database service.Through the multicloud clusters functionality, MongoDB Atlas users are spared the operational complexity of managing data replication and migration across clouds. The multicloud support lets users take advantage of unique capabilities and the reach of the different cloud providers while also providing for uptime assurances.Also on InfoWorld: The best open source software of 2020Starting today, MongoDB Atlas, which is available in 79 AWS, Google Cloud, and Microsoft Azure regions worldwide, now supports automatic fail-over to another cloud serving the same geographic area to preserve low-latency access and data resiliency requirements. Previously, MongoDB Atlas users had to select a single cloud provider per deployment.MongoDB cites several thematic areas of the multicloud support:Taking advantage of cloud-specific functionality, such as having a user whose primary cloud is AWS but wants to leverage Google Cloud machine learning capabilities.Data mobility, with users able to move data from one cloud to another with no downtime.High availability, with users able to spread redundancy across three providers and withstand global outages of any one of them.Enabling companies to expand their cloud access beyond their established platform. Some companies may be required to use or not use a specific cloud, based on customer requirements.MongoDB provides a document-oriented NoSQL database that features an object-native JSON-style data model, as opposed to storing data across tables as relational databases such as Oracle do, MongoDB said. Applications deployed on MongoDB Atlas range from major consumer financial apps to large-scale games. ", "pub_date": "2020-10-20"},
{"title": "3 cloud architecture secrets your cloud provider won’t tell you", "overview": "You may think you know everything about the proper configuration of a cloud computing solution, but cloud providers are keeping a few things to themselves.", "image_url": null, "url": "https://www.infoworld.com/article/3584411/3-cloud-architecture-secrets-your-cloud-provider-wont-tell-you.html", "body": "Do you have an optimized architecture? This means that your solution maximizes efficiency and minimizes costs. You’ve selected the right cloud resources to configure the best storage systems, databases, and compute platforms—at least that’s what you think.What I’m seeing out there, over and over again, is the selection of the wrong cloud resources for the wrong reasons. Cloud providers are pushing something that maximizes their revenue rather than being right for you. Also on InfoWorld: When hybrid multicloud has technical advantagesSo, here are three cloud architecture secrets that you’ll never hear from your cloud provider:You’ve probably heard that it’s better to go with a native database, cloudops system, or security system that’s part of a single public cloud offering. Now that we’ve moved to a mostly multicloud world, that’s just not the case.   It’s much better to pick general-purpose and heterogeneous solutions that span public clouds instead of a native solution that’s only good on a single public cloud. You’ll never see this in the architecture guide offered by your cloud provider. Non-native resources should be considered each and every time.   Cloud solutions that depend on a lot of data ingress and egress are almost never a good idea. No brainer, considering that you’ll see data leaving and entering a public cloud provider on your monthly cloud bill, and it is not cheap. However, this is often overlooked when considering a core architecture.   This is typically an issue for IT organizations that want to keep some data on-premises, usually due to outdated concerns about compliance and security. The providers won’t advise you otherwise, considering that they make bank on the exit and entrance charges. Keep your data in the cloud if you’re looking for the best performance and security and the lowest costs.I often see security systems bound to a single application’s workload. The application leverages its own encryption system, identity management systems, role-based security, etc. Typically, these are also native to a single cloud provider where the application is hosted. The issue here is that a cloud provider wants the workload in the cloud ASAP and will often advise for the speed of movement instead of a sound security architecture. This can’t scale, considering that you’ll be creating one-off security solutions for all applications, and it will create so much security complexity that you’ll have security issues just from the complexity.Security should be systemic to all things in the core architecture. Applications should use very similar security patterns—and the same security systems, if at all possible. Again, these are typically non-native, and your cloud provider won’t benefit as much.By the way, I’m not picking on cloud providers. They are only acting in their best interests. However, the savvier you are, the more you know when to accept and reject their advice.  ", "pub_date": "2020-10-02"},
{"title": "The best open source software of 2020", "overview": null, "image_url": null, "url": "https://www.infoworld.com/article/3575858/the-best-open-source-software-of-2020.html", "body": "", "pub_date": "2020-10-05"},
{"title": "Hybrid cloud is where the action is", "overview": "Enterprises should be more concerned with getting their hybrid cloud strategy right than worrying about multicloud.", "image_url": null, "url": "https://www.infoworld.com/article/3583974/hybrid-cloud-is-where-the-action-is.html", "body": "Multicloud is definitely a thing. However, it’s not exactly clear what that “thing” is. According to new survey data from database vendor MariaDB, 71% of survey respondents report running databases on at least two different cloud providers today. Yet when asked what would keep them from going all in on a cloud database, a vendor’s “lack of a multicloud offering” ranked dead last. In other words, everyone is doing multicloud, but no one knows why.Which perhaps supports Gartner analyst Lydia Leong’s contention that, “Most organizations multicloud, rather than  to be multicloud in a deliberate and structured way.” Pragmatism, not dogmatism, rules.The database is getting cloudyIf you’re bullish on cloud, MariaDB’s survey data will confirm your bias. According to the survey, roughly 89% of those surveyed expect to see at least 50% of their databases running in the cloud by 2025. Nor is MariaDB’s data an outlier here: Gartner goes one step further, projecting that 75% of all databases will be deployed or migrated to a cloud platform by 2022.As much as I hope this is true (I work for a cloud vendor, after all), I will admit that it seems overly ambitious. Let’s do some math. First, Gartner pegs global IT spending at $3.9 trillion in 2020. Cloud spending, by contrast, will hit $266 billion. I’m no math genius but I think that means just 7% of IT spending is going toward cloud this year (and, of course, a fraction of that will be spent on databases).It’s  that tens of billions of dollars in database cloud spend will shift from on-premises to cloud workloads within two years, but IT tends to move slowly, particularly with databases. This isn’t to suggest cloud isn’t a big deal. It is. But it’s going to take time.And yet...There are plenty of reasons for organizations to want to move their database workloads to the cloud. According to the MariaDB survey, here are the primary benefits companies already derive from cloud databases:Ease of use (24.0% of respondents)Time savings (23.6%)Modernization (21.3%)Cost savings (21.1%)[Frees up resources to] Focus on other business aspects (9.3%)Pretty compelling, right? Spurred by these benefits, respondents were asked what would keep them from going all in on a cloud database:Security (73.3%)Price (46.2%)Compatibility (44.9%)Scalability (35.2%)Migration (33.1%)Lack of multi-cloud offering (21.1%)Other (1.3%)It’s interesting that “multicloud” ranks last here, despite (as noted above) the fact that 71% report running databases in at least two clouds. Multicloud doesn’t seem to be a motivation to buy. Rather, it’s just a fact of enterprise existence.Enterprise is as enterprise doesMany years ago, Billy Marshall coined the phrase, “The CIO is the last to know.” He was referring to how open source software makes its way into enterprises through developer laptops, but the same phenomenon plays out today with cloud services, even at smaller companies. At one startup we first guessed how many SaaS applications we had running; I don’t remember the number but it was small, like 10. When we actually tallied up the true number it was closer to 80.As noted above, “ease of use” is the primary benefit companies derive from the cloud. This convenience factor opens the door to more cloud finding its way into the enterprise, not less. Leong calls out this longstanding enterprise principle with her typical style:Multicloud is inevitable in almost all organizations. Cloud IaaS+PaaS spans such a wide swathe of IT functions that it’s impractical and unrealistic to assume that the organization will be single-vendor over the long term. Just like the enterprise tends to have at least three of everything (if not 10 of everything), the enterprise is similarly not going to resist the temptation of being multicloud, even if it’s complex and challenging to manage, and significantly increases management costs. It is a rare organization that both has diverse business needs and can exercise the discipline to use a single provider.To Leong’s point, multicloud may be a fact of life but it doesn’t tend to be one that companies choose, for the reasons she cites.But what  an intentional, critically important factor in cloud (and cloud databases) today is hybrid. Asked how important hybrid cloud was to corporate objectives, respondents were very clear:Not at all important (2.5%)Somewhat important (19.4%)Very important (39.2%)Extremely important (38.9%)That is, 97.5% of respondents believe hybrid cloud is at least somewhat important, with a whopping 78.1% saying it’s very important or extremely important.The reason for this should be clear from the IT spending data I laid out earlier. However much enterprises may want to get to the cloud as quickly as possible, they still have workloads that will stay on-premises for a time, and sometimes forever. As such, enterprises rightly are much more concerned with getting their hybrid cloud strategy right than worrying about their multicloud non-strategy. It may not win enterprise IT any buzzword bingo contests, but hybrid is where the action is.", "pub_date": "2020-10-05"},
{"title": "Survey finds cloud complexity increases challenges", "overview": "Convoluted cloud solutions are a big reason cloud implementations don’t work as hoped.", "image_url": null, "url": "https://www.infoworld.com/article/3584360/survey-finds-cloud-complexity-increases-challenges.html", "body": "Bridging the Cloud Transformation Gap is a report that evaluates the findings of Aptum’s Global Cloud Impact Study. Aptum’s annual study seeks the opinions of 400 senior IT decision makers. Keep in mind that these sponsored reports can be self-serving, either for lead generation or to promote a publisher’s dogma. This report is behind a registration wall. Also on InfoWorld: Which multicloud architecture will win out?Despite possible biases, some good data points here address the intent to migrate to clouds versus the reality of doing so. I found the information realistic, considering that similar reports only cover the positive aspects of cloud migration. One of the more interesting bits of data points to the issue of complexity:The survey reveals this with 62 percent of respondents citing complexity and abundance of choice as a hindrance when planning a cloud transformation. One of the biggest sources of complexity that crops up in more advanced cloud projects are legacy systems.If these issues sound familiar, I’ve been waving my hands about them for a while now. I even took some early arrows as cloud providers, peers, and other cloud vendors felt my insistence in sounding the complexity alarm had a negative impact on their ability to sell the cloud. Back to the survey, 62 percent of respondents is a pretty big number, even more than I thought would be aware of the problem. Of course, these are only the IT leaders who understand they have a problem. Based on my experience, the percentage of IT execs who actually do have complexity problems is higher.What we’ve tried to figure out in the past few years is where complexity originates. My implementation experiences are backed up by this report. The “abundance of choice” or the need to select the best of breed is a prime culprit. This usually results in a technological smorgasbord, where hundreds of decoupled cloud dev and migration teams make their own calls around what technology to use. Complexity naturally arises when it’s time to join and coordinate those apples and oranges. The legacy issue is even less understood. The primary culprit here is that legacy systems don’t seem to be going away. Although you can create a different ops plan for these older systems, the need for them to work and play well with cloud-based systems results in a deeper problem. The end state of both issues? Too many technologies exist within the solution sets for the enterprise to cost-effectively or efficiently operationalize. That’s where cloud typically falls on its face. IT groups are forced to put off deployments or go back and mediate the complexity. Also on InfoWorld: When hybrid multicloud has technical advantagesIf there’s a standard approach for dealing with complexity, the approach itself will take some effort. It’s not an easy problem to solve, either before or after deployment. Some operations tools (such as AIops) can sometimes provide assistance here, as well as other monitoring and management tools that work with cloud and legacy workloads. I’ve had some success with these tools; they can abstract the complexity from the ops team, either using a single-pane-of-glass approach or layers of automation. Of course, tools can take you only so far. The real challenge is to first understand that you have a problem, and then take proactive steps to solve it. Be aware of potential complexity at all stages of the planning process, and you will greatly increase your odds of success with cloud implementations.", "pub_date": "2020-10-06"},
{"title": "Static versus dynamic cloud migration ", "overview": "Many plan to use the same tools from one cloud migration project to the next. A command and control center might give different advice.  ", "image_url": null, "url": "https://www.infoworld.com/article/3584430/static-versus-dynamic-cloud-migration.html", "body": "It’s 9:00 AM on a Wednesday. You’re in the boardroom giving a status update on the latest migration project that will remove most of the vulnerabilities found during the recent pandemic. This is the third migration project, all less than 100 workloads and 10 data sets. All have taken place in parallel, and all leverage different cloud migration teams.Company leadership notes that the metrics were very different between the projects. Project One shows nearly 80 percent efficiency in terms of code refactoring, testing, deployment, security implementation, etc. The others were closer to 30 and 40 percent. Why the differences? Also on InfoWorld: How to choose a cloud machine learning platformMost efficiency issues arise from dynamic versus static migration approaches and tools. Most people who currently do cloud migrations gravitate toward the specific processes, approaches, and migration tool suites that worked for past projects. This static approach to cloud migration forces a specific set of processes and tools onto a wide variety of migration projects and problem domains. The misuse of specific processes and tools as generic solutions often leads to failure. Core to this issue is our drive to find a specific suite of tools and technology bundles that the industry considers best-of-breed, and our desire to leverage best practices. We in IT love to follow the crowd: \"I read about these tools and this approach that worked for Joe and Jane and Bob at companies kind of like mine, so they’ll work for me, too.\" We make the erroneous assumption that we remove risk by not making our own choices, even if the choices are situational. As an expert in this field, I would love to list a standard set of migration tools that will address everyone’s needs—code scanners, configuration management, continuous integration and development, testing tools, and more. But the real answer is that your selections of tools and approaches must be based on the requirements of the applications and databases you are migrating to public clouds--or any other platform, for that matter. The project criteria and review processes for migration projects typically include, but are not limited to:“As is” platform assessmentApplication assessmentData assessmentConfiguration management planSecurity migration toolsGovernance migration toolsRefactoring and redeploymentCI/CDTesting and deploymentCloudops/IT operations…and more.The tool categories listed above will have different solutions based on the “as is” and “to be” platforms, development approaches, and databases used, as well as the identified storage, security, and governance requirements. Although a one-size-fits-all approach might work, it will rarely provide the promised efficiencies and could actually derail the entire project if the approaches and tools are too far off. My message here is that you need an extra step in the process. Select the tools and approaches based on what you want to move, where it needs to go, and the attributes it needs to have upon arrival. Almost always, this will drive the selection of a different set of tools for each migration project.The bottom-line message is that the same tools and processes can rarely be reused with optimal results from one migration project to the next.Now it’s time to hear about the exceptions. To take advantage of these exceptions, there needs to be centralized command and control of all migrations, such as a center of excellence where problem patterns and solutions during each migration project can be documented. With centralized command and control (CCC), your best practices will emerge, and they will have a much higher likelihood of working in future migration projects. This is the point in the overall enterprise migration where some tools and processes can begin to be reused. CCC needs to become part of the process when teams initially identify migration targets and begin migration plans. It should be CCC’s duty to compare and contrast planned migration projects with past projects. If CCC can recommend approaches, tools, and processes that worked for previous migrations with similar attributes, you will never need to reinvent the wheel. It’s a hard lesson to learn. The silver lining here is that even if one size rarely fits all, a few sizes can often fit most. As always, the key to success is to do your homework at the beginning instead of the end of a project, document everything, and make that documentation easily accessible to help plan future projects. ", "pub_date": "2020-10-09"},
{"title": "Understanding GraphQL engine implementations", "overview": "Among the many ways of implementing a GraphQL engine, only one approach offers the same  performance, scalability, and ACID guarantees as the underlying database. ", "image_url": null, "url": "https://www.infoworld.com/article/3575530/understanding-graphql-engine-implementations.html", "body": "When we talk about advantages of GraphQL we often hear one-liners such as “only fetch what you need,” “only requires one generic endpoint,” “data source agnostic,” or “more flexibility for developers.” But as always things are more subtle than a one-liner could ever describe.Generic and flexible are the key words here and it’s important to realize that it’s hard to keep generic APIs performant. Performance is the number one reason that someone would write a highly customized endpoint in REST (e.g. to join specific data together) and that is exactly what GraphQL tries to eliminate. In other words, it’s a tradeoff, which typically means we can’t have the cake and eat it too. However, is that true? Can’t we get both the generality of GraphQL and the performance of custom endpoints? It depends!Also on InfoWorld: The best free programming courses during quarantineLet me first explain what GraphQL is, and what it does really well. Then I’ll discuss how this awesomeness moves problems toward the back-end implementation. Finally, we’ll zoom into different solutions that boost the performance while keeping the generality, and how that compares to what we at Fauna call “native” GraphQL, a solution that offers an out-of-the-box GraphQL layer on top of a database while keeping the performance and advantages of the underlying database.GraphQL is a specificationBefore we can explain what makes a GraphQL API “native,” we need to explain GraphQL. After all, GraphQL is a multi-headed beast in the sense that it can be used for many different things. First things first: GraphQL is, in essence, a specification that defines three things: schema syntax, query syntax, and a query execution reference.The schema simply defines what your data looks like (attributes and types) and how it can be queried (query name, parameters, and return types). For example, a todo application could have Todos and a List of todos, if it only provides one way to read this data — e.g., get a list of todos via the id, then that schema would look like this:Once you have a schema that defines how your data and queries look, it’s super easy to retrieve data from your GraphQL endpoint. Do you want a list? Just call the getList item and specify what attributes of the list you want to return.Do you want to join that data with todos? No problem! Just add a small snippet of JSON to the mix.But how does this join happen? Of course we did not yet define how this query actually maps to data that comes from our data source.It’s relatively easy to understand the schema and see how you can query. It’s harder to understand what the performance implications are since there are so many different implementations out there. GraphQL is  an implementation to retrieve your data. Rather, GraphQL provides guidelines on how a request query should be broken down into multiple “resolvers” and turned into a response.Resolvers define how one element of the query (what GraphQL calls a field) can be turned into data. Then, depending on the framework or GraphQL provider, you either implement these resolvers yourself or they are provided automagically. When resolvers are provided for you, the implementation will determine whether we can talk about native GraphQL or not. In essence, the resolvers are just functions that have a certain signature. In JavaScript, such a resolver could look like this:And each of our “fields” will have a corresponding resolver. Fields are just the attributes in your schema.Each of these fields will have a resolver function (either generated by the library or implemented manually). The execution of a GraphQL query starts at a root field resolver, in this case, getList. Since getList promises to return a List, we will also need to fill in the fields of the List; therefore we need to call the resolvers for these fields and so on. In essence, it’s a process of recursive function calls. Let’s look at an example:For the above query, we would traverse three fields, each with a resolver function: returns a List with only the field todos receives the List item from getList and returns a list of Todos related to that list. receives a Todo item from the todos resolver and returns a string from the title.This is in itself a very elegant recursive way to answer the query. However, we will see that the choices made in the actual implementation will have a huge impact on performance, scalability, and the behavior of your API.Patterns for writing resolversIn order to understand the different approaches, we need to learn how to get started building a GraphQL execution engine. The syntax of that depends on the server library we choose, but each library adheres to the resolver guidelines described above.As we have explained, implementing a GraphQL engine is all about implementing functions called resolvers with a specific signature.The arguments serve different purposes for which you can find the details here. The ones that are most important for this implementation are: is the previous object. In the previous example, we mentioned that the todos resolver receives the List object that was resolved by getList. The object parameter is meant to pass on the result of the previous resolver. is the argument(s). In getList, we pass an ID, so args will be {id: \"some id\"}.We can’t do much with one resolver function, and our GraphQL server library will need to know how to map a query to the different resolvers and how we delegate work from one resolver to the next resolver. Each library has a slightly different syntax to specify this, but the general idea remains the same. For example, one syntax to specify the mapping of queries to resolvers could look as follows:If we write the query below, we can match this to resolvers by first looking into the root resolvers (the ones in Query) where we will find the getList resolver. Since that resolver returns a List, the GraphQL execution engine knows that getList returns a List and therefore it needs to go search in List for the resolver of the todos field. The way this resolves is called the resolver chain.Now that we know how GraphQL libraries want us to write resolvers, we can start thinking about how it affects performance.The above explanation might make you think that resolving a GraphQL query is quite linear, but in fact resolver chains are more like chains that keep on splitting... Oh wait, that’s just called a tree!GraphQL approach #1: The naive implementationWhen we implement resolvers naively in this rather elegant recursive system we can easily write an API that is slow. By playing human interpreter on the previous query we can see how a naive implementation results in a tree-like execution plan that sends many queries to the database.Although there are only two database calls in the above implementation, one of these calls, getTodo(id), is called for each Todo that is associated with the List. That means that we will be hammering our database with 1 (getting the list) + N (getting each todo) calls and joining all this data in the back end. This is known as the N+1 problem.Remember that, in contrast, a clean REST API would allow us to call each of these resources separately from the front end and require us to join these somewhere (e.g. in the front end) or require us to build custom endpoints. There were two things we wanted to improve upon by using GraphQL: Multiple calls from the front end, requiring that the data also has to be joined in the front end.Workarounds that require you to build a custom endpoint for performance or for each new front end or app requirement. For example, if you have a new UI for your mobile application, it might have different requirements. It might require other joins or fewer attributes.An example that depicts the number of calls in a REST back end, both when we follow REST practice by separating endpoints per entity type and when we optimize by writing custom endpoints to join.Although our naive GraphQL endpoint effectively solves these issues, it creates another problem further down the road.The introduction of GraphQL in combination with a naive implementation moves a problem that was transparent to the back end, where the problem might still be present but is hidden from the API user. In essence, we replaced the problem of multiple REST calls with the N+1 problem. It is not necessarily a problem to express generic queries in REST because similar things can be done with Odata. The difficulty is to provide an efficient implementation for such a generic endpoint.With the naive implementation, we now have an even higher number of queries between the back end and the database, and the results have to be merged in the back end. Joining many small requests requires memory and might cripple our database as we scale. This is clearly not ideal.GraphQL approach #2: Batching with DataloaderFacebook has created a great piece of software, called Dataloader, that allows you to solve a part of the problem. If you plug in Dataloader, similar queries will be essentially cached per frame or per tick. Consider the following execution, without Dataloader.Dataloader takes in all of these queries, waits for the end of the frame, then combines them into one query.Besides that, Dataloader does in-memory caching of certain queries’ results. With Dataloader, we have now improved the number of database calls significantly. However, it only helps for similar queries that fetch the same entity type, which means that we are still joining data in memory and we are still doing multiple calls. If we look at the queries that are sent to the database, the approach now looks like this: This is already great, but we can do much better. After all, joining in memory in the back end is just not scalable as data grows and our schema becomes more complex, requiring that multiple entities be fetched. For example, consider the following model from a Twitter-based example application called Fwitter.We need to retrieve not only the Fweets (which are like Twitter’s tweets), but their comments, hashtags, statistics, author, original fweets in case of a refweet, etc. We face a complex in-memory join, and again, many database calls. Also on InfoWorld: 7 best practices for remote agile teamsGraphQL approach #3: Generating the queryIdeally, if possible, we want a GraphQL query that translates into one database query. Sounds simple right? However, we’ll see that code generation can be hard or impossible depending on the underlying data storage and query language.For starters, this approach can raise many questions. Do we generate one query or multiple queries? Can we even generate one query that expresses a complex GraphQL statement? If we generate a big query with many joins, is it still efficient?Since native GraphQL is a special case of “generating the query,” let’s first (and finally) define what we at Fauna call “native” GraphQL. We’ll answer these questions in the subsequent section, “Why is native GraphQL so difficult to achieve?” GraphQL approach #4: Native GraphQLWe can take the previous approach one step further and run this translation layer as a part of our infrastructure close to the database. That means that an extra hop will be eliminated. And if our database is multi-region, our GraphQL API will not lose the latency benefits of that.If the GraphQL layer is a part of the database and allows you to do highly efficient queries straight from your application, then your response latencies will be much better. You’ll be fetching all the entities you need in one hop and only one call. At Fauna, we consider a GraphQL implementation native when the GraphQL layer lives on the infrastructure of the database and adheres to the following conditions:One GraphQL query = one database query = one transactionGraphQL queries offer the same ACID guarantees as the underlying databaseThe underlying database allows efficient execution of such queries", "pub_date": "2020-10-21"},
{"title": "Looking into the post-2020 future of Big Tech", "overview": "A strong showing during the pandemic leaves Apple, Amazon, Facebook, Google, and Microsoft poised for continued growth and increased regulatory scrutiny.", "image_url": null, "url": "https://www.infoworld.com/article/3586797/looking-into-the-post-2020-future-of-big-tech.html", "body": "The year isn’t over, not for more than two months. But it sure feels like 2020 should end soon, seeing as how one pandemic-stressed, politics-inflamed day bleeds into the next.Nevertheless, it might be healthy to start putting 2020 behind us. It’s been a disruptive year in high tech, as in every other industry, but it’s also been a highly public proving ground for cloud, streaming, artificial intelligence, and other pillars of 21st century civilization.Also on InfoWorld: How data analysis, AI, and IoT will shape the post-pandemic ‘new normal’Tech has been the economy’s mainstay during the pandemicAs we squint past the upcoming US presidential election into the new year, the following trends will shape the technology landscape for the rest of this decade, regardless of who occupies the White House on January 20.For starters, the pandemic will drag on longer than people would like to believe. Distancing, quarantining, and lockdowns will still hammer the global economy for at least another 6 to 12 months. However, the tech sector has already figured out how to keep operating in spite of these obstacles. Just as important, its customers now rely intimately on cloud, streaming, remote collaboration, robotics, smart sensors, and other digital technologies in order to stay alive.In 2021, enterprise technology professionals will adjust their strategies with one eye on COVID-19 trends and the other on their digital transformation initiatives. Predictive containment management will become a central element of every facilities administrator’s high-tech toolkit.Socially distanced living will persist long past the present pandemic’s end. People will continue to socially distance in most public interactions for at least the next three to five years. Through fits and starts, the global culture is getting used to personal protective equipment, ubiquitous biosensors, contactless interactions, remote collaboration, sanitization-intensive maintenance, and other changes in how we live and work. Many of these new living styles depend on AI, cloud services, mobile devices, robotics, and other technological platforms.The tech vendors who stand to gain the most from this trend are those—such as Amazon, Google, and Microsoft—that provide full, cloud-to-edge ecosystems that enable seamless “new normal” lifestyles.Furthermore, effective COVID-19 vaccines will come to market only after long delays in R&D and in clinical trials. Hopes for a quick resolution to the pandemic crisis will wither as the grinding process of developing and testing COVID-19 vaccines continues. If we consider the history of vaccines for recent pandemics such as Ebola, it may take two to three years before an effective COVID-19 vaccine arrives. However, the tech sector will benefit from the fact that vaccine development will critically depend on AI, high-performance computing, quantum technology, and other advanced tools.As the human race sifts through false hopes and counterproductive approaches for treatment and cure, tech companies’ solutions will literally become a lifeline for distinguishing what works from what doesn’t.FAANGs will cement their dominance in the global economyAgainst the backdrop of these trends, I wouldn’t bet against the FAANG (Facebook, Amazon, Apple, Netflix, Google) vendors (and also Microsoft) further cementing their tech-industry dominance throughout the 2020s.These six vendors—often grouped together as “Big Tech”—are the foremost business success stories in the present crisis. Tech stocks have been at the heart of this year’s counterintuitive stock market recovery.The mini crash of March is receding in the rearview mirror as the FAANG vendors have proven how essential cloud, streaming, remote collaboration, edge computing, and other digital technologies are to economic resilience. During the past several months, the tech giants have outperformed the rest of the stock market, with the Nasdaq-100 index up 24 percent this year.Also on InfoWorld: Coding together apart: Software development after COVID-19Even as the pandemic wanes, investors will flock to the Big Tech vendors as the safest place to put their money. Tech companies have proven to be low-risk investments, considering that many of today’s leading tech companies are highly profitable and can easily cover their respective debt holdings from available cash.Political and regulatory pressure will build on Big TechOne big wild card in Big Tech’s future may be legal and regulatory pressures to divest themselves of assets that give them what some call an unfair, monopolistic advantage in their core markets.Recently, the US House of Representatives antitrust subcommittee released its investigative findings that Google, Apple, Facebook, and Amazon “have become the kinds of monopolies we last saw in the era of oil barons and railroad tycoons.” The subcommittee specifically called out Amazon’s dominance in e-commerce, Google’s in search and advertising, Facebook’s in social networking, and Apple’s in both mobile content and apps. The companies were cited for such anticompetitive practices as acquiring potential competitors and using their platforms to limit competition, control access, and favor their own products.To address these concerns, the subcommittee recommended updating antitrust laws to reflect today’s digital economy. It also called for additional FTC oversight over Big Tech mergers and acquisitions. Most controversially, the legislators advocated breaking up the Big Tech companies or requiring them to divest themselves of key assets to encourage competition.At first glance, there doesn’t appear to be any natural way to partion these vendors to achieve the stated objective. Requiring Amazon to spin off its third-party seller marketplace wouldn’t lessen the dominance of its core “first-party sales” online retailing site. Splitting off Google’s search assets from its mobile OS, digital advertising, cloud computing, and office productivity assets wouldn’t dent its momentum in any of those other segments. Mandating that Facebook sell off its Instagram, WhatsApp, and Messenger products wouldn’t dilute its hold on its core social networking subscribers. And opening up Apple’s devices to other app stores, music channels, and streaming media channels probably wouldn’t diminish the first-to-market advantages of Apple-founded offerings on those devices.Dominant tech vendors will not back downIf legislators and regulators attempt to force Big Tech’s hand in this regard, they’re likely to set the stage for a long, drawn-out, lawyerly trench war.Even if any of these megalithic digital companies were forced to partition, M&A activity in the impacted segments might bring the pieces back together. This is not a farfetched scenario if we consider how the “Baby Bells” organically re-coalesced in the years after the 1984 AT&T divestiture or the “Baby Standards” reasserted their collective muscle in the decades after the 1911 Standard Oil antitrust breakup.Another likely outcome is that one or more of the post-breakup FAANG companies might rapidly rebuild itself to “800-pound gorilla” scale through the growth-hacking savvy that pervades this industry. All of the FAANG vendors began as startups, as did many of the assets they gained from strategic acquisitions. Nothing would be more natural than for the newly independent, split-off divisions to come roaring back with fresh capital, vision, and momentum.During this decade, Big Tech vendors will face greater regulatory scrutiny if they attempt to buy out direct competitors or substantial vendors in adjacent niches. However, any regulatory effort to hamstring the FAANGs in their core markets is likely to set off a new round of acquisitions in nontraditional (for them) industries. We expect that other Big Tech companies will follow Amazon’s lead in acquiring strong brands in brick-and-mortar sectors that have been severely impacted by the COVID-19 recession and from the tech-instigated disruptions of the past decade. In this way, defunct brands in retail, hospitality, airlines, and other sectors may resurface in new roles within predominantly virtual business models.This almost goes without saying. After all, the FAANGs are sitting on huge piles of cash and are avidly scouting for new investment opportunities, in the classic fashion of any self-respecting capitalist. As of this past May, Apple held cash, cash equivalents, and marketable securities to the tune of $192.8 billion, while Microsoft was sitting on $137.6 billion, Google parent Alphabet had $117.2 billion, Facebook had amassed $60.3 billion, and Amazon held $49.3 billion. Also, the sharp rise in all of their stock prices during the COVID-19 crisis gives them even more purchasing power for snatching up valuable competitive assets.Also on InfoWorld: Amazon, Google, and Microsoft take their clouds to the edgeDuring the coming year, it wouldn’t be surprising if one or more of the Big Tech vendors decide to voluntarily self-partition, without a specific legal or regulatory mandate to do so. We can see foreshadowings of this in Google’s restructuring of its business units into Alphabet subsidiaries four years ago. HP split into enterprise and consumer businesses in 2014 and IBM separated technical services from its core product portfolio earlier this year.If Apple, Amazon, Facebook, and other Big Tech vendors can find a shareholder rationale for partitioning their respective businesses in the coming year, they will probably waste no time putting it into effect. If doing so helps keep regulators from breathing down their necks, then, all the better.", "pub_date": "2020-10-22"},
{"title": "Cloud adoption in a post-COVID world ", "overview": "Many thought that COVID-19 would cause the Great Cloud Slowdown; instead, the opposite happened. What will cloud adoption look like when the masks come off?", "image_url": null, "url": "https://www.infoworld.com/article/3586597/cloud-adoption-in-a-post-covid-world.html", "body": "I drew some big lines in the sand by predicting that the pandemic would cause a run on cloud computing. The pandemic quickly proved that companies leveraging more public cloud resources than their competitors and peers have much less exposure to risk than those that do not. Priorities soon shifted to cloud migration on the fastest path possible. The result is a pandemic run on cloud migration tools, experienced cloud pros, cloud consulting, and public clouds themselves. Most of us didn’t believe the experts’ predictions that pandemic quarantines and restrictions could linger until the end of summer, with another spike during flu season in the fall. It’s now October, the start of flu season. Cases are spiking to record levels. Pandemic restrictions are still in place in many states. I don’t need a crystal ball to predict that the current run on cloud will continue until the end of the pandemic. Also on InfoWorld: When hybrid multicloud has technical advantagesThe good news is that the pandemic will end, whether by an effective vaccine, improved treatments, better disease management, or a combination of all three. If progress continues as forecast, the COVID-19 pandemic could be contained worldwide by next summer. Even without a vaccine or the scientific advances available to today’s research teams, the pandemic of 1918 ended after 26 months. We may never return to the old normal, but, one way or another, this virus will eventually run its course.Now, what does postpandemic cloud computing look like? First, for most enterprises, a reduction in momentum appears unlikely. However, priorities will shift. Today’s focus is on the low-hanging fruit to achieve migration at speed. Eventually we’ll run out of easily migrated workloads.Once the number of applications migrated slows down, the complexity and value of those workloads will increase. That means we’ll need at least the same amount of resources to refactor more complex applications to run correctly on cloud platforms. The focus will move from lift and shift at speed to application modifications and modernizations that take advantage of the native features of the target public clouds. Second, because we’ll be fixing mistakes made during the initial pandemic run, we’ll increase the required resources. This is due to excess complexity caused by little or no coordination between cloud migration and cloud development teams, all under pressure to migrate or build faster to fix the risk exposures revealed during the early days of lockdown. This will surprise most enterprises. Budgets will need to move away from migration to solve these issues. Overall I still expect budget increases because cloud is now considered strategic to the business. Finally, there’s a silver lining to today’s new and renewed interest in cloud computing. Changes in business that were once unthinkable happened overnight, and they happened worldwide. Today it’s easier to accept that cultures and processes must change to allow cloud computing to work properly. The acceptance of cloud computing within most enterprises is a major shift. Enterprises that embrace the changes are likely to adopt new technologies that were once career risks to even mention in meetings, such as cloud, artificial intelligence, edge computing, etc. This acceptance and desire for change and disruption will allow the business to become more agile, and thus respond better to customer and market dynamics. We will also see global, national, and local disasters in the future that will continue to change the way we use technology to run our businesses and our world. The sooner we understand that change is inevitable—and it’s coming—the better off we’ll be. ", "pub_date": "2020-10-23"},
{"title": "How edge analytics will drive smarter computing", "overview": "Tapping edge computing and IoT devices for real-time analytics holds great promise, but designing analytical models for edge deployment presents a challenge. ", "image_url": null, "url": "https://www.infoworld.com/article/3586550/how-edge-analytics-will-drive-smarter-computing.html", "body": "Many analytics and machine learning use cases connect to data stored in data warehouses or data lakes, run algorithms on complete data sets or a subset of the data, and compute results on cloud architectures. This approach works well when the data doesn’t change frequently. But what if the data does change frequently?Today, more businesses need to process data and compute analytics in real-time. IoT drives much of this paradigm shift as data streaming from sensors requires immediate processing and analytics to control downstream systems. Real-time analytics is also important in many industries including healthcare, financial services, manufacturing, and advertising, where small changes in the data can have significant financial, health, safety, and other business impacts.Also on InfoWorld: Amazon, Google, and Microsoft take their clouds to the edgeIf you’re interested in enabling real-time analytics—and in emerging technologies that leverage a mix of edge computing, AR/VR, IoT sensors at scale, and machine learning at scale—then understanding the design considerations for edge analytics is important. Edge computing use cases such as autonomous drones, smart cities, retail chain management, and augmented reality gaming networks all target deploying large scale, highly reliable edge analytics.Edge analytics, streaming analytics, and edge computingSeveral different analytics, machine learning, and edge computing paradigms are related to edge analytics:Edge analytics refers to analytics and machine learning algorithms deployed to infrastructure outside of cloud infrastructure and “on the edge” in geographically localized infrastructure.Streaming analytics refers to computing analytics in real time as data is processed. Streaming analytics can be done in the cloud or on the edge depending on the use case.Event processing is a way to process data and drive decisions in real time. This processing is a subset of streaming analytics, and developers use event-driven architectures to identify events and trigger downstream actions.Edge computing refers to deploying computation to edge devices and network infrastructure.Fog computing is a more generalized architecture that splits computation among edge, near edge, and cloud computing environments.When designing solutions requiring edge analytics, architects must consider physical and power constraints, network costs and reliability, security considerations, and processing requirements.  Reasons to deploy analytics on the edgeYou might ask why you would deploy infrastructure to the edge for analytics? There are technical, cost, and compliance considerations that factor into these decisions.Applications that impact human safety and require resiliency in the computing architecture are one use case for edge analytics. Applications that require low latency between data sources such as IoT sensors and analytics computing infrastructure are a second use case that often requires edge analytics. Examples of these use cases include: Self-driving cars, automated machines, or any transportation where control systems are automating all or parts of the navigation.Smart buildings that have real-time security controls and want to avoid having dependencies on network and cloud infrastructure to allow people to enter and exit the building safely.Smart cities that track public transportation, deploy smart meters for utility billing, and smart waste management solutions. Cost considerations are a significant factor in using edge analytics in manufacturing systems. Consider a set of cameras scanning the manufactured products for defects while on fast-moving conveyor belts. It can be more cost-effective to deploy edge computing devices in the factory to perform the image processing, rather than having high-speed networks installed to transmit video images to the cloud.Also on InfoWorld: When hybrid multicloud has technical advantagesI spoke with Achal Prabhakar, VP of engineering at Landing AI, an industrial AI company with solutions that focus on computer vision. “Manufacturing plants are quite different from mainstream analytics applications and therefore require rethinking AI including deployment,” Prabhakar told me. ”A big focus area for us is deploying complex deep learning vision models with continuous learning directly on production lines using capable but commodity edge devices.”Deploying analytics to remote areas such as construction and drilling sites also benefits from using edge analytics and computing. Instead of relying on expensive and potentially unreliable wide area networks, engineers deploy edge analytics infrastructure on-site to support the required data and analytics processing. For example, an oil and gas company deployed a streaming analytics solution with an in-memory distributed computing platform to the edge and reduced the drilling time by as much as 20 percent, from a typical 15 days to 12 days. Compliance and data governance is another reason for edge analytics. Deploying localized infrastructure can help meet GDPR compliance and other data sovereignty regulations by storing and processing restricted data in the countries where the data is collected.Designing analytics for the edgeUnfortunately, taking models and other analytics and deploying them to edge computing infrastructure isn’t always trivial. The computing requirements for processing large data sets through computationally intensive data models may require re-engineering before running and deploying them on edge computing infrastructure.For one thing, many developers and data scientists now take advantage of the higher-level analytics platforms that are available on public and private clouds. IoT and sensors often utilize embedded applications written in C/C++, which may be unfamiliar and challenging terrain for cloud-native data scientists and engineers.Another issue may be the models themselves. When data scientists work in the cloud and scale computing resources on-demand at relatively low costs, they are able to develop complex machine learning models, with many features and parameters, to fully optimize the results. But when deploying models to edge computing infrastructure, an overly complex algorithm could dramatically increase the cost of infrastructure, size of devices, and power requirements.I discussed the challenges of deploying AI models to the edge with Marshall Choy, VP of product at SambaNova Systems. “Model developers for edge AI applications are increasingly focusing more on highly-detailed models to achieve improvements in parameter reduction and compute requirements,” he noted. “The training requirements for these smaller, highly-detailed models remains daunting.”Another consideration is that deploying a highly reliable and secure edge analytics system requires designing and implementing highly fault-tolerant architectures, systems, networks, software, and models.I spoke with Dale Kim, senior director of product marketing at Hazelcast, about use cases and constraints when processing data at the edge. He commented that, while equipment optimizations, preventive maintenance, quality assurance checks, and critical alerts are all available at the edge, there are new challenges like limited hardware space, limited physical accessibility, limited bandwidth, and greater security concerns.“This means that the infrastructure you’re accustomed to in your data center won’t necessarily work,” Kim said. “So you need to explore new technologies that are designed with edge computing architectures in mind.”Also on InfoWorld: How to choose a cloud machine learning platformThe next frontier in analyticsThe more mainstream use cases for edge analytics today are data processing functions, including data filtering and aggregations. But as more companies deploy IoT sensors at scale, the need to apply analytics, machine learning, and artificial intelligence algorithms in real-time will require more deployments on the edge. The possibilities at the edge make for a very exciting future of smart computing as sensors become cheaper, applications require more real-time analytics, and developing optimized, cost-effective algorithms for the edge becomes easier. ", "pub_date": "2020-10-26"},
{"title": "Why you’re doing cloudops wrong", "overview": "Now that operational best practices for cloud computing are well known, why do mistakes keep piling up?  ", "image_url": null, "url": "https://www.infoworld.com/article/3587389/why-youre-doing-cloudops-wrong.html", "body": "Cloud operations, aka cloudops, is the long tail in the cloud computing migration and development story. It takes place after you deploy cloud-based solutions and then operate them over a long period of time. Cloudops determines the success of a migration or development effort and the success of user and customer experiences.Some things going wrong in cloudops right now need some attention. First is too many types of operational tools, such as management and monitoring. These tools drive more operational complexity, which can result in human errors that, in turn, cause operational issues. Another problem is enterprises that underestimate the resources required to drive cloudops. Because we’re moving to largely heterogeneous, multicloud deployments, we’ve tripled the number of resources under management in the past four years, yet the size of most ops staffs has remained the same. Third is a lack of cross-cloud security solutions. Generally speaking, you can’t scale native security services on each public cloud; risk and vulnerabilities begin to emerge. Security needs to be more than an afterthought.Also on InfoWorld: Which multicloud architecture will win out?More is going wrong than just the issues mentioned here, although these are the most common. Take time to understand each of these problems, one at a time. At the same time, look for common and holistic solutions. A few suggestions to do cloudops right:Strive to remove much of the complexity from operations but normalize the number of tools employed. This includes common security tools that span cloud and platform, and common management and monitoring tools, such as AIops tooling.With a bit of planning, you can cut the number of cloudops tools in half. This reduction comes with lower risk and fewer resources (people) needed to drive operations. But don’t kid yourself. This approach will change operational processing and playbooks. The goal is to find the most optimized solution that requires the least number of tools and people. At the same time, the solution should increase the effectiveness of operations and uptime. If you take this commonality approach seriously, most problems will disappear.I often observe ops teams doing something over and over again without question, even if they suspect there is a better way. This including processing and tooling. Continuous improvement of cloudops encourages teams to question all aspects of  procedures and tools. Those who promote this approach often find that change is not as readily accepted as they’d hoped. Reticence can typically be overcome if you empower those who do the daily cloudops jobs with the authority to launch proof-of-concepts for tools and procedures in search of new and better solutions. This should be at least 10 percent of the cloudops budget. I’m not saying that those who do cloudops wrong are not good at it. Like any other technical discipline, evolving and improving is just part of the game. If your team or your project experience problems, review your operational tools, resources, and security with an eye toward common tasks and tools. Then empower your staff to make continuous improvements. With the right approach, skills will improve and problems will get solved—or be avoided altogether. ", "pub_date": "2020-10-27"},
{"title": "GPipe and PipeDream: Scaling AI training in every direction", "overview": "New frameworks Google GPipe and Microsoft PipeDream join Uber Horovod in distributed training for deep learning", "image_url": null, "url": "https://www.infoworld.com/article/3539741/gpipe-and-pipedream-scaling-ai-training-in-every-direction.html", "body": "Data science is hard work, not a magical incantation. Whether an AI model performs as advertised depends on how well it’s been trained, and there’s no “one size fits all” approach for training AI models.The necessary evil of distributed AI trainingScaling is one of the trickiest considerations when training AI models. Training can be especially challenging when a model grows too resource hungry to be processed in its entirety on any single computing platform. A model may have grown so large it exceeds the memory limit of a single processing platform, or an accelerator has required developing special algorithms or infrastructure. Training data sets may grow so huge that training takes an inordinately long time and becomes prohibitively expensive.Also on InfoWorld: Artificial intelligence predictions for 2020Scaling can be a piece of cake if we don’t require the model to be particularly good at its assigned task. But as we ramp up the level of inferencing accuracy required, the training process can stretch on longer and chew up ever more resources. Addressing this issue isn’t simply a matter of throwing more powerful hardware at the problem. As with many application workloads, one can’t rely on faster processors alone to sustain linear scaling as AI model complexity grows.Distributed training may be necessary. If the components of a model can be partitioned and distributed to optimized nodes for processing in parallel, the time needed to train a model can be reduced significantly. However, parallelization can itself be a fraught exercise, considering how fragile a construct a statistical model can be.The model may fail spectacularly if some seemingly minor change in the graph—its layers, nodes, connections, weights, hyperparameters, etc.—disrupts the model’s ability to make accurate inferences. Even if we leave the underlying graph intact and attempt to partition the model’s layers into distributed components, we will then need to recombine their results into a cohesive whole.If we’re not careful, that may result in a recombined model that is skewed in some way in the performance of its designated task.New industry frameworks for distributed AI trainingThroughout the data science profession, we continue to see innovation in AI model training, with much of it focusing on how to do it efficiently in multiclouds and other distributed environments.In that regard, Google and Microsoft recently released new frameworks for training deep learning models: Google’s GPipe and Microsoft’s PipeDream. The frameworks follow similar scaling principles.Though different in several respects, GPipe and PipeDream share a common vision for distributed AI model training. This vision involves the need to:Free AI developers from having to determine how to split specific models given a hardware deployment.Train models of any size, type, and structure on data sets of any scale and format, and in a manner that is agnostic to the intended inferencing task.Partition models so that parallelized training doesn’t distort the domain knowledge (such as how to recognize a face) that it was designed to represent.Parallelize training in a fashion that is agnostic to the topology of distributed target environments.Parallelize both the models and the data in a complex distributed training pipeline.Boost GPU (graphics processing units) compute speeds for various training workloads.Enable efficient use of hardware resources in the training process. Reduce communication costs at scale when training on cloud infrastructure.Also on InfoWorld: AI, machine learning, and deep learning: Everything you need to knowScaling training when models and networks become extremely complexWhat distinguishes these two frameworks is the extent to which they support optimized performance of training workflows for models with sequential layers (which is always more difficult to parallelize) and in more complex target environments, such as multicloud, mesh, and cloud-to-edge scenarios.Google’s GPipe is well suited for fast parallel training of deep neural networks that incorporate multiple sequential layers. It automatically does the following:Partitions models and moves the partitioned models to different accelerators, such as GPUs or TPUs (Tensor processing units), which have special hardware that has been optimized for different training workloads.Splits a mini-batch of training examples into smaller micro-batches that can be processed by the accelerators in parallel.Enables internode distributed learning by using synchronous stochastic gradient descent and pipeline parallelism over a distributed machine learning library.Microsoft’s PipeDream also exploits model and data parallelism, but it’s more geared to boosting performance of complex AI training workflows in distributed environments. One of the AI training projects in Microsoft Research’s Project Fiddle initiative, PipeDream accomplishes this automatically because it can:Separate internode computation and communication in a way that leads to easier parallelism of data and models in distributed AI training.Partition AI models into stages that consist of a consecutive set of layers.Map each stage to a separate GPU that performs the forward and backward pass neural network functions for all layers in that stage.Determine how to partition the models based on a profiling run that is performed on a single GPU.Balance computational loads among different model partitions and nodes, even when a training environment’s distributed topology is highly complex.Minimize communications among the distributed worker nodes that handle the various partitions, given that each worker has to communicate only to a single other worker and to communicate only subsets of the overall model’s gradients and output activations.Further details on both frameworks are in their respective research papers: GPipe and PipeDream.The need for consensus and scalabilityTraining is a critical feature of AI’s success, and more AI professionals are distributing these workflows across multiclouds, meshes, and distributed edges.Going forward, Google and Microsoft should align their respective frameworks into an industry consensus approach for distributed AI training. They might want to consider engaging Uber in this regard. The ride sharing company already has a greater claim to the first-to-market distinction in distributed training frameworks. It open sourced its Horovod project three years ago. The project, which is hosted by the Linux Foundation’s AI Foundation, has been integrated with leading AI modeling environments such as TensorFlow, PyTorch, Keras, and Apache MXNet.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesScalability should be a core consideration of any and all such frameworks. Right now, Horovod has some useful features in that regard but lacks the keen scaling focus that Google and Microsoft have built into their respective projects. In terms of scalability, Horovod can run on single or multiple GPUs, and even on multiple distributed hosts without code changes. It is capable of batching small operations, automating distributed tuning, and interleaving communication and computation pipelines.Scalability concerns will vary depending on what training scenario you consider. Regardless of which framework becomes dominant—GPipe, PipeDream, Horovod, or something else—it would be good to see industry development of reference workflows for distributed deployment of the following specialized training scenarios:Semisupervised learning that uses small amounts of labeled data (perhaps crowdsourced from human users in mobile apps) to accelerate pattern identification in large, unlabeled data sets, such as those ingested through IoT devices’ cameras, microphones, and environmental sensors.Reinforcement learning that involves building AI modules, such as those deployed in industrial robots, that can learn autonomously with little or no “ground truth” training data, though possibly with human guidance.Collaborative learning that has distributed AI modules, perhaps deployed in swarming drones, that collectively explore, exchange and exploit optimal hyperparameters, thereby enabling all modules to converge dynamically on the optimal trade-off of learning speed versus accuracy.Evolutionary learning that trains a group of AI-driven entities (perhaps mobile and IoT endpoints) through a procedure that learns from an aggregate of self-interested decisions they make, based both on entity-level knowledge and on varying degrees of cross-entity model-parameter sharing.Transfer learning that reuses any relevant training data, feature representations, neural-node architectures, hyperparameters and other properties of existing models, such as those executed on peer nodes.On-device training that enables apps to ingest freshly sensed local data and rapidly update the specific AI models persisted in those devices.Robot navigation learning that works with raw sensory inputs, exploits regularities in environment layouts, and requires only a little bit of training data.This list doesn’t even begin to hint at the diversity of distributed AI training workflows that will be prevalent in the future. To the extent that we have standard reference frameworks in place in 2020, data scientists will have a strong foundation for carrying the AI revolution forward in every direction.", "pub_date": "2020-04-23"},
{"title": "AWS unveils open source model server for PyTorch", "overview": "Intended to ease production deployments of PyTorch models, TorchServe supports multi-model serving and model versioning for A/B testing", "image_url": null, "url": "https://www.infoworld.com/article/3540415/aws-unveils-open-source-model-server-for-pytorch.html", "body": "Amazon Web Services (AWS) has unveiled an open source tool, called TorchServe, for serving PyTorch machine learning models. TorchServe is maintained by AWS in partnership with Facebook, which developed PyTorch, and is available as part of the PyTorch project on GitHub.Released on April 21, TorchServe is designed to make it easy to deploy PyTorch models at scale in production environments. Goals include lightweight serving with low latency, and high-performance inference.Also on InfoWorld: 5 reasons to choose PyTorch for deep learningThe key features of TorchServe include:Default handlers for common applications such as object detection and text classification, sparing users from having to write custom code to deploy models.Multi-model serving.Model versioning for A/B testing.Metrics for monitoring.RESTful endpoints for application integration.Any deployment environment can be supported by TorchServe, including Kubernetes, Amazon SageMaker, Amazon EKS, and Amazon EC2. TorchServe requires Java 11 on Ubuntu Linux or MacOS. Detailed installation instructions can be found on GitHub. ", "pub_date": "2020-04-24"},
{"title": "The best free data science courses during lockdown", "overview": "6 excellent online courses and one book to learn statistics, machine learning, and deep learning while you’re locked in the house", "image_url": null, "url": "https://www.infoworld.com/article/3540434/the-best-free-data-science-courses-during-lockdown.html", "body": "If you are locked down because of the COVID-19 pandemic, you just might have some extra time on your hands. Binging Netflix is all well and good, but perhaps you are getting tired of that and you would like to learn something new.One of the most lucrative fields to open up in the last couple of years is data science. The resources I list below will help those technical enough to understand math at the level of statistics and differential calculus to incorporate machine learning into their skill sets. They might even help you start a new career as a data scientist. Also on InfoWorld: Artificial intelligence predictions for 2020If you already can program in Python or R, that skill will give you a leg up on applied data science. On the other hand, the programming isn’t the hard part for most people — it’s the numerical methods.Coursera offers many of the following courses. You can audit them for free, but if you want credit you need to pay for them.I recommend starting with the book  so that you can learn the math and the concepts before you start writing code.I should also note that there are several good courses at Udemy, although they are not free. They usually cost about $200 each for lifetime access, but I’ve seen many of them discounted to less than $20 in recent days.Jeff Prosise of Wintellectnow tells me that he’s planning to make a few more of his courses free, so stay tuned.The Elements of Statistical Learning, Second EditionBy Trevor Hastie, Robert Tibshirani, and Jerome Friedman, Springerhttps://web.stanford.edu/~hastie/Papers/ESLII.pdfThis free 764-page ebook is one of the most widely recommended books for beginners in data science. It explains the fundamentals of machine learning and how everything works behind the scenes, but contains no code. Should you prefer a version of the book with applications in R, you can buy or rent it through Amazon.Applied Data Science with Python SpecializationBy Christopher Brooks, Kevyn Collins-Thompson, V. G. Vinod Vydiswaran, and Daniel Romero, University of Michigan/Courserahttps://www.coursera.org/specializations/data-science-pythonThe five courses (89 hours) in this University of Michigan specialization introduce you to data science through the Python programming language. This specialization is intended for learners who have a basic Python or programming background, and who want to apply statistical, machine learning, information visualization, text analysis, and social network analysis techniques through popular Python toolkits such as Pandas, Matplotlib, Scikit-learn, NLTK, and NetworkX to gain insight into their data.Data Science: Foundations using R SpecializationBy Jeff Leek, Brian Caffo, and Roger Peng, Johns Hopkins/Courserahttps://www.coursera.org/specializations/data-science-foundations-rThis 68-hour specialization (five courses) covers foundational data science tools and techniques, including getting, cleaning, and exploring data, programming in R, and conducting reproducible research.Deep LearningBy Andrew Ng, Kian Katanforoosh, and Younes Bensouda Mourri, Stanford/deeplearning.ai/Courserahttps://www.coursera.org/specializations/deep-learningIn 77 hours (five courses) this series teaches the foundations of deep learning, how to build neural networks, and how to lead successful machine learning projects. You will learn about Convolutional networks (CNNs), Recurrent neural networks (RNNs), Long Short Term Memory networks (LSTM), Adam, Dropout, BatchNorm, Xavier/He initialization, and more. You will work on case studies from healthcare, autonomous driving, sign language reading, music generation, and natural language processing. In addition to the theory, you will learn how it is applied in industry using Python and TensorFlow, which they also teach.Fundamentals of Machine LearningBy Jeff Prosise, Wintellectnowhttps://www.wintellectnow.com/Videos/Watch?videoId=fundamentals-of-machine-learningIn this free two-hour introductory video course, Prosise takes you through regression, classification, Support Vector Machines, Principal Component Analysis, and more, using Scikit-learn, the popular Python library for machine learning. Machine LearningBy Andrew Ng, Stanford/Courserahttps://www.coursera.org/learn/machine-learningThis 56-hour video course provides a broad introduction to machine learning, data mining, and statistical pattern recognition. Topics include supervised learning (parametric/non-parametric algorithms, support vector machines, kernels, neural networks), unsupervised learning (clustering, dimensionality reduction, recommender systems, deep learning), and best practices in machine learning and AI (bias/variance theory and innovation process). You’ll also learn how to apply learning algorithms to building smart robots, web search, anti-spam, computer vision, medical informatics, audio, database mining, and other areas.Machine LearningBy Carlos Guestrin and Emily Fox , University of Washington/Courserahttps://www.coursera.org/specializations/machine-learningThis 143-hour (four course) specialization from leading researchers at the University of Washington introduces you to the exciting, high-demand field of Machine Learning. Through a series of practical case studies, you will gain applied experience in major areas of Machine Learning including Prediction, Classification, Clustering, and Information Retrieval. You will learn to analyze large and complex datasets, create systems that adapt and improve over time, and build intelligent applications that can make predictions from data.", "pub_date": "2020-04-27"},
{"title": "Major R language update brings big changes", "overview": "R 4.0.0 brings numerous and significant changes to syntax, strings, reference counting, grid units, and more", "image_url": null, "url": "https://www.infoworld.com/article/3540989/major-r-language-update-brings-big-changes.html", "body": "Version 4.0.0 of the R language for statistical computing has been released, with changes to the syntax of the language as well as features pertaining to error-checking and long vectors.The upgrade was published on April 24. Source code for R 4.0.0 is accessible at cran.r-project.org. A GNU project, R has gathered steam with the rise of data science and machine learning, currently ranking 10th in the Tiobe Index of language popularity and seventh in the PyPL Popularity of Programming Language index.Changes and features introduced in R 4.0.0 include:A new syntax is offered for specifying _raw_ character constants similar to the one used in C++, where  can be used to define a literal string. This makes it easier to write strings containing backslashes or both single and double quotes.The language now uses a  default, and thus by default no longer converts strings to factors in calls to  and . Many packages relied on the previous behavior and will need updating.The S3 generic function  now is in package base rather than package graphics; it is reasonable to have methods that do not use the graphics package. The generic currently is re-exported from the graphics namespace to allow packages importing it from there to keep working, but this could change in the future. Packages that define S4 graphics for  should be re-installed and package code using such generics from other packages must ensure they are imported rather than relying on being looked for on the search path.S3 methods for class array now are dispatched for matrix objects.Reference counting now is used instead of the NAMED mechanism for determining when objects can be safely mutated into base C code. This reduces the need to copy in some cases and should allow future optimizations. It also is expected to help make internal code easier to maintain. and  in package tools now can check for specifierror or warning classes via the new optional second argument ., the utility for the data frame method , now works without parsing and explicit evaluation.Long vectors now are supported as the  argument of a  loop. now converts character columns to factors and factors to integers. now explicitly lists all exports in the NAMESPACE file.The internal implementation of grid units has changed. The only visible effects at the user level should be a slightly different print format for some units, faster performance for unit operations, and two new functions,  and .Printing  now uses a new  method.Packages must be re-installed under the new version of R.This version of R is built against the PCRE2 library for Perl-like regular expressions if available.The beginnings of support for C++ 20.Time needed to start a homogeneous PSOCK cluster on localhost with many nodes has been significantly reduced.There also are a number of deprecations. For example, make macro F77_VISIBILITY has been removed and replaced with F_VISIBILITY; deprecated support for specifiying C++ 98 for package installation has been removed; and many defunct functions have been removed from the base and methods packages. ", "pub_date": "2020-04-28"},
{"title": "Quantum AI is still years from enterprise prime time", "overview": "Quantum computing’s greatest potential for widespread adoption during this decade is in  artificial intelligence ", "image_url": null, "url": "https://www.infoworld.com/article/3546010/quantum-ai-is-still-years-from-enterprise-prime-time.html", "body": "Quantum computing’s potential to revolutionize AI depends on growth of a developer ecosystem in which suitable tools, skills, and platforms are in abundance. To be considered ready for enterprise production deployment, the quantum AI industry would have to, at the very least, reach the following key milestones:Find a compelling application for which quantum computing has a clear advantage over classical approaches to building and training AI.Converge on a widely adopted open source framework for building, training, and deploying quantum AI.Build a substantial, skilled developer ecosystem of quantum AI applications.Also on InfoWorld: The 6 best programming languages for AI developmentThese milestones are all still at least a few years in the future. What follows is an analysis of the quantum AI industry’s maturity at the present time.Lack of a compelling AI application for which quantum computing has a clear advantageQuantum AI executes ML (machine learning), DL (deep learning), and other data-driven AI algorithms reasonably well.As an approach, quantum AI has moved well beyond the proof-of-concept stage. However, that’s not the same as being able to claim that quantum approaches are superior to classical approaches for executing the matrix operations upon which AI’s inferencing and training workloads depend.Where AI is concerned, the key criterion is whether quantum platforms can accelerate ML and DL workloads faster than computers built entirely on classical von Neumann architectures. So far there is no specific AI application that a quantum computer can perform better than any classical alternative. For us to declare quantum AI a mature enterprise technology, there would need to be at least a few AI applications for which it offers a clear advantage—speed, accuracy, efficiency—over classical approaches to processing these workloads.Nevertheless, pioneers of quantum AI have aligned its functional processing algorithms with the mathematical properties of quantum computing architectures. Currently, the chief algorithmic approaches for quantum AI include: This associates quantum-state amplitudes with the inputs and outputs of computations performed by ML and DL algorithms. Amplitude encoding allows for statistical algorithms that support exponentially compact representation of complex multidimensional variables. It supports matrix inversions in which the training of statistical ML models reduces to solving linear systems of equations, such as those in least-squares linear regressions, least-squares version of support vector machines, and Gaussian processes. It often requires the developer to initialize a quantum system in a state whose amplitudes reflect the features of the entire data set.: This uses an algorithm that finds with high probability the unique input to a black box function that produces a particular output value. Amplitude amplification is suitable for those ML algorithms that can be translated into an unstructured search task, such as k-medians and k-nearest neighbors. It can be accelerated through random walk algorithms where randomness comes from stochastic transitions between states, such as in that inherent to quantum superposition of states and the collapse of wave functions due to state measurements.: This determines the local minima and maxima of a machine-learning function over a given set of candidate functions. It starts from a superposition of all possible, equally weighted states of a quantum ML system. It then applies a linear, partial differential equation to guide the time evolution of the quantum-mechanical system. It eventually yields an instantaneous operator, known as the Hamiltonian, that corresponds to the sum of the kinetic energies plus the potential energies associated with the quantum system’s ground state.Leveraging these techniques, some current AI implementations use quantum platforms as coprocessors on select calculation workloads, such as autoencoders, GANs (generative adversarial networks), and reinforcement learning agents.As quantum AI matures, we should expect that these and other algorithmic approaches will show a clear advantage when applied to AI grand challenges that involve complex probabilistic calculations operating over highly multidimensional problem domains and multimodal data sets. Examples of heretofore intractable AI challenges that may yield to quantum-enhanced approaches include neuromorphic cognitive models, reasoning under uncertainty, representation of complex systems, collaborative problem solving, adaptive machine learning, and training parallelization.But even as quantum libraries, platforms, and tools prove themselves out for these specific challenges, they will still rely on classical AI algorithms and functions within end-to-end machine learning pipelines.Lack of a widely adopted open source modeling and training frameworkFor quantum AI to mature into a robust enterprise technology, there will need to be a dominant framework for developing, training, and deploying these applications. Google’s TensorFlow Quantum is an odds-on favorite in that regard. Announced this past March, TensorFlow Quantum is a new software-only stack that extends the widely adopted TensorFlow open source AI library and modeling framework.TensorFlow Quantum brings support for a wide range of quantum computing platforms into one of the dominant modeling frameworks used by today’s AI professionals. Developed by Google’s X R&D unit, it enables data scientists to use Python code to develop quantum ML and DL models through standard Keras functions. It also provides a library of quantum circuit simulators and quantum computing primitives that are compatible with existing TensorFlow APIs.Developers can use TensorFlow Quantum for supervised learning on such AI use cases as quantum classification, quantum control, and quantum approximate optimization. They can execute advanced quantum learning tasks such as meta-learning, Hamiltonian learning, and sampling thermal states. They can use the framework to train hybrid quantum/classical models to handle both the discriminative and generative workloads at the heart of the GANs used in deep fakes, 3D printing, and other advanced AI applications.Recognizing that quantum computing is not yet mature enough to process the full range of AI workloads with sufficient accuracy, Google designed the framework to support the many AI use cases with one foot in traditional computing architectures. TensorFlow Quantum enables developers to rapidly prototype ML and DL models that hybridize the execution of quantum and classic processors in parallel on learning tasks. Using the tool, developers can build both classical and quantum datasets, with the classical data natively processed by TensorFlow and the quantum extensions processing quantum data, which consists of both quantum circuits and quantum operators.Google designed TensorFlow Quantum to support advanced research into alternative quantum computing architectures and algorithms for processing ML models. This makes the new offering suitable for computer scientists who are experimenting with different quantum and hybrid processing architectures optimized for ML workloads.To this end, TensorFlow Quantum incorporates Cirq, an open source Python library for programming quantum computers. It supports programmatic creation, editing, and invoking of the quantum gates that constitute the Noisy Intermediate Scale Quantum (NISQ) circuits characteristic of today’s quantum systems. Cirq enables developer-specified quantum computations to be executed in simulations or on real hardware. It does this by converting quantum computations to tensors for use inside TensorFlow computational graphs. As an integral component of TensorFlow Quantum, Cirq enables quantum circuit simulation and batched circuit execution, as well as estimation of automated expectation and quantum gradients. It also enables developers to build efficient compilers, schedulers, and other algorithms for NISQ machines.In addition to providing a full AI software stack into which quantum processing can now be hybridized, Google is looking to expand the range of more traditional chip architectures on which TensorFlow Quantum can simulate quantum ML. Google also announced plans to expand the range of custom quantum-simulation hardware platforms supported by the tool to include graphics processing units from various vendors as well as its own Tensor Processing Unit AI-accelerator hardware platforms.Google’s latest announcement lands in a fast-moving but still immature quantum computing marketplace. By extending the most popular open source AI development framework, Google will almost certainly catalyze use of TensorFlow Quantum in a wide range of AI-related initiatives.However, TensorFlow Quantum comes into a market that already has several open source quantum-AI development and training tools. Unlike Google’s offering, these rival quantum AI tools come as parts of larger packages of development environments, cloud services, and consulting for standing up full working applications. Here are three full-stack quantum AI offerings:Azure Quantum, announced in November 2019, is a quantum-computing cloud service. Currently in private preview and due for general availability later this year, Azure Quantum comes with a Microsoft open-sourced Quantum Development Kit for the Microsoft-developed quantum-oriented Q# language as well as Python, C#, and other languages. The kit includes libraries for development of quantum apps in ML, cryptography, optimization, and other domains.Amazon Braket, announced in December 2019 and still in preview, is a fully managed AWS service. It provides a single development environment to build quantum algorithms, including ML, and test them on simulated hybrid quantum/classical computers. It enables developers to run ML and other quantum programs on a range of different hardware architectures. Developers craft quantum algorithms using the Amazon Braket developer toolkit and use familiar tools such as Jupyter notebooks.IBM Quantum Experience is a free, publicly available, cloud-based environment for team exploration of quantum applications. It provides developers with access to advanced quantum computers for learning, developing, training, and running AI and other quantum programs. It includes IBM Qiskit, an open source developer tool with a library of cross-domain quantum algorithms for experimenting with AI, simulation, optimization, and finance applications for quantum computers.TensorFlow Quantum’s adoption depends on the extent to which these and other quantum AI full-stack vendors incorporate it into their solution portfolios. That seems likely, given the extent to which all these cloud vendors already support TensorFlow in their respective AI stacks.TensorFlow Quantum won’t necessarily have the quantum AI SDK field all to itself going forward. Other open source AI frameworks—most notably, the Facebook-developed PyTorch—are contending with TensorFlow for the hearts and minds of working data scientists. One expects that rival framework to be extended with quantum AI libraries and tools during the coming 12 to 18 months.We can catch a glimpse of the emerging multitool quantum AI industry by considering a pioneering vendor in this regard. Xanadu’s PennyLane is an open-source development and training framework for AI, executing over hybrid quantum/classical platforms.Launched in November 2018, PennyLane is a cross-platform Python library for quantum ML, automatic differentiation, and optimization of hybrid quantum-classical computing platforms. PennyLane enables rapid prototyping and optimization of quantum circuits using existing AI tools, including TensorFlow, PyTorch, and NumPy. It is device-independent, enabling the same quantum circuit model to be run on different software and hardware back ends, including Strawberry Fields, IBM Q, Google Cirq, Rigetti Forest SDK, Microsoft QDK, and ProjectQ.Lack of a substantial and skilled developer ecosystemAs killer apps and open source frameworks mature, they are sure to catalyze a robust ecosystem of skilled quantum-AI developers who are doing innovative work driving this technology into everyday applications.Increasingly, we’re seeing the growth of a developer ecosystem for quantum AI. Each of the major quantum AI cloud vendors (Google, Microsoft, Amazon Web Services, and IBM) is investing heavily in enlarging the developer community. Vendor initiatives in this regard include the following:", "pub_date": "2020-05-28"},
{"title": "Data science during COVID-19: Some reassembly required", "overview": "Most likely, the assumptions behind your data science model or the patterns in your data did not survive the coronavirus pandemic. Here’s how to address the challenges of model drift", "image_url": null, "url": "https://www.infoworld.com/article/3561290/data-science-during-covid-19-some-reassembly-required.html", "body": "The enormous impact of the COVID-19 pandemic is obvious. What many still haven’t realized, however, is that the impact on ongoing data science production setups has been dramatic, too. Many of the models used for segmentation or forecasting started to fail when traffic and shopping patterns changed, supply chains were interrupted, and borders were locked down.In short, when people’s behavior changes fundamentally, data science models based on prior behavior patterns will struggle to keep up. Sometimes, data science systems adapt reasonably quickly when the new data starts to represent the new reality. In other cases, the new reality is so fundamentally different that the new data is not sufficient to train a new system. Or worse, the base assumptions built into the system just don’t hold anymore, so the entire process from model creation to production deployment must be revisited.Also on InfoWorld: The best free data science courses during quarantineThis post describes different scenarios and a few examples of what happens when old data becomes completely outdated, base assumptions are no longer valid, or patterns in the overall system change. I then highlight some of the challenges data science teams face when updating their production system and conclude with a set of recommendations for a robust and future-proof data science setup.Data science impact scenario: Data and process changeThe most dramatic scenario is a complete change of the underlying system — one that not only requires an update of the data science process but also a revision of the assumptions that went into its design in the first place. This requires a full new data science creation and productionization cycle: understanding and incorporating business knowledge, exploring data sources (possibly to replace data that doesn’t exist anymore), and selecting and fine-tuning suitable models. Examples include traffic predictions (especially near suddenly closed borders), shopping behavior under more or less stringent lockdowns, and healthcare-related supply chains.A subset of the above is the case where the availability of the data has changed. An illustrative example here is weather predictions, where quite a bit of data is collected by commercial passenger aircraft that are equipped with additional sensors. With the grounding of those aircraft, the volume of available data has been drastically reduced. Because base assumptions about weather systems remain the same (ignoring for a moment that changes in pollution and energy consumption may affect the weather as well) “only” a retraining of the existing models may be sufficient. However, if the missing data represents a significant portion of the information that went into model construction, the data science team would be wise to rerun the model selection and optimization process as well.Data science impact scenario: Data changes, process remains the sameIn many other cases, the base assumptions remain the same. For example, recommendation engines will still work very much the same, but some of the dependencies extracted from the data will change. This is not necessarily very different from, say, a new bestseller entering the charts, but the speed and magnitude of change may be far bigger — as we saw with the sudden spike in demand for health-related supplies. If the data science process has been designed flexibly enough, its built-in change detection mechanism should quickly identify the shift and trigger a retraining of the underlying rules. Of course, that presupposes that change detection was in fact built-in and that the retrained system achieves sufficient quality levels.Also on InfoWorld: Applying devops in data science and machine learningData science impact scenario: Data and process continue to workThis brief list is not complete without stressing that many data science systems will continue to work just as they always have. Predictive maintenance is a good example. As long as the usage patterns stay the same, engines will continue to fail in exactly the same ways as before. The important question for the data science team is: Are you sure? Is your performance monitoring setup thorough enough that you can be sure you are not losing quality? Do you even know when the performance of your data science system changes?As noted in the first two impact scenarios above, change to your data science system could happen abruptly (when borders are closed from one day to the next, for example) or only gradually over time. Some of the bigger economic impacts will become apparent in customer behavior only over time. For example, in the case of a SaaS business, customers may not cancel their subscriptions overnight but over coming months.Model drift detection is keyOne most often encounters two types of production data science setups. There are the older systems that were built, deployed, and have been running for years without any further refinements, and then there are the newer systems that may have been the result of a consulting project, possibly even a modern automated machine learning (AutoML) type of project. In both cases, if you are fortunate, automatic handling of partial model change has been incorporated into the system, so at least some model retraining is handled automatically. However, none of the currently available AutoML tools allow for performance monitoring and automatic retraining, and usually the older, “one shot” projects don’t worry about that either. As a result, you may not even be aware that your data science process has failed.If you are lucky to have a setup where the data science team has made numerous improvements over the years, chances are higher that automatic model drift detection and retraining have been built-in. However, even then (and especially in the case where a complete model change is required) it is far more likely that the system cannot easily be recreated. Unless all of the steps of your data science process are well documented, and the experts who wrote the code are still with the company, it will be difficult to revisit the assumptions and update the process. The only solution may be to start an entirely new project.Reinvention vs. reassemblyObviously, if your data science process was set up by an external consulting team, you don’t have much of a choice other than to bring them back in. If your data science process is the result of an automated machine learning service, you may be able to re-engage that service, but especially in the case of the change in business dynamics, you should expect to be involved quite a bit—similar to the first time you embarked on this project. Also on InfoWorld: How dataops improves data, analytics, and machine learningOne side note here: Be skeptical when someone pushes for supercool new methods. In many cases, a new approach is not needed. Rather, one should focus on carefully revisiting the assumptions and data used for the previous data science process. Only in very few cases is this really a “data 0” problem where one tries to learn a new model from very few data points. Even then, one should also explore the option of building on top of the previous models and keeping them involved in some weighted way. Very often, new behavior can be well represented as a mix of previous models with a sprinkle of new data.But if your data science development is done in-house, now is the time an integrative and uniform environment that is 100% backward compatible comes in very handy. In such a platform, the assumptions are modeled and documented in one place, allowing well-informed changes and adjustments to be made much more easily. It’s even better if you can validate, test, and deploy the changes into production from that same environment without the need for manual interaction.TwitterLinkedInKNIME blog—newtechforum@infoworld.com", "pub_date": "2020-06-10"},
{"title": "Working with the Azure Kinect Developer Kit", "overview": "Building applications on top of the Kinect depth sensor", "image_url": null, "url": "https://www.infoworld.com/article/3562738/working-with-the-azure-kinect-developer-kit.html", "body": "Microsoft announced its Azure Kinect camera modules alongside HoloLens 2 early in 2019. Both devices use the same mixed-reality camera module, using a time-of-flight depth sensor to map objects around the camera. But where HoloLens is a wearable mixed-reality device, the Azure Kinect modules are intended to provide Azure-hosted machine learning applications with connected sensors that can be mounted anywhere in a workspace.Azure Kinect is a direct descendent of the second-generation Kinect modules that shipped with the Xbox One, but instead of providing real-world inputs for gaming, it’s targeted at enterprise users and applications. Intended to work with Azure’s Cognitive Services, the first Azure Kinect developer kit started shipping at the end of 2019 in the United States, adding several other countries in early 2020.Also on InfoWorld: What is CUDA? Parallel programming for GPUsOpening the boxThe $399 Azure Kinect Developer Kit is a small white unit with two camera lenses, one for a wide-angle RGB camera and one for the Kinect depth sensor, and an array of microphones. It has an orientation sensor, allowing you to use the camera to build complex 3-D images of environments, ready for use in mixed reality. You can chain multiple devices together for quick 3-D scans or to provide coverage of an entire room, using the orientation sensor to help understand device position.Along with the camera unit, you get a power supply, an Allen key to remove the chaining ports cover, and a USB cable to connect to a development PC. I’d recommend getting a desktop tripod or another type of mount, as the bundled plastic stand is rather small and doesn’t work with most desks or monitors. There’s no software in the box, only a link to online documentation where you can download the device SDK.Before you get started, you should update the device firmware. This ships with the SDK and includes a command line installation tool. When you run the updater it first checks the current firmware state before installing camera and device firmware and then rebooting. Once the camera has rebooted, use the same tool to check that the update has installed successfully. If there’s a problem with an install you can use the camera’s hardware reset (hidden under the tripod mount) to restore the original factory image.Sensing the worldWith the SDK installed you get access to the device sensors from your code. There are three SDKs: one for low-level access to all the camera’s sensors, another to use the familiar Kinect body-tracking features, and one to link the camera’s microphone array to Azure’s speech services. A prebuilt Kinect Viewer app shows the available camera views and streams data from the device’s sensors. You get access to the wide-angle RGB camera, a depth camera view, and the image from the depth sensor’s infrared camera. SDKs are available for both Windows and for Linux, specifically Canonical’s Ubuntu 18.04 LTS release, and can be downloaded directly from Microsoft or from GitHub.It’s a good idea to spend some time playing with the Kinect Viewer. It lets you see how the different depth camera modes operate, helping you choose either a narrow or wide field of view. You can see data from the position sensors, both the accelerometer and gyroscope, and from the microphone array. With the Azure Kinect Developer Kit connected to a development PC and working, you can start to write code for it. A command line recorder app can be used to capture data for playback in the viewer, storing depth information in an MKV (Matroska Video) format file.Building your first depth-sensing applicationMicrosoft provides sample code for building a simple C application to work with the Azure Kinect Development Kit. There’s only one library needed, and this provides the objects and methods needed to work with the camera. Any application first needs to check how many cameras are connected to the host PC before you configure your device data streams. Devices are identified by their serial number, so you can use this to address a specific camera when working with several connected to the same PC or chained together.The Azure Kinect Developer Kit only delivers streaming data, so applications need to configure the data rate in frames per second, along with image color formats and resolutions. Once you’ve created a configuration object you can open a connection using your configuration object, ready to stream data. When you’re finished reading a data stream, stop and close the device.Images are captured in a capture object, with a depth image, an IR image, and a color image for each individual image, taken from the device’s stream. Once you have a capture, you can extract the individual images ready for use in your application. Image objects can be delivered to the Azure machine vision APIs, ready for object recognition or anomaly detection. One example Microsoft has used in its demonstrations is an application that uses captured video to detect when a worker on a factory floor gets too close to operating machinery; another detects someone smoking near a gas pump.Images are captured from the device in a correlated way. Each captured image has a depth image, an IR image, a color image, or a combination of images.A similar process gives you data from the position and motion sensors. As motion data is captured at a higher rate than image data, you must implement some form of synchronization in your code to avoid losing any data. Audio data is captured using standard Windows APIs, including those used by Azure’s speech services.Although the Azure Kinect hardware captures a lot of data, the SDK functions help transform it into a usable form; for example, adding depth data to an RGB image to produce RGB-D images that are transformed to the viewpoint of the RGB camera (and vice versa). As the two sensors are off-set, this requires warping an image mesh to merge the two cameras’ viewpoints, using your PC’s GPU. Another transform generates a point cloud, allowing you to get depth data for each pixel in your capture. One useful option in the SDK is the ability to capture video and data streams in a Matroska-format file. This approach allows bandwidth-limited devices to batch data and deliver it to, say, Azure Stack Edge devices with Cognitive Services containers for batch processing.Body tracking a digital skeletonThe original Kinect hardware introduced body tracking, with a skeletal model that could be used to quickly evaluate posture and gestures. That same approach continues in the Azure Kinect Body Tracking SDK, which uses Nvidia’s CUDA GPU parallel processing technology to work with 3-D image data from your device’s depth sensor. A bundled sample app shows some of the features of the SDK, including the ability to track more than one person at a time. The Azure Kinect Body Tracking Viewer shows a 3-D point cloud and tracked bodies.The Body Tracking SDK builds on the Azure Kinect SDK, using it to configure and connect to a device. Captured image data is processed by the tracker, storing data in a body frame data structure. This contains a collection of skeletal structures for identified bodies, a 2-D index map to help visualize your data, along with the underlying 2-D and 3-D images that were used to construct the tracking data. Each frame can be used to construct animations or to feed information to machine learning tools that can help process tracked positions in relation to a room map or to ideal positions.Azure’s Cognitive Services are a powerful tool for processing data, and the addition of Azure Kinect makes it possible to use them in a wide range of industrial and enterprise scenarios. With a focus on workplace 3-D image recognition, Microsoft is attempting to show how image recognition can be used to reduce risk and improve safety. There’s even the option of using an array of devices as a quick volumetric capture system, which can help build both mixed-reality environments and provide source data for CAD and other design tools. The result is a flexible device that, with a little code, becomes a very powerful sensing device.", "pub_date": "2020-06-16"},
{"title": "3 ways to apply agile to data science and dataops", "overview": "Take an agile approach to dashboards, machine learning models, cleansing data sources, and data governance", "image_url": null, "url": "https://www.infoworld.com/article/3562346/3-ways-to-apply-agile-to-data-science-and-dataops.html", "body": "Just about every organization is trying to become more data-driven, hoping to leverage data visualizations, analytics, and machine learning for competitive advantages. Providing actionable insights through analytics requires a strong dataops program for integrating data and a proactive data governance program to address data quality, privacy, policies, and security.Delivering dataops, analytics, and governance is a significant scope that requires aligning stakeholders on priorities, implementing multiple technologies, and gathering people with diverse backgrounds and skills. Agile methodologies can form the working process to help multidisciplinary teams prioritize, plan, and successfully deliver incremental business value.Also on InfoWorld: How to address data and architecture standards in agile developmentAgile methodologies can also help data and analytics teams capture and process feedback from customers, stakeholders, and end-users. Feedback should drive data visualization improvements, machine learning model recalibrations, data quality increases, and data governance compliance.  Defining an agile process for data science and dataopsApplying agile methodologies to the analytics and machine learning lifecycle is a significant opportunity, but it requires redefining some terms and concepts. For example:Instead of an agile product owner, an agile data science team may be led by an analytics owner who is responsible for driving business outcomes from the insights delivered.Data science teams sometimes complete new user stories with improvements to dashboards and other tools, but more broadly, they deliver actionable insights, improved data quality, dataops automation, enhanced data governance, and other deliverables. The analytics owner and team should capture the underlying requirements for all these deliverables in the backlog.Agile data science teams should be multidisciplinary and may include dataops engineers, data modelers, database developers, data governance specialists, data scientists, citizen data scientists, data stewards, statisticians, and machine learning experts. The team makeup depends on the scope of work and the complexity of data and analytics required.An agile data science team is likely to have several types of work. Here are three primary ones that should fill backlogs and sprint commitments.1. Developing and upgrading analytics, dashboards, and data visualizationsData science teams should conceive dashboards to help end-users answer questions. For example, a sales dashboard may answer the question, “What sales territories have seen the most sales activity by rep during the last 90 days?” A dashboard for agile software development teams may answer, “Over the last three releases, how productive has the team been delivering features, addressing technical debt, and resolving production defects?”Agile user stories should address three questions: Who are the end-users? What problem do they want addressed? Why is the problem important? Questions are the basis for writing agile user stories that deliver analytics, dashboards, or data visualizations. Questions address who intends to use the dashboard and what answers they need.It then helps when stakeholders and end-users provide a hypothesis to an answer and how they intend to make the results actionable. How insights become actionable and their business impacts help answer the third question (why is the problem important) that agile user stories should address.The first version of a Tableau or Power BI dashboard should be a “minimal viable dashboard” that’s good enough to share with end-users to get feedback. Users should let the data science team know how well the dashboard addresses their questions and how to improve. The analytics product owner should put these enhancements on the backlog and consider prioritizing them in future sprints.2. Developing and upgrading machine learning modelsThe process of developing analytical and machine learning models includes segmenting and tagging data, feature extraction, and running data sets through multiple algorithms and configurations. Agile data science teams might record agile user stories for prepping data for use in model development and then creating separate stories for each experiment. The transparency helps teams review the results from experiments, decide on the next priorities, and discuss whether approaches are converging on beneficial results.There are likely separate user stories to move models from the lab into production environments. These stories are devops for data science and machine learning, and likely include scripting infrastructure, automating model deployments, and monitoring the production processes.Once models are in production, the data science team has responsibilities to maintain them. As new data comes in, models may drift off target and require recalibration or re-engineering with updated data sets. Advanced machine learning teams from companies like Twitter and Facebook implement continuous training and recalibrate models with new training set data.3. Discovering, integrating, and cleansing data sourcesAgile data science teams should always seek out new data sources to integrate and enrich their strategic data warehouses and data lakes. One important example is data siloed in SaaS tools used by marketing departments for reaching prospects or communicating with customers. Other data sources might provide additional perspectives around supply chains, customer demographics, or environmental contexts that impact purchasing decisions.Analyst owners should fill agile backlogs with story cards to research new data sources, validate sample data sets, and integrate prioritized ones into the primary data repositories. When agile teams integrate new data sources, the teams should consider automating the data integration, implementing data validation and quality rules, and linking data with master data sources.Julien Sauvage, vice president of product marketing at Talend, proposes the following guidelines for building trust in data sources. “Today, companies need to gain more confidence in the data used in their reports and dashboards. It’s achievable with a built-in trust score based on data quality, data popularity, compliance, and user-defined ratings. A trust score enables the data practitioner to see the effects of data cleaning tasks in real time, which enables fixing data quality issues iteratively.”The data science team should also capture and prioritize data debt. Historically, data sources lacked owners, stewards, and data governance implementations. Without the proper controls, many data entry forms and tools did not have sufficient data validation, and integrated data sources did not have cleansing rules or exception handling. Many organizations have a mountain of dirty data sitting in data warehouses and lakes used in analytics and data visualizations.Just like there isn’t a quick fix to address technical debt, agile data science groups should prioritize and address data debt iteratively. As the analytics owner adds user stories for delivering analytics, the team should review and ask what underlying data debt must be itemized on the backlog and prioritized.Implementing data governance with agile methodologiesThe examples I shared all help data science teams improve data quality and deliver tools for leveraging analytics in decision making, products, and services.In a proactive data governance program, issues around data policy, privacy, and security get prioritized and addressed in parallel to the work to deliver and improve data visualizations, analytics, machine learning, and dataops. Sometimes data governance work falls under the scope of data science teams, but more often, a separate group or function is responsible for data governance.Organizations have growing competitive needs around analytics and data governance regulations, compliance, and evolving best practices. Applying agile methodologies provides organizations with a well-established structure, process, and tools to prioritize, plan, and deliver data-driven impacts.", "pub_date": "2020-06-18"},
{"title": "Redis 6: A high-speed database, cache, and message broker", "overview": "Redis is a powerful blend of speed, resilience, scalability, and flexibility, and Redis Enterprise takes it even further", "image_url": null, "url": "https://www.infoworld.com/article/3563354/redis-6-a-high-speed-database-cache-and-message-broker.html", "body": "Like many, you might think of Redis as only a cache. That point of view is out of date.Essentially, Redis is a NoSQL in-memory data structure store that can persist on disk. It can function as a database, a cache, and a message broker. Redis has built-in replication, Lua scripting, LRU eviction, transactions, and different levels of on-disk persistence. It provides high availability via Redis Sentinel and automatic partitioning with Redis Cluster.The core Redis data model is key-value, but many different kinds of values are supported: Strings, Lists, Sets, Sorted Sets, Hashes, Streams, HyperLogLogs, and Bitmaps. Redis also supports geospatial indexes with radius queries and streams.To open source Redis, Redis Enterprise adds features for additional speed, reliability, and flexibility, as well as a cloud database as a service. Redis Enterprise scales linearly to hundreds of millions of operations per second, has active-active global distribution with local latency, offers Redis on Flash to support large datasets at the infrastructure cost of a disk-based database, and provides 99.999% uptime based on built-in durability and single-digit-seconds failover.Further, Redis Enterprise extends the core Redis functionality to support any data modeling method with modules such as RediSearch, RedisGraph, RedisJSON, RedisTimeSeries, and RedisAI, and allows operations to be executed across and between modules and core. All this is provided while keeping database latency under one millisecond.Core Redis features and use casesWhat does it mean that Redis can now function as a database, cache, and message broker? And what are the use cases those roles support? is the classic function of Redis. Essentially, Redis sits in front of a disk-based database and saves queries and results; the application checks the Redis cache for stored results first, and queries the disk-based database for results not currently in the cache. Given the sub-millisecond response rate of Redis, this is usually a big win for application performance. Expiration timers and LRU (least recently used) eviction from the Redis cache help to keep the cache current and to use memory effectively.The  is an important part of modern web applications. It’s a convenient place to keep information about the user and her interactions with the application. In a web farm architecture, hosting the session store directly on the web server requires making the user “stick” to the same back-end server for future requests, which can limit the load balancer. Using a disk-based database for the session store removes the need to bind a session to a single web server, but introduces an additional source of latency. Using Redis (or any other fast in-memory database) as the session store often results in a low-latency, high-throughput web application architecture.Redis can function as a  using three different mechanisms, and one of the important use cases for Redis as a message broker is to act as glue between microservices. Redis has a low-overhead publish/subscribe notification mechanism that facilitates fire-and-forget messages, but can’t work when the destination service is not listening. For a more persistent, Kafka-like message queue, Redis uses streams, which are time-stamp ordered key-value pairs in a single key. Redis also supports doubly-linked lists of elements stored at a single key, which are useful as a first-in/first-out (FIFO) queue. Microservices can, and often do, use Redis as a cache as well as using it as a message broker, although the cache should run in a separate instance of Redis from the message queue.Basic replication allows Redis to scale without using the cluster technology of the Redis Enterprise version. Redis replication uses a leader-follower model (also called master-slave), which is asynchronous by default. Clients can force synchronous replication using a WAIT command, but even that doesn’t make Redis consistent across replicas.Redis has server-side Lua scripting, allowing programmers to extend the database without writing C modules or client-side code. Basic Redis transactions allow a client to declare a sequence of commands as a non-interruptible unit, using the MULTI and EXEC commands to define and run the sequence. This is  the same as relational transactions with rollbacks.Redis has different levels of on-disk persistence that the user can select. RDB (Redis database file) persistence takes point-in-time snapshots of the database at specified intervals. AOF (append-only file) persistence logs every write operation received by the server. You can use both RDB and AOF persistence for maximum data safety.Redis Sentinel, itself a distributed system, provides high availability for Redis. It does monitoring of the master and replica instances, notification if there is something wrong, and automatic failover if the master stops working. It also serves as a configuration provider for clients.Redis Cluster provides a way to run a Redis installation where data is automatically sharded across multiple Redis nodes. Redis Cluster also provides some degree of availability during partitions, although the cluster will stop operating if the majority of masters become unavailable.As I mentioned earlier, Redis is a key-value store that supports Strings, Lists, Sets, Sorted Sets, Hashes, Streams, HyperLogLogs, and Bitmaps as values. One of the simplest and most common use cases is using integer values as counters. In support of this, INCR (increment), DECR (decrement), and other single operations are atomic, and therefore safe in a multi-client environment. In Redis, when keys are manipulated they will automatically be created if they don’t already exist.The other kinds of value structures also have their own examples in the Try Redis tutorial. The tutorial was undergoing maintenance when I tried it myself; I expect that to be fixed soon, as Redis Labs has become involved in what was originally a community effort.There are a number of add-on modules for Redis including (in descending order of popularity) a neural network module, full-text search, SQL, a JSON data type, and a graph database. The licenses for modules are set by the authors. Some of the modules that work with Redis are primarily modules for Redis Enterprise.Redis Enterprise enhancementsUsing a shared-nothing cluster architecture, Redis Enterprise delivers infinite linear scaling without imposing any non-linear overheads in a scaled-out architecture. You can deploy multiple Redis instances on a single cluster node, to take full advantage of multi-core computer architecture. Redis Enterprise has demonstrated scaling to hundreds of millions of operations per second with five nines (99.999%) uptime. Redis Enterprise does automatic re-sharding and rebalancing while maintaining low latency and high throughput for transactional loads.Redis Enterprise offers active-active deployment for globally distributed databases, enabling simultaneous read and write operations on the same dataset across multiple geo-locations. To make that more efficient, Redis Enterprise can use conflict-free replicated data types (CRDTs) to maintain consistency and availability of data. Riak and Azure Cosmos DB are two other NoSQL databases that support CRDTs.While there is extensive academic literature on CRDTs, I admit I don’t completely understand how or why they work. The short summary of  they do is that CRDTs can resolve inconsistencies without intervention, using a mathematically derived set of rules. CRDTs are valuable for high-volume data that require a shared state, and can use geographically dispersed servers to reduce latency for users.One of the major differences between Redis and Redis Enterprise is that Redis Enterprise decouples the data path from cluster management. This improves the operation of both components. The data path is based on multiple zero-latency, multi-threaded proxies that reside on each of the cluster nodes to mask the underlying complexity of the system. The cluster manager is a governing function that provides capabilities such as resharding, rebalancing, auto-failover, rack-awareness, database provisioning, resource management, data persistence configuration, and backup and recovery. Because the cluster manager is entirely decoupled from the data path components, changes to its software components do not affect the data path components.Redis on Flash is a Redis Enterprise feature that can drastically reduce the cost of hardware for Redis. Instead of having to pay through the nose for terabytes of RAM or restrict the size of your Redis datasets, you can use Redis on Flash to place frequently accessed hot data in memory and colder values in Flash or persistent memory, such as Intel Optane DC.Redis Enterprise modules include RedisGraph, RedisJSON, RedisTimeSeries, RedisBloom, RediSearch, and RedisGears. All Redis Enterprise modules also work with open source Redis.What’s new in Redis 6?Redis 6 is a big release, both for the open source version and the Redis Enterprise commercial version. The performance news is the use of threaded I/O, which gives Redis 6 a 2x improvement in speed over Redis 5 (which was no slouch). That carries over into Redis Enterprise, which has additional speed improvements for clusters as described above.The addition of access control lists (ACLs) gives Redis 6 the concept of users, and allows developers to write more secure code. Redis Enterprise 6 builds on ACLs to offer role-based access control (RBAC), which is more convenient for the programmers and DBAs.Redis 6.0 open sourceAccess control lists (ACLs)Improved evictionsThreaded I/ORESP3 protocolRedis Enterprise 6.0Role-based access control (RBAC)Extending active-activeHyperLogLogStreamsRedis Enterprise 6.0 adds support for the Streams data type in active-active databases. That allows both high availability and low latency while concurrently reading and writing to and from a real-time stream in multiple data centers in multiple geographic locations.RedisGears is a dynamic framework that enables developers to write and execute functions that implement data flows in Redis. It lets users write Python scripts to run inside Redis, and enables a number of use cases including write-behind (Redis acts as a front-end to a disk-based database), real-time data processing, streaming and event processing, operations that cross data structures and models, and AI-driven transactions.RedisAI is a model serving engine that runs inside Redis. It can perform inference with PyTorch, TensorFlow, and ONNX models. RedisAI can run on CPUs and GPUs, and enables use cases such as fraud detection, anomaly detection, and personalization.Creating a new role in Redis Enterprise Cloud. RBAC is considerably easier to manage that ACLs.Installing RedisYou can install Redis by downloading and compiling a source tarball or by pulling a Docker image from the Docker Hub. Redis can be compiled and used on Linux, MacOS, OpenBSD, NetBSD, and FreeBSD. The source code repository is on GitHub. On Windows, you can run Redis either in a Docker container or under Windows Subsystem for Linux (WSL), which requires Windows 10.You can install Redis Enterprise on Linux or in Docker containers. The Linux downloads come in the form of binary packages (DEB or RPM depending on the flavor of Linux) and Bash shell scripts for cluster installation. The scripts check for the required four cores and 15 GB of RAM for installation.Redis Enterprise installs on Docker and on Linux systems. This page shows the Redis Enterprise 5.0 installers, as the Redis Enterprise 6.0 installers are still in the release candidate phase, although I was given the Linux installers for Redis Enterprise 6.0 RC privately when I asked.Redis Enterprise CloudThe fastest way to install Redis Enterprise is not to install it at all, but rather to run it in the Redis Enterprise Cloud. When I tried this myself for review purposes, I initially received a Redis 5 instance; I had to ask for an upgrade to Redis 6.You can run a small instance in the Redis Enterprise Cloud for free, or a full-featured instance for a monthly price that depends on the amount of RAM provisioned. While this description says that Redis modules are not available for low-throughput Essentials environments, Redis Labs is currently rolling out Module support at the Essential level starting with the AWS Mumbai zone.At the Essentials level, instances range from 30 MB of RAM (free) to 5 GB of RAM ($338 per month) and over. Redis Enterprise Cloud instances are available on AWS, GCP, and Azure. You can choose your own region or regions.Creating a database once you have a Redis Enterprise Cloud subscription is a simple matter of naming the database, choosing a Redis or Memcached protocol, choosing your replication and persistence options, choosing your data eviction policy, and setting your usage alerts.", "pub_date": "2020-06-22"},
{"title": "When not to use AIops for cloudops", "overview": "The excitement around AIops means that it’s sometimes being deployed for the wrong reasons.  Here are a few situations where AIops is contraindicated", "image_url": null, "url": "https://www.infoworld.com/article/3563908/when-not-to-use-aiops-for-cloudops.html", "body": "Artificial intelligence for IT operation platforms, better known as AIops, is an evolving and expanded use of technologies that for the past several years were categorized as IT operations analytics. The growth of AIops has been clear to anyone watching the market, but if you need some statistics, Gartner reports that by 2022, 40 percent of large enterprises will use AIops tools to support or replace monitoring and service desk tasks, up from 5 percent today.That’s a pretty big jump. However, it’s also an indication that many enterprises may pick AIops tools for the wrong purposes—mistakes that will likely cost millions. Here is what I’m seeing.Also on InfoWorld: AI, machine learning, and deep learning: Everything you need to know Those who haven’t planned a proper cloud solution for the business, and even an on-premises solutions paired with public clouds, are attempting to fix systemic issues. A poor plan will lead to performance problems and outages with AIops. Just like “you can’t fix stupid,” poorly planned architectures need to be corrected before you apply AIops tools and use them properly. AIops tools work on the assumption that the solution's configuration is sound before they can process alarms and resolutions properly. If not done in that order, you’ll just be teaching your AIops system how to correlate gigabytes of data coming from cloud and non-cloud systems, attempting fixes that are unlikely to be successful because they kick off other alerts and triggers. Those working cloudops now are, in essence, inventing a new discipline. Enterprises have seen the growth in cloudops specialists who are commanding some pretty good salaries. This has driven up costs, reducing the value they thought they would get by using public clouds.     I’ve seen enterprises invest in AIops tools based on a business case pointing to fewer ops team members needed, and the ability to automate costs out of the ops equation. Although there is a potential to reduce cost and staff much further down the road, AIops requires a great deal of ops expertise. You typically see AIops drive an expansion of the cloudops team, and costs initially rise for at least a few years. You have to invest in efficiency; you can’t remove dollars and expect good outcomes.  Those who do cloud security already understand that the ops automation processes is not a good way to defend your cloud-based applications and data. Indeed, the conflation of AIops with cloud security actually makes things less secure, considering that you’ll be dealing with security systems that are more complex.  AIops is one of those tools that needs to be implemented with great care in order to be effective. The market is moving at such speed that mistakes are going to happen; however, with a bit of common sense, you’ll find that AIops will eventually provide the value you’re looking for.       ", "pub_date": "2020-06-23"},
{"title": "How PagerDuty helps customer service and IT teams improve responses", "overview": "PagerDuty uses machine learning to anticipate issues and dramatically accelerate responses, so problems are addressed before they impact customers", "image_url": null, "url": "https://www.infoworld.com/article/3544929/how-pagerduty-helps-customer-service-and-it-teams-improve-responses.html", "body": "Predicting the outcome of the NCAA men’s Division I basketball tournament — an event where upsets are celebrated wildly and the outcome is notoriously difficult to foresee — is nearly as competitive as the tournament itself. For years, Warren Buffet held a contest offering a billion dollars for a perfect bracket, and nobody even came close. Speaking of unpredictability, just as fans were getting ready to make their picks for this year’s tournament, all major public sporting events were canceled. Who could have predicted ?Even though we can’t see the future, a deep understanding of variables does enable people to make better predictions and gain an edge over the competition. Picking winners by their school mascot may work every once in a while, but an in-depth study of the best teams, coaches, and athletes is a much more effective strategy.Likewise, customer service, devops, and IT issues are inherently unpredictable. It’s impossible for companies to know in advance when operational problems will arise, product defects will surface, or communications will go askew. Solutions driven by AI and machine learning can help teams improve their odds. These products can dramatically accelerate responses to issues, so problems are prevented or resolved before most customers encounter them. Companies can get thousands of alerts per minute when a problem arises within their digital app or service — a broken cart for an ecommerce website, for example — which is neither useful nor actionable for human interpreters to tackle. The overwhelming amount of noise simply leads to lost signals and many more contacts between customers and service teams before underlying problems can be addressed.Predictive solutions for customer services are built on understanding the drivers behind the signals. Quickly identifying patterns helps companies stay ahead of the curve. Machine learning tools free up a lot of cycles for response teams by cutting through the noise, rather than distracting them over and over again with alerts and information that may not be useful.When teams use machine learning in this way, they can boil down the signals to uncover the actual incidents that are driving the unmanageable number of alerts. Instead of scrambling to put out many small fires, they can see the big picture of where the problems actually lie and be more intelligent and informed in tackling a smaller group of larger issues.How predictive capabilities can improve service responsesPredictive processes must be conducted in real-time if they are going to help companies get ahead of the issues for the majority of customers. Developing problems that threaten to impact customers do not allow time to be paused for reflection or deliberation.The higher-level need for predictive customer and IT services is in training algorithms to recognize which alerts belong to which incidents. At PagerDuty, our main goal is to help companies identify issues before they cause problems within digital systems, and predict what may go wrong in the future so companies can get ahead of it. We use machine learning to group alerts together so teams can see the full scale of the issues and know precisely how to solve them.For example, multiple teams may each be working on individual complaints without understanding that they are all elements of a single issue. Insights from PagerDuty’s platform solve that problem and get everyone on the same page. Meanwhile, as responders are assigned specific issues to fix, the platform triages messages to each individual so they are not overwhelmed with issues outside of the one they are addressing.This is important because most systems don’t operate in isolation where a point failure in one place is the same as a point failure somewhere else. When problems arise, companies use PagerDuty to help find the origin point for cascading issues to try to prevent catastrophic failure. When teams can be more predictive and preventive, they gain a higher-level view of the problems and learn where their efforts will have the most impact.A structure that helps teams identify and solve problems quickly also gives much greater visibility to every level of the organization. Managers and directors can have better insights about how to deploy teams. Leaders who may have to explain problems or downtime to customers are likewise armed with information and a clear path forward.How PagerDuty uses machine learning Giving companies greater predictive and preventive power for customer service and IT starts with grouping issues in a way that helps identify underlying causes of digital issues. That grouping begins with the supposition that if two messages have similar text, then those messages are fundamentally similar. Although this is reasonable in theory, knowing whether those messages are truly similar is a fuzzy concept.At PagerDuty, the most impactful solution has been to apply a parser that takes messages and transforms them into less refined language. This process boils down the words in order to surface specific elements within the message.The system locates unique identifiers like dates, times, customer IDs, or websites with IDs inside that would only be issued in the context of customer messages and reports. These identifiers are generally unimportant to the parser in terms of content. The program simply identifies that they are present within the body of the message.After this overall blurring, the words and identifiers within each message can then be grouped together. This is where PagerDuty’s platform examines the incoming signals and determines the full extent to which messages share groups of words.This step is accomplished by vectorizing, which is the process of turning each of these series of words into a representative sequence of numbers. But it is still an imperfect system. Every sentence produces a vector representation, of course, but each vector could conceivably come from several different sentences. Usually, there is enough information to determine when sentences have the same information. But PagerDuty’s software engineers still have to account for the fact that there are many ways that a vector could have been put together.Once the system can identify a group of messages that have the same vectors, they are bundled together. These groups essentially have the same content. Their identifiers indicate they are full of all the same terms.Turning machine data into predictions and preventionFor example, a company usually learns that something is going wrong when it is suddenly flooded with reports and messages. Most of these will be machine-generated, some with custom templates and some even written by a person. Without some sort of grouping, teams have no way of seeing a higher-level view of the circumstances. They could build a grouping tool, but that would require a serious investment of time and effort, all while more incident reports pile up.Likewise, because so many of the messages have different content, simply grouping messages only when they are identical doesn't do much to diminish the volume of issues. Using AI to discern similarities lets the group accumulate relevant information over time. Instead of thousands of individual problems, each represented by a report or message, grouping alerts this way surfaces just a few core issues that are the source of other problems.At that point, the system has enabled the response teams to become both predictive and preventive. It becomes much easier to find the biggest issues and fix the underlying causes that will prevent future problems. Prioritizing a little engineering work on core issues causes the incident load to drop dramatically, all from basic AI-driven grouping.In theory, this should be a very reliable process. Once messages are parsed, identified, and vectorized, it should be easy for the system to group them together as similar. They are all textually related and the vectors let the platform measure the strength of the relatedness.In practice, of course, it’s not always so simple. The flexibility of language means that the system quite often gets it wrong. This is why PagerDuty builds ample and powerful feedback systems into our products.Improving results with human feedbackWhen end users provide feedback to the system, they are giving us new data points to help perfect the process. This is usually an acknowledgment that, yes, A and B look like they should be related. However, the human context of the message reveals that they don’t have much to do with each other.PagerDuty’s feedback system gives greater weight to messages that have been positively correlated because they share terms, but then human feedback reveals they are not similar. This evaluation and modification could be accomplished in software through a very large reinforcement learning system, but for the user it is a simple evaluation whether terms and messages should or should not be together.  Customers, of course, do not need to see the nuts and bolts of how this works. The customer service and IT teams should have simple tools to provide feedback that will delineate which terms don’t match.On a higher level, PagerDuty’s feedback systems give users broad options for merging and separating groups of terms within the alerts. This is simply grabbing items and moving them in or out of a group; in essence indicating that certain items belong with each other, but another does not match.Another less sophisticated but equally powerful product may only require literal thumbs-up and thumbs-down buttons. The user essentially approves of a match or indicates a flaw in the process.Anything can and will happen to frustrate and disappoint customers, as anyone who has worked in customer service will tell you. Improving your odds in those unpredictable circumstances requires learning, understanding and solving problems as quickly as they emerge. The foremost integrated event intelligence and incident response solutions in the space combine machine and human telemetry by looking at both digital signals and human response behavior.PagerDuty—newtechforum@infoworld.com", "pub_date": "2020-06-24"},
{"title": "What is face recognition? AI for Big Brother", "overview": "Facial recognition is becoming more accurate, but some systems exhibit racial bias and some uses of the technology are controversial ", "image_url": null, "url": "https://www.infoworld.com/article/3573069/what-is-face-recognition-ai-for-big-brother.html", "body": "Can Big Brother identify your face from street-level CCTV surveillance and tell whether you’re happy, sad, or angry? Can that identification lead to your arrest on an outstanding warrant? What are the odds that the identification is incorrect, and really connects to someone else? Can you defeat the surveillance entirely using some trick?On the flip side, can you get into a vault protected by a camera and facial identification software by holding up a print of an authorized person’s face? What if you put on a 3-D mask of an authorized person’s face?Welcome to face recognition — and the spoofing of facial recognition.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesWhat is face recognition?Face recognition is a method for identifying an unknown person or authenticating the identity of a specific person from their face. It’s a branch of computer vision, but face recognition is specialized and comes with social baggage for some applications, as well as some vulnerabilities to spoofing.How does face recognition work?The early facial recognition algorithms (which are still in use today in improved and more automated form) rely on biometrics (such as the distance between eyes) to turn the measured facial features from a two-dimensional image into a set of numbers (a feature vector or template) that describes the face. The recognition process then compares these vectors to a database of known faces which have been mapped to features in the same way. One complication in this process is adjusting the faces to a normalized view to account for head rotation and tilt before extracting the metrics. This class of algorithms is called .Another approach to face recognition is to normalize and compress 2-D facial images, and to compare these with a database of similarly normalized and compressed images. This class of algorithms is called .Three-dimensional face recognition uses 3-D sensors to capture the facial image, or reconstructs the 3-D image from three 2-D tracking cameras pointed at different angles. 3-D face recognition can be considerably more accurate than 2-D recognition.Skin texture analysis maps the lines, patterns, and spots on a person’s face to another feature vector. Adding skin texture analysis to 2-D or 3-D face recognition can improve the recognition accuracy by 20 to 25 percent, especially in the cases of look-alikes and twins. You can also combine all the methods, and add in multi-spectral images (visible light and infrared), for even more accuracy.Face recognition has been improving year over year since the field began in 1964. On average, the error rate has reduced by half every two years.Face recognition vendor testsNIST, the US National Institute of Standards and Technology, has been performing tests of facial recognition algorithms, the Face Recognition Vendor Test (FRVT), since 2000. The image datasets used are mostly law enforcement mug shots, but also include in-the-wild still images, such as those found in Wikimedia, and low-resolution images from webcams.The FRVT algorithms are mostly submitted by commercial vendors. The year-over-year comparisons show major gains in performance and accuracy; according to the vendors, this is primarily because of the use of deep convolutional neural networks.Related NIST face recognition testing programs have studied demographic effects, detection of face morphing, identification of faces posted on social media, and identification of faces in video. A previous series of tests was conducted in the 1990s under a different moniker, Face Recognition Technology (FERET).NISTFacial identification miss rates across the false positive range, from NIST.IR.8271 (September 2019). Note that both axes are logarithmic.Face recognition applicationsFace recognition applications mostly fall into three major categories: security, health, and marketing/retail. Security includes law enforcement, and that class of facial recognition uses can be as benign as matching people to their passport photos faster and more accurately than humans can, and as creepy as the “Person of Interest” scenario where people are tracked via CCTV and compared to collated photo databases. Non-law-enforcement security includes common applications such as face unlock for mobile phones and access control for laboratories and vaults.Health applications of facial recognition include patient check-ins, real-time emotion detection, patient tracking within a facility, assessing pain levels in non-verbal patients, detecting certain diseases and conditions, staff identification, and facility security. Marketing and retail applications of face recognition include identification of loyalty program members, identification and tracking of known shoplifters, and recognizing people and their emotions for targeted product suggestions.Face recognition controversies, biases, and bansTo say that some of these applications are controversial would be an understatement. As a 2019 New York Times article discusses, facial recognition has swirled in controversy, from its use for stadium surveillance to racist software.Stadium surveillance? Face recognition was used at the 2001 Super Bowl: the software identified 19 people thought to be subjects of outstanding warrants, though none were arrested (not for lack of trying).Racist software? There have been several issues, starting with the 2009 face tracking software that could track whites but not Blacks, and continuing with the 2015 MIT study that showed that the facial recognition software of the time worked much better on white male faces than female and/or Black faces.These sorts of issues have led to outright bans of facial recognition software in specific places or for specific uses. In 2019, San Francisco became the first major American city to block police and other law enforcement agencies from using face recognition software; Microsoft called for federal regulations on facial recognition; and MIT showed that Amazon Rekognition had more trouble determining female gender than male gender from face images, as well as more trouble with Black female gender than white female gender.In June 2020, Microsoft announced that it will not sell and has not sold its face recognition software to the police; Amazon banned police from using Rekognition for a year; and IBM abandoned its facial recognition technology. Banning face recognition entirely will not be easy, however, given its wide adoption in iPhones (Face ID) and other devices, software, and technologies.Not all face recognition software suffers from the same biases. The 2019 NIST demographic effects study followed up on the MIT work and showed that algorithmic demographic bias varies widely among developers of face recognition software. Yes, there are demographic effects on the false match rate and false non-match rate of facial identification algorithms, but they can vary by several orders of magnitude from vendor to vendor, and they have been decreasing over time.Hacking face recognition, and anti-spoofing techniquesGiven the potential privacy threat from face recognition, and the attraction of getting access to high-value resources protected by face authentication, there have been many efforts to hack or spoof the technology. You can present a printed image of a face instead of a live face, or an image on a screen, or a 3-D printed mask, to pass authentication. For CCTV surveillance, you can play back a video. To avoid surveillance, you can try the “CV Dazzle” fabrics and make-up, and/or IR light emitters, to fool the software into not detecting your face.Of course, there are efforts to develop anti-spoofing techniques for all these attacks. To detect printed images, vendors use a liveness test, such as waiting for the subject to blink, or perform motion analysis, or use infrared to distinguish a live face from a printed image. Another approach is to perform micro-texture analysis, since human skin is optically different from prints and mask materials. The latest anti-spoofing techniques are mostly based on deep convolutional neural networks.This is an evolving field. There’s an arms war going on between attackers and anti-spoofing software, as well as academic research on the effectiveness of different attack and defense techniques.Face recognition vendorsAccording to the Electronic Frontier Foundation, MorphoTrust, a subsidiary of Idemia (formerly known as OT-Morpho or Safran), is one of the largest vendors of face recognition and other biometric identification technology in the United States. It has designed systems for state DMVs, federal and state law enforcement agencies, border control and airports (including TSA PreCheck), and the state department. Other common vendors include 3M, Cognitec, DataWorks Plus, Dynamic Imaging Systems, FaceFirst, and NEC Global.The NIST Face Recognition Vendor Test lists algorithms from many more vendors from all over the world. There are also several open source face recognition algorithms, of varying quality, and a few major cloud services that offer face recognition.Amazon Rekognition is an image and video analysis service that can identify objects, people, text, scenes, and activities, including facial analysis and custom labels. The Google Cloud Vision API is a pretrained image analysis service that can detect objects and faces, read printed and handwritten text, and build metadata into your image catalog. Google AutoML Vision allows you to train custom image models.The Azure Face API does face detection that perceives faces and attributes in an image, performs person identification that matches an individual in your private repository of up to 1 million people, and performs perceived emotion recognition. The Face API can run in the cloud or on the edge in containers.Face datasets for recognition trainingThere are dozens of face datasets available for downloading that can be used for recognition training. Not all face datasets are equal: They tend to vary in image size, number of people represented, number of images per person, conditions of images, and lighting. Law enforcement also has access to non-public face datasets, such as current mugshots and driver’s license images.Some of the larger face databases are Labeled Faces in the Wild, with ~13K unique people; FERET, used for the early NIST tests; the Mugshot database used in the ongoing NIST FRVT; the SCFace surveillance camera database, also available with facial landmarks; and Labeled Wikipedia Faces, with ~1.5K unique identities. Several of these databases contain multiple images per identity. This list from researcher Ethan Meyers offers some cogent advice on picking a face dataset for a specific purpose.In summary, facial recognition is improving, and vendors are learning to detect most spoofing, but some applications of the technology are controversial. The error rate for face recognition is halving every two years, according to NIST. Vendors have improved their anti-spoofing techniques by incorporating convolutional neural networks.Meanwhile, there are initiatives to ban the use of face recognition in surveillance, especially by police. Banning face recognition entirely would be difficult, however, given how widespread it has become.Deep learning vs. machine learning: Understand the differencesWhat is machine learning? Intelligence derived from dataWhat is deep learning? Algorithms that mimic the human brainMachine learning algorithms explainedAutomated machine learning or AutoML explainedSupervised learning explainedSemi-supervised learning explainedUnsupervised learning explainedReinforcement learning explainedWhat is computer vision? AI for images and videoWhat is face recognition? AI for Big BrotherWhat is natural language processing? AI for speech and textKaggle: Where data scientists learn and competeWhat is CUDA? Parallel processing for GPUs", "pub_date": "2020-09-03"},
{"title": "Using robotics and immersive technologies to support work-from-home employees", "overview": "The COVID-19 crisis has accelerated the long-term shift toward remote working as a standard business practice", "image_url": null, "url": "https://www.infoworld.com/article/3572346/using-robotics-and-immersive-technologies-to-support-work-from-home-employees.html", "body": "Even when the pandemic wanes, the percentage of personnel working remotely will continue to grow. According to a recent enterprise survey by 451 Research, the emerging technology research unit of S&P Global Market Intelligence, 80 percent have implemented or expanded universal work-from-home policies, and 67 percent plan to keep at least some remote work policies in place long term or permanently.However, remote employees may find their organizations’ administrative and technical support capabilities unsatisfactory. Remote support tends to lack the hands-on, interactive, and immersive services that we’ve come to expect from on-site technical and administrative support personnel. As enterprises retool their internal processes to support a post-pandemic workforce, they should explore a growing range of immersive and robotics capabilities that can deliver high-quality support to remote, mobile, and other nontraditional worksites.10 professional organizations focused on diversity in techBeing Black in IT: 3 tech leaders share their storiesGender gapped: The state of gender diversity in ITMāori participation in ITIT snapshot: Ethnic diversity in the tech industryUsing robotics to automate remote supportChief among these immersive technologies are RPA (robotic process automation), EC (embodied cognition), AR (augmented reality), VR (virtual reality), and MR (mixed reality).RPA has already come to remote work environments. However, this software-centric approach is not the same as having a traditional human administrative assistant close at hand.In this regard, EC tools, which use sensor-driven artificial intelligence to anchor robotics in physical environments, may bridge the gap by powering traditional hardware-based robots as multifunctional robotic digital assistants. As discussed in this recent article, Facebook researchers are developing EC technology to power a “truly embodied robot” that, among other capabilities, “could check to see whether a door is locked…or retrieve a smartphone that’s ringing in an upstairs bedroom.” Automating physical tasks such as these might boost worker productivity by fending off distractions and interruptions in the remote worker’s day.Of course, smart robots would need to automatically sense the physical parameters of diverse home and other remote work environments. In that regard, Facebook’s R&D points to technological advances that will make that possible. Specifically, Facebook Research has just open sourced a new audio simulation platform called “SoundSpaces,” which trains robotic agents to navigate 3-D environments through AI-based processing of visual and acoustic sensor data.This technology, which the company describes as the “first audio-visual platform for embodied AI,” automatically creates high-fidelity, semantics-infused renderings of virtually any sound source from real-world sensed environments. It enables embodied AI agents to acquire multimodal sensory understandings and develop complex reasoning about objects and places. It runs on Facebook AI’s AI Habitat simulation platform and includes audio renderings from two sets of publicly available 3-D environments: Matterport3D and Replica Dataset.Using dynamic captioning to augment remote supportAR can support remote work environments in which elements of the physical setting are unfamiliar to the worker. It displays “augmented” descriptive captions in a 3-D format over an employee’s direct camera view of his or her physical environment.For support applications, AR might dynamically supplement the information in a worker’s remote physical environment with a dynamic display of descriptive labels and guidance. Wearable AR technology could automatically present remote workers with the information they need at the exact moment they need it.These captions may be prepackaged with the AR-based support application, or they may be automatically inferred from the physical environments using AI-based technologies such as Facebook’s SoundSpaces. For example, some AR-based training applications could display step-by-step instructions for complicated assembly work. This eliminates the need for workers to retrieve that information manually from desktop computer or paper manuals.If the captions need to help remote workers understand human behaviors and intentions (such as in caregiving contexts), technology such as this recent MIT project, may come in handy. Researchers at MIT’s Computer Science and Artificial Intelligence Laboratory have developed a system that can detect and caption the behaviors of people within a room from Wi-Fi and other RF signals. Researchers claim that their approach can observe people through walls and other obstructions, even in complete darkness, and it can learn to detect those people’s interactions with objects, such as a cup of water.Though this may give people the jitters on privacy grounds, it can be very useful in support contexts such as remotely detecting when disabled personnel need assistance or when a fully abled worker is still struggling to master a new physical work skill. The technology, called RF-Diary, can be set up to notify support personnel when the detected behaviors cross some threshold requiring remote intervention.Using virtualization to simulate remote supportRemote employees typically lack direct physical access to all or most of the physical business process in which they’re participating.Simulating the parts of the process that the remote employee can’t interact with directly is where VR comes in handy. It is well suited to support scenarios where important participants, tools, and activities are entirely missing from the remote employee’s immediate physical environment. It may involve avatars to represent unseen personnel and computer-generated imagery to stand in for end-to-end workflows or specific projects, tasks, or outcomes. VR has long been a staple of remote training in medical, engineering, aerospace, military, and other fields. It can provide simulated learning and support capabilities in situations in which remote personnel can cultivate skills within their normal work routines without needing to visit in-person training sites.Another promising use of VR is to enable head-office support staff to interactively visualize the physical environment of the remote worker. One emerging technology that might be useful in this regard is this Stanford-developed “neural holography” system for 3-D rendering. It incorporates AI that is trained with a camera-in-the-loop simulator to generate high-quality, immersive, 3-D visualizations in real time.Then there’s MR, which hybridizes AR and VR by seamlessly blending real and simulated work environments. It is well suited to supporting remote environments where some, but not all, important participants and activities are not always within a worker’s immediate field of vision. This may call for dynamic labeling of those aspects of the physical environment whose meaning is not entirely evident alongside simulation of those aspects that are missing from the environment.Military and police agencies are using MR training tools in a variety of simulated scenarios, including active shooter, domestic violence, and traffic stops. The blend of physical and virtual environments creates realistic training without endangering the lives of soldiers or police officers.What’s nextIn the coming three to five years, immersive technologies will become fundamental to delivering high-quality administrative and technical support to remote workers. The “bring your own device” revolution of the past 20 years will ensure that many future workers will be very comfortable with immersive technologies such as AR, VR, and MR in their professional lives.Using these immersive tools in conjunction with at-home robotics devices will enable enterprises to deliver 24x7 support to remote workers that’s almost as hands-on as what they might have received if they’d remained at their company’s traditional offices.", "pub_date": "2020-09-03"},
{"title": "Amazon, Google, and Microsoft take their clouds to the edge", "overview": "As enterprises ramp up edge computing deployments, the big three clouds are ponying up a surprising array of edge options for a broad range of needs.", "image_url": null, "url": "https://www.infoworld.com/article/3575071/amazon-google-and-microsoft-take-their-clouds-to-the-edge.html", "body": "It might surprise you to learn that the big three public clouds – AWS, Google Could Platform, and Microsoft Azure – are all starting to provide edge computing capabilities. It’s puzzling, because the phrase “edge computing” implies a mini datacenter, typically connected to IoT devices and deployed at an enterprise network’s edge rather than in the cloud.The big three clouds have only partial control over such key edge attributes as location, network, and infrastructure. Can they truly provide edge computing capabilities?Also on InfoWorld: How to choose a cloud machine learning platformThe answer is yes, although the public cloud providers are developing their edge computing services via strategic partnerships and with some early-stage limitations.Edge Computing4 essential edge computing use cases (Network World)Edge computing's epic turf war (CIO)Securing the edge: 5 best practices (CSO)Edge computing and 5G give business apps a boost (Computerworld)Amazon, Google, and Microsoft take their clouds to the edge (InfoWorld)Cloud-based edge computing offerings are a clear sign that the boundaries between public cloud, private cloud, and edge computing are blurring. The unifying goal is to provide businesses and architects with a range of choices based on the type of workload and its performance, reliability, regulatory, and safety requirements.Unfortunately, a glut of new options always means new jargon and branding, so we’ll need to do a fair bit of demystifying as we sort through the big three cloud offerings for edge computing. Before jumping in, though, let’s start with a quick primer on some key edge computing architecture considerations.Understanding edge computing requirements and architecturesFirst and foremost, engineering teams must understand the edge computing requirements. Connecting a globally dispersed network of inexpensive sensors that generate a few terabytes of data daily has different computing requirements than servicing a dozen factory floors with an array of video sensors processing petabytes of data in real-time. The architecture must address the specific data processing, analytics, and workflows needed.Then, just as important, consider the regulatory, security, and safety requirements. Medical devices deployed at hospitals or controllers for autonomous vehicles both capture and process highly personal, life-critical information. Reliability and performance demands should dictate the location, network, security, and infrastructure requirements.Understanding these requirements helps architects determine where to locate edge computing infrastructure physically, what types of infrastructure are required, the minimal connectivity requirements, and other design considerations.But the unique benefit public cloud edge computing offers is the ability to extend underlying cloud architecture and services, particularly for customers already heavily invested in one public cloud or another. Do architects and developers want to leverage AWS, Azure, or Google Cloud services deployed to the edge? That’s what the public clouds are betting on – and they are also considering 5G-enabled mobile applications that require low-latency data and machine learning processing at telco endpoints.With these questions in mind, here’s an overview of what the three major public clouds provide.Extend to Azure Edge Zones with Azure StackAzure is betting that architects and developers want to focus on the application and less on the infrastructure. Azure has three options that enable a hybrid edge, where architects can leverage 5G networks and deploy data processing, machine learning models, streaming applications, and other real-time data-intensive applications optimally.Azure Edge Zones are managed deployments of the Azure stack that can be purchased through Microsoft and are currently available in New York, Los Angeles, and Miami.Microsoft partnered with AT&T to offer Azure End Zones with Carrier in a number of locations, including Atlanta, Dallas, and Los Angeles. This option is best for 5G-enabled mobile applications that require low-latency data processing or machine learning capabilities.Lastly, businesses can also deploy a private Azure Edge Zone. Microsoft has partnered with several data center providers that enable this capability.These options provide location choices and network flexibility, while Azure Stack Edge brings Azure computing and services to the edge. Azure Stack Edge is a 1U, 2x10 Core Intel Xeon, 128GB appliance that can be configured with containers or VMs and managed as a Kubernetes cluster of appliances. This model is optimized for machine learning and IoT applications.Microsoft also offers Azure Stack HCI, a hyperconverged infrastructure for modernizing data centers, and Azure Stack Hub for deploying cloud-native applications.Like other cloud services, Microsoft sells Azure Stack Edge by subscription, with costs calculated by the utility. Microsoft manages the device and offers 99.9 percent service level availability.Extending AWS services from 5G devices to large scale analyticsAWS has a similar set of offerings to distribute AWS services to edge data centers and telco networks.AWS is starting to support edge data centers with AWS Local Zones that are currently available only in Los Angeles.AWS Wavelength is designed for low-latency applications running on 5G devices, including connected vehicles, AR/VR applications, smart factories, and real-time gaming.AWS is partnering with Verizon to deliver AWS Wavelength, which is currently available in Boston and the San Francisco Bay area.AWS offers two flavors of edge infrastructure, starting with the AWS Snow line of appliances. AWS Snowcone is the smallest appliance with two vCPUs and 4GB that is primarily used for edge data storage and transfer. Memory-intensive data processing and machine learning applications deployed to the edge likely require AWS Snowball Edge that comes in storage and compute-optimized models with up to 52 vCPUs and 208GB. For the largest scale applications, AWS Outposts are 42U racks deployed to data centers for running different EC2 instance types, containers (Amazon ECS), Kubernetes (Amazon EKS), databases (Amazon RDS), data analytics (Amazon EMR), and other AWS services.Google lags as all three clouds vie for the edgeJust as Google occupies third place in the public cloud wars, it’s also trying to catch up with its edge offerings. Google’s most recent announcements include Anthos at the Edge, collaborations with AT&T on 5G connectivity, and the Google Mobile Edge Cloud. The offering is part of Anthos, a hybrid and multi-cloud application modernization platform enabling businesses to deploy applications on GCP and in the data center.The public cloud vendors all recognize that the next wave of innovation is coming from the intersection of IoT, 5G, and machine learning analytics deployed at the edge. They’re not going to let infrastructure and data center companies like Dell or HPE dominate this new market without a fight, so their answer is to bring their cloud platforms, containers, orchestration, and services to edge data centers and telco endpoints. And they’re not doing this alone: The public clouds are partnering with telcos, infrastructure providers, and major service providers to enable their offerings.But these are early days for public cloud edge computing solutions. The fact that the big three clouds are getting serious about edge computing only serves to underscore the promise of the edge frontier. Whether enterprises choose public cloud edge solutions or opt to build their own infrastructure, network, and computing platforms, few will want to be left out of the growing wave of edge innovation.", "pub_date": "2020-09-14"},
{"title": "Rethinking 'cloud bursting'", "overview": "Nothing is the same in the world of cloud technology year-to-year. Could cloud bursting make a comeback in new packaging?", "image_url": null, "url": "https://www.infoworld.com/article/3575078/rethinking-cloud-bursting.html", "body": "About two years ago I wrote a piece here on the concept of cloud bursting, where I pointed out a few realities:Private clouds are no longer a thing, considering the current state of private cloud systems compared to the features and functions of the larger hyperscalers.You need to maintain workloads on both private and public clouds for hybrid cloud bursting to work; in essence, using two different platforms.It’s clear the bursting hybrid clouds concept just adds too much complexity and cost for a technology stack (the cloud) to be widely adopted by companies that want to do the most with the least.Also on InfoWorld: Tiny clouds taking on AWS, Microsoft Azure, and Google CloudIn case you missed the fervor in the tech press a few years ago, cloud bursting is the concept of leveraging public clouds only when capacity of the on-premises cloud runs out. Somehow companies thought that they could invoke a public cloud-based part of the application and have access to the same data without latency. Mostly it did not work.I stand by that posting, but now I have a few additional things to say about the concept of cloud bursting in 2020 and 2021.First, a few on-premises solutions exist today that are close analogs to public clouds, because they are sold by the public cloud providers. The larger hyperscalers, including Google, Microsoft, and AWS, have hardware and software solutions that exist in traditional data centers. Simply put, these are a scaled-down version of their public cloud solutions packaged as appliances.Cloud bursting is possible with these solutions, considering that both the on-premises platform and the public cloud platform are purpose-built to work and play well together. The objective is to eventually move the on-premises workloads to the public clouds by using these on-premises solutions as an intermediate step. Second, edge computing is a thing now. The use of IoT devices connected to public clouds has always been around, but the formal use of edge-based systems that are both devices and legit servers is part of the cloud architecture zeitgeist. This means that edge computing has processing and data storage outside of the public cloud providers that are also purpose-built to work with specific public clouds. Moreover, the public cloud providers support edge directly now. Those deploying systems that leverage edge computing infrastructure don’t have to build things from scratch to use public clouds for back-end processing.Although there are more and more architectural patterns that look like cloud bursting, the notion is really about distributing processing and storage, which is not at all new. My purpose in pointing out what’s changed is really to point out that things do indeed change. This is why I love the cloud computing business.", "pub_date": "2020-09-15"},
{"title": "How BT is shifting its engineers into the fast lane", "overview": "BT’s head of software engineering excellence lays out the telco’s route to streamlining development, modernizing applications, automating deployments", "image_url": null, "url": "https://www.infoworld.com/article/3575414/how-bt-is-shifting-its-engineers-into-the-fast-lane.html", "body": "The broad industry shift toward the cloud and containerized workloads can often ignore the reality for many established enterprises: What happens when the unstoppable force of cloud native progress meets the immovable object of legacy technology?This is the current challenge being faced by BT, the 170-year old British telecoms business, which is looking to adopt cloud native tooling and techniques in a safe and sustainable way across a large and diverse developer population.Only then will the business be able to move fast enough to keep up with rapidly changing customer expectations and competitor efforts in the era of over-the-top media proliferation and 5G connectivity.“We have a lot of legacy and code that served us in the past,” Rajesh Premchandran, head of software engineering excellence at BT told InfoWorld. “Our developers right now are constrained by an existing stack — that’s important to acknowledge — it’s not all greenfield, there’s a constraint of architecture and design.”Engineering leaders at BT are hoping that a common, modern set of cloud services, combined with a gradual shift toward containers and Kubernetes can reduce the amount of wasted effort its developers expend on a day-to-day basis.Slowly but surely BT is looking to consolidate and modernize its existing workloads. This is where containers and Kubernetes enter the equation, but Premchandran urges caution to anyone who sees the container orchestration tool as a silver bullet.“What really happens is you’ve got to tease out what is containerizable,” he explains. “Say you have a monolith and you’ve got 50 components, all intertwined with hard-coded dependencies and some legacy stack. First you ask, what do I containerize? Where is that unit that I can tease out without impacting the others? That is the biggest challenge that most firms which have been around as long as we have face.”[Also on InfoWorld: Kubernetes meets the real world: 3 success stories]To overcome this challenge, BT has established a platform team, dedicated to helping application teams identify these containerizable elements and find the best environment in which to host them, be it in the public cloud or on a platform-as-a-service (PaaS).This has led to a common set of considerations BT engineers take when looking to modernize their workloads, namely how loosely coupled it is and whether you can effectively isolate a service and containerize it, without impacting the existing stack.In terms of infrastructure options, there is some choice, but the idea is to simplify these decisions to gain some level of consistency.Of course all three major infrastructure-as-a-service (IaaS) options — Amazon Web Services, Microsoft Azure, and Google Cloud Platform — are on the table. Where possible the platform team will push developers to use a managed Kubernetes service or, better still, the Tanzu Application Service, a PaaS from the vendor VMware, and its own Tanzu Kubernetes Grid (TKG).“You have to handhold them, otherwise they will take the biggest unit they can handle, put that into a Docker container and then lo and behold you’ve got the same monster on better infrastructure — that doesn’t solve your business problem,” Premchandran says of his developers.That being said, when push comes to shove, the preference would always be for TKG, which promises a consistent managed Kubernetes layer across on-premises and public cloud environments.“You may want to swap out your cloud provider tomorrow, you might want to move your workloads from on prem to cloud, so you need a control plane for Kubernetes that makes the underlying IaaS irrelevant,” Premchandran explains.That doesn’t mean that a certain subset of developers at BT aren’t still looking to get their hands dirty with Kubernetes.“It’s like driving an automatic car, there are aficionados who like the feel of changing gears and the clutch. With TKG available it’s more important to ask what is your specific need, not your want, and if you’ve done it the hard way, here is the easy way and it’s your choice,” Premchandran says.“Kubernetes is hard... we won’t enjoy doing that at scale, so that is where we ask developers to focus on the application logic and let the hard things be automated for you,” he adds.That doesn’t mean that self-managed Kubernetes is off the table, but really it should be reserved for use cases where developers really need that fine-grained level of control. For everything else, let the vendor take care of it.“This is a constant tussle,” he admits, “where people want Kubernetes by default, but I think you’ve got to be a bit more prescriptive in the beginning to developers and then as you see them maturing, have them make those independent choices.”Once those principles and frameworks have been established across BT’s developer community, the next task is to scale it out via documentation and word of mouth buzz, both for in-house engineers and with external partners.The advantage of having some developers getting their hands dirty with Kubernetes does mean that a new area of expertise starts to spring up across your organization. This is never a bad thing, especially when you are looking to evangelize a new technology, where a few respected voices can go a long way to getting that buy-in.“If you look at how standards are democratized, you’ve got enterprise architecture that looks at the overall architecture standards and policies, then you have solution architects, who are a level down and are specific to a product, and finally we have distinguished engineers — we call them expert engineers — who are also key influencers for other developers, who look up to them,” Premchandran explains.For partners, BT embarked on a pre-pandemic roadshow, running events and sessions to lay out the new direction and get them up to speed with the latest documentation, including a new Software Engineering Handbook and internal wiki.“You can’t have a top-down diktat saying ‘march towards the cloud and let developers figure out their own learning part’,” he says. “We’ve invested heavily in certification programs, getting online e-learning platforms that allow career paths to be developed around the cloud and agile and DevOps and all of the associated skills.”Also on InfoWorld: Tim O’Reilly: The golden age of the programmer is overNow it is about scaling that knowledge and those skills across the organization. “We’ve learned the ropes, what happens when workloads are portable, now developers need to be full-stack engineers understanding hybrid cloud workloads and have the skills to train other people to modernize their own stacks,” he says.Once all of this has been achieved, Premchandran hopes that he is overseeing a happier developer community deploying at much higher velocity than was previously possible.“Infrastructure provisioning needs to be at a much faster clip to allow DevOps to happen,” he says, but it’s not just speed that he wants.“This is an interconnected problem set. You want to go fast because your customers need things fast, but you’ve got disconnected teams,” he says. Kubernetes and other cloud native approaches promise a rote to bringing all of these teams together onto a common methodology.Over the next couple of years Premchandran will focus on getting more applications onto a modern, cloud native, microservice-based architecture; implement DevSecOps practices for “completely zero touch deployments”; and, finally, increase the number of reusable software components across the group.For example, if someone decides to build a billing component, can this be used by anyone needing to bill customers within an application? In an ideal world, yes.The goal for Premchandran is a developer community that can move as fast as it needs to, without being encumbered by legacy or infrastructure concerns.“Agile is not just about going fast, it’s about having the choice to go fast. There’s a difference. Just because your car can hit 250 miles-per-hour, that doesn’t mean you’re at that speed all the time. But I want a car that can go that fast if need be,” he says. ", "pub_date": "2020-09-17"},
{"title": "When a digital twin becomes the evil twin", "overview": "The use of digital twins is becoming more pervasive, but things are going very wrong in some instances. Here’s how to fix them now", "image_url": null, "url": "https://www.infoworld.com/article/3574905/when-a-digital-twin-becomes-the-evil-twin.html", "body": "A digital twin is a digital replica of some physical entity, such as a person, a device, manufacturing equipment, or even planes and cars. The idea is to provide a real-time simulation of a physical asset or human to determine when problems are likely to occur and to proactively fix them before they actually arise.Although the roles of digital twins vary a great deal, the connection is established using real-time data from sensors that are able to sync the digital twin living in a virtual world with the physical twin that we can actually touch. This new synchronized simulation leverages IoT (Internet of Things), AI (artificial intelligence), machine learning, and analytics with spatial graphics to create a working simulation of the model that updates as the physical entity changes. There are all types of use cases for digital twins; the most common is a digital twin to represent machines, such as factory equipment and robotics. The twin simulates when proactive maintenance should occur, and if implemented properly should provide better machine productivity and uptime.At issue is that most digital twins exist in public clouds, for the obvious reason that they are much cheaper to run and can access all of the cloud's storage and processing, as well as special services such as AI and analytics, to support the twin. Moreover, the cloud offers purpose-built services for creating and running twins. The ease of building, provisioning, and deploying twins has led to a few issues where the digital twin becomes the evil twin and does more harm than good. Some of the examples that I’ve seen include:In manufacturing, the twin over- or understates the proactivity needed to fix issues before they become real problems. Companies are fixing things identified in the twin simulation that actually don’t need to be fixed. For example, they replace hydraulic fluid three times more often than needed in a factory robot. Or worse, the twin suggests configuration changes that result in overheating and a fire. That last one really happened.In the transportation industry, a digital twin could shut down a jet engine due to what the twin simulated to be a fire but turned out to be a faulty sensor.In the world of healthcare, a patient was indicated as having factors that would likely lead to a stroke but it was determined to be a problem with the predictive analytics model.The point I’m making is that attaching simulations to real devices, machines, and even humans has a great deal of room for error. Most of these can be tracked back to the people creating twins for the first time who don’t find their mistakes until shortly after deployment. The problem I have is that you could crash an airplane, scare the hell out of a patient, or have a factory robot go aflame.With the cloud making the use of digital twins much more affordable and speedier, I see issues like these increasing. Perhaps the problems aren't evil, but they're certainly avoidable.", "pub_date": "2020-09-18"},
{"title": "The 2020 Enterprise Architecture Awards", "overview": "The 2020 Forrester and InfoWorld EA Awards contest winners focus on business architecture, digital transformation, and governance", "image_url": null, "url": "https://www.infoworld.com/article/3575829/the-2020-enterprise-architecture-awards.html", "body": "In the Forrester/InfoWorld Enterprise Architecture Awards competition, we look for the most dramatic stories of EA’s strategic leadership and concrete business impact. The winners of the 2020 Forrester/InfoWorld Enterprise Architecture Awards show the value of a close relationship with the business, a solid vision for enabling digital transformation, and effective governance practices — not to mention the need for a high-priority response to a global pandemic!In alphabetical order, the winners this year are the EA teams of:CSL BehringThomas Jefferson University and Jefferson HealthLexmarkSun LifeVerizon CommunicationsCongrats to all the winners — each has a compelling story of EA best practices. You can read those stories below.", "pub_date": "2020-09-21"},
{"title": "Will cloud architecture change after COVID?", "overview": "Cloud computing has exploded in popularity during the pandemic. The systemic changes it has caused are likely here to stay", "image_url": null, "url": "https://www.infoworld.com/article/3575854/will-cloud-architecture-change-after-covid.html", "body": "A few things have changed since the start of the COVID-19 lockdowns. The Global 2000, as well as governments, now understand that traditional data centers are more vulnerable to natural disasters (such as pandemics) than once thought.  Indeed, the use of cloud computing means that there are no physical data centers and servers to protect, and that we don’t rely on humans for physical operations as much as we did in the past.  Also on InfoWorld: How to choose a cloud machine learning platformPutting the obvious aside for now, let’s look at the future of cloud computing, specifically at the future of cloud computing architecture. I see a few changes happening now that won’t likely go away after the current crisis ends: Only a few enterprise IT shops selected data consolidation as an option, cloud or not. The reasons ranged from security risks, to the lack of willingness to go through the painful process of migrating data from complex and distributed databases, to a consolidated database in the cloud, or even on-premises.  Today, driven by the risks around COVID-19, enterprises are accelerating data consolidation efforts, typically moving to a public cloud provider or providers. This means changing schemas and even databases models, for example, from relational to object.This also means changing the applications that leverage the databases, which is where the costs and risk come in. Some stuff is just going to break. Most enterprises think the risk of not doing this outweighs the risk of data consolidation. You’ll find this will be the preferred approach post-COVID as well. I’m often taken aback by the number of enterprises that moved to public clouds yet still don’t use identity-based security and governance, such as IAM (identity and access management). With the new distributed and remote workforce, IT is finding that traditional security layers, and even role-based security, are way more risky than fined-grained security and governance configurations using identity. This will likely lead to breaches if left alone, not to mention it is more costly to operate.Of course, much like data consolidation, making the change is hard and expensive. Each device, human, API, server, etc., needs to use an identity in order to configure authorization and deauthorization of access. This means standing up directory services if they don’t exist and investing in IAM technology.   The common theme is that enterprises are stepping up and doing what’s hard, risky, and costly in order to remove much of the greater risk that has been identified by the impact of the pandemic. This is a silver lining, considering that these two things needed to be done anyway as a major best practice going forward. ", "pub_date": "2020-09-22"},
{"title": "How MariaDB achieves global scale with Xpand", "overview": "A new MariaDB storage engine provides distributed SQL and massive scalability with a shared nothing architecture, fully distributed ACID transactions, and strong consistency", "image_url": null, "url": "https://www.infoworld.com/article/3574077/how-mariadb-achieves-global-scale-with-xpand.html", "body": "As information and processing needs have grown, pain points such as performance and resiliency have necessitated new solutions. Databases need to maintain ACID compliance and consistency, provide high availability and high performance, and handle massive workloads without becoming a drain on resources. Sharding has offered a solution, but for many companies sharding has reached its limits, due to its complexity and resource requirements. A better solution is distributed SQL.In a distributed SQL implementation, the database is distributed across multiple physical systems, delivering transactions at a globally scalable level. MariaDB Platform X5, a major release that includes upgrades to every aspect of MariaDB Platform, provides distributed SQL and massive scalability through the addition of a new smart storage engine called Xpand. With a shared nothing architecture, fully distributed ACID transactions, and strong consistency, Xpand allows you to scale to millions of transactions per second.Also on InfoWorld: Tim O’Reilly: The golden age of the programmer is overOptimized pluggable smart enginesMariaDB Enterprise Server is architected to use pluggable storage engines (like Xpand) to optimize for particular workloads from a single platform. There is no need for specialized databases to handle specific workloads. MariaDB Xpand, our smart engine for distributed SQL, is the most recent addition to our lineup. Xpand adds massively scalable distributed transactional capabilities to the options provided by our other engines. Our other pluggable engines provide optimization for analytical (columnar), read-heavy workloads, and write-heavy workloads. You can mix and match replicated, distributed, and columnar tables to optimize every database for your specific requirements.Adding MariaDB Xpand enables enterprise customers to gain all the benefits of distributed SQL – speed, availability, and scalability – while retaining the MariaDB benefits they are accustomed to.Let’s take a high-level look at how MariaDB Xpand provides distributed SQL.Distributed SQL down to the indexesXpand provides distributed SQL by slicing, replicating, and distributing data across nodes. What does this mean? We’ll use a very simple example with one table and three nodes to demonstrate the concepts. Not shown in this example is that all slices are replicated.Figure 1. Sample table with indexesIn Figure 1 above, we have a table with two indexes. The table has some dates and we have an index on column 2, and another on columns 3 and 1. Indexes are in a sense tables themselves. They’re subsets of the table. The primary key is , the first index in the table. That’s what will be used to hash and spread the table data out around the database.Figure 2. Xpand slices and distributes data, including indexes, across nodes. (Replication is not shown for reasons of simplicity. All slices have at least two replicas.)Now we add the notion of . Slices are essentially horizontal partitions of the table. We have five rows in our table. In Figure 2, the table has been sliced and distributed. Node #1 has two rows. Node #2 has two rows, and Node #3 has one row. The goal is to have the data distributed as evenly as possible across the nodes.The  have also been sliced and distributed. This is a key difference between Xpand and other distributed solutions. Typically, distributed databases have local indexes, so every node has an index of its own data. In Xpand, indexes are distributed and stored independently of the table. This eliminates the need to send a query to all nodes (scatter/gather). In the example above, Node #1 contains rows 2 and 4 of the table, and also contains indexes for rows 32 and 35 and rows April and March. The table and the indexes are independently sliced, distributed, and replicated across the nodes.The query engine uses the distributed indexes to determine where to find the data. It looks up only the index partitions needed and then sends queries only to the locations where the needed data reside. Queries are all distributed. They’re done concurrently and in parallel. Where they go depends entirely on the data and what is needed to resolve the query.All slices are replicated at least twice. For every slice, there are replicas residing on other nodes. By default, there will be three copies of that data – the slice and two replicas. Each copy will be on a different node, and if you were running in multiple availability zones, those copies would also be sitting in different availability zones.Read and write handlingLet’s take another example. In Figure 3, we have five instances of MariaDB Enterprise Server with Xpand (nodes). There’s a table to store customer profiles. The slice with Shane’s profile is on Node #1 with copies on Node #3 and Node #5. Queries can come in on any node and will be processed differently depending on if they are reads or writes.Figure 3. Writes are processed simultaneously to all copies in a distributed transaction.Writes are made to all copies synchronously inside a distributed transaction. Any time I update my “Shane” profile because I changed my email or I changed my address, those writes go to all copies at the same time within a transaction. This is what provides strong consistency.In Figure 3, the UPDATE statement went to Node #2. There is nothing on Node #2 regarding my profile but Node #2 knows where my profile is and sends updates to Node #1, Node #3, and Node #5, then commits that transaction and returns back to the application.Reads are handled differently. In the diagram, the slice with my profile on it is on Node #1 with copies on Node #3 and Node #5. This makes Node #1 the . Every slice has a ranking replica, which could be said to be the node that “owns” the data. By default, no matter which node a read comes in on, it always goes to the ranking replica, so every SELECT that resolves to me will go to Node #1.Providing elasticityDistributed databases like Xpand are continuously changing and evolving depending on the data in the application. The rebalancer process is responsible for adapting the data distribution to current needs and maintaining the optimal distribution of slices across nodes. There are three general scenarios that call for redistribution: adding nodes, removing nodes, and preventing uneven workloads or “hot spots.”For example, say we are running with three nodes but find traffic is increasing and we need to scale – we add a fourth node to handle the traffic. Node #4 is empty when we add it as shown in Figure 4. The rebalancer automatically moves slices and replicas to make use of Node #4, as shown in Figure 5.Figure 4. Node 4 has been added to handle increased traffic. Nodes are empty when they are added to the Xpand cluster.Figure 5. The Xpand rebalancer redistributes slices and replicas to take advantage of the increased capacity.If Node #4 should fail, the rebalancer automatically goes to work again; this time recreating slices from their replicas. No data is lost. Replicas are also recreated to replace those that were residing on Node #4, so all slices again have replicas on other nodes to ensure high availability.Figure 6. If a node fails, the Xpand rebalancer recreates the slices and the replicas that resided on the failed node from the replica data on the other nodes.Balancing the workloadIn addition to scale out and high availability, the rebalancer mitigates unequal workload distribution – either hot spots or underutilization. Even when data is randomly distributed with a perfect hash algorithm, hot spots can occur. For example, it could happen just by chance that the 10 products on sale this month happen to be sitting on Node #1. The data is evenly distributed but the workload is not (Figure 7). In this type of scenario, the rebalancer will redistribute slices to balance resource utilization (Figure 8).Figure 7. Xpand has evenly distributed the data but the workload is uneven. Node 1 has a significantly higher workload than the other three nodes.Figure 8. Xpand’s rebalancer redistributes data slices to balance the workload across nodes.Scalability, speed, availability, balanceInformation and processing needs will continue to grow. That’s a given. MariaDB Xpand provides a consistent, ACID-compliant scaling solution for enterprises with requirements that can’t be met with other alternatives like replication and sharding.Distributed SQL provides scalability, and MariaDB Xpand provides the flexibility to choose how much scalability you need. Distribute one table or multiple tables or even your whole database, the choice is yours. Operationally, capacity is easily adjusted to meet changing workload demands at any given time. You never have to be over-provisioned.Xpand also transparently protects against uneven resource utilization, dynamically redistributing data to balance the workload across nodes and prevent hot spots. For developers, there’s no need to worry about scalability and performance. Xpand is elastic. Xpand also provides redundancy and high availability. With data sliced, replicated, and distributed across nodes, data is protected and redundancy is maintained in the event of hardware failure.Also on InfoWorld: Beyond NoSQL: The case for distributed SQLAnd, with MariaDB’s architecture, your distributed tables will play nicely – including cross-engine JOINs – with your other MariaDB tables. Create the database solution you need by mixing and matching replicated, distributed, or columnar tables all on a single database on MariaDB Platform.MariaDB Corporation—newtechforum@infoworld.com", "pub_date": "2020-09-23"},
{"title": "PyTorch 1.5 adds C++ power, distributed training", "overview": "The powerful deep learning system for Python now makes it easier to integrate high performance C++ code and train models on multiple machines at once", "image_url": null, "url": "https://www.infoworld.com/article/3539275/pytorch-15-adds-c-power-distributed-training.html", "body": "PyTorch, the Python framework for quick-and-easy creation of deep learning models, is now out in version 1.5.PyTorch 1.5 brings a major update to PyTorch’s C++ front end, the C++ interface to PyTorch functionality. The C++ front end now offers complete feature parity with the Python API. Normally one would write PyTorch applications exclusively in Python, but this change makes it readily possible to prototype the application in Python, then move it to C++ without losing any features, or to freely mix C++ and Python.Also on InfoWorld: 5 reasons to choose PyTorch for deep learningPyTorch 1.5 also adds a way to bind custom C++ classes to TorchScript and Python. TorchScript lets you create models in Python and run them without Python dependencies—e.g., in C++. The new binding system allows your C++ code to be visible to TorchScript, so C++ objects and memory spaces can be created and manipulated using TorchScript or Python. This feature is still considered experimental.PyTorch 1.5 also drops support for Python 2 and requires Python 3.5 and higher. This is in line with other major Python frameworks, as Python 2 is now at end-of-life status and is no longer receiving any updates.Finally, PyTorch 1.5 brings a stable version of the distributed RPC framework and RPC API. Introduced in PyTorch 1.4 as an experimental feature, the RPC framework provides mechanisms for running PyTorch functions on remote machines and thus allows training models across multiple machines for faster training results. Also on InfoWorld: 8 great Python libraries for natural language processingWith PyTorch 1.5, the RPC framework can be used to build training applications that make use of distributed architectures if they’re available. The RPC framework is designed to minimize the amount of data copying across nodes, so work is always done as close to the data as possible.", "pub_date": "2020-04-22"},
{"title": "10 questions about deep learning ", "overview": "Learn why neural networks are so powerful, how and where they’re used, and how to get started — no programming necessary", "image_url": null, "url": "https://www.infoworld.com/article/3532058/10-questions-about-deep-learning.html", "body": "It seems everywhere you look nowadays, you will find an article that describes a winning strategy using deep learning in a data science problem, or more specifically in the field of artificial intelligence (AI). However, clear explanations of deep learning, why it’s so powerful, and the various forms deep learning takes in practice, are not so easy to come by.In order to know more about deep learning, neural networks, the major innovations, the most widely used paradigms, where deep learning works and doesn’t, and even a little of the history, we have asked and answered a few basic questions.Also on InfoWorld: Artificial intelligence today: What’s hype and what’s realDeep learning is the modern evolution of traditional neural networks. Indeed, to the classic feed-forward, fully connected, backpropagation trained, multilayer perceptrons (MLPs), “deeper” architectures have been added. Deeper means more hidden layers and a few new additional neural paradigms, as in recurrent networks and in convolutional networks.There is no difference. Deep learning networks are neural networks, just with more complex architectures than were possible to train in the 1990s. For example, long short-term memory (LSTM) units in recurrent neural networks (RNNs) were introduced in 1997 by Hochreiter and Schmidhuber but never found extensive adoption because of the long computational times and high computational resources that were required. Multilayer perceptrons with more than one hidden layer have also been around for a long time, and their benefits were clear. The main difference is that modern computational resources have made their implementation feasible. In general, faster and more powerful computational resources have allowed for the implementation and experimentation of more powerful and more promising neural architectures. It is clear that spending days in network training cannot rival the few minutes spent in training the same network with the help of GPU acceleration.The big breakthrough was in 2012 when the deep learning-based AlexNet network won the ImageNet challenge with an unprecedented margin. The top-five error rate of AlexNet was 15 percent, while the next best competitor ended up with 26 percent. This victory kicked off a surge in deep learning networks, and the best models nowadays attain error rates below the 3 percent mark.That’s particularly impressive if you consider that the human error rate is around 5 percent.In a word, flexibility. On the one hand, neural networks are universal function approximators, which is smart talk for saying that you can approximate almost anything using a neural network—if you make it complex enough. On the other hand, you can use the trained weights of a network to initialize the weights of another network that performs a similar task. This is called transfer learning, and you would be surprised how well it works, even for tasks that seem quite dissimilar at first glance.Also on InfoWorld: Artificial intelligence predictions for 2020There are four very successful and widely adopted deep learning paradigms: LSTM units in recurrent neural networks, convolutional layers in convolutional neural networks (CNNs), encoder-decoder structures, and generative adversarial networks (GANs).RNNs are a family of neural networks used for processing sequential data, like text (e.g., a sequence of words or characters) or time series data. The idea is to apply a copy of the same network at each time step and connect the different copies via some state vectors. This allows the network to remember information from the past. Popular unit network structures in RNNs are gated recurrent units (GRUs) and LSTMs.CNN layers are especially powerful for data with spatial dependencies like images. Instead of connecting every neuron to the new layer, a sliding window is used, which works like a filter.  Some convolutions may detect edges or corners, while others may detect cats, dogs or street signs inside an image.Another often used neural network structure is the encoder-decoder network. A simple example is an autoencoder where a neural network with a bottleneck layer is trained to reconstruct the input to the output. A second application of encoder-decoder networks is neural machine translation where encoder-decoder structure is used in an RNN. The LSTM-based encoder extracts a dense representation of the content in the source language, and the LSTM-based decoder generates the output sequence in the target language.And, of course, the generative adversarial networks. A generative adversarial network is composed of two deep learning networks, the generator and the discriminator. Both networks are trained in alternating steps competing to improve themselves. GANs have been successfully applied to image tensors to create anime, human figures and even van Gogh-like masterpieces.No, at least not yet. There are certain domains, like computer vision, where you can’t get around deep learning anymore, but there are other areas, such as tabular data, that have proven to be a challenge for deep learning.In the case of tabular data, which is still the main format used for storing business data, deep learning is not doing terribly poorly there. However, training a deep learning model for days on an expensive GPU server is hard to justify if you can get similar accuracy using random forests or gradient boosted trees, which you can train within a few minutes on a decent laptop.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesNot really. It is true that most deep learning paradigms are available in TensorFlow and Keras and that both of them require Python skills. However, in our open source KNIME Analytics Platform, we provide a graphical user interface (GUI) to handle exactly those Keras and deep learning libraries using TensorFlow in the back end. You can build a neural architecture as complex as you wish just by dragging and dropping the appropriate nodes one after the other.An example is shown in Figure 1 below, where we trained an LSTM-based RNN to generate free text. The model creates fake names that resemble mountain names for a new outdoor clothing line. At the top (the brown nodes), you can see where we built the neural architecture, which we then trained using the Keras Network Learner node. The trained network, opportunely modified, is then saved in a TensorFlow format.Figure 1. Constructing and training an LSTM-based RNN to generate free text. At the top, the brown nodes build the network architecture. Then, the Keras Network Learner node trains the network, which after some appropriate post-processing is saved in a TensorFlow file.You can find plenty on our community KNIME Hub. For example, recurrent neural networks with LSTM units can be found in this example for free text generation (also shown in Figure 1) or in this other example for time series prediction. Also, there are several example workflows using convolutional neural networks to process images, such as “Building a CNN from scratch” or “Train simple CNN.” A simple feed-forward, fully connected multilayer autoencoding structure was built and used as a solution to a fraud detection task. I am sure many more have been uploaded by the community as we speak.Keep up with advances in machine learning, AI, and big data analytics with InfoWorld’s Machine Learning and Analytics Report newsletterKNIME Analytics Platform is an open source application and so are its integrations, including the Keras and TensorFlow integrations. You can install them wherever you wish, i.e. in the public cloud of your choosing or on your machine. It is clear, though, that the more powerful the machine, the faster the execution. You can even apply GPU acceleration in the KNIME Keras integration. You just need a GPU-equipped machine with CUDA installed, a Conda environment with Keras for GPU installation, and the KNIME Keras integration on top of that.KNIMEPracticing Data Science: A Collection of Case StudiesTwitterLinkedInKNIME blogLinkedInLinkedInFrom Alteryx to KNIMELinkedInwww.knime.comKNIME blog", "pub_date": "2020-03-12"},
{"title": "AI companies plant the seeds for quantum machine learning", "overview": "Quantum computing promises to accelerate analytics faster than the speed of light, but it still feels slightly unreal, in spite of the first signs of broad commercialization", "image_url": null, "url": "https://www.infoworld.com/article/3532436/ai-companies-plant-the-seeds-for-quantum-machine-learning.html", "body": "Quantum isn’t the next big thing in advanced computing so much as a futuristic approach that could potentially be the biggest thing of all.Considering the theoretical possibility of quantum fabrics that enable seemingly magical, astronomically parallel, unbreakably encrypted, and faster-than-light subatomic computations, this could be the omega architecture in the evolution of AI (artificial intelligence).Also on InfoWorld: Artificial intelligence predictions for 2020No one doubts that the IT industry is making impressive progress in developing and commercializing quantum technologies. But this mania is also shaping up to be the hype that ends all hype. It will take time for quantum technology to prove itself a worthy successor to computing’s traditional von Neumann architecture.Though the splashy headlines boast of quantum supremacy, which refers to claims that programmable quantum devices can solve problems beyond the reach of von Neumann architectures, there has been far less focus on quantum practicality. In other words, there is still little evidence that quantum computers are being applied to real-world use cases in AI, ML (machine learning), and other advanced analytics. AI has been singled out as a quantum killer app for quite some time, but quantum so far has minimal presence in the commercial data analytics arena.We need to ask ourselves whether all the recent industry activity is setting ourselves up for the dreaded “quantum winter,” analogous to the long AI winter period of backlash against the early hype for that technology. It’s one thing to talk about the mind-blowing potential of quantum analytics that executes across parallel universes as if it were the holy grail. It’s quite another to point to a mature technology with clear killer apps that make a huge difference in our lives today.Nevertheless, there is a growing sense among researchers and even among analytics professionals that ML could become the core use case for quantum in our lives.This is not a recent revelation. An MIT professor declared in 2013 that the “first quantum application” is ML. Specifically, the professor, Seth Lloyd, proposed a “q-app” that “encodes Google-like queries with q-bits that enable quantum computers to not only perform real-time searches through even the most gigantic databases, but which also ensures their absolute privacy, since attempts to eavesdrop on the query by the search engine provider would disturb the delicate q-bit’s superposition of states.”More significantly, recent product launches and other announcements by Amazon Web Services, Microsoft, IBM, and Honeywell in the quantum computing space address AI and ML use cases to varying degrees. None of these announcements pertains to a generally available quantum product or service that’s solving practical business problems. However, most of these announcements include hooks for programmers to develop such solutions on quantum hardware platforms or cloud services.In November 2019, Microsoft announced Azure Quantum. This quantum-computing cloud service is currently in private preview and expected to become generally available later this year. It comes with a Microsoft open source Quantum Development Kit for the Microsoft-developed quantum-oriented Q# language as well as Python, C#, and other languages. The kit includes libraries for development of quantum apps in ML, cryptography, optimization, and other domains.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesA month later, AWS announced the Amazon Braket service. Still in preview, this is a fully managed AWS service that enables scientists, researchers, and developers to begin experimenting with computers from quantum hardware providers (including D-Wave, IonQ, and Rigetti) in a single place. It provides a single development environment to build quantum algorithms—including ML—and test them on simulated quantum computers. Developers can run ML and other quantum programs on a range of different hardware architectures. And it allows users to design quantum algorithms using the Amazon Braket developer toolkit and use familiar tools, such as Jupyter notebooks.Then in January, IBM announced the expansion of its Q Network, in which more than 200,000 users are running hundreds of billions of executions on IBM’s quantum systems and simulators through the IBM Cloud. Participants in the network have access to IBM’s quantum expertise and resources, open source Qiskit software and developer tools, as well as cloud-based access to the IBM Quantum Computation Center. Many of the workloads being run include ML, as well as real-time simulations of quantum computing architectures.Less than two weeks ago, Honeywell announced that its high-capacity quantum computer will be generally available within three months. It also announced that Honeywell Ventures is making investments in Cambridge Quantum Computing and Zapata Computing. Both companies have expertise in ML and other cross-vertical algorithms and software for quantum computing applications.The most important announcement, coming just a few days ago, was Google’s launch of TensorFlow Quantum. This new software-only stack extends the widely adopted TensorFlow open-source ML library and modeling framework to support building and training of ML models to be processed on quantum computing platforms.Developed by Google’s X R&D unit, TensorFlow Quantum enables data scientists to use Python code to develop quantum ML models through standard Keras functions. It provides a library of quantum circuit simulators and quantum computing primitives that are compatible with existing TensorFlow APIs.TensorFlow Quantum’s release is no big surprise, coming several months after Google declared  “quantum supremacy,” which refers to its achievement of a quantum computing feat that would have been impossible on traditional computing architecture.In addition to providing a full AI/ML software stack into which quantum processing can now be hybridized, Google intends to expand the range of more traditional chip architectures on which TensorFlow Quantum can simulate quantum ML. It has announced plans to expand the range of custom, quantum-simulation hardware platforms supported by the tool to include graphics processing units from various vendors as well as its own Tensor Processing Unit AI-accelerator hardware platforms.Recognizing that quantum computing is not yet mature enough to process the full range of ML workloads with sufficient accuracy, Google has wisely designed its new open source tool to support the many AI use cases with one foot in traditional computing architectures. TensorFlow Quantum enables developers to rapidly prototype ML models that hybridize the execution of quantum and classic processors in parallel on learning tasks. Using the tool, developers can build both classical and quantum datasets, with the classical data natively processed by TensorFlow, and the quantum extensions processing quantum data, which consists of both quantum circuits and quantum operators.Also on InfoWorld: 5 reasons to choose PyTorch for deep learningDevelopers can use TensorFlow Quantum for supervised learning on such ML use cases as quantum classification, quantum control, and quantum approximate optimization. They can also execute advanced quantum learning tasks such as meta-learning, Hamiltonian learning, and sampling thermal states.In addition, Google has designed TensorFlow Quantum to support the growing range of AI use cases, such as “deepfakes” that do video, voice, and image generation with a high degree of verisimilitude. Google ML developers can use TensorFlow Quantum to train hybrid quantum/classical models to handle both the discriminative and generative workloads at the heart of the generative adversarial networks used in such applications.Strategically, Google’s likely next move will be to combine TensorFlow Quantum with its pre-existing Quantum Computing Playground into a full-featured, managed quantum ML service. Considering the fact that Google’s top public-cloud rivals (Microsoft, AWS, and IBM) all have such services either on the market or in preview, it would be shocking if the Mountain View, Calif.-based company doesn’t try to one-up them with a quantum ML service of its own.Building “write once run anywhere” quantum ML will be tricky for quite some time, even in TensorFlow Quantum. This is because quantum researchers are experimenting with a wide range of alternative architectures even while they try to build practical applications.Here are some of the principal ways the leading quantum ML vendors are supporting these more fundamental quantum R&D requirements:Google designed TensorFlow Quantum to support advanced research in alternative quantum computing architectures and algorithms for processing ML models. This makes the new offering an invaluable research tool for computer scientists who are experimenting with different quantum and hybrid processing architectures optimized for ML workloads. To this end, TensorFlow Quantum incorporates Cirq, an open source Python library for programming quantum computes. It supports programmatic creation, editing, and invoking of the quantum gates that constitute the Noisy Intermediate-Scale Quantum (NISQ) circuits characteristic of today’s quantum systems. It also enables developer-specified quantum computations to be executed in simulations or on real hardware.Microsoft’s Azure Quantum includes libraries for simulation of alternative quantum circuit processing scenarios and prediction of likely program performance in these environments.Amazon Braket allows users to explore, evaluate, and experiment with quantum computing hardware, design quantum algorithms, and simulate performance of their programs’ execution either on low-level quantum circuits or fully managed hybrid algorithms.IBM Q Network supports real-time simulations of alternative quantum computing architectures.Quantum computing has been in wait-and-see mode for so long that we tend to overlook the fact that it’s being rapidly put to practical uses.Keep up with advances in machine learning, AI, and big data analytics with InfoWorld’s Machine Learning and Analytics Report newsletterEven as quantum computing platform vendors experiment with new materials, methodologies, and architectures, researchers around the world have been demonstrating that quantum processing of ML models is in fact feasible. We can expect that AI/ML researchers at Google and elsewhere will probably use TensorFlow Quantum to do some fairly amazing things that were never feasible on traditional AI-accelerator hardware platforms.It’s clear from all these recent industry announcements that we’ll not only see commercialized quantum ML in our lifetime but that it has already begun to emerge and will gain steady adoption in this decade.", "pub_date": "2020-03-13"},
{"title": "What is Apache Spark? The big data platform that crushed Hadoop", "overview": "Fast, flexible, and developer-friendly, Apache Spark is the leading platform for large-scale SQL, batch processing, stream processing, and machine learning", "image_url": null, "url": "https://www.infoworld.com/article/3236869/what-is-apache-spark-the-big-data-platform-that-crushed-hadoop.html", "body": "Apache Spark definedApache Spark is a data processing framework that can quickly perform processing tasks on very large data sets, and can also distribute data processing tasks across multiple computers, either on its own or in tandem with other distributed computing tools. These two qualities are key to the worlds of big data and machine learning, which require the marshalling of massive computing power to crunch through large data stores. Spark also takes some of the programming burdens of these tasks off the shoulders of developers with an easy-to-use API that abstracts away much of the grunt work of distributed computing and big data processing.From its humble beginnings in the AMPLab at U.C. Berkeley in 2009, Apache Spark has become one of the key big data distributed processing frameworks in the world. Spark can be deployed in a variety of ways, provides native bindings for the Java, Scala, Python, and R programming languages, and supports SQL, streaming data, machine learning, and graph processing. You’ll find it used by banks, telecommunications companies, games companies, governments, and all of the major tech giants such as Apple, Facebook, IBM, and Microsoft.InfoWorld’s 2020 Technology of the Year Award winners: The best software development, cloud computing, data analytics, and machine learning products of the yearApache Spark architectureAt a fundamental level, an Apache Spark application consists of two main components: a  which converts the user's code into multiple tasks that can be distributed across worker nodes, and  which run on those nodes and execute the tasks assigned to them. Some form of cluster manager is necessary to mediate between the two.Out of the box, Spark can run in a standalone cluster mode that simply requires the Apache Spark framework and a JVM on each machine in your cluster. However, it’s more likely you’ll want to take advantage of a more robust resource or cluster management system to take care of allocating workers on demand for you. In the enterprise, this will normally mean running on Hadoop YARN (this is how the Cloudera and Hortonworks distributions run Spark jobs), but Apache Spark can also run on Apache Mesos, Kubernetes, and Docker Swarm.If you seek a managed solution, then Apache Spark can be found as part of Amazon EMR, Google Cloud Dataproc, and Microsoft Azure HDInsight. Databricks, the company that employs the founders of Apache Spark, also offers the Databricks Unified Analytics Platform, which is a comprehensive managed service that offers Apache Spark clusters, streaming support, integrated web-based notebook development, and optimized cloud I/O performance over a standard Apache Spark distribution.Apache Spark builds the user’s data processing commands into a , or DAG. The DAG is Apache Spark’s scheduling layer; it determines what tasks are executed on what nodes and in what sequence.  Spark vs. Hadoop: Why use Apache Spark?It’s worth pointing out that Apache Spark vs. Apache Hadoop is a bit of a misnomer. You’ll find Spark included in most Hadoop distributions these days. But due to two big advantages, Spark has become the framework of choice when processing big data, overtaking the old MapReduce paradigm that brought Hadoop to prominence.The first advantage is speed. Spark’s in-memory data engine means that it can perform tasks up to one hundred times faster than MapReduce in certain situations, particularly when compared with multi-stage jobs that require the writing of state back out to disk between stages. In essence, MapReduce creates a two-stage execution graph consisting of data mapping and reducing, whereas Apache Spark’s DAG has multiple stages that can be distributed more efficiently. Even Apache Spark jobs where the data cannot be completely contained within memory tend to be around 10 times faster than their MapReduce counterpart.The second advantage is the developer-friendly Spark API. As important as Spark’s speedup is, one could argue that the friendliness of the Spark API is even more important.Spark CoreIn comparison to MapReduce and other Apache Hadoop components, the Apache Spark API is very friendly to developers, hiding much of the complexity of a distributed processing engine behind simple method calls. The canonical example of this is how almost 50 lines of MapReduce code to count words in a document can be reduced to just a few lines of Apache Spark (here shown in Scala):By providing bindings to popular languages for data analysis like Python and R, as well as the more enterprise-friendly Java and Scala, Apache Spark allows everybody from application developers to data scientists to harness its scalability and speed in an accessible manner.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesSpark RDDAt the heart of Apache Spark is the concept of the Resilient Distributed Dataset (RDD), a programming abstraction that represents an immutable collection of objects that can be split across a computing cluster. Operations on the RDDs can also be split across the cluster and executed in a parallel batch process, leading to fast and scalable parallel processing.RDDs can be created from simple text files, SQL databases, NoSQL stores (such as Cassandra and MongoDB), Amazon S3 buckets, and much more besides. Much of the Spark Core API is built on this RDD concept, enabling traditional map and reduce functionality, but also providing built-in support for joining data sets, filtering, sampling, and aggregation.Spark runs in a distributed fashion by combining a  core process that splits a Spark application into tasks and distributes them among many  processes that do the work. These executors can be scaled up and down as required for the application’s needs.Spark SQLOriginally known as Shark, Spark SQL has become more and more important to the Apache Spark project. It is likely the interface most commonly used by today’s developers when creating applications. Spark SQL is focused on the processing of structured data, using a dataframe approach borrowed from R and Python (in Pandas). But as the name suggests, Spark SQL also provides a SQL2003-compliant interface for querying data, bringing the power of Apache Spark to analysts as well as developers.Alongside standard SQL support, Spark SQL provides a standard interface for reading from and writing to other datastores including JSON, HDFS, Apache Hive, JDBC, Apache ORC, and Apache Parquet, all of which are supported out of the box. Other popular stores—Apache Cassandra, MongoDB, Apache HBase, and many others—can be used by pulling in separate connectors from the Spark Packages ecosystem.Selecting some columns from a dataframe is as simple as this line:Using the SQL interface, we register the dataframe as a temporary table, after which we can issue SQL queries against it:Behind the scenes, Apache Spark uses a query optimizer called Catalyst that examines data and queries in order to produce an efficient query plan for data locality and computation that will perform the required calculations across the cluster. In the Apache Spark 2.x era, the Spark SQL interface of dataframes and datasets (essentially a typed dataframe that can be checked at compile time for correctness and take advantage of further memory and compute optimizations at run time) is the recommended approach for development. The RDD interface is still available, but recommended only if your needs cannot be addressed within the Spark SQL paradigm.Spark 2.4 introduced a set of built-in higher-order functions for manipulating arrays and other higher-order data types directly.Spark MLlibApache Spark also bundles libraries for applying machine learning and graph analysis techniques to data at scale. Spark MLlib includes a framework for creating machine learning pipelines, allowing for easy implementation of feature extraction, selections, and transformations on any structured dataset. MLlib comes with distributed implementations of clustering and classification algorithms such as k-means clustering and random forests that can be swapped in and out of custom pipelines with ease. Models can be trained by data scientists in Apache Spark using R or Python, saved using MLlib, and then imported into a Java-based or Scala-based pipeline for production use.Note that while Spark MLlib covers basic machine learning including classification, regression, clustering, and filtering, it does not include facilities for modeling and training deep neural networks (for details see InfoWorld’s Spark MLlib review). However, Deep Learning Pipelines are in the works.Also on InfoWorld: Artificial intelligence predictions for 2020Spark GraphXSpark GraphX comes with a selection of distributed algorithms for processing graph structures including an implementation of Google’s PageRank. These algorithms use Spark Core’s RDD approach to modeling data; the GraphFrames package allows you to do graph operations on dataframes, including taking advantage of the Catalyst optimizer for graph queries.Spark StreamingSpark Streaming was an early addition to Apache Spark that helped it gain traction in environments that required real-time or near real-time processing. Previously, batch and stream processing in the world of Apache Hadoop were separate things. You would write MapReduce code for your batch processing needs and use something like Apache Storm for your real-time streaming requirements. This obviously leads to disparate codebases that need to be kept in sync for the application domain despite being based on completely different frameworks, requiring different resources, and involving different operational concerns for running them.Spark Streaming extended the Apache Spark concept of batch processing into streaming by breaking the stream down into a continuous series of microbatches, which could then be manipulated using the Apache Spark API. In this way, code in batch and streaming operations can share (mostly) the same code, running on the same framework, thus reducing both developer and operator overhead. Everybody wins.A criticism of the Spark Streaming approach is that microbatching, in scenarios where a low-latency response to incoming data is required, may not be able to match the performance of other streaming-capable frameworks like Apache Storm, Apache Flink, and Apache Apex, all of which use a pure streaming method rather than microbatches.Structured StreamingStructured Streaming (added in Spark 2.x) is to Spark Streaming what Spark SQL was to the Spark Core APIs: A higher-level API and easier abstraction for writing applications. In the case of Structure Streaming, the higher-level API essentially allows developers to create infinite streaming dataframes and datasets. It also solves some very real pain points that users have struggled with in the earlier framework, especially concerning dealing with event-time aggregations and late delivery of messages. All queries on structured streams go through the Catalyst query optimizer, and can even be run in an interactive manner, allowing users to perform SQL queries against live streaming data.Structured Streaming originally relied on Spark Streaming’s microbatching scheme of handling streaming data. But in Spark 2.3, the Apache Spark team added a low-latency Continuous Processing Mode to Structured Streaming, allowing it to handle responses with latencies as low as 1ms, which is very impressive. As of Spark 2.4, Continuous Processing is still considered experimental. While Structured Streaming is built on top of the Spark SQL engine, Continuous Streaming supports only a restricted set of queries.Structured Streaming is the future of streaming applications with the platform, so if you’re building a new streaming application, you should use Structured Streaming. The legacy Spark Streaming APIs will continue to be supported, but the project recommends porting over to Structured Streaming, as the new method makes writing and maintaining streaming code a lot more bearable.Deep Learning PipelinesApache Spark supports deep learning via Deep Learning Pipelines. Using the existing pipeline structure of MLlib, you can call into lower-level deep learning libraries and construct classifiers in just a few lines of code, as well as apply custom TensorFlow graphs or Keras models to incoming data. These graphs and models can even be registered as custom Spark SQL UDFs (user-defined functions) so that the deep learning models can be applied to data as part of SQL statements.Apache Spark tutorialsReady to dive in and learn Apache Spark? We highly recommend Evan Heitman’s A Neanderthal’s Guide to Apache Spark in Python, which not only lays out the basics of how Apache Spark works in relatively simple terms, but also guides you through the process of writing a simple Python application that makes use of the framework. The article is written from a data scientist’s perspective, which makes sense as data science is a world in which big data and machine learning are increasingly critical.Keep up with advances in machine learning, AI, and big data analytics with InfoWorld’s Machine Learning and Analytics Report newsletterIf you’re looking for some Apache Spark examples to give you a sense of what the platform can do and how it does it, check out Spark By {Examples}. There is plenty of sample code here for a number of the basic tasks that make up the building blocks of Spark programming, so you can see the components that make up the larger tasks that Apache Spark is made for.Need to go deeper? DZone has what it modestly refers to as The Complete Apache Spark Collection, which consists of a slew of helpful tutorials on many Apache Spark topics. Happy learning!", "pub_date": "2020-03-16"},
{"title": "Kaggle calls data scientists to action on COVID-19", "overview": "In newest challenge, Kaggle asks AI researchers to apply machine learning tools and techniques to answering questions about COVID-19", "image_url": null, "url": "https://www.infoworld.com/article/3533269/kaggle-calls-data-scientists-to-action-on-covid-19.html", "body": "Kaggle, an online community for data scientists and a platform for data science competitions, has unveiled a new and timely bounty-paying challenge: the COVID-19 Open Research Dataset Challenge, or CORD-19.CORD-19 asks AI and machine learning researchers to develop text and data mining tools to analyze a dataset comprising tens of thousands of articles on virology and infectious disease. The goal is to help provide answers for 10 tasks, or lines of inquiry about the disease.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesThe prize for each of the tasks in the CORD-19 challenge is $1,000, delivered as cash or as a charitable donation to reseach and relief efforts.The world isn’t lacking for research about COVID-19. Kaggle’s dataset contains “over 29,000 scholarly articles, including over 13,000 with full text, about COVID-19, SARS-CoV-2, and releated coronaviruses,” according to the challenge introduction. But there’s little time to paw manually through those haystacks of research for the needles we need, so Kaggle is encouraging the use of machine learning techniques like natural language processing to get relevant data into the right hands more quickly.The CORD-19 tasks revolve around common questions about COVID-19. Each of the high-level tasks (e.g., What do we know about COVID-19 risk factors?) includes a number of subtasks (e.g., What populations are more susceptible? What roles do smoking or pre-existing pulmonary diseases play?).Other COVID-19-related datasets are also available on Kaggle. These include a full RNA sequencing of the virus and details about previous infectious disease outbreaks like Ebola and SARS.Keep up with advances in machine learning, AI, and big data analytics with InfoWorld’s Machine Learning and Analytics Report newsletterPrevious Kaggle challenges related to medical science featured projects with longer and less urgent time frames, such as devising better ways to screen for cervical cancer. Because the COVID-19 outbreak requires answers immediately, the Kaggle community is facing its first major test in real time.", "pub_date": "2020-03-18"},
{"title": "Explaining machine learning models to the business", "overview": "How to create summaries of machine learning system decisions that business decision makers can understand", "image_url": null, "url": "https://www.infoworld.com/article/3533369/explaining-machine-learning-models-to-the-business.html", "body": "Explainable machine learning is a sub-discipline of artificial intelligence (AI) and machine learning that attempts to summarize how machine learning systems make decisions. Summarizing how machine learning systems make decisions can be helpful for a lot of reasons, like finding data-driven insights, uncovering problems in machine learning systems, facilitating regulatory compliance, and enabling users to appeal — or operators to override — inevitable wrong decisions.Also on InfoWorld: Artificial intelligence predictions for 2020Of course all that sounds great, but explainable machine learning is not yet a perfect science. The reality is there are two major issues with explainable machine learning to keep in mind:Some “black-box” machine learning systems are probably just too complex to be accurately summarized.Even for machine learning systems that are designed to be interpretable, sometimes the way summary information is presented is still too complicated for business people. (Figure 1 provides an example of machine learning explanations for data scientists.)Figure 1: Explanations created by H2O Driverless AI. These explanations are probably better suited for data scientists than for business users.For issue 1, I’m going to assume that you want to use one of the many kinds of “glass-box” accurate and interpretable machine learning models available today, like monotonic gradient boosting machines in the open source frameworks h2o-3, LightGBM, and XGBoost. This article focuses on issue 2 and helping you communicate explainable machine learning results clearly to business decision-makers.This article is divided into two main parts. The first part addresses explainable machine learning summaries for a machine learning system and an entire dataset (i.e. “global” explanations). The second part of the article discusses summaries for machine learning system decisions about specific people in a dataset (i.e. “local” explanations). Also, I’ll be using a straightforward example problem about predicting credit card payments to present concrete examples.General summariesTwo good ways, among many other options, to summarize a machine learning system for a group of customers, represented by an entire dataset, are variable importance charts and surrogate decision trees. Now, because I want business people to care about and understand my results, I’m going to call those two things a “main drivers chart” and a “decision flowchart,” respectively.The main drivers chart provides a visual summary and ranking of which factors are most important to a machine learning system’s decisions, in general. It’s a high-level summary and decent place to start communicating about how a machine learning system works.In the example problem, I’m trying to predict missed credit card payments in September, given payment statuses, payment amounts, and bill amounts from the previous six months. What Figure 2 tells me is that, for the machine learning system I’ve constructed, the previous month’s repayment status is by far the most important factor for most customers in my dataset. I can also see that July and June repayment statuses are the next most important factors.Figure 2: Main drivers of model decisions about missing credit card payments in September for an entire dataset of credit card customers.How did I make this chart? It’s just a slightly modified version of a traditional variable importance chart. To make sure the displayed information is as accurate as possible, I chose an interpretable model and matched it with a reliable variable importance calculation.When I know my results are mathematically solid, then I think about the presentation. In this case, first I removed all numbers from this chart. While numeric variable importance values might be meaningful to data scientists, most business people don’t have time to care about numbers that aren’t related to their business. I also replaced raw variable names with directly meaningful data labels, because no business person actually wants to think about my database schema.Once I’ve summarized my system in an understandable chart, I can go to my business partners and ask really important questions like: Is my system putting too much emphasis on August repayment status? Or, does it make sense to weigh April payment amount more than August payment amount? In my experience, accounting for those kinds of domain knowledge insights in my machine learning systems leads to the best technical and business outcomes.The decision flowchart shows how predictive factors work together to drive decisions in my machine learning system and Figure 3 boils my entire machine learning system down to one flowchart!Figure 3: A flowchart showing roughly how a complex model makes decisions about missing credit card payments in September for an entire dataset of credit card customers.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesHow did I summarize an entire machine learning system into a flowchart? I used an old data mining trick known as a surrogate model. Surrogate models are simple models of complex models. In this case my simple model is a decision tree, or a data-derived flowchart, and my complex model is the input factors and decisions of my machine learning system. So, the decision flow chart is simple machine learning on more complex machine learning.Unfortunately, this trick is not guaranteed to work every time. Sometimes machine learning systems are just too sophisticated to be accurately represented by a simple model. So a key consideration for data scientists when creating a chart like Figure 3 is: How accurate and stable is my decision tree surrogate model? On the business side, if a data scientist shows you a chart like Figure 3, you should challenge them to prove that it is an accurate and stable representation of the machine learning system.If you want to make a decision flow chart, remember to try to limit the complexity of the underlying machine learning system, keep your flow chart to a depth of say three to five decisions (Figure 3 uses a depth of three), and use human-readable data formats and not your favorite label encoder.Specific summariesIf you work in financial services, you probably get that sometimes each individual machine learning system decision for each customer has to be explained or summarized. For the rest of the data science world, explaining individual machine learning system decisions might not be a regulatory requirement, but I would argue it’s a best practice. And regulations are likely on the way. Why not get ready?No one wants to be told “computer says no,” especially when the computer is wrong. So, consumer-level explanations are important for data scientists, who might want to override or debug bad machine learning behavior, and for consumers, who deserve to be able to appeal wrong decisions that affect them negatively.I’m going to focus on two types of explanations that summarize machine learning systems decisions for specific people, Shapley values (like in Figure 2) and counterfactual explanations. Since data science jargon isn’t helpful in this context, I’m going to call these two approaches main decision drivers (again) and “counter-examples.” Also, keep in mind there are many other options for creating consumer-specific explanations.   Shapley values can be used to summarize a machine learning system for an entire dataset (Figure 2) or at the individual decision level (Figure 4). When you use the right underlying machine learning and Shapley algorithm, these individual summaries can be highly accurate.Where I think most data scientists go wrong with Shapley values is in explaining them to business partners. My advice is don’t ever use equations, and probably don’t use charts or tables either. Just write out the elegant Shapley value interpretation in plain English (or whatever language you prefer). To see this approach in action, check out Figure 4. It displays the three most important drivers for a decision about a customer that my machine learning system has decided is at an above average risk for missing their September payment.Counter-examples explain what a customer could do differently to receive a different outcome from a machine learning system. You can sometimes use software libraries to create counter-examples or you can use trial and error, changing the inputs to a machine learning system, and observing the change in the system outputs, to create your own counter-examples. It turns out that for the high-risk customer portrayed in Figure 5, if they had made their recent payments on time, instead of being late, my machine learning system would put them at a much lower risk for missing their upcoming September payment.Once you can see the logic and data points behind how a machine learning system makes a given decision, it becomes much easier for data scientists to catch and fix bad data or wrong decisions. It also becomes much easier for customers interacting with machine learning systems to catch and appeal the same kinds of wrong data or decisions.These kinds of explanations are also potentially useful for compliance with regulations like the Equal Credit Opportunity Act (ECOA) and the Fair Credit Reporting Act (FCRA) in the United States, and the General Data Protection Regulation (GDPR) in the European Union. Responsible machine learningThe myriad risks of depending on a system you don’t understand are major barriers to the adoption of AI and machine learning in the business world. When you can break down those barriers, that’s a big step forward. Hopefully you’ll find the techniques I present here useful for just that, but do be careful. Aside from the accuracy and communication concerns I’ve already brought up, there are some security and privacy concerns with explainable ML too. Keep up with advances in machine learning, AI, and big data analytics with InfoWorld’s Machine Learning and Analytics Report newsletterAlso, explainability is just one part of mitigating machine learning risks. A machine learning system could be totally transparent and still discriminate against certain groups of people, or could be both transparent and wildly unstable when deployed to make decisions using real-world data. For these reasons and more, it’s always a good idea to consider privacy, security, and discrimination risks, and not just GPUs and Python code, when white-boarding a machine learning system.All of this leads me into the responsible practice of machine learning, but that’s food for thought for my next article. Suffice to say, communicating machine learning system outcomes is incumbent on all data scientists in today’s data-driven world, and explaining AI and machine learning to a business decision maker is becoming more of a possibility with the right approach and right technology. —1. Other great options for interpretable models include elastic net regression, explainable neural networks (XNN), GA2M, and certifiably optimal rule lists (CORELS).2. I recommend monotonic gradient boosting machines plus TreeSHAP to generate accurate summaries.3. Decision tree surrogates go back to at least 1996, but alternative methods have been put forward in recent years as well.4. Governments of at least Canada, Germany, the Netherlands, Singapore, the United Kingdom, and the United States have proposed or enacted ML-specific regulatory guidance.  5. Like cleverhans or foolbox.6. Risks of model extraction and inversion attacks and membership inference attacks are all generally exacerbated by presenting explanations along with predictions to consumers of ML-based decisions.7. For a more thorough technical discussion of responsible machine learning see: “A Responsible Machine Learning Workflow.”", "pub_date": "2020-03-19"},
{"title": "O’Reilly pulls the plug on in-person events", "overview": "The company believes the events business is ‘forever changed’ due to the coronavirus pandemic, so is moving its conferences online", "image_url": null, "url": "https://www.infoworld.com/article/3534429/oreilly-pulls-the-plug-on-in-person-events.html", "body": "In the wake of the COVID-19 virus pandemic, prominent technology conference producer O’Reilly has shut down its events business, permanently. From now on, O’Reilly events will be held online.The producer of events such as OSCON (O’Reilly Open Source Software Conference) and the Strata Data & AI conference, O’Reilly noted in a March 24 bulletin the impact of the virus on its in-person events division. In response, the company recently switched its Strata conference, which was to be held in San Jose last week, to an online format, drawing more than 4,600 remote attendees.Also on InfoWorld: Artificial intelligence predictions for 2020“Without understanding when this global health emergency may come to an end, we can’t plan for or execute on a business that will be forever changed as a result of this crisis,” said Laurie Baldwin, O’Reilly president. “With large technology vendors moving their events completely on-line, we believe the stage is set for a new normal moving forward when it comes to in-person events.”Baldwin noted that large technology vendors have moved events online as well. Microsoft, for one, is moving its Microsoft Build 2020 developer conference, originally planned for Seattle in May, to be all-digital.Keep up with the latest developments in software development, cloud computing, data analytics, and machine learning with the InfoWorld Daily newsletterO’Reilly employees who had been involved in the in-person events business have been let go. In addition to the events business, O’Reilly has a technology publishing business and provides interactive coding events and custom training.", "pub_date": "2020-03-25"},
{"title": "TensorFlow deepens its advantages in the AI modeling wars", "overview": "Despite complaints about its complexity, TensorFlow supports every AI development, training, and deployment scenario you can imagine", "image_url": null, "url": "https://www.infoworld.com/article/3534474/tensorflow-deepens-its-advantages-in-the-ai-modeling-wars.html", "body": "TensorFlow remains the dominant AI modeling framework. Most AI (artificial intelligence) developers continue to use it as their primary open source tool or alongside PyTorch, in which they develop most of their ML (machine learning), deep learning, and NLP (natural language processing) models.In the most recent O’Reilly survey on AI adoption in the enterprise, more than half of the responding data scientists cited TensorFlow as their primary tool. This finding is making me rethink my speculation, published just last month, that TensorFlow’s dominance among working data scientists may be waning. Neverthless, PyTorch remains a strong second choice, having expanded its usage in the O’Reilly study to more than 36 percent of respondents, up from 29 percent in the previous year’s survey.Also on InfoWorld: PyTorch vs. TensorFlow: How to chooseBoosting the TensorFlow stack’s differentiation vis-à-vis PyTorchAs the decade proceeds, the differences between these frameworks will diminish as data scientists and other users value feature parity over strong functional differentiation. Nevertheless, TensorFlow remains by far the top AI modeling framework, not just in adoption and maturity, but in terms of the sheer depth and breadth of the stack in supporting every conceivable AI development, training, and deployment scenario. PyTorch, though strong for 80 percent of the core AI, deep learning, and machine learning challenges, has a long way to go before it arrives at feature parity.Last week, Google’s TensorFlow team distanced its stack further from PyTorch. It held its yearly TensorFlow Dev Summit via livestream (for reasons everybody knows). In spite of the absence of in-person buzz, plenty of important news and analysis came from this purely online event.What follows is my discussion of the announcements organized into three broad categories: enhancements to the core TensorFlow and its ecosystem, new TensorFlow devops tools, and new TensorFlow add-ons for specialized AI development challenges.Enhancements to core TensorFlow and its ecosystemIn terms of enhancements to the core open source code, the team announced TensorFlow 2.2. This latest release—which, unlike prior TensorFlow versions, is now compatible with the entire TensorFlow ecosystem—focuses on the performance and stability of the core library. It also adds eager execution at the core, support of NumPy arrays, tf.data, and TensorFlow datasets.Ensuring full support for cross-device compilation of models built in TensorFlow, the team announced TensorFlow Runtime, which will ship with the core open source code and will ensure that ML models work on different devices and platforms that implement the Multi-Level Intermediate Representation standard, which is supported by 95 percent of hardware manufacturers.Making sure that models developed in TensorFlow can run on Google’s own neural network processor, the team announced that the previous version, TensorFlow 2.1, also supports Cloud Tensor processing units.New TensorFlow devops toolsMany enterprise AI developers use TensorFlow to build, train, and deploy deep learning, ML, and NLP models within complex devops workflows. To address these requirements, the team announced new capabilities to accelerate team productivity across the TensorFlow ecosystem: The team revealed new features for TensorFlow Hub, which contains more than 1,000 ready-to-use models and code snippets.: The team announced Keras Tuner, which simplifies the task of tuning model hyperparameters and works with Keras, TensorFlow, and scikit-learn.: Google Cloud AI Pipelines makes it simpler and easier to manage pipelines using TensorFlow Extended.: The team announced dev, a new tool that lets developers and researchers host and share their model-based experiments for free. It provides information and analyses on model performance, on-device workloads, and input pipelines.: The team unveiled Neural Structured Learning, a tool for training neural networks by using structured signals in addition to feature inputs. Structured signals are often used to represent relations or similarity among samples that may be labeled or unlabeled. Leveraging these signals during neural network training harnesses both labeled and unlabeled data and can improve model accuracy, particularly when the amount of labeled data is relatively small.New TensorFlow add-ons for specialized AI development challengesTensorFlow users are working in an astonishing range of sophisticated projects that push AI’s boundaries from every direction. To support these requirements, the team announced the following new tools, extensions, and add-ons that plug into TensorFlow’s ecosystem.: Computer vision is one of the hottest niche markets for AI-driven solutions. To support these developers, the team announced TensorFlow Graphics, which provides graphics functions as a set of differentiable graphics layers to build more efficient computer-vision network architectures. It includes spatial transformers, graphics renderers, and mesh convolutions. It also includes a 3D viewer that works in TensorBoard. It enables explicit modeling of geometric priors and constraints into neural networks for the purpose of building architectures that can be trained robustly, efficiently, and in a self-supervised fashion.: Natural language processing is at the heart of many practical AI applications. The TensorFlow team announced T5 Text-to-Text Transfer Transformer, a framework that applies transfer learning to natural language processing. It also released Meena, a neural conversational model that learns to respond sensibly to a given conversational context.: More AI models are being built in TensorFlow for deployment into browsers and various client- and server-side Web application frameworks. To accelerate developer time-to-value on such projects, the team announced new domain NPM packages for browser-based execution in TensorFlow.js. These include HandPose (detects landmarks on hands using pose estimation), Face Mesh (predict 3D facial landmarks on a human face), and Mobile BERT (Bidirectional Encoder Representations from Transformers) (natural language processing on mobile devices). The team released a new command-line converter wizard to make the conversion of models to TensorFlow.js easier. It launched a new WebAssembly back end for TensorFlow.js in support of fast execution on CPUs, as well as a new Hugging Face NPM package that allows developers to perform question and answering in NodeJS.: More AI projects involve distributing computations in parallel across supercomputers, meshes, and other complex computing fabrics. The team announced Mesh TensorFlow, which provides a language for model parallelism and distributed deep learning. It specifies a broad class of distributed tensor computations for various scenarios involving n-dimensional array of processors connected by a network. These scenarios might include deployments in which model parameters are too many to fit on one device, a 3D image model or other data example is so large that its activations do not fit on one device, or in which lower-latency parallel inference is required.: Probabilistic programming tools enable easier development of AI applications that involve complex statistical reasoning. The team announced TensorFlow Probability, a library that makes it easy to program with Python models that combine statistical reasoning with deep learning and can deploy efficiently on GPUs (graphics processing units) and TPUs (tensor processing units). The library enables data scientists and other developers to encode domain knowledge for understanding data and making predictions. It includes a wide selection of probabilistic models, layers, distributions, and optimizers. RL is the core of many edge, robotics, autonomous vehicle, and other new frontiers in AI. The TensorFlow team announced TF-Agents, a library for designing, implementing, testing, and benchmarking new RL algorithms in TensorFlow.: Fairness, ethics, and bias reduction are becoming more prevalent as objectives that AI development teams must build and train their models to achieve. The team announced a tool and library that will allow developers to visualize the fairness of their models. The Fairness Indicators library enables the computation of common fairness metrics for binary and multiclass classifications, which can be visualized on TensorBoard.: Quantum computing is a proving ground for AI applications of potentially mind-boggling sophistication. The team discussed TensorFlow Quantum, a library for hybrid quantum-classical machine learning. This new software-only stack extends TensorFlow to support building and training ML models to be processed on quantum computing platforms. Developed by Google’s X R&D unit, TensorFlow Quantum enables data scientists to use Python code to develop quantum ML models through standard Keras functions. It provides a library of quantum circuit simulators and quantum computing primitives that are compatible with existing TensorFlow APIs. To see where TensorFlow Quantum fits in this emerging niche, check out my recent InfoWorld article discussing the tool, which was announced just before TensorFlow DevWorld 2020.TensorFlow remains the AI modeling framework to beatThat’s an extraordinarily deep and broad set of announcements, reflecting the deep pockets and vast brainpower that Google and its parent Alphabet are investing in the TensorFlow ecosystem. Taken together, they may not stanch grumbling among AI developers about TensorFlow, which many deem confusing, hard to use, bloated, slow, inefficient, and excessively complex. In fact, the glut of new capabilities may exacerbate those issues to the point where they cause defections to simpler AI modeling frameworks.Nevertheless, TensorFlow is still very much the framework to beat from the perspective of professional data scientists who are building mission-critical AI projects for production environments. This week’s announcements have made its advantages for heavy-hitting AI development even more pronounced.", "pub_date": "2020-03-26"},
{"title": "Kubeflow 1.0 solves machine learning workflows with Kubernetes", "overview": "Google's machine learning toolkit for Kubernetes helps data scientists manage machine learning workflows and deploy and scale models in production ", "image_url": null, "url": "https://www.infoworld.com/article/3529977/kubeflow-10-solves-machine-learning-workflows-with-kubernetes.html", "body": "Kubeflow, Google’s solution for deploying machine learning stacks on Kubernetes, is now available as an official 1.0 release.Kubeflow was built to address two major issues with machine learning projects: the need for integrated, end-to-end workflows, and the need to make deploments of machine learning systems simple, manageable, and scalable. Kubeflow allows data scientists to build machine learning workflows on Kubernetes and to deploy, manage, and scale machine learning models in production without learning the intricacies of Kubernetes or its components.Also on InfoWorld: Artificial intelligence predictions for 2020Kubeflow is designed to manage every phase of a machine learning project: writing the code, building the containers, allocating the Kubernetes resources to run them, training the models, and serving predictions from those models. The Kubeflow 1.0 release provides tools, such as Jupyter notebooks for working with data experiments and a web-based dashboard UI for general oversight, to help with each phase.Google claims Kubeflow provides repeatability, isolation, scale, and resilience not just for model training and prediction serving, but also for development and research work. Jupyter notebooks running under Kubeflow can be resource-limited and process-limited, and can re-use configurations, access to secrets, and data sources.InfoWorld’s 2020 Technology of the Year Award winners: The best software development, cloud computing, data analytics, and machine learning products of the yearSeveral Kubeflow components are still under development and will be rolled out in the near future. Pipelines allow complex workflows to be created using Python. Metadata provides a way to track details about individual models, data sets, training jobs, and prediction runs. Katib gives Kubeflow users a mechanism to perform hyperparameter tuning, an automated way to improve the accuracy of predictions from models.", "pub_date": "2020-03-03"},
{"title": "3 cloud architecture problems that need solutions", "overview": "With cloud architecture becoming more art than science, some problems are stumping those who are supposed to have all the answers\r\n", "image_url": null, "url": "https://www.infoworld.com/article/3531630/3-cloud-architecture-problems-that-need-solutions.html", "body": "For the most part, cloud architecture is not that exciting. By now we know basically what works, what does not, and the process to get to the right target architecture. This means both the meta or logical architecture and added technology to get to the physical architecture.Although we know the best patterns for most of what cloud architecture requires, some problems are still being debated. No de facto solution or best practice has emerged yet. Here are my top three:Also on InfoWorld: What AI can really do for your business (and what it can’t) Edge computing has benefits, such as placing data processing closer to the source of the data. However, the question remains: How does one partition data and processes between a cloud-based server and an edge computer?Many push as much as they can to the edge, but realize that you’re moving away from a centralized system (the public cloud), to many decentralized systems (the edge devices or servers). You need to understand that you must maintain these edge systems, and they are much more difficult to monitor, govern, secure, update, and configure. Multiply that effort by hundreds of edge computing devices and you've got an operational nightmare. Many enterprises say containers are their strategy and not just an enabling technology. This almost religious belief in the power of containers has pushed many an application to the cloud in containers, but that’s really not how business should be moving there.The issue is that there are no hard and fast rules as to what can—and should—exist in a container. Legacy applications that will take a great deal of effort to refactor (rewrite) for containers are not likely candidates; however, in many instances, the cloud migration team attempts to move them first.This means that enterprises will fail to find value in containers for some of their applications that move to the cloud. It’s a million-dollar mistake that a good number of cloud architects will make. Machine learning is cheap in the clouds, as well as much easier to use than it was. This has led to many instances where enterprise IT AI-enabled an application when the use of cognitive systems as an application component was ultimately contraindicated.Much like the container trade-off outlined above, there is no definitive rule when and how to use machine learning within existing or net-new applications.Keep up with the latest developments in cloud computing with InfoWorld’s Cloud Computing Report newsletterA few things work against the use of machine learning, including the fact that you must refactor the application to take advantage of machine learning at all. However, the larger issue is if AI is even needed in the first place. Many never even ask that question.We’ll always have topics that are not easy to resolve. What’s most productive is that we’re talking about them. ", "pub_date": "2020-03-10"},
{"title": "Review: Nvidia’s Rapids brings Python analytics to the GPU", "overview": "An end-to-end data science ecosystem, open source Rapids gives you Python dataframes, graphs, and machine learning on Nvidia GPU hardware", "image_url": null, "url": "https://www.infoworld.com/article/3532009/review-nvidias-rapids-brings-python-analytics-to-the-gpu.html", "body": "Building machine learning models is a repetitive process. Often rote and routine, this is a game of “fastest through the cycle wins,” as the faster you can iterate, the easier it is to explore new theories and get good answers. This is one of the reasons practical enterprise use of AI today is dominated by the largest enterprises, which can throw enormous resources at the problem.Rapids is an umbrella for several open source projects, incubated by Nvidia, that puts the entire processing pipeline on the GPU, eliminating the I/O bound data transfers, while also substantially increasing the speed of each of the individual steps. It also provides a common format for the data, easing the burden of exchanging data between disparate systems. At the user level, Rapids mimics the Python API in order to ease the transition for that user base.Typical machine learning workflowRapids ecosystem architectureThe Rapids project aims to replicate, for the most part, the machine learning and data analytics APIs of Python, but for GPUs rather than CPUs. This means that Python developers already have everything they need to run on the GPU, without having to learn the low-level details of CUDA programming and parallel operations. Pythonistas can develop code on a non-GPU enabled machine, then, with a few tweaks, run it on all the GPUs available to them.Also on InfoWorld: The 6 best programming languages for AI developmentThe Nvidia CUDA toolkit provides lower level primitives for math libraries, parallel algorithms, and graph analytics. At the heart of the architecture is the GPU data frame, based on Apache Arrow, that provides a columnar, in-memory data structure that is programming language agnostic. The user interacts with the GPU dataframe via cuDF and a Pandas-like API. Dask, a Python library for parallel computing, mimics the upstream Python APIs and works with CUDA libraries for parallel computation. Think of Dask as Spark for Python.Rapids ecosystem architectureThe three main projects, cuDF, cuML and cuGraph, are developed independently, but designed to work seamlessly together. Bridges to the broader Python ecosystem are also being developed as part of the project.Rapids installationInstallation via Anaconda on a Linux machine in AWS was mostly straightforward, barring a few hiccups due to a change in dependencies in version 0.11. Installing the C/C++ libraries to use libcudf was not so easy, and I’d recommend sticking to the Python APIs and Conda installation process. Rapids includes a Jupyter notebook, also available on Google’s free Colab, that makes getting started simple. I used the Jupyter notebook version 0.10 to run the code on Google Colab, which includes an Nvidia Tesla T4 GPU.Rapids’ GPU dataframeAt the heart of any data science workflow is the dataframe. This is where feature engineering happens, and where the majority of time is spent, as data scientists wrangle dirty data. cuDF is the Rapids project for a GPU-based, Pandas-like dataframe. Underpinning cuDF is libcudf, a C++ library implementing low-level primitives for importing Apache Arrow data, performing element-wise mathematics on arrays, and executing sort, join, group by, reduction, and other operations on in-GPU memory matrices. The basic data structure of libcudf is the GPU DataFrame (GDF), which in turn is modeled on Apache Arrow’s columnar data store.cuDF technology stack The Rapids Python library presents the user with a higher level interface resembling dataframes, like those in Pandas. In many cases, Pandas code runs unchanged on cuDF. Where this is not the case, usually only minor changes are required.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesUser defined functions in cuDFOnce you’re past basic data manipulation, it is sometimes necessary to process rows and columns with user defined functions (UDFs). cuDF provides a PyData style API to write code to process more course-grained data structures like arrays, series, and moving windows. Currently only numeric and Boolean types are supported. UDFs are compiled using the Numba JIT compiler, which uses a subset of LLVM to compile numeric functions to CUDA machine code. This results in substantially faster run times on the GPU.Strings in cuDFAlthough GPUs are fantastic for rapidly processing float vectors, they haven’t typically been used for processing string data, and the reality is that most data comes to us in the form of strings. cuStrings is a GPU string manipulation library for splitting, applying regexes, concatenating, replacing tokens, etc. in arrays of strings. Like other functions of cuDF, it is implemented as a C/C++ library (libnvStrings) and wrapped by a Python layer designed to mimic Pandas. Although the string data type is not optimized for execution on GPUs, parallel execution of the code should provide a speedup over CPU-based string manipulation.Getting data in or out of cuDFDataframe I/O is handled by a dedicated library, cuIO. All of the most commonly encountered formats are supported, including Arrow, ORC, Parquet, HDF5, and CSV. If you are lucky enough to be running on DGX-2 hardware, you can use GPU Direct Storage integration to move data directly from high-speed storage to the GPU without involving the CPU. Mortal users will still appreciate the speedup the GPU gives when decompressing large data sets, and the tight integration with the Python ecosystem.GPU Direct Storage is currently in alpha, and when released will be available on most Tesla GPUs. You can create a GPU dataframe from NumPy arrays, Pandas DataFrames, and PyArrow tables with just a single line of code. Other projects can exchange data via the  for libraries that fall within the Numba ecosystem. DLPack for neural network libraries is also a supported interface.Probably the biggest drawback in using cuDF is the lack of interoperability outside of Python. I think a focus on a strong foundation of C/C++ APIs, as Arrow has done, would enable a broader ecosystem and benefit the project as a whole.Rapids’ cuMLcuML’s stated goals are to be “Python’s Scikit-learn powered by GPUs.” In theory this means you should only have to change your import statement and perhaps tune a few of the parameters to account for the differences in running on a CPU, where sometimes a brute force approach is better. The benefit of having a GPU-based Scikit-learn is hard to understate. The speedups are substantial, and data analysts can be many times more productive. The C++ API isn’t quite ready for broad consumption outside of its Python bindings, but this is expected to improve.cuML also includes APIs for helping with hyperparameter tuning via Dask, a library for scaling Python across multiple nodes. Many machine learning algorithms can be effectively made parallel, and cuML is actively developing both multi-GPU and multi-node, multi-GPU algorithms.cuML algorithmsAlso on InfoWorld: Artificial intelligence predictions for 2020Rapids’ cuGraphcuGraph is the third member of the Rapids ecosystem, and like the others, cuGraph is fully integrated with cuDF and cuML. It offers a good selection of graph algorithms, primitives, and utilities, all with GPU-accelerated performance. The selection of APIs in cuGraph is somewhat more extensive than in other parts of Rapids, with NetworkX, Pregel, GraphBLAS, and GQL (Graph Query Language) all available.cuGraph stackcuGraph is more like a toolkit in spirit than cuML. Graph technology is a fast-moving space both in academia and industry. Thus, by design, cuGraph gives developers access to the C++ layer and graph primitives, encouraging third parties to develop products using cuGraph. Several universities have contributed, and projects from Texas A&M (GraphBLAS), Georgia Tech (Hornet), and UC Davis (Gunrock) have been “productized” and included under the cuGraph umbrella. Each project provides a different set of capabilities, all GPU-accelerated, and all backed by the same cuDF dataframe.NetworkX is the Python API targeted by the Rapids team for its native interface. There are a number of algorithms available via that interface. While only page rank is multi-GPU, the team is actively working on multi-GPU versions of the others, where applicable.cuGraph algorithmsOne of the cuGraph sub-projects I found interesting is cugraphBLAS, an effort to standardize building blocks for graph algorithms in the language of linear algebra. Based on GraphBLAS (graphblas.org), a custom data structure designed for sparse dynamic graphs processing.Another cuGraph sub-project, Hornet provides a system independent format for containing graph data, analogous to the way Apache arrow provides a system independent way to process dataframes. Hornet supports most of the popular graph formats including SNAP, mtx, metis, and edges.In keeping with the spirit of being close to the Python community, Python’s native NetworkX package can be used for the study of complex networks. This includes data structures for graphs and multi-graphs, reimplemented using CUDA primitives, allowing you to reuse many of the standard graph algorithms and perform network structure and analysis measures. The majority of algorithms are single-GPU, like NetworkX. Nevertheless, running them on the GPU alone offers significant speedup, while work continues to move to multi-GPU implementations.Also on InfoWorld: What is CUDA? Parallel programming for GPUsOn the Rapids roadmap Given the tremendous speed up that GPU-based analytics provides, there are a few new projects coming into the mix in future versions.Multi-layer neural networks were one of the first workloads moved to GPUs, and a sizable body of code exists for this machine learning use case. Previously DLPack was the de-facto standard for data interchange among deep learning libraries. Nowadays the array_interface is commonly supported. Rapids supports both.Like most other projects at Rapids, cuSignal is a GPU-accelerated version of an existing Python library, in this case the SciPy Signal library. The original SciPy Signal library is based on NumPy, which is replaced with its GPU-accelerated equivalent, CuPy in cuSignal. This is a good example of the Rapids design philosophy at work. With the exception of a few custom CUDA kernels, the port to the GPU mostly involves replacing the import statement and tweaking a few function parameters. Bringing signal processing into the Rapids fold is a smart move. Signal processing is everywhere and has many immediately useful commercial applications in industry and defense.Spatial and spatiotemporal operations are great candidates for GPU acceleration, and they solve many real-world problems we face in everyday life, such as analyzing traffic patterns, soil health/quality, and flood risk. Much of the data collected by mobile devices, including drones, has a geospatial component, and spatial analysis is at the heart of the Smart City. Architected like the other components, cuSpatial is a C++ library built on CUDA primitives and the Thrust vector processing library, using cuDF for data interchange. Consumers of the C++ library can read point, polyline, and polygon data using a C++ reader. Python users are better off using existing Python packages like Shapely or Fiona to fill a NumPy array, then using the cuSpatial Python API or converting to cuDF dataframes. Visualizing data is fundamental, both within the analytics workflow and for presenting or reporting results. Yet for all the magic that GPUs can work on the data itself, getting that data out to a browser is not a trivial task. cuxfilter, inspired by the Crossfilter JavaScript library, aims to bridge that gap by providing a stack to enable third-party visualization libraries to display data in cuDF dataframes.There have been a few iterations of cuxfilter as the team sorts out the best architecture and connector patterns. The latest iteration leverages Jupyter notebooks, Bokeh server, and PyViz panels, while integration experiments include projects from Uber, Falcon, and PyDeck. This component isn’t quite yet ready for prime time, but is slated for release in Rapids 0.13. There are a lot of moving parts, and I did not get to experiment with it first hand, but if it lives up to its promise this will be a great addition to the Rapids toolkit.Also on InfoWorld: 5 reasons to choose PyTorch for deep learningScaling up and out with DaskDask is distributed task scheduler for Python, playing a similar role for Python that Apache Spark plays for Scala. Dask-cuDF is a library that provides partitioned, GPU-backed dataframes. Dask-cuDF works well when you plan to use cuML or when you are loading a data set that is larger than GPU memory or is spread across multiple files.Like a Spark RDD (Resilient Distributed Dataset), the Dask-cuDF distributed dataframe mostly behaves just like a local one, so you can experiment with your local machine and move to a distributed model when you need to scale up. Dask-cuML gives cuML multi-node capabilities, making it a good option when you do not have the budget for a DGX workstation.", "pub_date": "2020-03-11"},
{"title": "How AI helped Domino’s improve pizza delivery", "overview": "Sharing a container environment and Nvidia GPU server has enabled Domino’s data scientists to create more complex and accurate models to improve store and delivery operations", "image_url": null, "url": "https://www.infoworld.com/article/3535230/how-ai-helped-domino-s-improve-pizza-delivery.html", "body": "When the words artificial intelligence (AI) and machine learning (ML) are used, people often think of advanced industries such as space exploration and biomedicine that rely heavily on research and development. The fact is, AI and ML should be something all industries are looking at, including retail. We are now in the customer service era and small differences in service can make a big difference in market share.This past week Nvidia held a virtual version of its annual GPU Technology Conference (GTC), which has become a showcase for real life AI/ML use cases. Historically, the show has been a highly technical one, but over the years, it has evolved into an event where companies showcase how they use advanced technologies to transform their businesses.Also on InfoWorld: Artificial intelligence predictions for 2020Domino’s is an example of a familiar retail business that presented how it’s using AI and ML. The company has come up with a successful recipe to change the way it operates. The secret ingredient is Nvidia’s technology, which the leading pizza chain is using to improve store and online operations, provide a better customer experience, and route orders more efficiently.As a result, Domino’s is seeing happier customers and more tips for its drivers. But that’s only a small piece of the multifaceted pie. So, what does it take to get pizza from a Domino’s store to someone’s house? The answer is quite complex.The data science team at Domino’s tested the company’s speed and efficiency by leveraging Nvidia’s DGX-1 server, an integrated software and hardware system for deep learning research. For those not familiar with the DGX server line, Nvidia has created a series of turnkey appliances businesses can drop in and start using immediately. The alternative is to cobble together hardware, software, and AI platforms and tune the entire system correctly. This can take weeks to do.The Domino’s team created a delivery prediction model that forecasts when an order would be ready, using attributes of the order and what is happening in a Domino’s store, such as the number of employees, managers, and customers present at that moment. The model was based on a large dataset of five million orders, which isn’t massive but large enough to create accurate models. All future orders are fed back into the system to further increase model accuracy.Domino’s previous models used GPU-enabled laptops and desktops and would take more than 16 hours to train. The long timeframe made it extremely difficult to improve on the model, said Domino’s data science and AI manager Zachary Fragoso during a presentation at virtual GTC 2020.The extra compute power of the DGX-1 enabled Domino’s data scientists to train more complex models in less time. The system reduced the training time to under an hour and increased accuracy for order forecasts from 75 percent to 95 percent. The test demonstrated how Domino’s could boost productivity by training models faster, Fragoso said.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesDomino’s uncovered another benefit in the process: resource sharing. Each individual GPU on the DGX-1 is so large—with 32 GB of RAM—Domino’s data scientists could use a fraction of the GPUs and run multiple tests simultaneously. With eight such GPUs at their fingertips, the data scientists found themselves sharing resources and knowledge, as well as collaborating across teams.In the past, sharing work across teams—including code reviews and quality assurance testing—was challenging, since data scientists worked in their own local environments. Now that data scientists are working with a common DGX-1 server, they’re easily able to share Docker containers that are fully customizable and reproducible. This gives the data scientists a large resource pool to work with and access to resources when needed, so they’re not sitting idle. The Docker solution that Domino’s integrated with DGX-1 also makes it easier to reproduce code across different environments because all the data is contained within the Docker image.Domino’s recently purchased a second DGX-1 and started adding the Kubernetes container management system to the mix. With Kubernetes managed by an optimization engine, Domino’s can dynamically allocate resources to all its data scientists and launch containers faster. According to Fragoso, even data scientists who aren’t familiar with Linux can point-and-click to launch Docker containers.On the deployment side, Domino’s created an inferencing stack, which includes a Kubernetes cluster and four Nvidia GPUs. This way, data scientists can interact with and build their models using the same Docker container framework they use on the DGX-1.Domino’s also acquired a machine learning operations platform called Datatron, which sits on top of the Kubernetes cluster with the GPUs and assists Domino’s with ML-specific functionalities. Datatron allows for model performance monitoring in real time, so data scientists can be notified if their model requires retraining.Bringing the inference stack in-house allows Domino’s to have all the benefits that the cloud providers offer for hosting ML models, while keeping all data and resources on premises. It has changed the way the data scientists deploy models, giving them much more control over the deployment process, Fragoso explained in his presentation.Keep up with the latest developments in software development, cloud computing, data analytics, and machine learning with the InfoWorld Daily newsletterFragoso concluded with advice for other companies looking to bring these technologies in-house: “Think about how your data scientists will work together and collaborate. In our case, the DGX-1 and our data scientists are interacting in a common workspace. It was something that our team didn’t really consider when we first acquired this product and has been a real value for us.”Historically, data scientists operated as an independent silo within companies. More and more, IT organization are being asked to take on the task of providing the right technology to AI and ML initiatives. Data scientists are expensive resources for most companies and having them sit around waiting for models to finish is akin to tossing good pizza out the window. The right infrastructure, such as the DGX server series, enables companies to speed up processing time to let the data scientists work more and wait less.", "pub_date": "2020-03-31"},
{"title": "Review: Amazon SageMaker plays catch-up", "overview": "With Studio, Autopilot, and other additions, Amazon SageMaker is now competitive with the machine learning environments available in other clouds", "image_url": null, "url": "https://www.infoworld.com/article/3534699/review-amazon-sagemaker-plays-catch-up.html", "body": "When I reviewed Amazon SageMaker in 2018, I noted that it was a highly scalable machine learning and deep learning service that supports 11 algorithms of its own, plus any others you supply. Hyperparameter optimization was still in preview, and you needed to do your own ETL and feature engineering. Since then, the scope of SageMaker has expanded, augmenting the core notebooks with IDEs (SageMaker Studio) and automated machine learning (SageMaker Autopilot) and adding a bunch of important services to the overall ecosystem, as shown in the diagram below. This ecosystem supports machine learning from preparation through model building, training, and tuning to deployment and management — in other words, end to end.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesAmazon SageMaker Studio improves on the older SageMaker notebooks, and a number of new services have enhanced the SageMaker ecosystem to support end-to-end machine learning.What’s new in SageMaker?What’s new? Given that I last looked at SageMaker just after it was released, the list is rather long, but let’s start with the most visible services.SageMaker Studio, an IDE based on JupyterLabSageMaker Autopilot, which automatically builds and trains up to 50 feature-engineered models that can be examined in SageMaker StudioSageMaker Ground Truth, which helps to build and manage training datasetsSageMaker Notebooks now offer elastic compute and single-click sharingSageMaker Experiments, which helps developers visualize and compare machine learning model iterations, training parameters, and outcomesSageMaker Debugger, which provides real-time monitoring for machine learning models to improve predictive accuracy, reduce training times, and facilitate greater explainabilitySageMaker Model Monitor, which detects concept drift to discover when the performance of a model running in production begins to deviate from the original trained modelOther notable improvements include the optional use of spot instances for notebooks to reduce the cost; a new P3dn.24xl instance type that includes eight V100 GPUs; an AWS-optimized TensorFlow framework, which achieves close to linear scalability when training multiple types of neural networks; Amazon Elastic Inference, which can dramatically decrease inference costs; AWS Inferentia, which is a high-performance machine learning inference chip; and new algorithms, both built-in to SageMaker and  available in the AWS Marketplace. In addition, SageMaker Neo compiles deep learning models to run on edge computing devices, and SageMaker RL (not shown on the diagram) provides a managed reinforcement learning service.SageMaker StudioJupyterLab is the next-generation, web-based user interface for Project Jupyter. SageMaker Studio uses JupyterLab as the basis for an IDE that’s a unified online machine learning and deep learning workstation with collaboration features, experiment management, Git integration, and automatic model generation.The screenshot below shows how to install the SageMaker examples into a SageMaker Studio instance, using a terminal tab and the Git command line. The instructions for doing this are in the README for this example, which is kind of a Catch-22. You can read them by browsing to the Getting Started example on GitHub, or by cloning the repository to your own machine and reading it there.SageMaker Studio showing the folder view at the left, and a terminal at the right. In the terminal, we have run a Git command line to populate the instance with Amazon’s SageMaker examples from GitHub.Amazon’s Getting Started example contains a notebook called xgboost_customer_churn_studio.ipynb, which was adapted from a blog post about predicting customer churn. As Jupyter notebooks go, it has lots of explanations, as you can see in the screenshots below.Also on InfoWorld: The 6 best programming languages for AI developmentAmazon SageMaker Studio Walkthrough. This example touches on four of the major features of SageMaker Studio: Experiments, the debugger, model hosting, and the model monitor. It starts with prepping the data, then training, then hosting and monitoring.In steps 4 and 5 of the XGBoost notebook we take a quick look at the training data and upload the files to Amazon S3. If the required bucket doesn’t exist, we create it.In steps 6 through 9 we set up the Experiment and define the hyperparameters to use for the XGBoost training. We start with the built-in XGBoost algorithm.In step 10 we create a trial and run the first training. Note that we’re using one ml.m4.xlarge instance for the training.In step 11 we run five more trainings, for different values of the min_child_weight hyperparameter. The chart at the bottom is a static example, but I was able to follow the instructions to create my own chart from the components of this trial.The example goes on to run an additional training with an external XGBoost algorithm modified to save debugging information to Amazon S3 and to invoke three debugging rules. This is in what’s called  mode, meaning that it’s not a built-in algorithm.When the trainings are all done, you can compare the results in the Experiments tab.Here we are comparing the results of six trials in algorithm mode in the Experiments tab. The results are ordered by validation error in this view.The example then hosts the model using its  method and tests the deployed endpoint using its  method. Finally, it creates a baselining job with the training dataset and a scheduled monitoring job that reports any constraint violations.Also on InfoWorld: 8 great Python libraries for natural language processingBy the way, XGBoost is only one of the many algorithms built into SageMaker. A full list is shown in the table below — and you can always create your own model.Amazon SageMaker’s built-in algorithms. You can always use other algorithms in “framework” mode by supplying the code.SageMaker AutopilotSuppose you don’t know how to do feature engineering and you aren’t very familiar with the different algorithms available for the various machine learning tasks. You can still use SageMaker — just let it run on autopilot. SageMaker Autopilot is capable of handling datasets up to 5 GB.In the screenshot below we are running the Direct Marketing with Amazon SageMaker Autopilot example. It starts by downloading the data, unzipping it, uploading it to an S3 bucket, and launching an Autopilot job by calling the create_auto_ml_job API. Then we track the progress of the job as it analyzes the data, does feature engineering, and does model tuning, as shown below.At right, we see a SageMaker Autopilot job running in SageMaker Studio. It completed in about four hours. At left, we see SageMaker Studio’s Git support.The example then picks the best model, uses it to create and host an endpoint, and runs a transform job to add the model predictions to a copy of the test data. Finally, it finds the two notebooks created by the Autopilot job.There is a user interface to the Autopilot results, although it’s not obvious. If you right-click on the automl experiment you can see all the trials with their objective values, as shown below.Click on the Objective column title to sort the results and bring the best one to the top; click on the buttons at the top right to open the candidate generation and data exploration notebooks.Also on InfoWorld: 5 reasons to choose PyTorch for deep learningSageMaker Ground TruthIf you’re lucky, all your data will be labeled, or otherwise annotated, and ready to be used as a training dataset. If not, you can annotate the data manually (the standard joke is that you give the task to your grad students), or you can use a semi-supervised learning process that combines human annotations with automatic annotations. SageMaker Ground Truth is such a labeling process.As you can see in the diagram below, Ground Truth can be applied to a number of different tasks. With Ground Truth, you can use workers from either Amazon Mechanical Turk, or a vendor company that you choose, or an internal, private workforce along with machine learning to enable you to create a labeled dataset.Amazon SageMaker Ground Truth combines human annotations with automatic annotations to turn raw data into training data for model building.Amazon provides seven walkthroughs that demonstrate various ways of using SageMaker Ground Truth.SageMaker NeoUntil recently, deploying trained models on edge devices — smartphones and IoT devices, for example — has been difficult. There have been specific solutions, such as TensorFlow Lite for TensorFlow models and TensorRT for Nvidia devices, but SageMaker Neo compiles and automatically optimizes TensorFlow, Apache MXNet, PyTorch, ONNX, and XGBoost models for deployment on ARM, Intel, and Nvidia processors as well as Qualcomm, Cadence, and Xilinx devices.According to AWS, Neo can double the performance of models and shrink them enough to run on edge devices with limited amounts of memory.SageMaker inference deployment optionsIn terms of compute, storage, network transfer, etc., deploying models for production inference often accounts for 90 percent of the cost of deep learning, while the training accounts for only 10 percent of the cost. AWS offers many ways to reduce the cost of inference.One of these is Elastic Inference. AWS says that Elastic Inference can speed up the throughput and decrease the latency of getting real-time inferences from your deep learning models that are deployed as Amazon SageMaker hosted models, but at a fraction of the cost of using a GPU instance for your endpoint. Elastic Inference accelerates inference by allowing you to attach fractional GPUs to any Amazon SageMaker instance.Elastic Inference is supported in Elastic Inference-enabled versions of TensorFlow, Apache MXNet, and PyTorch. To use any other deep learning framework, export your model by using ONNX, and then import your model into MXNet.Keep up with the latest developments in software development, cloud computing, data analytics, and machine learning with the InfoWorld Daily newsletterIf you need more than the 32 TFLOPS per accelerator you can get from Elastic Inference, you can use EC2 G4 instances, which have Nvidia T4 GPUs, or EC2 Inf1 instances, which have AWS Inferentia custom accelerator chips. If you need the speed of Inferentia chips, you can use the AWS Neuron SDK to compile your deep learning model into a Neuron Executable File Format (NEFF), which is in turn loaded by the Neuron runtime driver to execute inference input requests on the Inferentia chips.At this point, the Amazon SageMaker Studio preview is good enough to use for end-to-end machine learning and deep learning: data preparation, model training, model deployment, and model monitoring. While the user experience still leaves a few things to be desired, such as better discovery of functionality, Amazon SageMaker is now competitive with the machine learning environments available in other clouds.— $0.0464 to $34.272 per instance hour for compute, depending on number of CPUs and GPUs; SSD storage: $0.14 per GB-month; Data transfer: $0.016 per GB in or out.  Hosted on Amazon Web Services. ", "pub_date": "2020-04-01"},
{"title": "Is your data lake open enough? What to watch out for", "overview": "Like yesterday’s data warehouses, today’s data lakes threaten to lock us into proprietary formats and systems that restrict innovation and raise costs", "image_url": null, "url": "https://www.infoworld.com/article/3534516/is-your-data-lake-open-enough-what-to-watch-out-for.html", "body": "A data lake is a system or repository that stores data in its raw format along with transformed, trusted data sets, and provides both programmatic and SQL-based access to this data for diverse analytics tasks such as data exploration, interactive analytics, and machine learning. The data stored in a data lake can include structured data from relational databases (rows and columns), semi-structured data (CSV, logs, XML, JSON), unstructured data (emails, documents, PDFs), and binary data (images, audio, video).A challenge with data lakes is not getting locked into proprietary formats or systems. This lock-in restricts the ability to move data in and out for other uses or to process data using other tools, and can also tie a data lake to a single cloud environment. That’s why businesses should strive to build open data lakes, where data is stored in an open format and accessed through open, standards-based interfaces. Adherence to an open philosophy should permeate every aspect of the system, including data storage, data management, data processing, operations, data access, governance, and security. Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesAn open format is one based on an underlying open standard, developed and shared through a public, community-driven process without vendor-specific proprietary extensions. For example, an open data format is a platform-independent, machine-readable data format, such as ORC or Parquet, whose specification is published to the community, such that any organization can create tools and applications to read data in the format.A typical data lake has the following capabilities:Data ingestion and storageData processing and support for continuous data engineeringData access and consumptionData governance including discoverability, security, and complianceInfrastructure and operationsIn the following sections, we will describe openness requirements for each capability.Data ingestion and storageAn open data lake ingests data from sources such as applications, databases, data warehouses, and real-time streams. It formats and stores the data into an open data format, such as ORC and Parquet, that is platform-independent, machine-readable, optimized for fast access and analytics, and made available to consumers without restrictions that would impede the re-use of that information. An open data lake supports both pull-based and push-based ingestion of data. It supports pull-based ingestion through batch data pipelines and push-based ingestion through stream processing. For both these types of data ingestion, an open data lake supports open standards such as SQL and Apache Spark for authoring data transformations. For batch data pipelines, it supports row-level inserts and updates—UPSERT—to data sets in the lake. Upsert capability with snapshot isolation—and more generally, ACID semantics—greatly simplifies the task, as opposed to rewriting data partitions or entire data sets. The ingest capability of an open data lake ensures zero data loss and writes exactly-once or at-least-once, handles schema variability, writes in the most optimized data format into the right partitions, and provides the ability to re-ingest data when needed.Data processing and support for continuous data engineeringAn open data lake stores the raw data from various data sources in a standardized open format. However, use cases such as data exploration, interactive analytics, and machine learning require that the raw data be processed to create use-case driven trusted data sets. For data exploration and machine learning use cases, users continually refine data sets for their analysis needs. As a result, every data lake implementation should enable users to iterate between data engineering and use cases such as interactive analytics and machine learning. This can be thought of as continuous data engineering, which involves the interactive ability to author, monitor, and debug data pipelines. In an open data lake, these pipelines are authored using standard interfaces and open source tools such as SQL, Python, Apache Spark, and Apache Hive.Data access and consumptionThe most visible outcome of the data lake is the types of use cases it enables. Whether the use case is data exploration, interactive analytics, or machine learning, access to data is vital. The access to data can be through SQL or programmatic languages such as Python, R, and Scala. While SQL is the norm for interactive analysis, programmatic languages are used for more advanced applications like machine learning and deep learning. An open data lake supports data access through a standards-based implementation of  SQL with no proprietary extensions. It enables external tools to access that data through standards such as ODBC and JDBC. Also, an open data lake supports programmatic access to data via standard programming languages such as R, Python, and Scala, and standard libraries for numerical computation and machine learning, such as TensorFlow, Keras, PyTorch, Apache Spark MLlib, MXNet, and Scikit-learn.Data governance – discoverability, security, and complianceWhen data ingestion and data access are implemented well, data can be made widely available to users in a democratized fashion. When multiple teams start accessing data, data architects need to exercise oversight for governance, security, and compliance purposes. Data itself is hard to find and comprehend and not always trustworthy. Users need the ability to discover and profile data sets for integrity before they can trust them for their own use case. A data catalog enriches metadata through different mechanisms, uses it to document data sets, and supports a search interface to aid discovery.Since the first step is to discover the required data sets, it’s essential to surface metadata to end-users for exploration purposes, to see where the data resides and what it contains, and to determine if it is useful for answering a particular question. Discovery includes data profiling capabilities that support interactive previews of data sets to shine a light on formatting, standardization, labels, data shape, and so on.An open data lake should have an open metadata repository. As an example, the Apache Hive metadata repository is an open repository that prevents vendor lock-in for metadata.Increasing accessibility to the data requires data lakes to support strong access control and security features. To be open, a data lake should do this through non-proprietary security and access control APIs. As an example, deep integration with open source frameworks such as Apache Ranger and Apache Sentry can facilitate table-level, row-level, and column-level granular security. This enables administrators to grant permissions against already-defined user roles in enterprise directories such as Active Directory. By basing access control on open source frameworks, open data lakes avoid vendor lock-in that results from a proprietary security implementation.New or expanded data privacy regulations, such as GDPR and CCPA, have created new requirements around “Right to Erasure” and “Right to Be Forgotten.” These govern consumers’ rights about their data and involve stiff financial penalties for non-compliance (as much as four percent of global turnover), so they must not be overlooked. Therefore, the ability to delete specific subsets of data without disrupting a data management process is essential. An open data lake supports this ability through open formats and open metadata repositories. In this way, they enable a vendor-agnostic solution to compliance needs.Infrastructure and operationsWhether the data lake is deployed in the cloud or on-premises, each cloud provider has a specific implementation to provision, configure, monitor, and manage the data lake as well as the resources it needs. An open data lake is cloud-agnostic and portable across any cloud-native environment, including public and private clouds. This allows administrators to leverage the benefits of both public and private cloud from an economics, security, governance, and agility perspective.  Keep up with the latest developments in software development, cloud computing, data analytics, and machine learning with the InfoWorld Daily newsletterOpen for innovationThe increase in the volume, velocity, and variety of data, combined with new types of analytics and machine learning, make data lakes a necessary complement to more traditional data warehouses. Data warehouses exist largely in a world of proprietary formats, proprietary SQL extensions, and proprietary metadata repositories, and lack programmatic access to data. Data lakes don’t need to follow this proprietary path, which leads to restricted innovation and higher costs. A well designed, open data lake provides a robust, future-proof data management system that supports a wide range of data processing needs including data exploration, interactive analytics, and machine learning.Qubole", "pub_date": "2020-04-09"},
{"title": "Not all AIops tools are created equal", "overview": "These tools can solve multicloud complexity issues, but they use very different approaches", "image_url": null, "url": "https://www.infoworld.com/article/3537414/not-all-aiops-tools-are-created-equal.html", "body": "AIops is becoming the new norm for operational tools supporting cloud operations. This technology can be applied to all types of operational tasks, providing intelligent automation that learns as it solves operational issues.These tools must carry out preprogrammed self-corrective processes, and the AIops tools’ ability to learn during those processes creates a huge advantage. For instance, understanding that performance issues could be saturation caused by cyberattacks should kick off security processes to mount a defense. Or moving out of a performance threshold should automatically launch more resources to bring performance back to an acceptable range.Also on InfoWorld: What AI can really do for your business (and what it can’t)The number of things you can do with these tools increases each day, and it’s likely to be standard equipment for those of you deploying and operating multicloud.Most important, AIops tools can deal with thousands of data points and make correlations that most humans would not make. Moreover, as they correlate these data points the tool itself is smarter—it knows what the information actually means and how to assist the cloudops team.The trouble is that many products in this space are actually old technology made new. We’ve been using operational tools for years. Those tools were redone to support public clouds; now they have been rebranded as AIops tools with some built-in AI capabilities.The trouble with this type of evolution is that it’s happening so fast, the tools are naturally going to take different approaches. Some are very data driven, capable of analyzing historical data; other focus on real-time monitoring.Data-oriented tools look for patterns in the data—typically assisted by an AI engine—in order to find cause and effect. They get to the root cause of an issue without the cloudops staff having to cull through gobs of data.Also, AI is leveraged in different ways. Some have pretrained AI systems within the tools, which means the tools comes with a predetermined amount of knowledge. Others focus on training from scratch. Each approach has advantages.The bottom line is that AIops is still an emerging space, despite many vendors being more traditional technology players. Solution patterns differ from tool to tool, and thus you need to be more diligent to understand the basic functionality of the tools, as well as how to match your ops requirements with the proper offering.", "pub_date": "2020-04-10"},
{"title": "Google unveils TensorFlow tool for making mobile-ready models", "overview": "TensorFlow Lite Model Maker shrinks TensorFlow models to more efficiently serve predictions on mobile devices", "image_url": null, "url": "https://www.infoworld.com/article/3538913/google-unveils-tensorflow-tool-for-making-mobile-ready-models.html", "body": "Google has announced TensorFlow Lite Model Maker, a tool for converting an existing TensorFlow model to the TensorFlow Lite format used to serve predictions on lightweight hardware such as mobile devices.TensorFlow models can be quite large, and serving predictions remotely from beefy hardware capable of handling them isn’t always possible. Google created the TensorFlow Lite model format to make it more efficient to serve predictions locally, but creating a TensorFlow Lite version of a model previously required some work.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesIn a blog post, Google described how TensorFlow Lite Model Maker adapts existing TensorFlow models to the Lite format with only a few lines of code. The adaptation process uses one of a small number of task types to evaluate the model and generate a Lite version. The downside is that only a couple of task types are available for use right now — i.e., image and text classification — so models for other tasks (e.g., machine vision) aren’t yet supported.Other TensorFlow Lite tools announced in the same post include a tool to automatically generate platform-specific wrapper code to work with a given model. Because hand-coding wrappers for models can be error-prone, the tool automatically generates the wrapper from metadata in the model autogenerated by Model Maker. The tool is currently available in a pre-release beta version, and supports only Android right now, with plans to eventually integrate it into Android Studio.", "pub_date": "2020-04-17"},
{"title": "Checking AI bias is a job for the humans", "overview": "By pre-processing or post-processing data, or even setting datasets to expire, humans can step in to correct machine learning models", "image_url": null, "url": "https://www.infoworld.com/article/3537968/checking-ai-bias-is-a-job-for-the-humans.html", "body": "One of the primary problems with artificial intelligence (AI) is the “artificial” part. The other is the “intelligence.” While we like to pretend that we’re setting robotic intelligences free from our human biases and other shortcomings, in reality we often transfer our failings into the AI, one dataset at a time.Hannah Davis, a data scientist, calls this out, arguing that “a dataset is a worldview,” filled with subjective meanings. But rather than leave our AI hopes moribund, she also offers some ways we might improve the data that informs our AI.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesAI has always been about peopleIt has become de rigueur to posture how very “data driven” we are, and nowhere more so than in AI, which is completely dependent on data to be of use. One of the wonders of machine learning algorithms, for example, is how fast they can sift through mountains of data to uncover patterns and respond accordingly. Such models, however, must be trained, which is why data scientists tend to congregate around established, high-quality datasets.Unfortunately, those datasets aren’t neutral, as Davis points out:[A] dataset is a worldview. It encompasses the worldview of the people who scrape and collect the data, whether they’re researchers, artists, or companies. It encompasses the worldview of the labelers, whether they labeled the data manually, unknowingly, or through a third-party service like Mechanical Turk, which comes with its own demographic biases. It encompasses the worldview of the inherent taxonomies created by the organizers, which in many cases are corporations whose motives are directly incompatible with a high quality of life.See the problem? Machine learning models are only as smart as the datasets that feed them, and those datasets are limited by the people shaping them. This could lead, as one Guardian editorial laments, to machines making our same mistakes, just more quickly: “The promise of AI is that it will imbue machines with the ability to spot patterns from data, and make decisions faster and better than humans do. What happens if they make worse decisions faster?”Complicating matters further, our own errors and biases are, in turn, shaped by machine learning models. As Manjunath Bhat has written, “People consume facts in the form of data. However, data can be mutated, transformed, and altered—all in the name of making it easy to consume. We have no option but to live within the confines of a highly contextualized view of the world.” We’re not seeing data clearly, in other words. Our biases shape the models we feed into machine learning models that, in turn, shape the data available for us to consume and interpret.Time to abandon hope, all we who enter here?Data problems are people problemsNot necessarily. As Davis goes on to suggest, one key thing we can do is to set our datasets to expire:Machine learning datasets are treated as objective. They’re treated as ground truth by both the machine learning algorithms and the creators. And datasets are hard, time-consuming, and expensive to make, so once a dataset is created, it is often in use for a long time. But there is no reason to be held to the past’s values when we as a society are moving forward; similarly, there is no reason to hold future society to our current conditions. Our datasets can and should have expiration dates.At any given point in time, the people, places, or things that are top of mind will tend to find their way into our datasets. (Davis uses the example of ImageNet, created in 2009, which returns flip phones when “cell phone” is searched.) By setting datasets to expire, we force our models to keep up with society.Also on InfoWorld: How dataops improves data, analytics, and machine learningThis calls out another option, one suggested by McKinsey research, which is to re-introduce people into AI. Whether through pre-processing of data or post-processing of data, humans can step in to correct machine learning models. The math involved in the model may be impeccable, but adding humans (yes, with biases) can help to take account of the model’s outcome and prevent biases from operating unchecked.Unless we’re careful, Davis warns, “It is easy to accidentally cause harm through something as seemingly simple as collecting and labeling data.” But with extra care, we can gain much of the benefits of AI while minimizing the potential biases and other shortcomings that the machines inherit from us humans.", "pub_date": "2020-04-21"},
{"title": "Some observations on cloud computing observability", "overview": "Observability is an approach to watching applications, data, and infrastructure in a way that provides more holistic value ", "image_url": null, "url": "https://www.infoworld.com/article/3573801/some-observations-on-cloud-computing-observability.html", "body": "The term and the concept of observability aren't new at all. Observability first appeared in the world of engineering and control theory. Although the definition depends on whom you’re asking, some common patterns and understandings are emerging. Mostly it’s a new buzzword by the AIops providers and some thought leaders in the cloudops space (meaning me). At its essence, observability is the measure of how well internal system states can be inferred from knowledge of all external data and states. It’s a bit different than monitoring which is something you do (a verb); observability is an attribute of a system (a noun).How data analysis, AI, and IoT will shape the post-pandemic ‘new normal’If that’s a bit confusing, here's a more pragmatic definition: the ability to leverage the concept of observability and the tools that support that concept. Most monitoring and operations tools (such as AIops tools) claim some role in observability. However in some respects, to me it's the same old bourbon (monitoring) in a much fancier bottle (observability). I look at it with a bit of well-placed skepticism.On the bright side, more tools are being purpose-built for the notion of observability, including most of the emerging AIops and cloudops toolsets, as well as other monitoring platforms that deal with complex operational data, such as security, database, and network monitoring. The purpose of observability is leveraging all of these data points holistically, which most enterprises don't do these days.         Observability is important to managing and monitoring modern systems, considering we deal with both modern applications and the pace at which these applications are delivered. The widely held practice of bolting on monitoring and management after applications are deployed does not scale very well.   Part of the notion of observability is the need for up-to-date instrumentation to better understand the properties, performance, and behaviors of an application. We need tools to deal with underlying complexity and distribution. Of all the barriers to the cloud moving forward, what scares me most is that we’ll build too much complexity with no clear way to manage it at scale.    Basically, monitoring has these attributes:Passive consumption of information from systems under management to determine state and status.Dashboards provide information about the state of systems and rely on humans to respond.Tools are purpose-built for static systems and platforms that don’t change much over time.Observability takes a different approach: Holistically considers all data from systems under management to determine a proactive and automated response.Asks questions based on a continuous understanding or hypotheses, continuously improving how systems are observed, tuned, and fixed using automation.Purpose-built for dynamic systems and platforms with widely changing complexity. The tool adapts to the systems under management, not the other way around.To be honest I take these new concepts as something to consider, not to follow religiously. The history of IT is full of ideas that seemed important for a time and faded away, never to be heard from again. However, considering the importance of operations and tools than can allow us to operate under increasingly complex environments, this concept could go wide.  ", "pub_date": "2020-09-08"},
{"title": "Using OPA to safeguard Kubernetes", "overview": "Open Policy Agent addresses Kubernetes authorization challenges with a full toolkit for integrating declarative policies into any number of application and infrastructure components", "image_url": null, "url": "https://www.infoworld.com/article/3573078/using-opa-to-safeguard-kubernetes.html", "body": "As more and more organizations move containerized applications into production, Kubernetes has become the de facto approach for managing those applications in private, public and hybrid cloud settings. In fact, at least 84% of organizations already use containers in production, and 78% leverage Kubernetes to deploy them, according to the Cloud Native Computing Foundation.Part of the power and allure of Kubernetes is that, unlike most modern APIs, the Kubernetes API is intent-based, meaning that people using it only need to think about what they want Kubernetes to do — specifying the “desired state” of the Kubernetes object — not  they want Kubernetes to achieve that goal. The result is an incredibly extensible, resilient, powerful, and hence popular system. The long and short of it: Kubernetes speeds app delivery.Also on InfoWorld: Cloud tech certifications count more than degrees nowHowever, changes in a cloud-native environment are constant by design, which means that runtime is extremely dynamic. Speed plus dynamism plus scale is a proven recipe for risk, and today’s modern environments do indeed introduce new security, operational, and compliance challenges. Consider this: How do you control the privilege level of a workload when it only exists for microseconds? How do you control which services can access the internet — or be accessed — when they are all built dynamically and only as needed? Where is your perimeter in a hybrid cloud environment? Because cloud-native apps are ephemeral and dynamic, the attack surface and the requirements for securing it are considerably more complex.Kubernetes authorization challengesMoreover, Kubernetes presents unique challenges regarding authorization. In the past, just that simple word, “authorization” brought up the concept of which people can perform which actions, or “who can do what.” But in containerized apps, that concept has greatly expanded to also include the concept of which software or which machines can perform which actions, aka “what can do what.” Some analysts are starting to use the term “business authorization” to refer to account-centric rules, and “infrastructure authorization” for everything else. And when a given app has a team of, say, 15 developers, but is made up of dozens of clusters, with thousands of services, and countless connections between them, it’s clear that “what can do what” rules are more important that ever — and that developers need tools for creating, managing, and scaling these rules in Kubernetes.Because the Kubernetes API is YAML-based, authorization decisions require analyzing an arbitrary chunk of YAML to make a decision. Those chunks of YAML should define the configuration for each workload. For instance, enforcing a policy, such as “ensure all images come from a trusted repository,” requires scanning the YAML to find a list of all containers, iterating on that list, extracting the particular image name, and string-parsing that image name. Another policy might be, for example, “prevent a service from running as root,” which would require scanning the YAML to find the list of containers, iterating on that list to check for any container-specific security setting, and then combining those settings with global security parameters. Unfortunately, no legacy “business authorization” access control solutions — think role-based or attribute-based access controls, IAM policies, and so on — are powerful enough to enforce policies as basic as the one above, or even things as simple as changing the labels on a pod. They simply were not designed to do so.Even in the rapidly evolving world of containers, one thing has remained constant: Security is often pushed out to the end. Today, DevOps and DevSecOps teams are striving to shift security left in development cycles, but, without the proper tools, are often left to identify and remediate challenges and compliance issues much later on. Indeed, to truly meet the time-to-market goals of a DevOps process, security and compliance policy must be implemented much earlier in the pipeline. It’s been proven that security policy works best when risk is eliminated in the early phases of development, meaning it’s less likely that security concerns will arise toward the end of the delivery pipeline.Yet, not all developers are security experts, and manual reviews of all YAML configurations is a guaranteed path to failure for already overburdened DevOps teams. But you shouldn’t have to sacrifice security for efficiency. Developers need appropriate security tooling that speeds development by implementing hard guardrails that eliminate missteps and risk — ensuring that their Kubernetes deployments are in compliance. What’s needed is a way to improve the overall process that is beneficial to developers, operations, security teams, and the business itself. The great news is there are solutions built to work with modern pipeline automation and “as-code” models that reduce both error and exhaustion.Also on InfoWorld: How to improve CI/CD with shift-left testingEnter Open Policy AgentIncreasingly, the preferred “who can do what” and “what can do what” tool for Kubernetes is Open Policy Agent (OPA). OPA is an open-source policy engine, created by Styra, that provides a domain-agnostic, standalone rules engine for business and infrastructure authorization. Developers often find OPA to be a perfect match for Kubernetes because it was designed around the premise that sometimes you need to write and enforce access control policies — and plenty of other policies — over arbitrary JSON/YAML. As a policy-as-code tool, OPA leads to increased speed and automation in Kubernetes development, while improving security and reducing risk. In fact, Kubernetes is one of the most popular use cases of OPA. If you don’t want to write, support, and maintain custom code for Kubernetes, you can use OPA as a Kubernetes admission controller and put its declarative policy language, Rego, to good use. For instance, you can take all of your Kubernetes access control policies — which are typically stored in wikis and PDFs and in people’s heads — and translate them into policy-as-code. That way, those policies can be enforced directly on the cluster, and developers running apps on Kubernetes don’t need to constantly refer to internal wiki and PDF policies while they work. This leads to fewer errors and eliminates rogue deployments earlier in the development process, all of which results in higher productivity.Another way that OPA can help address the unique challenges of Kubernetes is with context-aware policies. These are policies that condition the decisions Kubernetes makes for one resource on information about all the other Kubernetes resources that exist. For example, you might want to avoid accidentally creating an application that steals another application’s internet traffic by using the same ingress. In that case, you could create a policy to “prohibit ingresses with conflicting hostnames” to require that any new ingresses are compared to existing ingresses. More importantly, OPA ensures that Kubernetes configurations and deployments are in compliance with internal policies and external regulatory requirements — a win-win-win for developers, operations and security teams each.Securing Kubernetes across hybrid cloudOftentimes, when people say “Kubernetes,” they’re really referring to the applications that run on top of the Kubernetes container management system. That’s also a popular way to use OPA: have OPA decide whether microservice and/or end-user actions are authorized within the application itself. Because when it comes to Kubernetes environments, OPA offers a full toolkit for testing, dry-running, auditioning, and integrating declarative policies into any number of application and infrastructure components.Also on InfoWorld: 11 tools that make Kubernetes better12 tools that make Kubernetes easierIndeed, developers often expand their use of OPA to enforce policies and increase security across all of their Kubernetes clusters, particularly in hybrid cloud environments. For that, a number of users also leverage Styra DAS, which helps to validate OPA security policies in pre-runtime to see their impact, distribute them to any number of Kubernetes clusters, and then continuously monitor policies to ensure they’re having their intended effect.Regardless of where organizations are on their cloud-native and container journeys, what’s clear is that Kubernetes is now the standard for deploying containers in production. Kubernetes environments bring new, unique challenges that organizations must solve to ensure security and compliance in their cloud and hybrid-cloud environments — but solutions do exist to limit the need for ground-up thinking. For solving these challenges at speed and scale, OPA has emerged as the de facto standard for helping companies mitigate risk and accelerate app delivery through automated policy enforcement.Open Policy AgentStyra—newtechforum@infoworld.com", "pub_date": "2020-09-09"},
{"title": "As edge computing evolves, the cloud's role changes", "overview": "Edge computing is becoming more mainstream and more complex. Keep it under control by leveraging cloud-based technologies", "image_url": null, "url": "https://www.infoworld.com/article/3574850/as-edge-computing-evolves-the-clouds-role-changes.html", "body": "The notion of edge computing typically conjures an image of a device in a factory someplace, providing rudimentary computing and data collection to support a piece of manufacturing equipment. Perhaps it keeps the factory temperature and humidity optimized for the manufacturing process.Those who deal with edge computing these days understand that what was considered “edge” just a few years ago has morphed into something a bit more involved. Here are the emerging edge computing architecture patterns that I’m seeing:InfoWorld’s 2020 Technology of the Year Award winners: The best software development, cloud computing, data analytics, and machine learning products of the year No longer are edge devices connected directly to some centralized system, such as one residing in the cloud. They connect to other edge devices that may connect to larger edge devices that ultimately connect to a centralized system or cloud.This means that we’re leveraging very small and underpowered devices at the true edge, such as a thermostat on the wall. That device connects to a local server in the physical building, which is also considered an edge device, in a one-to-many configuration (one edge server to many thermostats). Then another edge server aggregates the data of many buildings and finally transmits to the public cloud where the edge-based data is stored, analyzed, and results returned down the hierarchy.Although this seems like we’re killing ourselves with complexity by adding layers to the edge architecture, the motivations are pragmatic. There is no need to send all of the data to the centralized storage and processing system in the cloud when it can be processed better and cheaper by edge server that are closer to the devices, especially if the collection devices (the thermostat) are not powerful enough to do any real processing.The advantages here are better performance and resiliency. The data did not have to be transmitted out of the building. The architecture is much more agile; you can repurpose each edge device without forcing changes to the centralized data storage and processing systems in the cloud.This edge architecture, simply put, allows data to flow from edge device to edge device, as well as to the back-end system in the cloud using autonomous AI-based agents charged with relocating data based on predefined rules. This is typically for storage and processing. Data from an edge device may be moved to another edge device or to a centralized server based on what needs to be done to the data.This has a clear advantage. It avoids saturating edge device that typically don’t have a lot of storage. Many edge storage devices use only one to three percent of their storage; others hover around 90 percent, which is scary.If the data does not need to be transmitted to the centralized processing systems (typically in a public cloud) and can be stored locally more efficiently, then edge architects will find autonomous edge data movement compelling, compared to ongoing upgrades to all edge devices and edge servers. The role of the cloud within these emerging architectures is to provide command and control, not just to be place for processing. The clear pattern has been to avoid sending data to the back end if it can be avoided. But there has to be a centralized “big brain” for all of this to work, and automation should exist in a central and configurable space, putting volatility into a domain.Although these architectures are rare today, I’m seeing more and more of them as enterprises attempt to move to edge computing in innovative ways adapted for their specific use and geographical distribution. Edge is likely to become even more complex, and the patterns will expand. Keeping the data away from the cloud seems counterintuitive, but we’re leveraging the cloud to be the master of all edge devices that will end up storing even more data. ", "pub_date": "2020-09-11"},
{"title": "What TikTok teaches enterprises about tech entanglement", "overview": "Our infrastructure, apps, and data must be designed in a way that makes them portable and flexible enough to be separated cleanly if needed", "image_url": null, "url": "https://www.infoworld.com/article/3574958/what-tiktok-teaches-enterprises-about-tech-entanglement.html", "body": "The flurry of interest in acquiring TikTok shows the value of a popular social network with vast troves of consumer data. The deal also holds an important lesson about technology use that other businesses can learn from: the need to design applications in a flexible, portable way. One challenge for any company that buys TikTok will be how to carve up its technical infrastructure outside of China without destroying its value in the process. The impetus for the deal is that the US doesn’t want China “spying” on its citizens through the app. That will require untangling the data and application code on the back end. Also on InfoWorld: How cloud-native technologies defeat cloud lock-inTikTok owner ByteDance also runs a similar app called Douyin, available only in China, and the apps reportedly share technical resources including user data, server code, and the algorithms that determine which content and ads a user sees. ByteDance needs to separate those elements in the narrow timeframe it’s been given, and its ability to do that effectively will be a factor in how much a suitor is willing to pay — and perhaps whether a deal happens at all.There are lessons here for other businesses. It’s not uncommon to sell or spin off a product or company division for strategic reasons, and how those assets are architected can impact their value and the ease with which they can be sold. The days of building giant monolithic applications are long gone. Most new apps are built in the cloud using smaller microservices that can be scaled up and down as needed and updated independently. That’s good for introducing new features quickly but it can be hard to disentangle apps if they’re not designed carefully using open standards and technologies.There are three main areas to consider: the cloud infrastructure, the application code, and the data. Here’s how to design each in a way that makes them portable and flexible enough that they can be separated cleanly if needed. Keep it cloud-neutralMost businesses have a preferred cloud provider, and any app being acquired may need to be moved onto a different service. In TikTok’s case, ByteDance recently signed an $800 million deal to run the app in Google’s cloud. Certainly, Microsoft or Oracle would want to move the app to their own cloud as soon as possible to collect the revenue. One way to get locked into a cloud is using high-level managed services, like Amazon Redshift or Google Cloud Big Table. These services are alluringly simple to deploy but notorious for locking customers into proprietary formats. Even the managed versions of popular open source products like MongoDB and Elasticsearch can have subtle differences depending on the cloud provider, making it hard to port apps when you need to.Think of the public cloud as an infrastructure service, not the basis for architectural decisions. If you’re building the next TikTok, select services that aren’t specific to one public cloud and don’t limit your options by going all in on one provider.Make apps modular and portableApplications should be developed as microservices and deployed in containers based on a widely used standard like Docker. Containers package everything an app needs to run into a self-contained bundle, including the application code and any dependencies or configuration files it requires. That makes it easier to move or copy an app onto a different cloud or hardware platform.Because containerized apps are modular by nature, each application service can be dealt with individually as needed. For example, maybe the user interface code is copied over while the security services are rewritten. Or a business could sell off specific functionality and retain other parts of an app. The algorithms themselves can be updated for each country or region, depending on local market needs.Keep the data independentByteDance has a trove of data about its users around the world. Given the US’s privacy and security concerns, it will need to unbundle the data associated with its international users and ensure ByteDance has no way of accessing that information after the deal is complete. Managing data in the event of a sale starts with knowing where all your data lives, including any backups. Data should be architected intentionally and segmented by country or zone of operation. This is a good practice in any event, for compliance with regulations like GDPR or nation-specific data sovereignty laws, but may also be essential in the event of a sale.Also on InfoWorld: Kubernetes meets the real world: 3 success storiesIn addition, data shouldn’t be hard-wired to the underlying storage system. Cloud native technologies like Kubernetes and containers allow data to reside anywhere and connect to an app as needed. In the event of a sale, data can be moved to a different cloud, data center, or region without having to rewrite the application or rip out the storage hardware. In short, data should be kept highly portable and there are standards-based cloud technologies that make that possible.It’s unclear who will acquire TikTok but the deal could be a lucrative one. Facebook paid $1 billion for Instagram and the service generated $20 billion in ad revenue last year. TikTok has about 80 million users in the US and it is still growing fast. But these deals can be complex when data and applications have to be disentangled on the back end. How you architect your infrastructure can have a big impact when it comes to a sale.So let this deal serve as a lesson. Design for maximum flexibility, because you never know what the future holds.Portworxnewtechforum@infoworld.com", "pub_date": "2020-09-11"},
{"title": "Visual Studio Codespaces is moving to GitHub", "overview": "Microsoft is moving Visual Studio Codespaces into GitHub Codespaces to simplify the developer experience", "image_url": null, "url": "https://www.infoworld.com/article/3575092/visual-studio-codespaces-is-moving-to-github.html", "body": "Microsoft’s Visual Studio Codespaces, which provide cloud-hosted development environments on Microsoft Azure, will be incorporated into GitHub Codespaces, which provide hosted Visual Studio Code environments on GitHub. The current Azure-based offering will be retired in February 2021.Microsoft said the service is moving because, during a preview stage, the company found that transitioning from a repository to a codespace was the most-criticized part of the workflow. The vast majority of users preferred an integrated, native, one-click experience. GitHub being the home of 50 million developers, Microsoft decided it made sense to partner with GitHub to address the issue. Also on InfoWorld: Visual Studio Code vs. Visual Studio: How to chooseGitHub Codespaces is still in a limited public beta; developers can sign up here. When developers connect to a GitHub Codespace through a portal or the Visual Studio Code editor, they will be prompted to submit add a GitHub account to the beta. Visual Studio Codespaces users will receive an email requesting their preferred GitHub account.While GitHub Codespaces provides an optimized experience for GitHub repos, developers still can use Git repos hosted elsewhere, such as on Azure or Bitbucket, by taking a few additional configuration steps. Also, the private preview of Windows-based Codespaces support in Visual Studio 2019 will move to GitHub.Microsoft has provided an FAQ about the consolidation. The compay set the following timeline for plan:September 4, 2020: Current users begin moving to the GitHub private beta.November 20, 2020: Creation of new Visual Studio Codespaces will no longer be allowed, but existing Visual Studio Codespaces still can be used.February 17, 2021: The Visual Studio Codespaces portal will be retired.", "pub_date": "2020-09-11"},
{"title": "Kubernetes and cloud portability — it’s complicated", "overview": "Kubernetes doesn’t offer the magical application portability you might expect, but something better", "image_url": null, "url": "https://www.infoworld.com/article/3574853/kubernetes-and-cloud-portability-its-complicated.html", "body": "I’m so sorry. You were told that Kubernetes was your key to the golden age of multicloud nirvana. You believed that Kubernetes would give you portability to move applications seamlessly between clouds, whether running in your data center or on a public cloud. It’s not really your fault. Vendors have been promising all sorts of magical things with regard to portability and Kubernetes.Unfortunately, Gartner just weighed in to put the kibosh on the Kubernetes-for-portability panacea. As analyst Marco Meinardi writes, when asked whether companies should embrace “Kubernetes to make their applications portable... the answer is: no.”Also on InfoWorld: Tim O’Reilly: The golden age of the programmer is overIt’s not that Kubernetes can’t be used to make applications portable. It can. But the nature of that portability differs from how it’s commonly conceived. So how  companies think about Kubernetes-driven portability?Can’t get there from hereFirst, let me suggest that the whole idea of multicloud might be wrongheaded. Now, I could be biased. After all, I do work for a cloud provider (AWS). However, I like to think that my bias against multicloud, magical-application-portability thinking stems from it being a really bad idea. As I wrote long before I joined AWS, “Vendors are cleaning up by selling multicloud snake oil while customers keep getting stuck with lowest common denominator cloud strategies and sky-high expenses.” Of course I’m not the only one with this view. Corey Quinn of the Duckbill Group, who has made a living by snarkily trashing bad IT practices, believes multicloud is “the worst practice” for a host of reasons. As Quinn has written:[T]he idea of building workloads that can seamlessly run across any cloud provider or your own data centers with equal ease… is compelling and something I would very much enjoy.However, it’s about as practical as saying “just write bug-free code” to your developers—or actually trying to find the spherical cow your physics models dictate should exist. It’s a lot harder than it looks.”Which brings us to Gartner.There are many, many great reasons to use Kubernetes, Meinardi writes. Portability just doesn’t happen to be one of them:Inquiries show that [the] likelihood [of moving applications between cloud providers] is actually very low. Once deployed in a provider, applications tend to stay there. This is due to data lakes being hard — and expensive — to port and, therefore, end up acting as centers of gravity.Why hard? Analyst Kurt Marko provides some clues, saying that “transparent, incognizant workload movement is only possible on vanilla container platforms for the simplest of applications.” He then lists a range of hurdles that get in the way of Kubernetes-based portability, including dependence on native cloud features (managed databases, serverless functions, etc.), the difficulty of federating user identity and security policies across platforms, and more.So if multicloud isn’t necessarily the best of ideas, following Quinn, and Kubernetes isn’t going to get us there, even if it were a great idea, following Marko, what kind of Kubernetes-fueled portability  a good idea?Portability is really about peopleIt’s not that you  architect an application in such a way as to make it portable across clouds. You can, as Peter Benjamin notes, and that portability makes more sense if you think of the timeframes in terms of years rather than days or weeks, as Paul Nash posits. (Miles Ward concurs, arguing, “Our customers [want to] evaluate every few years, not every week; they just want it easy if the landscape changes.”) Rather, it’s just that this isn’t common (for the reasons identified above), nor is it as important as other aspects of portability.Perhaps the most important way to think about portability comes from Weaveworks CEO Alexis Richardson:The point is “skills portability” due to using a standard operating model and tool chain. Large organisations want developers to use standard ways of working because this reduces training costs, and removes obstacles to staff moving from project to project. It also makes it easier and cheaper to apply policy if your “platform” (or platforms) are based on the same core set of cloud native tools.This view is echoed by Knative co-founder Matt Moore (“the real win is the portability of concepts and tools”). Or, as EJ Campbell nicely summarizes, “It’s nice to have transferable skills.”Also on InfoWorld: Cloud tech certifications count more than degrees nowImportantly, those skills help both employers and employees. Kubernetes may not make it simple to move applications between environments (on-premises or cloud), but it does level-set the skills and concepts that a developer can use in each of these environments. Or, as Rancher Labs CTO and co-founder Darren Shepherd puts it, “The portability is that the same approach can be used regardless of cloud, data center, edge, laptop, stateless, stateful, AI/ML, etc.”In this way, Kubernetes is incredibly important, offering “people portability” that is good for individuals and corporations. It’s not magical application portability. It’s something much better.", "pub_date": "2020-09-14"},
{"title": "Does Snowflake mean the end of open source?", "overview": "The cloud-based enterprise data platform may mark the end of a decades-long run in the dominance of open source infrastructure ", "image_url": null, "url": "https://www.infoworld.com/article/3575855/does-snowflake-mean-the-end-of-open-source.html", "body": "The Snowflake IPO was a big deal, and not merely because of the company’s enormous valuation.In 2013 Cloudera co-founder Mike Olson confidently (and accurately) declared “a stunning and irreversible trend in enterprise infrastructure.” That trend? “No dominant platform-level software infrastructure has emerged in the last 10 years in closed-source, proprietary form.” Snowflake, a cloud-based enterprise data platform, may spell the end of that run. Sure, we had Splunk, but Splunk squeaked through the hypothesis police before open source had found its feet, as Lightspeed partner Gaurav Gupta told me. MySQL, Apache Hadoop, MongoDB, Apache Spark... all of them (at least initially) open source.But now... Snowflake. Is Snowflake a snowflake? Or is the era of open source infrastructure coming to a close?Also on InfoWorld: Cloud tech certifications count more than degrees nowClosing up shop?In part the answer to that question depends on just how fiercely you’re prepared to defend the underlying assumption. After all, it’s simply not the case that all “dominant platform-level software infrastructure” is open source. This isn’t really to dispute Olson’s central thesis, because it’s absolutely true that the bulk of enterprise infrastructure has trended toward open source over the past 10 to 20 years.As Gordon Haff puts it, “You can certainly construct a narrative for the infrastructure being heavily driven by open source: Most NoSQL, Hadoop, Kafka, Spark, Ceph, Jupyter, etc. But a lot in the space isn’t as well: lots of cloud services, Tableau, Splunk, etc.” And Snowflake, of course.Though you’d never guess it from the energetic proselytizing of yesteryear, developers have never been overly religious about open source. The reason for that “stunning” trend is simply that open source made it easier for developers to get their jobs done thanks to high-quality, easily accessible open source data infrastructure. There are, of course, other benefits, such as the communities that often accompany open source projects, coupled with a desire to have more granular control of one’s software stack. But ultimately open source has won because it enables developers to “get ---- done.”Which is why, for example, you’ll find developers happy to use open source software like Apache Airflow to load data into their proprietary Snowflake data platform. It’s not cognitive dissonance. It’s pragmatism.The shift to managed servicesSpeaking of such pragmatism, Tom Barber suggests that the shift to managed cloud services somewhat negates “people’s interest in open source... because with SaaS you’re not paying for licenses but for a service, which changes the thinking somewhat.” After all, he continues, “Open source meant you didn’t pay for licenses but you still had to pay someone internal or external to install it, tune it, run it…. Most people can apt/yum install MySQL but tuning it requires in-depth knowledge.”Or let’s express that another way, as Redmonk analyst James Governor does: “Cloud is a better distribution and packaging mechanism than open source ever was…. Convenience is the killer app. Managed services win.” Or, as Olson himself suggested to me,I still believe that open source software provides strategic advantage. But “elimination of friction” isn’t the differentiator it seemed a decade ago. Smart cloud folks learned that lesson; proprietary infra in the cloud is super easy to acquire and use. That’s not to say open source is irrelevant. Far from it. “Open source is not a business model but is a great way to build software, build trust, and foster community,” Governor continues.That “great way to build software” also applies to SaaS vendors like Snowflake. While services like Snowflake might not be open source, they’re actively using open source under the hood, as Gordon Haff suggests. For example, Snowflake relies on open source FoundationDB as “a key part of our architecture [because it] has allowed us to build some truly amazing and differentiating features.”A Whitesource analysis in 2019 found that 99 percent of software includes open source. Snowflake, in this respect, is no snowflake.Open source, in sum, still matters. A lot. Open source under the hoodBut for would-be buyers of services like Snowflake, open source might not be the primary attraction. As Ken Horn posits, data, not source code, needs to be the “first-class citizen” for something like Snowflake. And “once on cloud, the whole open source software thing is a bit :shrug:.”It’s not “shrug” for Snowflake and other vendors who may choose to deliver data warehousing and other such services, because open source affords them the chance to build on a rich ecosystem of open source foundational building blocks. But for the would-be buyers, they just need “to get ---- done,” and this may mean they don’t want to perform the spade work sometimes associated with open source.So is Olson’s 2013 declaration wrong? No, but perhaps we can rephrase it: No dominant platform-level software infrastructure has emerged in the last  20 years in closed-source, proprietary form .”Linux is still the standardDo developers really care about open source?Open source has a people problemThe community hat rules the company hat in open sourceWhat is Google’s Open Usage Commons — and why?What does an open source maintainer do after burnout?Do we need so many databases?How GraphQL turned web development on its headHow Redis scratched an itch — and changed databases foreverWill the solo open source developer survive the pandemic?Open source projects take all kindsThe most important part of an open source projectGatsby JS stands on the shoulders of thousandsRemember when open source was fun?Open source made the cloud in its imageZeek and Jitsi: 2 open source projects we need nowWTH? OSS knows how to WFH IRLThe secret to Kubernetes’ successOpen source companies are thriving in the cloudOpen source should learn from Linux, not MySQLMaking open source JavaScript payCustomer-driven open source is the future of softwareMagical Magento figures out how to make and takeThe real number of open source developersWhy the Rust language is on the riseShould open source licenses fight evil?Should open source software advertise?Why open source has never been stronger", "pub_date": "2020-09-23"},
{"title": "Azure Databricks previews parallelized Photon query engine", "overview": "Microsoft and Databricks say the vectorized query engine written in C++ accelerates Apache Spark workloads by up to 20x", "image_url": null, "url": "https://www.infoworld.com/article/3583657/azure-databricks-previews-parallelized-photon-query-engine.html", "body": "Microsoft has unveiled a preview of a C++-based vectorized query engine for the Azure Databricks cloud analytics and AI service based on Apache Spark. Azure Databricks, which is delivered in partnership with Databricks, introduced the Photon-powered Delta Engine September 22.Written in C++ and compatible with Spark APIs, Photon is a vectorized query engine that leverages modern CPU architecture and the Delta Lake open source transactional storage layer to enhance Apache Spark 3.0 performance by as much as 20x. Microsoft said that as organizations embrace data-driven decision-making, it is now imperative for them to have a platform that can quickly analyze massive amounts and types of data.Also on InfoWorld: How to choose a cloud machine learning platformPhoton offers greater parallelism of CPU processing at the data and instruction levels. Other components in Delta Engine include an improved query optimizer and a caching layer. The combination of these technologies boosts big data use cases including data engineering, machine learning, data science, and data analytics.Azure Databricks is intended to allow users to quickly set up optimized Apache Spark environments. It offers native integration with the Azure Active Directory and other Azure cloud services such as Azure Synapse Analytics and Azure Machine Learning, with customers able to build end-to-end data warehouses, machine learning, and real-time analytics solutions. Users can request access to the Photon Preview by filling out a questionnaire.", "pub_date": "2020-09-28"},
{"title": "2 egregious cloud security threats the CSA missed", "overview": "The latest Cloud Security Alliance report highlights the ‘Egregious 11’ cloud security threats. Here are a couple more to consider", "image_url": null, "url": "https://www.infoworld.com/article/3583637/2-egregious-cloud-security-threats-the-csa-missed.html", "body": "My interesting weekend reading was this Cloud Security Alliance (CSA) report, which was vendor sponsored, highlighting 11 cloud security threats that should be on top of everyone’s mind. These threats are described as “egregious.”CSA surveyed 241 experts on security issues in the cloud industry and came up with these top 11 threats:Data breachesMisconfiguration and inadequate change controlLack of cloud security architecture and strategyInsufficient identity, credential, access, and key managementAccount hijackingInsider threatInsecure interfaces and APIsWeak control planeMetastructure and applistructure failuresLimited cloud usage visibilityAbuse and nefarious use of cloud servicesThis is a pretty good report, by the way. It’s free to download, and if you’re interested in the evolution of cloud computing security, it’s a good read.  Also on InfoWorld: Cloud tech certifications count more than degrees nowHowever, no report can be so comprehensive that it lists all threat patterns, or even derivatives to the threat patterns listed. I have a couple to add that I’m seeing over and over again.By the time attacks are identified they often do not look like attacks. Some tool watches something change over time, such as CPU and storage system saturation, and a non-security-focused ITops tool, such as an AIops tool, spots the issue. There needs to be a way for that alert to be shared with the cloud security system so it can take evasive action using automation.I’ve heard too many stories of attacks using any number of vectors that were discovered by an ITops tool and not by the security system. The reality is that security is systemic to all that is cloud, including usage and performance monitoring, governance systems, database monitoring, etc. Chances are these systems will pick up the shenanigans before the security system knows what’s going on. This is why the various systems need to be integrated and talk to each other. Most are not these days.Many in the cloud security space use the phrase “You never can be too secure.” Guess what? You can.As we get into the whole world of multifactor identification, passwords that have to change monthly, and encryption that hinders performance, we can make security a burden that costs way too much. What’s interesting is that the more complex the security systems, the less secure they seem to be. How is this the case?It comes down to human behavior. If cloud users are asked to change their passwords every month, guess what?  They just write the passwords down in digital memo systems, or I’ve seen them stuck to the screen using sticky notes. Moreover, I’ve seen people bypass encryption because it slows things down too much, even if there are compliance issues. Basically, humans will trade security for convenience or ease of doing their jobs.  The answers are not easy. Sure, you can be a jerk and come down on those violating security policies like a ton of bricks, but that will backfire as well.  The answer is to move to a more passive security plan. This means leveraging security solutions such as biometrics, where looking into a retinal scanner takes the place of frequently changed passwords. Also, encryption services can run on separate servers, thus reducing the impact on performance. Of course, we can go on for days identifying threats, either existing or emerging. The smarter approach is to look at your own cloud deployment rather than focusing on what others are calling “threats.”  ", "pub_date": "2020-09-29"},
{"title": "What is quantum computing? Solutions to impossible problems", "overview": "Quantum computing has great promise to solve problems that are too hard for classical computers to solve in reasonable amounts of time, but they are not yet practical", "image_url": null, "url": "https://www.infoworld.com/article/3574488/what-is-quantum-computing-solutions-to-impossible-problems.html", "body": "There’s no lack of hype in the computer industry, although even I have to admit that sometimes the technology does catch up to the promises. Machine learning is a good example. Machine learning has been hyped since the 1950s, and has finally become generally useful in the last decade.Quantum computing was proposed in the 1980s, but still isn’t practical, although that hasn’t dampened the hype. There are experimental quantum computers at a small number of research labs, and a few commercial quantum computers and quantum simulators produced by IBM and others, but even the commercial quantum computers still have low numbers of qubits (which I’ll explain in the next section), high decay rates, and significant amounts of noise.Also on InfoWorld: A hands-on look at Amazon Braket quantum computingQuantum computing explainedThe clearest explanation of quantum computing that I’ve found is in this video by Dr. Talia Gershon of IBM. In the video, Gershon explains quantum computing to a child, a teenager, a college student, and a graduate student, and then discusses quantum computing myths and challenges with Professor Steve Girvin from Yale University.To the child, she makes the analogy between bits and pennies. Classical bits are binary, like pennies lying on the table, showing either heads or tails. Quantum bits () are like pennies spinning on the table, which could eventually collapse into states that are either heads or tails.To the teenager, she uses the same analogy, but adds the word  to describe the states of a spinning penny. Superposition of states is a quantum property, commonly seen in elementary particles and in the electron clouds of atoms. In popular science, the usual analogy is the thought experiment of Schrödinger’s Cat, which exists in its box in a superposed quantum state of both alive and dead, until the box is open and it is observed to be one or the other.Gershon goes on to discuss quantum  with the teenager. This means that the states of two or more entangled quantum objects are linked, even if they are separated.By the way, Einstein hated this idea, which he dismissed as “spooky action at a distance,” but the phenomenon is real and observable experimentally, and has recently even been photographed. Even better, light entangled with quantum information has been sent over a 50-kilometer optical fiber.Finally, Gershon shows the teenager IBM’s quantum computer prototype with its dilution refrigerator, and discusses possible applications of quantum computers, such as modeling chemical bonds.With the college student, Gershon goes into more detail about the quantum computer, the quantum chip, and the dilution refrigerator that takes the temperature of the chip down to 10 mK (milliKelvin). Gershon also explains quantum entanglement in more detail, along with quantum superposition and interference. Constructive quantum interference is used in quantum computers to amplify signals leading to the right answer, and destructive quantum interference is used to cancel signals leading to the wrong answer. IBM makes qubits out of superconducting materials.With the grad student, Gershon discusses the possibility of using quantum computers to speed up key parts of the training of deep learning models. She also explains how IBM uses calibrated microwave pulses to manipulate and measure the quantum state (the qubits) of the computing chip.The principal algorithms for quantum computing (discussed below), which were developed before even one qubit had been demonstrated, assumed the availability of millions of perfect, fault-tolerant, error-corrected qubits. We currently have computers with 50 qubits, and they are not perfect. New algorithms under development are intended to work with the limited numbers of noisy qubits we have now.Steve Girvin, a theoretical physicist from Yale, tells Gershon about his work on fault-tolerant quantum computers, which don’t yet exist. The two of them discuss the frustration of quantum decoherence — “You can only keep your information quantum for so long” — and the essential sensitivity of quantum computers to noise from the simple act of being observed. They took a stab at the myths that in five years quantum computers will solve climate change, cancer, and <laughter>. Girvin: “We are currently at the vacuum tube or transistor stage of quantum computing, and we are struggling to invent quantum integrated circuits.”Quantum algorithmsAs Gershon mentioned in her video, the older quantum algorithms assume millions of perfect, fault-tolerant, error-corrected qubits, which are not yet available. Nevertheless, it’s worth discussing two of them to understand their promise and what countermeasures can be used to protect against their use in cryptographic attacks.Grover’s algorithm, devised by Lov Grover in 1996, finds the inverse of a function in O(√N) steps; it can also be used to search an unordered list. It provides a quadratic speedup over classical methods, which need O(N) steps.Other applications of Grover’s algorithm include estimating the mean and median of a set of numbers, solving the collision problem, and reverse-engineering cryptographic hash functions. Because of the cryptographic application, researchers sometimes suggest that symmetric key lengths be doubled to protect against future quantum attacks.Shor’s algorithm, devised by Peter Shor in 1994, finds the prime factors of an integer. It runs in polynomial time in log(N), making it exponentially faster than the classical general number field sieve. This exponential speedup promises to break public-key cryptography schemes, such as RSA, if there were quantum computers with “enough” qubits (the exact number would depend on the size of the integer being factored) in the absence of quantum noise and other quantum-decoherence phenomena.If quantum computers ever become large and reliable enough to run Shor’s algorithm successfully against the sort of large integers used in RSA encryption, then we would need new “post-quantum” cryptosystems that don’t depend on the difficulty of prime factorization.Quantum computing simulation at AtosAtos makes a quantum simulator, the Quantum Learning Machine, which acts as though it has 30 to 40 qubits. The hardware/software package includes a quantum assembly programming language and a Python-based high-level hybrid language. The device is in use at a few national labs and technical universities.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesQuantum annealing at D-WaveD-Wave makes quantum annealing systems such as the DW-2000Q, which are a little different and less useful than general-purpose quantum computers. The annealing process does optimization in a way that is similar to the stochastic gradient descent (SGD) algorithm popular for training deep learning neural networks, except that it allows for many simultaneous starting points and quantum tunneling through local hills. D-Wave computers cannot run quantum programs such as Shor’s algorithm.D-Wave claims that the DW-2000Q system has up to 2,048 qubits and 6,016 couplers. To reach this scale, it uses 128,000 Josephson junctions on a superconducting quantum processing chip, cooled to less than 15 mK by a helium dilution refrigerator. The D-Wave package includes a suite of open-source Python tools hosted on GitHub. The DW-2000Q is in use at a few national labs, defense contractors, and global enterprises.Quantum computing at Google AIGoogle AI is doing research on superconducting qubits with chip-based scalable architecture targeting two-qubit gate error < 0.5%, on quantum algorithms for modeling systems of interacting electrons with applications in chemistry and materials science, on hybrid quantum-classical solvers for approximate optimization, on a framework to implement a quantum neural network on near-term processors, and on quantum supremacy.In 2018 Google announced the creation of a 72-qubit superconducting chip called Bristlecone. Each qubit can connect with four nearest neighbors in the 2D array. According to Hartmut Neven, the director of Google’s Quantum Artificial Intelligence lab, quantum-computing power is increasing on a double-exponential curve, based on the number of conventional CPUs that the lab needs to replicate results from their quantum computers.In late-2019, Google announced that it had achieved quantum supremacy, the condition where quantum computers can solve problems that are intractable on classical computers, using a new 54-qubit processor named Sycamore. The Google AI Quantum team published the results of this quantum supremacy experiment in the  article, “Quantum Supremacy Using a Programmable Superconducting Processor.” Quantum computing at IBMIn the video that I discussed earlier, Dr. Gershon mentions that “There are three quantum computers sitting in this lab that  can use.” She is referring to IBM Q systems, which are built around transmon qubits, essentially niobium Josephson junctions configured to behave like artificial atoms, controlled by microwave pulses that fire microwave resonators on the quantum chip, which in turn address and couple to the qubits on the processor.IBM offers three ways to access its quantum computers and quantum simulators. For “anyone” there is the Qiskit SDK, and a hosted cloud version called the IBM Q Experience (see screenshot below), which also provides a graphical interface for designing and testing circuits. At the next level, as part of IBM Q Network, organizations (universities and large companies) are provided with access to IBM Q’s most advanced quantum computing systems and development tools.Qiskit supports Python 3.5 or later and runs on Ubuntu, macOS, and Windows. To submit a Qiskit program to one of IBM’s quantum computers or quantum simulators, you need IBM Q Experience credentials. Qiskit includes an algorithm and application library, Aqua, which provides algorithms such as Grover’s Search and applications for chemistry, AI, optimization, and finance.IBM unveiled a new generation of IBM Q system with 53 qubits in late-2019, as part of an expanded fleet of quantum computers in the new IBM Quantum Computation Center in New York State. These computers are available in the cloud to IBM’s over 150,000 registered users and nearly 80 commercial clients, academic institutions and research laboratories.IBM Q Experience home screen. Note that only the smaller IBM Q systems (5 to 14 qubits, plus a simulator) are currently available for general use. The larger ones are reserved for commercial clients.Quantum computing at IntelResearch at Intel Labs has led directly to the development of Tangle Lake, a superconducting quantum processor that incorporates 49 qubits in a package that is manufactured at Intel’s 300-millimeter fabrication facility in Hillsboro, Oregon. This device represents the third-generation of quantum processors produced by Intel, scaling upward from 17 qubits in its predecessor. Intel has sent Tangle Lake processors to QuTech in the Netherlands for testing and work on system-level design.Intel is also doing research on spin qubits, which function on the basis of the spin of a single electron in silicon, controlled by microwave pulses. Compared to superconducting qubits, spin qubits far more closely resemble existing semiconductor components operating in silicon, potentially taking advantage of existing fabrication techniques. Spin qubits are expected to remain coherent far longer than superconducting qubits, and to take much less space.Quantum computing at MicrosoftMicrosoft has been researching quantum computers for over 20 years. In the public announcement of Microsoft’s quantum computing effort in October 2017, Dr. Krysta Svore discussed several breakthroughs, including the use of topological qubits, the Q# programming language, and the Quantum Development Kit (QDK). Eventually, Microsoft quantum computers will be available as co-processors in the Azure cloud.The topological qubits take the form of superconducting nanowires. In this scheme, parts of the electron can be separated, creating an increased level of protection for the information stored in the physical qubit. This is a form of topological protection known as a Majorana quasi-particle. The Majorana quasi-particle, a weird fermion that acts as its own anti-particle, was predicted in 1937 and was detected for the first time in the Microsoft Quantum lab in the Netherlands in 2012. The topological qubit provides a better foundation than Josephson junctions since it has lower error rates, reducing the ratio of physical qubits to logical, error-corrected qubits. With this reduced ratio, more logical qubits are able to fit inside the dilution refrigerator, creating the ability to scale.Microsoft has variously estimated that one topological Majorana qubit is worth between 10 and 1,000 Josephson junction qubits in terms of error-corrected logical qubits. As an aside, Ettore Majorana, the Italian theoretical physicist who predicted the quasi-particle based on a wave equation, disappeared in unknown circumstances during a boat trip from Palermo to Naples on March 25, 1938.", "pub_date": "2020-09-09"},
{"title": "Overcoming the core hurdle to edge computing adoption", "overview": "Enterprises are figuring out that edge computing comes with its own set of challenges. Here’s how to work through the most difficult. ", "image_url": null, "url": "https://www.infoworld.com/article/3600452/overcoming-the-core-hurdle-to-edge-computing-adoption.html", "body": "Edge computing is picking up steam. According to this recent report by Turbonomic (requires registration), nearly 50 percent of organizations use or plan to use edge computing in the next 18 months.For those of you watching this market, many existing development projects listed as “edge computing” barely qualify for the title. Still, considering the state of edge computing just a few years ago, this is a huge leap in growth.Also on InfoWorld: Amazon, Google, and Microsoft take their clouds to the edgeThe factors that drive enterprise movement to the edge include:Edge-based solutions in the public cloud. In essence, these are pared down, private cloud versions of public clouds, such as AWS Outpost and Microsoft Stack. They often serve as a jumping off point from legacy systems to public clouds—like a public cloud with training wheels.IoT-based projects. Data storage and compute that’s closer to the edge of the network and to the source of the data provides better performance because less data is sent back to the centralized public cloud server.Edge computing architectures. This architecture involves more substantial and traditional servers, such as traditional storage and compute servers housed in specific offices or branches. Consider a restaurant chain that needs to place storage and compute at all locations but also wants to use a centrally managed paradigm.What stops the forward progress? No surprise here: It’s managing complexity without added cost and risk. According to the Turbonomic report: “Complexity, at 39 percent, is overwhelmingly considered the leading barrier to edge computing becoming conventional.” Complexity is almost double the second-place and third-place barriers: security (23 percent) and technology limitations in network/bandwidth throughput (22 percent). If this survey had been done a few years ago, I suspect that security and technology limitations would have been in the top two spots. What happened? In short, actual edge computing projects took the place of conceptual ones, with as many as 20 to 30 percent failing outright due to the inability to manage complexity.It isn't easy to manage widely distributed systems. There are challenges around configuration management, patching and software updates, CI/CD (continuous integration/continuous delivery), acceptance testing, distributed data storage, and security operations within edge-based implementations. This list is only a fraction of the complexity issues that must be managed at the edge.For now, these problems are difficult but not impossible to manage. There are any number of AIops, governance, and configuration management tools for cloud computing; however, very few tools are focused at the edge.Why? It’s difficult to nail down a repeatable approach and a technology stack for edge solutions. Edge-based systems can include almost any hardware and software, with a wide range of capabilities and limitations. In contrast, developers can depend on the consistencies of public cloud platforms.Edge computing will need sound, repeatable approaches to mediate complexity, as well as tools that provide consistent ways to approach the problem. We are just not there yet.", "pub_date": "2020-12-08"},
{"title": "Using OPA for multicloud policy and process portability", "overview": "How Open Policy Agent allows developer teams to write and enforce consistent policy and authorization across multicloud and hybrid cloud environments", "image_url": null, "url": "https://www.infoworld.com/article/3596599/using-opa-for-multicloud-policy-and-process-portability.html", "body": "As multicloud strategies become fully mainstream, companies and dev teams are having to figure out how to create consistent approaches among cloud environments. Multicloud, itself, is ubiquitous: Among companies in the cloud, a full 93% have multicloud strategies—meaning they use more than one public cloud vendor like Amazon Web Services, Google Cloud Platform, or Microsoft Azure. Furthermore, 87% or those companies have a hybrid cloud strategy, mixing public cloud and on-premises cloud environments.The primary reason that companies move to the cloud at all is to improve the performance, availability, scalability, and cost-effectiveness of compute, storage, network, and database functions. Then, organizations adopt a multicloud strategy largely to avoid vendor lock-in.OPA: A general-purpose policy engine for cloud-nativeUsing OPA to safeguard KubernetesUsing OPA for cloud-native app authorizationBut multicloud also presents a second alluring possibility, an extension of that original cloud-native logic: the ability to abstract cloud computing architectures so they can port automatically and seamlessly (if not just quickly) between cloud providers to maximize performance, availability, and cost savings—or at least maintain uptime if one cloud vendor happens to goes down. Cloud-agnostic platforms like Kubernetes, which run the same in any environment—whether that’s AWS, GCP, Azure, private cloud, or wherever—offer a tantalizing glimpse of how companies could achieve this kind of multicloud portability.But while elegant in theory, multicloud portability is complicated in practice. Dependencies like vendor-specific features, APIs, and difficult-to-port data lakes make true application and workload portability a complicated journey. In practice, multicloud portability only really works—and works well—when organizations achieve consistency across cloud environments. For that, businesses need a level of policy abstraction that works across said vendors, clouds, APIs, and so on—enabling them to easily port skills, people, and processes across the cloud-native business. While individual applications may not always port seamlessly between clouds, the organization’s overall approach should.Using OPA to create consistent policy and processes across cloudsOne of the tools that has become popular, precisely because it’s domain agnostic, is Open Policy Agent (OPA). Developed by Styra and donated to the Cloud Native Computing Foundation, OPA is an open-source policy engine that lets developer teams build, scale, and enforce consistent, context-aware policy and authorization across the cloud-native realm. Because OPA lets teams write and enforce policies across any number of environments, at any number of enforcement points—for cloud infrastructure, Kubernetes, microservices APIs, databases, service meshes, application authorization, and much more—it allows organizations to take a portable approach to policy enforcement across multicloud and hybrid cloud environments.Moreover, as a policy-as-code tool, OPA enables organizations to take the policies that are otherwise in company wikis and people’s heads and codify them into machine-processable policy libraries. Policy as code not only lets organizations automatically enforce policy in any number of clouds, but also shift left and inject policies upstream, closer to the development teams who are working across clouds, in order to catch and prevent security, operational, and compliance risk sooner.Pairing OPA with Terraform and KubernetesAs one example, many developers now use OPA in tandem with infrastructure-as-code (IaC) tools like Terraform and AWS CDK. Developers use IaC tools to make declarative changes to their vendor-hosted cloud infrastructure—describing the desired state of how they want their infrastructure configured, and letting Terraform figure out which changes need to be made. Developers then use OPA, a policy-as-code tool, to write policies that validate the changes that Terraform suggests and test for misconfigurations or other problems, before they are applied to production.At the same time, OPA can automatically approve routine infrastructure changes to cut down on the need for manual peer review (and the potential for human error that comes with it). This creates a vital safety net and sanity check for developers, and allows them to experiment risk-free with different configurations. While the cloud infrastructure itself is not portable between vendors, the approach is, by design.In a similar way, developers also use OPA to control, secure, and operationalize Kubernetes across clouds, and even across various Kubernetes distributions. Kubernetes has become a standard for deploying, scaling, and managing fleets of containerized applications. Just as Kubernetes is portable, so, too, are the OPA policies that you run on top of it.There are many Kubernetes use cases for OPA. One popular use case, for example, is to use OPA as a Kubernetes admission controller to ensure containers are deployed correctly, with appropriate configuration and permissions. Developers can also use OPA to control Kubernetes ingress and egress decisions, for example writing policies that prohibit ingresses with conflicting hostnames to ensure that applications never steal each other’s internet traffic. Most important for the multicloud cloud, perhaps, is the ability to ensure that each Kubernetes distribution, across clouds, is provably in compliance with enterprise-wide corporate security policies.Creating standard cloud-native building blocksBefore companies can port applications seamlessly across public clouds, they must first create standard building blocks for developers across every cloud-native environment. Along these lines, developers not only use OPA to create policy, but to automate the enforcement of security, compliance, and operations standards across the CI/CD pipeline. This enables repeatable scale for any multicloud deployment, while speeding development and reducing manual errors.Also on InfoWorld: Devops for remote engineers and distributed teamsOPA’s enabling of policy as code means that companies can use tools like Terraform for their public clouds and OPA for policy, Kubernetes for container management and OPA for policy, plus any number of microservices API and app authorization tools and OPA for policy, while running those same OPA policies in the CI/CI pipeline, or on the laptops of developers.In short, organizations need not waste any time reverse-engineering applications for multicloud portability. Instead, they can focus on building a repeatable process, using common skills, across the entire cloud-native stack.Open Policy AgentStyra—newtechforum@infoworld.com", "pub_date": "2020-12-09"},
{"title": "A foolproof process to move and improve cloud data", "overview": "Moral of the story: Data migration is never simple, cloud or not. ", "image_url": null, "url": "https://www.infoworld.com/article/3600792/a-foolproof-process-to-move-and-improve-cloud-data.html", "body": "I’m often asked, “How can I relocate data to the cloud, and improve the databases, applications, security, governance, and dataops as the migration occurs?” Everyone is looking for a shortcut or a magical tool that will migrate and automatically improve the state of the data. Sorry, that magic does not yet exist. In the meantime, a nonmagical migration process provides the best odds of success. Before we explore that process, I’ll mention a few things:  Also on InfoWorld: Microsoft Azure cloud migration: 3 success storiesFirst, cloud relocation does not use a waterfall approach. Certain tasks need to be completed to move on to the next tasks, but not all. These dependences will be readily apparent, but feel free to do any of the tasks below out of sequence. Second, to get this right the first time, follow the process outlined below with the correct mix of talent. You’ll need subject matter experts for databases, security, ops, governance, cloud-specific services, etc. Those people are difficult to find right now. Finally, this is a general approach. You will need to add or remove some items. For instance, if you’re a health care company, you need to deal with more compliance and governance issues around the use, migration, and deployment of data. With all that said, here’s the process:Assess the “as is” state of the data, including models (object, relational, in memory, special purpose, or other), metadata, application coupling, and requirements (security, governance, business continuity/disaster recovery, and management). Tagging starts here. Look for opportunities to reduce redundancy and increase efficiency. This can be as impactful as moving from one model to another (relational to object) which requires a great deal of application refactoring, normalization of all data schemas, defining a single source of truth, etc. You need to consider security, governance, and data ops as well, which are redundant to everything listed here, just to be clear.Define the “to be” state with the changes and requirements defined above. One of the paths I recommend is the development of a CDM (common metadata model). A CDM, at its essence, provides a single source of truth for most and sometimes all of the data that exists in an enterprise. It’s made up of many different databases that may use different database models, such as relational and object, and many different structures or schemas. However, it appears to all who use the CDM as a single, unified, abstract database that, when asked a question, provides a common and consistent answer. Define a migration and implementation plan, focusing on the target cloud platforms. The devil is in the details, and some changes still need to be made in flight, but minor ones.Create a staging and testing platform for applications and databases. This may also include CI/CD (continuous integration/continuous delivery) links. Moving forward, they should be maintained by devsecops teams, as well as DBAs. Make a plan for that maintenance.Test deployment on the staging and testing platforms to determine performance, security, governance, fit-to-purpose, etc. Repeat for each application and database. Testing will help determine cost of ops and provide the data to project those costs for the next several years. Now that real cost metrics exist, this is easy. Projections also help avoid sticker shock when you get your first cloud bill. Implement cost governance. Define ops planning, including monitoring and management approaches, playbooks, and tools. Take advantage of abstraction and automation to remove humans from the ops processes as much as possible. Begin phased deployments, starting with the smallest and least important databases and applications, progressing to the biggest and most important. Try to deploy with flexible deadlines. Don’t worry, you’ll get better at it as you learn. Rushing this portion of the process is where failure typically occurs because important tasks are tossed out in favor of meeting an arbitrary deadline.Execute acceptance testing after each phase.Begin dataops.Take a vacation.On average, this process takes three weeks for each database. If you have 100 databases to migrate, realistically speaking, it will take about 42 to 52 weeks to complete. The move-and-improve processes are not magical or automatic, but they can be baked into the migration. Good luck.", "pub_date": "2020-12-11"},
{"title": "Boosting AI’s smarts in the absence of training data", "overview": "Zero-shot learning repurposes knowledge through statistical or semantic approaches without needing huge amounts of fresh training data", "image_url": null, "url": "https://www.infoworld.com/article/3529871/boosting-ais-smarts-in-the-absence-of-training-data.html", "body": "AI (artificial intelligence) is the most perfect field of dreams in modern culture. If you ask the average person on the street what AI runs on, they probably won’t mention training data. Instead, they might mumble something about computer programs that magically learn how to do useful stuff from thin air.However, some of today’s most sophisticated AI comes close to that naïve dream. I’m referring to a still-developing approach known as “zero-shot learning.” This methodology—which is being explored at Microsoft, Uber, Baidu, Alibaba, and other AI-driven businesses—enables useful pattern recognition with little or no training data.Also on InfoWorld: What AI can really do for your business (and what it can’t)Zero-shot pattern learning will enable intelligent robots to dynamically recognize and respond to unfamiliar objects, behaviors, and environmental patterns that they may never have encountered in training. I predict that zero-shot approaches will increasingly be combined with reinforcement learning in order to enable robots to take the best actions iteratively in environments that are chaotic and one-off.In addition, gaming applications will use zero-shot approaches such as iterative self-play as an alternative to training on voluminous data derived from successful gameplay. This will enable the training of agents to master complex winning strategies in spite of knowing nothing about these games at the outset.Furthermore, zero-shot learning promises to make object recognition applications more versatile, due to its ability to drive:on-the-fly recognition of rare, unfamiliar, and unseen objects that may be substantially missing from training data.recognition of patterns for which it is hard to obtain training data that has been labeled with a sufficiently high degree of expert knowledge.detection of instances of object classes where the proliferation of fine-grained categories has made it difficult or prohibitively expensive to acquire sufficient quantities of statistically diverse, labeled training data.What makes zero-shot learning possible is the existence of prior knowledge that can be discovered and repurposed through statistical or semantic approaches. Zero-shot methods use this knowledge to predict the larger semantic space of features that encompasses both the seen instances (those in the training data) and the unseen instances (those missing from training data). Regarding automated knowledge discovery, some of the most promising technical approaches for zero-shot learning include:Building classification models from statistical knowledge that was gained in prior supervised learning projects that are in distinct but semantically adjacent object-recognition domains (identifying a never-seen class of vertebrate based on features extracted from a related species).Extracting semantic knowledge of target objects from textual descriptions of the targeted classes (crawled Web articles that describe the visual features of the species to be recognized).Using word vectors and other graph approaches to refine inferences of the semantic features of the target classes from those of the source classes, given the availability of textual descriptions of the target classes.Zero-shot learning can’t realize its potential as an AI pipeline accelerator unless data scientists acquire tools that provides simplified access to these techniques. That, in turn, requires deep learning toolkits that support easy visual design of new models from pre-existing functional building blocks under a larger paradigm known as “transfer learning.” This depends on workbenches that provide data scientists with reuseable feature representations, neural-node layerings, weights, training methods, learning rates, and other relevant features of prior models that can be quickly brought into zero-shot AI projects.As zero-shot techniques gain adoption, and the pool of prior knowledge grows, developers of high-quality AI will grow less reliant on training data. During the next few years, we’ll see data scientists build more intelligent robotics, gaming, and pattern-recognition applications by configuring pre-existing statistical and semantic knowledge, without needing to acquire, prepare, and label huge amounts of fresh training data.When that day arrives, more AI-based applications will be able to automate the bootstrapping of their intelligence from a state of pure ignorance to one of deep knowledge through techniques that are ad-hoc, zero-shot, and situationally adaptive.That will mark the true beginning of artificial general intelligence, a dream that has motivated the AI community from the days of Alan Turing all the way to the present.", "pub_date": "2020-02-27"},
{"title": "Interested in machine learning? Better learn PyTorch", "overview": "Don’t look now, but easy, straightforward PyTorch has become the hottest product in data science", "image_url": null, "url": "https://www.infoworld.com/article/3519411/interested-in-machine-learning-better-learn-pytorch.html", "body": "Building on the rampant popularity of Python was always going to be a good idea for the Facebook-born PyTorch, an open source machine learning framework. Just how good of an idea, however, few could have guessed. That’s because no matter how many things you get right when launching an open source project (great docs, solid technical foundation, etc.), there is always an element of luck to a project’s success.Machine learning with Python: An introductionWell, consider PyTorch lucky, then. Or blessed. Or something. Because it’s booming and, if analyst Thomas Dinsmore is to be believed, “By the end of [2020] PyTorch will have more active contributors than TensorFlow.” More contributors and more adoption? That’s a big jump for a rival to TensorFlow, long considered the industry default since its public release in 2015.Also on InfoWorld: Artificial intelligence predictions for 2020Wild and crazy adoptionAs detailed in OpenHub, TensorFlow and PyTorch are running neck-and-neck in terms of 12-month contributor totals: TensorFlow (906) and PyTorch (900). This represents huge progress by the PyTorch community, given TensorFlow’s head start, and is reflected in the growth in PyTorch’s user community, as reflected in Jeff Hale’s analysis of job posting sites for data scientist roles:To be clear, this analysis reflects  growth or decline over the past year. The TensorFlow user community is still much larger than PyTorch’s, though in academics PyTorch has gone from distant minority to overwhelming majority almost overnight. All things considered, it’s not hard to see PyTorch quickly bridging the gap at this pace.Particularly given PyTorch’s comparative advantages. Did I mention Python?Lowering the bar to data scienceAs Serdar Yegulalp wrote back in 2017 at the launch of PyTorch, “A chief advantage to PyTorch is that it lives in and allows the developer to plug into the vast ecosystem of Python libraries and software. Python programmers are also encouraged to use the styles they’re familiar with, rather than write code specifically meant to be a wrapper for an external C/C++ library.” This means that PyTorch has always had the advantage of approachability. The documentation is excellent and there’s a healthy community of developers happy to help out.This advantage is further accentuated by PyTorch’s computational graph setup. As Savan Visalpara explains:TensorFlow is ‘Define-and-Run,’ whereas PyTorch is ‘Define-by-Run.’ In [a] Define-and-Run framework, one would define conditions and iterations in the graph structure then run it. In [a] Define-by-Run [framework, the] graph structure is defined on-the-fly during forward computation, [which is a more] natural way of coding.Dhiraj Kumar concurs, arguing that such a dynamic model allows data scientists to “fully see each and every computation and know exactly what is going on.”To be sure, with the release of TensorFlow 2.0, Google has made TensorFlow “eager by default.” As Martin Heller explains, “Eager execution means that TensorFlow code runs when it is defined, as opposed to adding nodes and edges to a graph to be run in a session later, which was TensorFlow’s original mode.”Also on InfoWorld: PyTorch vs. TensorFlow: How to chooseWhile this sounds great for TensorFlow because it helps the framework compete better with PyTorch in terms of ease of use, “In enabling Eager mode by default, TensorFlow forces a choice onto their users — use eager execution for ease of use and require a rewrite for deployment, or don’t use eager execution at all.While this is the same situation that PyTorch is in, the opt-in nature of PyTorch’s TorchScript is likely to be more palatable than TensorFlow’s ‘Eager by default,’” warns Horace He. TensorFlow Eager mode also suffers from performance issues, though we’d expect these to improve over time.In sum, while the market still leans heavily on TensorFlow, PyTorch’s easy-to-learn, straightforward-to-use approach that ties into the world’s most popular programming language for data science is proving a winner. Although academia has been fastest to embrace PyTorch, we should expect to see ever-increasing adoption with the enterprise set, too.", "pub_date": "2020-01-30"},
{"title": "InfoWorld’s 2020 Technology of the Year Award winners", "overview": null, "image_url": null, "url": "https://www.infoworld.com/article/3518995/infoworlds-2020-technology-of-the-year-award-winners.html", "body": "", "pub_date": "2020-02-05"},
{"title": "Microsoft speeds up PyTorch with DeepSpeed", "overview": "A new open source project from Microsoft accelerates machine learning framework PyTorch without needing major code rewrites", "image_url": null, "url": "https://www.infoworld.com/article/3526449/microsoft-speeds-up-pytorch-with-deepspeed.html", "body": "Microsoft has released DeepSpeed, a new deep learning optimization library for PyTorch, that is designed to reduce memory use and train models with better parallelism on existing hardware.According to a Microsoft Research blog post announcing the new framework, DeepSpeed improves PyTorch model training through a memory optimization technology that increases the number of possible parameters a model can be trained with, makes better use of the memory local to the GPU, and requires only minimal changes to an existing PyTorch application to be useful.Also on InfoWorld: Artificial intelligence predictions for 2020It’s the minimal impact on existing PyTorch code that has the greatest potential impact. As machine learning libraries grow entrenched, and more applications become dependent on them, there is less room for new frameworks, and more incentive to make existing frameworks more performant and scalable.PyTorch is already fast when it comes to both computational and development speed, but there’s always room for improvement. Applications written for PyTorch can make use of DeepSpeed with only minimal changes to the code; there’s no need to start from scratch with another framework.One way DeepSpeed enhances PyTorch is by improving its native parallelism. In one example, provided by Microsoft in the DeepSpeed documentation, attempting to train a model using PyTorch’s Distributed Data Parallel system across Nvidia V100 GPUs with 32GB of device memory “[ran] out of memory with 1.5 billion parameter models,” while DeepSpeed was able to reach 6 billion parameters on the same hardware.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesAnother touted DeepSpeed improvement is more efficient use of GPU memory for training. By partitioning the model training across GPUs, DeepSpeed allows the needed data to be kept close at hand, reduces the memory requirements of each GPU, and reduces the communication overhead between GPUs.A third benefit is allowing for more parameters during model training to improve prediction accuracy. Hyperparameter optimization, which refers to tuning the parameters or variables of the training process itself, can improve the accuracy of a model but typically at the cost of manual effort and expertise.To eliminate the need for expertise and human effort, many machine learning frameworks now support some kind of automated hyperparameter optimization. With DeepSpeed, Microsoft claims that “deep learning models with 100 billion parameters” can be trained on “the current generation of GPU clusters at three to five times the throughput of the current best system.”Also on InfoWorld: Why you should use Python for machine learningDeepSpeed is available as free open source under the MIT License. Tutorials in the official repo work with Microsoft Azure, but Azure is not required to use DeepSpeed.", "pub_date": "2020-02-10"},
{"title": "InfoWorld Technology of the Year Awards", "overview": "About InfoWorld’s annual awards for the best hardware, software, and cloud services of the year", "image_url": null, "url": "https://www.infoworld.com/article/2871319/infoworld-technology-of-the-year.html", "body": "InfoWorld’s annual Technology of the Year Awards recognize the best and most innovative products in the areas of software development, cloud computing, big data analytics, and machine learning.Note that we do not have a formal submission process for the Technology of the Year Awards. Winners are chosen based on our coverage of products throughout the year. However, we welcome suggestions regarding products we should consider for coverage and for awards at any time. To have your hardware, software, or cloud service considered for a 2019 Technology of the Year Award, please send the name of the product and a link to product information to Executive Editor Doug Dineley before September 30, 2020.The 2020 Technology of the Year Award winners will be announced February 3, 2021.Previous Technology of the Year Award winners2020 Technology of the Year Awards2019 Technology of the Year Awards2018 Technology of the Year Awards2017 Technology of the Year Awards2016 Technology of the Year Awards2015 Technology of the Year Awards2014 Technology of the Year Awards2013 Technology of the Year Awards2012 Technology of the Year Awards2011 Technology of the Year Awards2010 Technology of the Year AwardsMarketing/PR:  Congratulations on winning a Technology of the Year award! Look here for all the information you’ll need to tell the world.", "pub_date": "2020-02-11"},
{"title": "Swift language targets machine learning", "overview": "Swift 6 roadmap also emphasizes expressive and elegant APIs and ‘fantastic’ development experience", "image_url": null, "url": "https://www.infoworld.com/article/3526594/swift-language-targets-machine-learning.html", "body": "Moving toward Swift 6, the core development team behind Apple’s Swift programming language has set priorities including refining the language for use in machine learning.Ambitions in the machine learning space are part of plans to invest in “user-empowering directions” for the language. Apple is not the only company with machine learning ambitions for Swift; Google has integrated Swift with the TensorFlow machine learning library in a project called Swift for TensorFlow. And the Swift community has created Swift Numerics, a library that can be used for machine learning.Also on InfoWorld: Artificial intelligence predictions for 2020In addition to machine learning, directions eyed for Swift include building APIs such as variadic generics and DSL capabilities such as function builders. Solutions for major language features such as memory ownership and concurrency also are part of the plan. Other specific goals for Swift, cited in a January 2020 bulletin, include:Creating a “fantastic development experience,” with developers able to be highly productive and joyful when programming in the language. These investments include faster builds, better diagnostics, responsive code completion, and reliable debugging. Most current engineering work in the project covers these areas.Growing the Swift software ecosystem, including expanding the number of supported platforms and improving how software written in Swift is deployed. Also planned is support for cross-platform tools such as Language Server Protocol, the Swift Package Manager, code formatting, and refactoring. Cultivation of a rich open source library ecosystem also is eyed.Keep up with hot topics in software development with InfoWorld’s App Dev Report newsletterIntroduced in June 2014, Swift has been rising steadily in the Tiobe index of programming language popularity, jumping from 20th place a year ago to 10th place in the February 2020 index. Its predecessor, Objective-C, has done the reverse, dropping from 10th a year ago to 20th this month. The release currently in development is Swift 5.2. A succession of Swift 5.x releases are expected before Swift 6.", "pub_date": "2020-02-11"},
{"title": "InfoWorld Technology of the Year Awards promotional information", "overview": "Guidelines for InfoWorld Technology of the Year Award winners", "image_url": null, "url": "https://www.infoworld.com/article/2871225/infoworld-technology-of-the-year-awards-promotional-information.html", "body": "Congratulations! To help you promote your win, InfoWorld has supplied the following PR information and usage guidelines.Note that companies named as an InfoWorld Technology of the Year Award winner may be referenced in press outreach to publicize the awards and InfoWorld’s content about the awards. InfoWorld will not disclose proprietary corporate information, but it may highlight information included by your organization in the original InfoWorld Technology of the Year Awards content.If you would like a physical award, or print or electronic reprints, you can place an order through the YGS Group via phone at (800) 290-5460 x129 or via email at infoworld@theYGSgroup.com. Note that YGS solely determines the cost of the awards’ production.InfoWorld logo usage and guidelinesAs an InfoWorld Technology of the Year winner, you have the opportunity to purchase a license for the rights to use the Technology of the Year logo. Please contact the YGS Group via phone at (800) 290-5460 x129 or via email at infoworld@theYGSgroup.com.The InfoWorld Technology of the Year brand is a valuable asset that International Data Group needs to protect. We ask that you help us by properly using the logo in accordance with our guidelines listed below. Accordingly, we ask that your business partners, customers, and other third parties adhere to the guidelines listed below.Parties given permission to use the InfoWorld Technology of the Year logotype must adhere to the following rules:The logotype may not be altered in any manner, including size, proportions, colors, elements, type, or in any other respect. You may not animate, morph, or otherwise distort its perspective or dimensional appearance.The logotype may not be combined with any other graphic or textural elements and may not be used as a design element of any other logo or trademark.The logotype must be separated from your company name and product names by the space of one logo width or one inch, whichever is greatest.If you are unsure if your usage is within these guidelines, please email Stacey Raap in IDG Communications Marketing.PR opportunitiesPlease read the following guidelines carefully before preparing a press release that references InfoWorld and the InfoWorld Technology of the Year rankings.If your company wants to include a quote attributed to an InfoWorld spokesperson, please use the following quote to reinforce InfoWorld Technology of the Year key messages. Modified versions of this quote are subject to approval from InfoWorld. All quotes should be attributed to Doug Dineley, Executive Editor, InfoWorld.“If digital transformation means anything, it means taking advantage of the latest advances in software development, cloud computing, data analytics, and AI to improve your business,” said Doug Dineley, executive editor of InfoWorld. “Our 2020 Technology of the Year Award winners are the platforms and tools that the most innovative companies are using to tap the power of data, streamline business processes, and respond more quickly to customers and new business opportunities.” All communications involving InfoWorld and the InfoWorld Technology of the Year Awards must be consistent with the style and content listed below:The full feature name is the InfoWorld Technology of the Year Awards.InfoWorld is always one word, and the “w” is capitalized.On first reference, list the publication as “IDG’s InfoWorld.” InfoWorld as a stand-alone name may be used after the first reference.If a subsidiary of an organization is included in the rankings, press releases or marketing material should specify that the unit—not the parent organization—received the award. If the parent unit is cited, the name of the subsidiary unit should be more prominent in placement, size, and usage than that of the parent unit.The following approved InfoWorld corporate boilerplate and short InfoWorld Technology of the Year Awards description may be used, where appropriate, in press releases referencing inclusion in the InfoWorld Technology of the Year feature.Selected by InfoWorld editors and reviewers, the annual awards identify the best and most innovative products on the IT landscape. Winners are drawn from products tested during the past year, with the final selections made by InfoWorld’s Reviews staff.   InfoWorld is the leading resource for content and tools on modernizing enterprise IT. Our editors and writers provide first-hand experience from testing, deploying, and managing implementation of emerging enterprise technologies. InfoWorld’s website (InfoWorld.com) and custom solutions provide a deep dive into specific technologies to help IT decision-makers excel in their roles and provide opportunities for IT vendors to reach this audience. InfoWorld is published by IDG Communications, a subsidiary of International Data Group (IDG), the world’s leading media, events, and research company. Company information is available at www.idg.com.IDG Communications, an International Data Group (IDG) company, brings together the leading editorial brands (CIO, Computerworld, CSO, InfoWorld, JavaWorld, and Network World) to serve the information needs of our technology and security-focused audiences. As the premier high-tech B2B media company, we leverage the strengths of our premium owned and operated brands, while simultaneously harnessing their collective reach and audience affinity. We provide market leadership and converged marketing solutions for our customers to engage IT and security decision-makers across our portfolio of award-winning websites, events, magazines, products, and services. Company information is available at www.idg.com.", "pub_date": "2020-02-11"},
{"title": "8 great Python libraries for natural language processing", "overview": "With so many NLP resources in Python, how to choose? Discover the best Python libraries for analyzing text and how to use them", "image_url": null, "url": "https://www.infoworld.com/article/3519413/8-great-python-libraries-for-natural-language-processing.html", "body": "Natural language processing, or NLP for short, is best described as “AI for speech and text.” The magic behind voice commands, speech and text translation, sentiment analysis, text summarization, and many other linguistic applications and analyses, natural language processing has been improved dramatically through deep learning.The Python language provides a convenient front-end to all varieties of machine learning including NLP. In fact, there is an embarrassment of NLP riches to choose from in the Python ecosystem. In this article we’ll explore each of the NLP libraries available for Python—their use cases, their strengths, their weaknesses, and their general level of popularity.Also on InfoWorld: 6 great new Python features you don’t want to missNote that some of these libraries provide higher-level versions of the same functionality exposed by others, making that functionality easier to use at the cost of some precision or performance. You’ll want to choose a library well-suited both to your level of expertise and to the nature of the project.CoreNLPThe CoreNLP library — a product of Stanford University — was built to be a production-ready natural language processing solution, capable of delivering NLP predictions and analyses at scale. CoreNLP is written in Java, but multiple Python packages and APIs are available for it, including a native Python NLP library called StanfordNLP.CoreNLP includes a broad range of language tools—grammar tagging, named entity recognition, parsing, sentiment analysis, and plenty more. It was designed to be human language agnostic, and currently supports Arabic, Chinese, French, German, and Spanish in addition to English (with Russian, Swedish, and Danish support available from third parties). CoreNLP also includes a web API server, a convenient way to serve predictions without too much additional work.The easiest place to start with CoreNLP’s Python wrappers is StanfordNLP, the reference implementation created by the Stanford NLP Group. In addition to being well-documented, StanfordNLP is also maintained regularly; many of the other Python libraries for CoreNLP have not been updated in some time.CoreNLP also supports the use of NLTK, a major Python NLP library discussed below. As of version 3.2.3, NLTK includes interfaces to CoreNLP in its parser. Just be sure to use the correct API.The obvious downside of CoreNLP is that you’ll need some familiarity with Java to get it up and running, but that’s nothing a careful reading of the documentation can’t achieve. Another hurdle could be CoreNLP’s licensing. The whole toolkit is licensed under the GPLv3, meaning any use in proprietary software that you distribute to others will require a commercial license.GensimGensim does just two things, but does them exceedingly well. Its focus is statistical semantics—analyzing documents for their structure, then scoring other documents based on their similarity.Gensim can work with very large bodies of text by streaming documents to its analysis engine and performing unsupervised learning on them incrementally. It can create multiple types of models, each suited to different scenarios: Word2Vec, Doc2Vec, FastText, and Latent Dirichlet Allocation.Gensim’s detailed documentation includes tutorials and how-to guides that explain key concepts and illustrate them with hands-on examples. Common recipes are also available on the Gensim GitHub repo.Don’t miss InfoWorld’s 2020 Technology of the Year Award winners: The best software development, cloud computing, data analytics, and machine learning products of the yearNLTKThe Natural Language Toolkit, or NLTK for short, is among the best-known and most powerful of the Python natural language processing libraries. Many corpora (data sets) and trained models are available to use with NLTK out of the box, so you can start experimenting with NLTK right away.As the documentation states, NLTK provides a wide variety of tools for working with text: “classification, tokenization, stemming, tagging, parsing, and semantic reasoning.” It can also work with some third-party tools to enhance its functionality.Keep in mind that NLTK was created by and for an academic research audience. It was not designed to serve NLP models in a production environment. The documentation is also somewhat sparse; even the how-tos are thin. Also, there is no 64-bit binary; you’ll need to install the 32-bit edition of Python to use it. Finally, NLTK is not the fastest library either, but it can be sped up with parallel processing.If you are determined to leverage what’s inside NLTK, you might start instead with TextBlob (discussed below).PatternIf all you need to do is scrape a popular website and analyze what you find, reach for Pattern. This natural language processing library is far smaller and narrower than other libraries covered here, but that also means it’s focused on doing one common job really well.Pattern comes with built-ins for scraping a number of popular web services and sources (Google, Wikipedia, Twitter, Facebook, generic RSS, etc.), all of which are available as Python modules (e.g., ). You don’t have to reinvent the wheels for getting data from those sites, with all of their individual quirks. You can then perform a variety of common NLP operations on the data, such as sentiment analysis.Pattern exposes some of its lower-level functionality, allowing you to to use NLP functions, n-gram search, vectors, and graphs directly if you like. It also has a built-in helper library for working with common databases (MySQL, SQLite, and MongoDB in the future), making it easy to work with tabular data stored from previous sessions or obtained from third parties.Also on InfoWorld: Artificial intelligence predictions for 2020PolyglotPolyglot, as the name implies, enables natural language processing applications that deal with multiple languages at once.  The NLP features in Polyglot echo what’s found in other NLP libraries: tokenization, named entity recognition, part-of-speech tagging, sentiment analysis, word embeddings, etc. For each of these operations, Polyglot provides models that work with the needed languages.Note that Polyglot’s language support differs greatly from feature to feature. For instance, the tokenization system supports almost 200 languages (largely because it uses the Unicode Text Segmentation algorithm), and sentiment analysis supports 136 languages, but part-of-speech tagging supports only 16.PyNLPIPyNLPI (pronounced “pineapple”) has only a basic roster of natural language processing functions, but it has some truly useful data-conversion and data-processsing features for NLP data formats.Most of the NLP functions in PyNLPI are for basic jobs like tokenization or n-gram extraction, along with some statistical functions useful in NLP like Levenshtein distance between strings or Markov chains. Those functions are implemented in pure Python for convenience, so they’re unlikely to have production-level performance.But PyNLPI shines for working with some of the more exotic data types and formats that have sprung up in the NLP space. PyNLPI can read and process GIZA, Moses++, SoNaR, Taggerdata, and TiMBL data formats, and devotes an entire module to working with FoLiA, the XML document format used to annotate language resources like corpora (bodies of text used for translation or other analysis). You’ll want to reach for PyNLPI whenever you’re dealing with those data types.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesSpaCySpaCy, which taps Python for convenience and Cython for speed, is billed as “industrial-strength natural language processing.” Its creators claim it compares favorably to NLTK, CoreNLP, and other competitors in terms of speed, model size, and accuracy. SpaCy’s main drawback is it’s relatively new, so it only covers English and a few other (chiefly European) languages. That said, SpaCy has already reached version 2.2 as of this writing.SpaCy includes most every feature found in those competing frameworks: speech tagging, dependency parsing, named entity recognition, tokenization, sentence segmentation, rule-based match operations, word vectors, and tons more. SpaCy also includes optimizations for GPU operations—both for accelerating computation, and for storing data on the GPU to avoid copying.Spacy’s documentation is excellent. A setup wizard generates command-line installation actions for Windows, Linux, and macOS and for different Python environments (pip, conda, etc.) as well. Language models install as Python packages, so they can be tracked as part of an application’s dependency list.TextBlobTextBlob is a friendly front-end to the Pattern and NLTK libraries, wrapping both of those libraries in high-level, easy-to-use interfaces. With TextBlob, you spend less time struggling with the intricacies of Pattern and NLTK and more time getting results.TextBlob smooths the way by leveraging native Python objects and syntax. The quickstart examples show how texts to be processed are simply treated as strings, and common NLP methods like part-of-speech tagging are available as methods on those string objects.Keep up with hot topics in software development with InfoWorld’s App Dev Report newsletterAnother advantage of TextBlob is you can “lift the hood” and alter its functionality as you grow more confident. Many default components, like the sentiment analysis system or the tokenizer, can be swapped out as needed. You can also create high-level objects that combine components—this sentiment analyzer, that classifier, etc.—and re-use them with minimal effort. This way, you can prototype something quickly with TextBlob, then refine it later.", "pub_date": "2020-02-12"},
{"title": "Amazon Braket: Get started with quantum computing", "overview": "Amazon’s quantum computing service is currently good for learning about quantum computing and developing NISQ-regime quantum algorithms, but stay tuned", "image_url": null, "url": "https://www.infoworld.com/article/3573372/amazon-braket-get-started-with-quantum-computing.html", "body": "While IBM, Microsoft, and Google have made major commitments and investments in quantum computing, Amazon has, until recently, been fairly quiet about the field. That changed with the introduction of Amazon Braket.Amazon still isn’t trying to build its own quantum computers, but with Braket it is making other companies’ quantum computers available to cloud users via AWS. Braket currently supports three quantum computing services, from D-Wave, IonQ, and Rigetti.Also on InfoWorld: A hands-on look at the Microsoft Quantum Development Kit and IBM Q and Qiskit quantum computing SDKsD-Wave makes superconducting quantum annealers, which are usually programmed using D-Wave Ocean software, although there is also an annealing module in the Braket SDK. IonQ makes trapped ion quantum processors, and Rigetti makes superconducting quantum processors. In Braket, you can program both IonQ and Rigetti processors using the Braket Python SDK circuits module. The same code also runs on local and hosted quantum simulators.The name Braket is kind of an in-joke for physicists. Bra-ket notation is the Dirac formulation of quantum mechanics, which is an easier way of expressing Schrödinger’s equation than partial differential equations. In Dirac notation, a bra  is a row vector, and a ket  is a column vector. Writing a bra next to a ket implies matrix multiplication.Amazon Braket and the Braket Python SDK compete with IBM Q and Qiskit, Azure Quantum and Microsoft Q#, and Google Cirq. IBM already has its own quantum computers and simulators available to the public online. Microsoft’s simulator is generally available, but its quantum offerings are currently in limited preview for early adopters, including access to quantum computers from Honeywell, IonQ, and Quantum Circuits, and optimization solutions from 1QBit. Microsoft hasn’t announced when its own topological superconducting quantum computers will become available, nor has Google announced when it will make its quantum computers or Sycamore chips available to the public.Amazon Braket overviewAmazon Braket is a fully managed service that helps you get started with quantum computing. It has three modules, Build, Test, and Run. The Build module centers around managed Jupyter notebooks pre-configured with sample algorithms, resources, and developer tools, including the Amazon Braket SDK. The Test module provides access to managed, high-performance, quantum circuit simulators. The Run module provides secure, on-demand access to different types of quantum computers (QPUs): gate-based quantum computers from IonQ and Rigetti, and a quantum annealer from D-Wave.Tasks may not run immediately on the QPU. The QPUs only execute tasks during execution windows.Amazon Braket SDK APIThe Braket Python SDK defines all of the operations you need to build, test, and run quantum circuits and annealers. It is organized into five packages: braket.annealing, braket.aws, braket.circuits, braket.devices, and braket.tasks.The braket.annealing package allows you to define two kinds of binary quadratic models (BQMs): Ising (a mathematical model of ferromagnetism in statistical mechanics, using magnetic dipole moments of atomic “spins”) and QUBO (Quadratic Unconstrained Binary Optimization) problems, to solve on a quantum annealer, such as a D-Wave unit. The braket.circuits package lets you define quantum circuits based on a set of gates, to solve on gate-based quantum computers, such as ones from IonQ and Rigetti.The other three packages control the running of your problem. The braket.aws package allows you to select quantum devices, load problems into tasks, and connect tasks to AWS sessions. The braket.devices package lets you run tasks on quantum devices and simulators. The braket.tasks package lets you manage, track, cancel, and get results from quantum tasks.Amazon Braket circuits and gatesCircuits in a quantum computer such as the ones from IonQ or Rigetti (or IBM or Honeywell, for that matter) are built from a standard set of gates (see figure below), although not every QPU may have an implementation of every kind of gate. In the Braket SDK you define a circuit using the  method from the braket.circuits package, qualified by the gates in the circuit and their parameters.For example, this Braket code (from Amazon’s Deep_dive_into_the_anatomy_of_quantum_circuits example) defines a circuit that initializes four qubits to a Hadamard (equal probability of 1 and 0) state, then entangles qubit 2 with qubit 0 and qubit 3 with qubit 1 using Controlled Not operations.The Braket SDK seems to have a nearly full set of quantum logic gates, as shown in this enumeration of the  class. I don’t see a Deutsch gate listed, but as far as I know it hasn’t yet been implemented on a real QPU.Rxtreme(CC BY-SA 4.0)D-Wave OceanOcean is the native Python-based software stack for D-Wave quantum annealers. For use via Braket, you can combine the Ocean software with the Amazon Braket Ocean plug-in, which translates between Ocean and Braket formats.Also on InfoWorld: Cloud tech certifications count more than degrees nowQuantum annealers function quite differently than gate-based QPUs. Essentially, you formulate your problem as a binary quadratic model (BQM) that has a global minimum at the solution you want to find. Then you use the annealer to sample the function many times (since the annealer isn’t perfect) to find the minimum. You can create the BQM for a given problem mathematically or generate the BQM using Ocean software. The code that follows, from Amazon’s D-Wave_Anatomy example, uses the Braket Ocean plug-in to solve a BQM on a D-Wave device.D-Wave SystemsTo use a quantum annealer, you need to formulate your problem as a quadratic objective function, or BQM (binary quadratic model). Then you sample the solution space to find the global minimum of the BQM.Enabling Amazon Braket and using notebooksBefore you can use Braket, you need to enable it in your AWS account.The Anatomy of Quantum Circuits example, one of the simpler tutorials, demonstrates running circuits on a simulator and controlling tasks.Then you need to create a notebook instance. Notebooks use Amazon SageMaker (read my review).Creating a Braket notebook instance is a matter of naming the notebook, picking an instance type, and either creating or reusing an IAM role with the correct permissions. Using an encryption key and VPC are both optional.When you open a notebook, you can enter new code or use one of Amazon’s examples.The Braket examples supplied by Amazon are divided into four categories: simple circuits, advanced circuits, hybrid, and annealing.You need to check the status of the QPU devices, as they aren’t always available.The Devices screen shows you the current status of each quantum processing unit. An hour after this screenshot, the D-Wave annealer was online and available, but the Rigetti QPU had a two-hour queue.While you can run them yourself, the Braket example notebooks have been saved with results from a previous run.The Anatomy of Quantum Circuits example, one of the simpler Braket tutorials, demonstrates running circuits on a simulator and controlling tasks.There are examples for both gate-based QPUs, as above, and quantum annealers, as below.The D-Wave anatomy example demonstrates how to perform quantum annealing, both with a simulator and with a D-Wave device. Block 20 in the example uses the D-Wave device, takes 1,000 samples, and performs an aggregate operation to simplify the output.Learn today, useful tomorrowAmazon Braket is a reasonable way to get your feet wet with quantum computers and simulators. Since we’re still in the NISQ (Noisy Intermediate Scale Quantum) phase of quantum computing, you can’t really expect useful results from Braket. We’ll need more qubits, less noise, and longer coherence times, all of which are being actively researched.Braket’s current QPU offerings are modest. The 2048-qubit D-Wave annealer is mostly useful for optimization problems; it’s about half the size of D-Wave’s latest-generation annealer. The 11-qubit IonQ QPU, which has relatively long coherence times, is  too small to implement the algorithms for quantum computers that should exhibit useful quantum supremacy, such as Grover’s algorithm for finding the inverse of a function and Shor’s algorithm for finding the prime factors of an integer. The 30-qubit Rigetti Aspen-8 is also too small.Braket is not free, although it is relatively cheap to use. By comparison, IBM Q is completely free, although the publicly available IBM QPUs are very small: they range from a 1 qubit QPU in Armonk to a 15-qubit QPU in Melbourne. IBM also offers a paid premium QPU service.Also on InfoWorld: Review: Amazon SageMaker plays catch-upIBM also rates its QPUs by their quantum volume (QV), a measure that combines the number of qubits with their error rate and coherence time. There are five-qubit IBM QPUs ranging from QV8 to QV64: higher is better. Honeywell has also announced achieving QV64.What Braket is currently good for is learning about quantum computing and developing NISQ-regime quantum algorithms. Stay tuned, though. As QPUs improve and are plugged into AWS, Braket will become more and more useful.— Managed notebooks: $0.04 to $34.27 per instance-hour; quantum simulator: $4.50 per hour; quantum computers: $0.30 per task plus $0.00019 to $0.01 per shot (repetition of a circuit).  AWS; installing the Braket SDK locally requires Python 3.7.2 or greater, and Git.", "pub_date": "2020-09-07"},
{"title": "Cloud migration gets harder", "overview": "Now that we’ve migrated the easy stuff, the level of difficulty is increasing—but it still needs to be done", "image_url": null, "url": "https://www.infoworld.com/article/3572373/cloud-migration-gets-harder.html", "body": "The chart below depicts the number of applications migrated over time in blue, and the degree of difficulty of moving those applications in orange. This is a fictional collection of applications; however, the concept that difficulty increases the more you migrate affects enterprises large and small as they move to the public cloud.  See this chart: What’s occurring is easy to explain, but the solution to the problem is not. Also on InfoWorld: Microsoft Azure cloud migration: 3 success storiesSimply put, the applications and databases that are more modern, better designed, and built to be portable are the first to relocate to the clouds. This for good reason: The cloud teams want to get some wins on the scoreboard and can do so by removing risk and reducing the degree of difficulty. This is actually my advice as enterprises begin their journey.Predictably, as the number of well-designed and modern applications that can be easily migrated decreases, migration teams are forced to face applications that are not as well designed or are built on older platforms that may not have an analog platform yet in a public cloud. Legacy applications come to mind, but also any application that needs significant refactoring to get it running correctly on a public cloud. As many of the enterprises approach the middle of the migration process, the degree of difficulty increases significantly, as shown in the chart, and it’s killing migration productivity. So, how does an enterprise that’s already freaked out about the vulnerabilities exposed in the pandemic get the remaining applications out of the enterprise data center?Here are two approaches that seem to be working: Chances are, if you’re focused on a public cloud as a primary target for your workloads, you may now be considering some valid alternatives. MSPs offer more platform analogs for the more difficult-to-replicate platforms and provide an A-to-A migration path to move to a public cloud.If you think that this is a cop-out, considering that you’re going to need to fix or modernize those applications and databases at some point, you’re right. But using an MSP will accelerate the downsizing of the existing physical data center faster. Once you’ve migrated to an MSP, you can focus on fixing or sunsetting applications on that quasi-cloud platform. From there, you can leave them or ultimately migrate them to a pubic cloud. You’ll have to do the heavy lifting up front, but it’s really the right way to do things. In essence, it’s an application migration factory optimized using automation. Too many enterprises fix each application as a one-off. That won’t scale or be consistent.  Of course, there are other approaches, including doing nothing and just fighting through it, degree of difficulty be dammed. I would encourage those enterprises to be a bit more open-minded to other solutions that may be difficult but ultimately remove problems moving forward. That’s a no brainer for me.          ", "pub_date": "2020-08-25"},
{"title": "Gitpod open-sources cloud IDE platform", "overview": "Gitpod applies CI/CD to development environments, allowing them to be maintained as code ", "image_url": null, "url": "https://www.infoworld.com/article/3572561/gitpod-open-sources-cloud-ide-platform.html", "body": "Development environment technology provider Gitpod has open-sourced its self-named cloud-based IDE platform for automatically spinning up ready-to-code development environments.The open-sourcing will allow the Gitpod community to participate in the technology’s development and make it easier for developers to integrate Gitpod into their workflows, the company said.Also on InfoWorld: 5 reasons we don’t write code like we used toA Kubernetes application, Gitpod allows developers to maintain development environments as code, turning manual steps into a machine-executable part of a project’s source code. The platform monitors changes in the repository, and preps development environments for every change. This preparation includes:Setting up tools.Checking out the correct Git branch.Compiling code.Downloading dependencies.Initializing whatever is needed.Developer workflows are streamlined, with teams able to build applications quicker, the company said. Coding can begin from a branch, issue, or merge or pull request, applying CI/CD concepts to development environments. Gitpod works with code-hosting platforms including GitLab, GitHub Enterprise, and Bitbucket.Benefits of Gitpod cited by the company include:Shorter lead times, with reductions in time it takes to switch contexts and maintain development environments.Elimination of “configuration drift,” with the GitOps approach embraced through versioning of configuration in the Git repository. This ensures consistent, reproducible development environments.Enabling remote collaboration, with developers able to work on code reviews, mentoring, and sharing snapshots of work. Gitpod is available under an Affero GPL license on GitHub. The technology was architected by Sven Efftinge, who co-created the Eclipse Theia IDE development platform.", "pub_date": "2020-08-25"},
{"title": "Why you should look beyond the big 3 cloud providers", "overview": "There are appealing enterprise options beyond AWS, Azure, and GCP, but buyer beware — not just any cloud service qualifies", "image_url": null, "url": "https://www.infoworld.com/article/3572388/why-you-should-look-beyond-the-big-3-cloud-providers.html", "body": "The cloud has, for the most part, become a commodity. Most enterprise cloud workloads require a small variety of hardware. After all, how many organizations are actually doing quantum computing or even AI training workloads in the cloud? The fact is, the majority of businesses simply need core cloud services: compute, block and object storage, CI/CD and testing environments, backup and failover, and low-risk options for hybrid cloud or multicloud deployments.Also on InfoWorld: 6 ways Alibaba Cloud challenges AWS, Azure, and GCPThat’s why technology analysts have recognized “alternative cloud providers” as a legitimate and growing segment of the cloud services market, clearly distinct from the Big Three hyperscale public cloud providers—Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP). 451 Research, for example, in its 2020 Trends in Managed Services and Hosting Report observes:Hyperscale public clouds make up a significant majority of public cloud market revenue. However, there is still room for alternative cloud providers to add value as enterprises increasingly adopt multicloud strategies, and smaller companies opt for services that intentionally skew away from the complexity of hyperscale cloud.For the first time in the 12-year history of cloud computing, companies have legitimate and arguably better-suited choices in providers. We’re talking about alternative cloud providers like DigitalOcean, Linode, and Vultr, who are now in many ways on par with (and, in the case of price performance, outperform) the Big Three mega clouds.An alternative cloud provider must have an extensive global network capable of equipping their customers with the following:The means to physically locate data close to users to reduce or eliminate lag times in data, media, and website page loads.The means to comply with laws in various countries regarding user privacy requirements, specifically in geographic user data storage mandates, even in recovery failovers.The ability to scale instantly and with little fuss.A significant layer of security protections.Disaster recovery and loss prevention capabilities via failover and backup redundancies.The means to “pay by the sip,” which means according to actual cloud usage. Further cost savings are delivered through the reduction of data center capex costs and IT payroll costs.Automatic updates, upgrades, patching, and other security, maintenance, and service updates so that users are always using the latest and greatest versions of hardware, firmware, middleware, and software.Analytics and dashboards that can give you control and insights over your data.Worldwide data centersOne of the myths about hyperscalers is that they offer superior networks and data center hardware. For all intents and purposes, there is no difference between the hardware on the racks of alternative cloud providers and what AWS has on its racks. The compute power is basically identical.If you’re wondering about global reach and scalability, rest assured that alternative cloud providers have got you covered. Granted, alternative cloud providers are not big enough to directly compete with AWS, Azure, or GCP, and they have no designs to. But they have extensive global networks that deliver ubiquitous availability via world-class data centers placed strategically around the world.The resulting global network is capable of blanketing users, scaling massive workloads, complying with variations in regional laws, and providing instant recoveries in flawlessly orchestrated failovers. Today, the global networks established by alternative cloud providers are already scaling to meet the needs of Fortune 50 companies, so scaling to support the workloads of small and medium-size businesses (SMBs) is not an issue.Enterprise-grade computing servicesCertainly, some people choose to pay for AWS, Azure, or GCP to have access to the hundreds of products they offer, just in case you need them. In reality, though, how many companies really need computer vision, speech understanding, super-advanced networking, and highly optimized database queries, for example? In reality, the percentage of workloads that need any “special sauce” from AWS or the other hyperscale cloud providers is relatively low.Alternative cloud providers can provide scale and quality of infrastructure needed for most enterprise workloads. The overall market has commoditized around basic services, what analysts call “core primitives” — things like servers, databases, bare metal, Kubernetes, and other core infrastructure components. Alternative cloud providers focus on delivering these core primitives rather than offering hundreds of proprietary services that, once engaged, lock you in. (Further, most of those special services are available through third parties, many of them open source, so it’s pretty easy to replicate everything that you would do on AWS, usually for much cheaper and without the lock-in.)Developers and other users find the reduction in the immense complexity encountered in using top-tier public clouds to be a significant advantage. According to 451 Research, the message of simplicity from alternative cloud providers “resonates with developers who need to deploy applications quickly, don’t need access to the full suite of advanced cloud functions, and prioritize price, performance, and access to a level of technical support.” The same holds true for SMBs, which typically have a small technical staff who would rather focus on getting work done than navigating the pitfalls and distractions of too many bells and whistles.An undeniable cost advantage The bottom line about why you should use alternative cloud providers is… your bottom line.By focusing on core services, alternative cloud providers offer developers and SMBs a legitimate alternative: typically the same or better performance at a much better price. In most cases, you can save over half of your infrastructure costs by moving from a hyperscaler to an alternative cloud provider. Why wouldn’t you want the option that gives you all the performance you need at half the cost?Also on InfoWorld: PaaS, CaaS, or FaaS? How to chooseIf you are ready to consider your alternatives, start by making sure the companies you consider meet the network qualifications of alternative cloud providers (see sidebar). Then dig a little deeper. Not all alternative cloud providers are the same. You’ll want to choose an alternative cloud provider who also offers a great track record, exceptional service, transparent and affordable pricing, and no lock-in. Most importantly, choose a cloud provider that makes your cloud computing experience simple and accessible — what we all dreamed cloud computing could be.Linode—newtechforum@infoworld.com", "pub_date": "2020-08-26"},
{"title": "Cloud is going to take time", "overview": "Cloud spend will need to grow many more years before it becomes even a meaningful fraction of total IT spending", "image_url": null, "url": "https://www.infoworld.com/article/3572407/cloud-is-going-to-take-time.html", "body": "$233.4 billion. That’s how much analyst firm IDC said the world spent on cloud (SaaS, IaaS, PaaS) in 2019. It sounds like a lot of money, right? According to a quick Internet search, $233 billion is how much China plans to spend on defense. It’s how much Bill Gates could save by “reinventing the toilet.” And it’s how much the meeting industry expects to lose due to the pandemic curtailing travel.But $233.4 billion is a rounding error compared to the $4 trillion that organizations will spend on IT in 2020, according to IDC. Just 5.8 percent. The fixation on cloud revenues only serves to distort the reality of how organizations spend their IT budgets today.Also on InfoWorld: 6 ways Alibaba Cloud challenges AWS, Azure, and GCPBut growth!Sure, if we look at where the growth in IT is, cloud is hot. (Note: IDC includes Devices, Infrastructure (server/storage/network hardware and cloud services), Software, and IT services in its IT spending number.) As IDC vice president Stephen Minton recently said,Where there is growth, most of it is in the cloud. Overall software spending is now expected to decline as businesses delay new projects and application roll-outs…. On the other hand, the amount of data that companies must store and manage is not going anywhere. Increasingly, even more of that data will be stored, managed, and increasingly also analysed in the cloud.Let’s take China as an example.On Alibaba’s recent earnings call, executive vice-chair Joe Tsai pointed out that China’s total cloud market is still relatively small compared to the western markets, just $15 to $20 billion. Even so, that’s a bigger share of the total Chinese IT market ($297 billion in 2020, according to IDC) than the global percentage.As for growth, as Tsai went on to project, “The China market is going to be a much faster-growing market in cloud than the U.S. market.” At Alibaba’s roughly $7 billion annual run rate, coupled with 59 percent annual growth in the most recent quarter, there’s a long way to go to catch up to IDC’s other prediction: 90 percent of applications in China will be cloud-native by 2025. If the Chinese market gets anywhere near that number, that represents impressive growth, indeed.And frictionBut most economies don’t function like China’s, with its five-year plans. Nor do most companies, though we’ve seen the coronavirus pandemic kickstart a torrid era of digital transformation. As CircleCI CEO Jim Rose told me a few months back, “The pandemic has compressed the time that companies are taking to get to CI/CD [and cloud]. Everything we forecasted for the next year is now happening in the next three months.”Writing about steps organizations should take to get ahead of pandemic-induced restrictions on employee movement and customer spend, InfoWorld’s David Linthicum notes a few to-do’s:Move rapidly to public cloud-based resources, including IaaS and SaaS.Eliminate data centers, either owned or leased.Reduce or eliminate facilities to a functional minimum.Change corporate culture to support remote collaboration.Some of these are easier said than done. For example, companies like GitLab that have always had a remote-first policy found it much easier to adapt to work-from-home requirements than peers used to an office culture. You can give everyone a camera and headset, but enabling people to be productive in a work-from-home environment requires cultural change, and cultural change takes time.Hurry up and waitAs such, even as we rightly expect to see a hastening toward more cloud adoption over the next few months (and years), we would be wrong to think we’ll magically go from $233 billion to $4 trillion by 2025. Don’t bet on it.Also on InfoWorld: Kubernetes meets the real world: 3 success storiesBut also don’t bet on the fanciful notion that workloads are going to “repatriate” from cloud to on-premises data centers. There are all sorts of good reasons to pick that fever dream apart (analyst Corey Quinn highlights a few of them), but here’s perhaps the biggest: organizations are slow to change. Even the “hastening” I mentioned above about digital transformation will take years. It’s simply not the case that companies will move to cloud (and then back to their data centers) on a whim. That’s not how the real world works.Don’t believe me? Let’s rewind to the beginning of this article. Yes, cloud is a big deal... but it’s still just 5.8 percent of total IT spending. Over time, I suspect we’ll see those percentages flip, with the 5.8 percent number relating to on-premises workloads, not cloud. But that’s not going to happen next year. Or the year after that. Change takes time.", "pub_date": "2020-08-26"},
{"title": "How to find cloud talent during a boom", "overview": "The pandemic has led to an explosion of cloud projects. How do you hire in an environment where 10 job reqs are chasing a single qualified candidate?", "image_url": null, "url": "https://www.infoworld.com/article/3572323/how-to-find-cloud-talent-during-a-boom.html", "body": "We made the call from the start: COVID-19 would spike the need for cloud computing and cloud computing talent. IDC’s latest Worldwide Quarterly Cloud IT Infrastructure Tracker noted an increase in cloud spending with traditional infrastructure taking a dirt nap. The pandemic was the primary driver behind the shift in IT spending, with IDC noting that widespread remote work triggered demand for enterprise cloud-based services.Of course, that was last quarter. The outlook for the remainder of the year is explosive cloud growth with funding and acceleration of cloud projects underway right now or about to begin. What currently hinders an enterprise’s movement to the cloud is the lack of cloud talent, including architects, security specialists, developers, operations, and secops engineers, to name just a few. Also on InfoWorld: 9 career pitfalls every software developer should avoidSo, how do you find talent in a seller’s market? Here are a few creative ideas:The days of driving to an office or moving because of a job should be pretty much over. Unless the employee needs to be physically present (and I can’t think of an example when it comes to cloud computing), an employee should be able to work from anywhere with a reliable Internet connection.The pandemic helped some companies, such as online sellers, and hurt others, such as the aviation industry. Companies that have cut staff, including those with cloud skills, should be targeted by recruiters. Hire someone with IT skills but few or no cloud skills. Provide the training needed to take on a cloud-related role. I like this approach because you need continuous learners anyway in the cloud space, and hiring someone who can quickly learn and adjust is typically more valuable than limiting the hunt to a candidate who already has the right certifications. This may seem old school, but in the days when I had to hire a lot of people and had no budget for recruiters or advertising, I would host meetings for special interest groups in my office. Many of these groups already exist today; you can either start a new group or join an existing one. They are always looking for volunteers to provide meeting space. Of course, there are more traditional HR tricks to try. You’ll probably find one or two approaches that work best for your situation. Stick with what works to gather the talent you need. This is a time when the recruiting process can keep an enterprise on track or make it fail fast. ", "pub_date": "2020-08-28"},
{"title": "Do developers really care about open source?", "overview": "If all a developer really wants is an API that’s open and a cost model that works, then maybe cloud is the best answer", "image_url": null, "url": "https://www.infoworld.com/article/3572324/do-developers-really-care-about-open-source.html", "body": "FaunaDB founder Evan Weaver has a crazy thought. Even as open source projects like Linux and Kubernetes continue to thrive, he suggests that maybe, just maybe, “As long as you give developers an API that’s open and a cost model that works for them then they don’t care [about open source]. They just don’t want to operate anything.” Cloud, in other words, not code.It’s a bold thought, and not an unreasonable one. When I floated this idea with a range of industry heavyweights, however, they pushed back for a number of different reasons. Among them? Well, according to Guarav Gupta, an investor with Lightspeed (and former Elastic and Splunk product executive), “There is a deep amount of developer love and appreciation and almost like an addiction” for open source, something developers don’t feel for an API.Is there a way to have the convenience of APIs without losing developers’ sense of belonging for open source communities? The answer seems to be yes, but it’s a bit complicated to get there.Don’t forget the dataFor Aurélien Georget, co-founder and chief product officer at Strapi, which offers an open source headless CMS, one of the enduring draws of open source isn’t really about code, though sometimes that’s simply a necessity. For example, in Strapi’s case many customers want to heavily customize their CMS. In this instance, a cloud service doesn’t meet their needs. They need the code.Or even if they don’t want to tinker with the code, data drives them to it: “Our users aren’t interested in ownership of their code but of their data. For data privacy reasons, [or] sometimes from a legal point of view (e.g. banks, insurances, public administration, etc.),” they need to run their code — and keep their data — within their own data center. This isn’t to suggest that Weaver is wrong to insist on an API-centric approach, however: “Every solution should be API-oriented,” Georget concurs, as “It lets the developers be creative, imagine new use cases, and innovate.”Even so, Georget acknowledges, “Being independent and having 100 percent ownership of our data has a price.” It would perhaps be more convenient to use a cloud service but this simply isn’t always possible for certain classes of application or customer, he says.And then there’s the possibility, argues Patrick McFadin, chief evangelist at DataStax, which contributes code and operational expertise to the Apache Cassandra database community, that APIs can become a one-way door: “APIs that used to be open are getting locked down over time or put behind a paywall. The war for data is only going to get worse and the trend will be heavily proprietary.” Code, by contrast, can be given away because it’s “not the business” of most enterprises, including software vendors.Ben Bromhead, CTO at Instaclustr, might have the ideal compromise. Instaclustr runs open source software like Apache Kafka as a managed service. So long as a company builds with cloud services that closely adhere to open source standards, they never really lose their independence of code or data:Open source data-layer technologies guarantee companies full control over their own data and processes. By choosing 100 percent open source technologies, companies own their own code and maintain freedom from vendor or technical lock-in. But, more important than the code itself, true open source technologies ensure that companies’ critical information supply lines cannot be disrupted by the whims of entities that provide proprietary solutions, and might interfere with their ability to fully leverage their own data no matter what.In other words, perhaps it needn’t be a cloud  open source decision, but rather a cloud  open source choice.It’s about trustWhich brings us back to Gupta’s contention that there’s something different, and perhaps better, about open source. Even as Gupta acknowledges that “Ultimately people want to consume things as a [cloud] service,” he suggests that code is critical for forging real affinity — even affection — for technology. “There is a deep amount of developer love and appreciation and almost like an addiction to [an open source] product that you develop by being able to feel it and understand it and be part of a community.”Can you achieve this “cosmic closeness” with an API? Not really, Gupta argues. Communities, he says, are “a lot easier to build if you’re open source,” rather than a “black box cloud service with an API. People like Twilio, [but] do they love it?” He doesn’t answer his rhetorical question, but it seems to demand an unequivocal, “No!” Because, as Gupta goes on to suggest, “As a developer, you want to be part of a movement.”At the heart of such an open source movement is trust: open source, open roadmaps, open communication, open decision-making. This is where cloud vendors like Instaclustr arguably can successfully give customers what they want: the ease of cloud and the trust and control of open source. “Good open source companies build incredible amounts of trust,” says Gupta. Wise cloud companies do the same by not forfeiting the trust and affection born within open source communities.Open source has a people problemThe community hat rules the company hat in open sourceWhat is Google’s Open Usage Commons — and why?What does an open source maintainer do after burnout?Do we need so many databases?How GraphQL turned web development on its headHow Redis scratched an itch — and changed databases foreverWill the solo open source developer survive the pandemic?Open source projects take all kindsThe most important part of an open source projectGatsby JS stands on the shoulders of thousandsRemember when open source was fun?Open source made the cloud in its imageZeek and Jitsi: 2 open source projects we need nowWTH? OSS knows how to WFH IRLThe secret to Kubernetes’ successOpen source companies are thriving in the cloudOpen source should learn from Linux, not MySQLMaking open source JavaScript payCustomer-driven open source is the future of softwareMagical Magento figures out how to make and takeThe real number of open source developersWhy the Rust language is on the riseShould open source licenses fight evil?Should open source software advertise?Why open source has never been stronger", "pub_date": "2020-08-31"},
{"title": "9 ways to build privacy into your cloud applications", "overview": "Look to these strategies to balance privacy with functionality and protect your applications and data against attacks in the cloud", "image_url": null, "url": "https://www.infoworld.com/article/3572600/9-ways-to-build-privacy-into-your-cloud-applications.html", "body": "Privacy is one of those nebulous ideas that everyone loves. Delivering it, though, is a job that’s full of nuance and tradeoffs. Turn the dial too far to one side and the databases are useless. Turn it too far in the other direction and everyone is upset about your plan to install camera arrays in their shower to automatically reorder soap.The good news is that there is a dial to turn. In the early days, everyone assumed that there was just a switch. One position delivered all of the wonderful magic of email, online ordering, and smartphones. The other position was the cash-only world of living off the grid in a cabin wearing an aluminum foil hat.Also on InfoWorld: 6 ways Alibaba Cloud challenges AWS, Azure, and GCPPrivacy enhancing technologies let you control how much privacy to support but limit that control to preserve functionality. They mix in encryption functions with clever algorithms to build databases that can answer some questions correctly — but only for the right people.In my book, Translucent Databases, I explored building a babysitter scheduling service that could let parents book babysitters without storing personal information in the central database. The parents and babysitters could get the correct answer from the database, but any attacker or insider with root privileges would get only scrambled noise.The field has grown dramatically over the years and there are now a number of approaches and strategies that do a good job of protecting many facets of our personal lives. They store just enough information for businesses to deliver products while avoiding some of the obvious dangers that can appear if hackers or insiders gain access.The approaches all have their limits. They will defend against the most general attacks but some start to crumble if the attackers are better equipped or the attacks are more targeted. Often the amount of protection is proportional to the amount of computation power required for the encryption calculations. Basic protections may not add noticeable extra load to the system, but providing perfect security may be out of reach for even the cloud companies.But these limits shouldn’t stop us from adding the basic protections. The perfectly secure approach may not be out there, but adding some of these simpler solutions can protect everyone against some of the worst attacks that can be enabled by the new cloud services.Here are nine strategies for balancing privacy with functionality. Use the featuresThe cloud providers understand that customers are nervous about security and they’ve slowly added features that make it easier to lock up your data. Amazon, for instance, offers more than two dozen products that help add security. The AWS Firewall Manager helps make sure the firewalls let in only the right packets. AWS Macie will scan your data looking for sensitive data that’s too open. Google Cloud and Microsoft Azure have their own collections of security tools. Understanding all of these products may take a team but it’s the best place to start securing your cloud work.Watch the secretsSecuring the passwords, encryption keys, and authentication parameters is hard enough when we’re just locking down our desktops. It’s much trickier with cloud machines, especially when they’re managed by a team. A variety of different tools are designed to help. You’ve still got to be careful with source code management, but the tools will help juggle the secrets so they can be added to the cloud machines safely. Tools like Hashicorp’s Vault,  Doppler’s Enclave,  AWS’s Key Management System, and Okta’s API management tools are just some of the options that simplify the process. All still require some care but they are better than writing down passwords in a little notebook and locking it in someone’s office.Consider dedicated hardwareIt’s hard to know how paranoid to be about sharing computer hardware with others. It’s hard to believe that an attacker may finagle a way to share the right machine and then exploit some of the different extreme approaches like rowhammer, but some data might be worth the hard work. The cloud companies offer dedicated hardware just for occasions like this. If your computing load is fairly constant, it may even make economic sense to use local servers in your own building. Some embrace the cloud company’s hybrid tools and others want to set up their own machines. In any case, taking complete control of a computer is more expensive than sharing, but it rules out many attacks.HashingOne of the simplest solutions is to use a one-way function to hide personal information. These mathematical functions are designed to be easy to compute but practically impossible to reverse. If you replace someone’s name with , someone browsing the database will only see the random encrypted noise that comes out of the one-way function.This data may be inscrutable to casual browsers, but it can still be useful. If you want to search for Bob’s records, you can compute  and use this scrambled value in your query. This approach is secure against casual browsers who may find an interesting row in a database and try to unscramble the value of . It won’t stop targeted browsing by attackers who know they are looking for Bob. More sophisticated approaches can add more layers of protection.The most common one-way functions may be the Secure Hash Algorithm or SHA, a collection of functions approved by the US National Institute of Standards and Technology. There are several different versions, and some weaknesses have been found in the earlier versions, so make sure you use a new one. Also on InfoWorld: 17 clever APIs for every developer whimPure encryptionGood encryption functions are built into many layers of the operating system and file system. Activating them is a good way to add some basic security against low-level attackers and people who might gain physical access to your device. If you’re storing data on your laptop, keeping it encrypted saves some of the worry if you lose the machine.Regular encryption functions, though, are not one-way. There’s a way to unscramble the data. Choosing regular encryption is often unavoidable because you’re planning on using the data, but it leaves another pathway for the attackers. If you can apply the right key to unscramble the data, they can find a copy of that key and deploy it too.  Make sure you read the section above about guarding secrets.Fake dataWhile some complain about “fake news” corrupting the world, fake data has the potential to protect us. Instead of opening up the real data set to partners or insiders who need to use it for projects like AI training or planning, some developers are creating fake versions of the data that have many of the same statistical properties.RTI, for instance, created a fake version of the US Census complete with more than 110 million households holding more than 300 million people. There’s no personal information of real Americans but the 300 million fake people are more or less in the same parts of the country and their personal details are pretty close to the real information. Researchers predicting the path of infectious diseases were able to study the US without access to real personal data.An AI company, Hazy, is delivering a Python-based tool that will run inside secure data centers and produce synthetic versions of your data that you can share more freely.Differential privacyThe term describes a general approach to adding just enough noise to the data to protect the private information in the data set while still leaving enough information to be useful. Adding or subtracting a few years to everyone’s age at random, for instance, will hide the exact birth years of the people but the average won’t be affected.The approach is most useful for larger statistical work that studies groups in aggregate. The individual entries may be corrupted by noise, but the overall results are still accurate.Microsoft has started sharing White Noise, an open source tool built with Rust and Python, for adding a finely tuned amount of noise to your SQL queries.Homomorphic encryptionMost encryption algorithms scramble the data so completely that no one can make any sense of the results without the proper key. Homomorphic approaches use a more sophisticated framework so that many basic arithmetic operations can be done on the encrypted data without the key. You can add or multiply without knowing the underlying information itself.The simplest schemes are practical but limited. Chapter 14 of Translucent Databases describes simple accounting tools that can, for instance, support addition but not multiplication. More complete solutions can compute more arbitrary functions, but only after much more expensive encryption.IBM is now sharing an open source toolkit for embedding homomorphic encryption in iOS and MacOS applications with the promise that versions for Linux and Android will be coming soon. The tools are preliminary, but they offer the ability to explore calculations as complicated as training a machine learning model without access to the unencrypted data.Also on InfoWorld: 5 reasons we don’t write code like we used toKeep nothingProgrammers may be packrats who keep data around in case it can be useful for debugging later. One of the simplest solutions is to design your algorithms to be as stateless and log-free as possible. Once the debugging is done, quit filling up the disk drives with lots of information. Just return the results and stop.Keeping as little information as possible has dangers. It’s harder to detect abuse or fix errors. But on the flip side, you don’t need to worry about attackers gaining access to this digital flotsam and jetsam. They can’t attack anyone’s personal data if it doesn’t exist.", "pub_date": "2020-08-31"},
{"title": "2 cloud architecture problems are still unsolved", "overview": "If you think we’ve figured out all cloud design problems, you’d be wrong. Edge devices and multicloud security are still sticking points", "image_url": null, "url": "https://www.infoworld.com/article/3570737/2-cloud-architecture-problems-are-still-unsolved.html", "body": "As 2021 begins to appear on the horizon, most people will be happy to see 2020 in the rearview mirror. Despite the pandemic and all its chaos and uncertainties, 2020 was the year of the cloud. Pandemic-weary enterprises rushed to the cloud for safety, scalability, and agility.We still have some architectural obstacles to overcome. There’s no time like the present to review the two architectural problems on the top of my list.Also on InfoWorld: What is CI/CD? Continuous integration and continuous delivery explained. When a tiered architecture includes an edge device, we need best practices and processes to figure out where to place data and knowledge bases and the back-end hyperscalers.This seems like an easy problem to solve: Simply understand the workload requirements and then determine the proper tiering. Then the number of edge devices grows exponentially, the organic growth of data saturates the edge devices, and the requirements become quicksand.We need a dynamic approach that uses automation to migrate the data and the knowledge bases as needed. But even that approach can be problematic through the negative effects of over-engineering. I’m working on this aspect of the solution right now.. You will probably point to IAM (identity and access management) solutions that span clouds, but I find that most are not yet ready for prime time.Cloud security architects are forced to use whatever cloud-native security is available for each cloud brand, which makes this solution more complex and more difficult to operate by the secops team. The outcome is a higher risk of being compromised. Also, the integration of the native directory services of each cloud often leads to manually restarting the integration processes because right now they seem to just jam up.I think IAM providers will solve this security problem now that they recognize that multicloud is their future, and that the market demands the ability to provide all patterns of security to span clouds. We know what the solution to the security problem is; we just need to build one that consistently works.I’m tracking dozens of cloud implementation problems that I see over and over again in my consulting practice. Many people believe there are limitations right now that can’t be overcome. Just watch me.", "pub_date": "2020-08-11"},
{"title": "A reference architecture for multicloud", "overview": "Complexity is the order of the day for multicloud. Here's my vision of how all the pieces fit together", "image_url": null, "url": "https://www.infoworld.com/article/3570619/a-reference-architecture-for-multicloud.html", "body": "Today’s definition of multicloud depends on who you ask. This is not a new trend. Whenever a new technology gets some hype, suddenly there is semantic confusion. My definition is a bit more holistic than most: Multicloud is a collection of public clouds, private clouds, and legacy systems (yes, I went there). Two or more public or private clouds must be present to be true multicloud. Legacy systems are optional but also good to include.InfoWorld’s 2020 Technology of the Year Award winners: The best software development, cloud computing, data analytics, and machine learning products of the yearA charted representation of my definition looks like this: Something I’ve learned over the years is that frameworks of any kind have to be generic and applicable to different problem domains. My representation includes most of what you’ll find in common multicloud deployments I see today. If this seems complex, that’s the point. If the goal is to get a healthy multicloud deployment up and running, you’ll need most of these services. That list includes everything from cost governance, storage management, and cloud service brokers, as well as orchestration and process management—and those skills require qualified talent.You’ll need a few more things, depending on your industry, such as compliance management. Figure on at least 20 percent to 30 percent more services to ultimately meet your business needs.Note that my multicloud definition includes two core meta-layers:  deals with everything that’s stored inside and outside of the public clouds. Cloud-native databases exist here, as do legacy databases that still remain on-premises. The idea is to manage these systems using common layers, such as management and monitoring, security, and abstraction.  means that we deal with behavior/services and the data bound to those services from the lower layers of the architecture. It’s pretty much the same general idea as data-focused multicloud, in that we develop and manage services using common layers of technology that span from the clouds back to the enterprise data center.Of course, there is much more to both layers. Remember that the objective is to remove humans from having to deal with the cloud and noncloud complexity using automation and other approaches. This is the core objective of multicloud complexity management, and it seems to be growing in popularity as a rising number of enterprises get bogged down by manual ops processes and traditional tools.Also note that this diagram depicts a multicloud that has very little to do with clouds, as I covered a few weeks ago. Indeed, the clouds become the source of the services and data, but the tougher problem is how you build systems in and between the public clouds, private clouds, and traditional data centers that can manage how both data and services are leveraged by the applications, depicted at the top of the diagram.This is a new paradigm for staff who have only dealt with cloud-native services for the past several years. Now we must configure architectures that span technologies and create a more valuable layer to help build and deploy business solutions. This requires a step to define exactly what that architecture looks like at each level. ", "pub_date": "2020-08-14"},
{"title": "Review: AWS Bottlerocket vs. Google Container-Optimized OS", "overview": "Container-Optimized OS offers tremendous advantages for container workloads on Google Cloud Platform, but Bottlerocket is a better choice everywhere else", "image_url": null, "url": "https://www.infoworld.com/article/3570158/review-aws-bottlerocket-vs-google-container-optimized-os.html", "body": "Running large numbers of containers to deploy an application requires a rethink of the role of the operating system. Google’s Container-Optimized OS and AWS’s Bottlerocket take the traditional virtualization paradigm and apply it to the operating system, with containers the virtual OS and a minimal Linux fulfilling the role of the hypervisor.Various flavors of Linux optimized for containers have been around for a few years and have evolved ever smaller footprints as the management and user-land utilities moved to the  cluster management layer or to containers. These container-optimized operating systems are ideal when you need to run applications in Kubernetes with minimal setup and do not want to worry about security or updates, or want OS support from your cloud provider.Also on InfoWorld: 13 ways Google Cloud beats AWSContainer OSs solve several issues commonly encountered when running large container clusters, such as keeping up with OS vulnerabilities and patching potentially hundreds of instances, updating packages while dealing with potentially conflicting dependencies, degraded performance from a large dependency tree, and other OS headaches. The job is challenging enough with a few racks of servers and nearly impossible without infrastructure support when managing thousands.AWS BottlerocketBottlerocket is purpose-built for hosting containers in Amazon infrastructure. It runs natively in Amazon Elastic Kubernetes Service (EKS), AWS Fargate, and Amazon Elastic Container Service (ECS).Bottlerocket is essentially a Linux 5.4 kernel with just enough added from the user-land utilities to run containerd. Written primarily in Rust, Bottlerocket is optimized for running both Docker and Open Container Initiative (OCI) images. There’s nothing that limits Bottlerocket to EKS, Fargate, ECS, or even AWS. Bottlerocket is a self-contained container OS and will be familiar to anyone using Red Hat flavors of Linux.Bottlerocket integrates with container orchestrators such as Amazon EKS to manage and orchestrate updates, and support for other orchestrators can be adding by building variants of the operating system to add the necessary orchestration agents or custom components to the build.Bottlerocket’s approach to security is to minimize the attack surface to protect against outside attackers, minimize the impact that a vulnerability would have on the system, and provide inter-container isolation. To isolate containers, Bottlerocket uses container control groups (cgroups) and kernel namespaces for isolation between containers running on the system. eBPF (enhanced Berkeley Packet Filter) is used to further isolate containers and to verify container code that requires low-level system access. The eBPF secure mode prohibits pointer arithmetic, traces I/O, and restricts the kernel functions the container has access to.The attack surface is reduced by running all services in containers. While a container might be compromised, it’s less likely the entire system will be breached, due to container isolation. Updates are automatically applied when running the Amazon-supplied edition of Bottlerocket via a Kubernetes operator that comes installed with the OS. An immutable root filesystem, which creates a hash of the root filesystem blocks and relies on a verified boot path using dm-verity, ensures that the system binaries haven’t been tampered with. The configuration is stateless and /etc/ is mounted on a RAM disk. When running on AWS, configuration is accomplished with the API and these settings are persisted across reboots, as they come from file templates within the AWS infrastructure. You can also configure network and storage using custom containers that implement the CNI and CSI specifications and deploy them along with other daemons via the Kubernetes controllers.SELinux is enabled by default, with no way to disable it. Normally that might be a problem, but in the container OS use case relaxing this requirement isn’t necessary. The goal is to prevent modification of settings or containers by other OS components or containers. This security feature is a work in progress.The Bottlerocket build system is based on Rust, which is fine considering there’s nothing to build except for support for Docker and Kubernetes. Rust just broke into the top 20 programming languages and seems to be gaining traction due to its C++ like syntax and automatic memory management. Rust is licensed under the MIT or Apache 2 license. Amazon does a good job of leveraging GitHub for their development platform, making it easy for developers to get involved. The toolchain and code workflow will be familiar to any developer, and by design end users are encouraged to create variants of the OS. This is to cater to support for multiple orchestration agents. In order to keep the OS footprint as small as possible, each Bottlerocket variant runs on a specific orchestration plane. Amazon includes variants for Kubernetes and local development builds. You could, for example, create your own update operator or your own control container by changing the URL of the container.Bottlerocket isn’t intended to be managed with a shell. Indeed, there is little of the OS that requires management, and what is required is accomplished by the HTTP API, the command-line client (eksctl), or the web console.To update you need to deploy an update container onto the instance. See the bottlerocket-update-operator (a Kubernetes operator) on GitHub. Bottlerocket accomplishes single-step updates using the “two partition pattern,” where the image has two bootable partitions on disk. Once an update has been successfully written to the inactive partition, the priority bits in the GUID partition table of each partition are swapped and the “active” and “inactive” partitions roles are reversed. Upon reboot, the system is upgraded, or, in the event of an error, rolled back to the last known-good image. There are no packages that can be installed, only containers, and updates are image based, as in NanoBSD and other embedded operating systems. The reason behind this decision was explained by Jeff Barr, AWS evangelist:Instead of a package update system, Bottlerocket uses a simple, image-based model that allows for a rapid and complete rollback if necessary. This removes opportunities for conflicts and breakage, and makes it easier for you to apply fleet-wide updates with confidence using orchestrators such as EKS.To access a Bottlerocket instance directly you run a “control” container, which is managed by a separate instance of containerd. This container runs the AWS SSM agent so you can execute remote commands or start a shell on one or more instances. The control container is enabled by default.There is also an administrative container that runs on the internal control plane of the instance (I.e. on a separate containerd instance). Once enabled, this admin container runs an SSH server that allows you to log in as ec2-user using your Amazon-registered SSH key. While this is useful for debugging, it is not really suitable for making configuration changes due to the security policies of these instances.", "pub_date": "2020-08-17"},
{"title": "6 tips for rapidly scaling applications", "overview": "How to address dramatic usage increases during COVID-19 or to start planning for growth in the post-pandemic recovery", "image_url": null, "url": "https://www.infoworld.com/article/3571186/6-tips-for-rapidly-scaling-applications.html", "body": "While the COVID-19 pandemic continues to ravage communities and the economy, many companies in ecommerce, logistics, online learning, food delivery, online business collaboration, and other sectors are experiencing massive spikes in demand for their products and services. For many of these companies, the evolving usage patterns caused by shelter-in-place and lockdown orders have created surges in business. These surges have pushed applications to their limits or beyond, potentially resulting in outages and delays that frustrate customers.If your company is experiencing a dramatic increase in business and application load, what can you do? How can you rapidly increase the performance and scalability of your applications to ensure a great customer experience without breaking the bank? Here are six tips for rapidly scaling applications the right way.How data analysis, AI, and IoT will shape the post-pandemic ‘new normal’Tip 1: Understand the full challengeAddressing only part of the problem may not achieve the desired results. Be sure to consider all of the following. – Application performance under load (and as experienced by end users) is determined by the interplay between latency and concurrency. Latency is the time required for a specific operation, such as the time it takes for a website to respond to a user request. Concurrency is how many simultaneous requests a system can handle. When concurrency is not scalable, a significant increase in demand can cause an increase in latency because the system cannot immediately respond to all requests as they are received. This can lead to a poor customer experience as response times increase from fractions of a second to several seconds or worse, even potentially to an inability to respond to all requests. So while ensuring low latency for a single request may be essential, by itself it may not solve the challenge created by surging concurrency. You must find a way to scale the number of concurrent users while simultaneously maintaining the required response time. Further, applications must be able to scale seamlessly across hybrid environments that may span multiple cloud providers and on-premises servers. – A strategy that will take years to implement, such as rearchitecting an application from scratch, isn’t helpful for addressing immediate needs. The solution you adopt should enable you to begin scaling in weeks or months. – Few companies approach this challenge without budget restrictions, so a strategy that minimizes upfront investments and minimizes increased operational costs can be critical.Tip 2: Plan for both the short term and the long termEven as you address the challenge of increasing concurrency while maintaining latency, do not rush into a short-term fix that can lead to a costly dead end. If a complete redesign of the application isn’t planned or feasible, adopt a strategy that will enable the existing infrastructure to scale massively, as needed.Tip 3: Choose the right technologyOpen source in-memory computing solutions have proven to be the most cost-effective way to rapidly scale up system concurrency while maintaining or improving latency. Apache Ignite, for example, is a distributed in-memory computing solution which is deployed on a cluster of commodity servers. It pools the available CPUs and RAM of the cluster and distributes data and compute to the individual nodes. Deployed on-premises, in a public or private cloud, or in a hybrid environment, Ignite can be deployed as an in-memory data grid (IMDG) inserted between existing application and data layers without major modifications to either. Ignite also supports ANSI-99 SQL and ACID transactions.With an Apache Ignite in-memory data grid in place, relevant data from the database is “cached” in the RAM of the compute cluster and is available for processing without the delays caused by normal reads and writes to a disk-based data store. The Ignite IMDG uses a MapReduce approach and runs application code on the cluster nodes to execute massively parallel processing (MPP) across the cluster with minimal data movement across the network. This combination of in-memory data caching, sending compute to the cluster nodes, and MPP dramatically increases concurrency and reduces latency, providing up to a 1,000X increase in application performance compared to applications built on a disk-based database.The distributed architecture of Ignite makes it possible to increase the compute power and RAM of the cluster simply by adding new nodes. Ignite automatically detects the additional nodes and redistributes data across all nodes in the cluster, ensuring optimal use of the combined CPU and RAM. The ability to add nodes to the cluster easily also enables massive scalability to support rapid growth. Finally, the IMDG ensures data consistency by writing changes made to the data in the IMDG by the application layer back to the source data stores.Apache Ignite can also future proof your infrastructure by powering two increasingly important strategies. – A DIH architecture can power real-time business processes that require 360-degree data views. It provides a common data access layer for aggregating and processing data from a combination of data streams and on-premises and cloud-based sources, including on-premises and cloud databases, data lakes, data warehouses, and SaaS applications. Multiple customer-facing business applications can then access the aggregated data and process the data at in-memory speeds without moving the data over the network. The DIH automatically synchronizes changes to the data made by the consuming applications to the back-end data stores while reducing or eliminating the need for API calls to those data sources. – HTAP is high-speed processing of the same in-memory dataset for both transactions and analytics. This eliminates the need for a time-consuming extract, transform, and load (ETL) process to periodically copy data from an online transactional processing (OLTP) system to a separate online analytical processing (OLAP) system. Powered by an in-memory computing platform, HTAP enables running pre-defined analytics queries on the operational data without impacting overall system performance.Tip 4: Consider an open source stackTo continue creating a cost-effective, rapidly scalable infrastructure, consider these other proven open source solutions:Apache Kafka or Apache Flink for building real-time data pipelines for delivering data from streaming sources, such as stock quotes or IoT devices, into the Apache Ignite in-memory data grid.Kubernetes for automating the deployment and management of applications that have been containerized in Docker or other container solutions. Putting applications in containers and automating the management of them is key to successfully building real-time, end-to-end business processes in a distributed, hybrid, multi-cloud world.Apache Spark for processing and analyzing large amounts of distributed data. Spark takes advantage of the Ignite in-memory computing platform to more effectively train machine learning models using the huge amounts of data being ingested via a Kafka or Flink streaming pipeline.Tip 5: Build, deploy, and maintain it rightWith the desire to deploy these solutions in an accelerated timeframe – and with the consequences of delays potentially very high – it is essential to make a realistic assessment of the in-house resources available for the project. If your company lacks the expertise or the availability, don’t hesitate to consult with third-party experts. You can easily obtain support for all these open source solutions on a contract basis, making it possible to gain the required expertise without the cost and time required to expand your in-house team.Tip 6: Learn moreMany online resources are available to help you get up to speed on the potential of these technologies and determine which strategies may be right for your organization. Start by exploring the following.What is Apache Ignite?Breakout session and keynote recordings from past In-Memory Computing Summits (videos and slides)Apache Ignite communityHow-to for Apache Ignite Deployments in Kubernetes (video)What is Apache Kafka?Session recordings from the Apache Kafka Summit (videos and slides)A simple introduction to Apache FlinkApache Flink communityStep-by-Step Introduction to Basic Concept of KubernetesKubernetes communityApache Spark 101Apache Spark communityWhether your goal is to ensure an optimal customer experience in the face of surging business activity or to start planning for growth in the post-pandemic economic recovery, an open source infrastructure stack powered by in-memory computing is a cost-effective path to combining unprecedented speed with massive scalability to enable real-time business processes.GridGain Systemsnewtechforum@infoworld.com", "pub_date": "2020-08-19"},
{"title": "2021: The year of the great enterprise disconnect", "overview": "Next year enterprise cloud usage will become completely virtual and disconnected from centralized infrastructure. Here’s how to be ready", "image_url": null, "url": "https://www.infoworld.com/article/3572272/2021-the-year-of-the-great-enterprise-disconnect.html", "body": "Due to the unprecedented shutdown of businesses during the pandemic, most white-collar workers went from hours-long commutes to working fulltime from a guest bedroom somewhere in the suburbs. In the past, most enterprises resisted or refused to consider viable telecommuting options. Suddenly, new regulations, laws, and liability policies forced the hands of most enterprises around the world. Companies had to accommodate a remote workforce in a short amount of time. Adapt or die. IT departments spent the better part of the year putting out fires caused by the sudden shift to a remote workforce. VPNs had to scale to handle the influx, networks needed upgrades, security required adjustments, and there was even work to be done with employees' ISPs to provide better and more reliable bandwidth. ISPs suddenly moved bandwidth upgrades to the top of the priority list. Also on InfoWorld: 6 ways Alibaba Cloud challenges AWS, Azure, and GCPEven the most remote homes in the country will begin to support telecommuting as early as next year, as 5G becomes a pervasive reality nationwide. If staff can live and work from anywhere, why not live where they can get the most home for the least amount of money? This trend naturally eliminates the drawbacks an enterprise once had when all of its proverbial eggs were in one geographic basket. Five or ten years ago, a major fire, flood, or weather event in the immediate vicinity would have shut down a firm’s headquarters. In some of today’s cutting-edge enterprises, staff can be so geographically scattered that most white-collar workers could continue to telecommute each morning even if the place they once reported to no longer exists. Virtual IT systems can be load-balanced across cloud platforms in deliberately staggered locations so that systems experience little or no impact on operations. To turn these opportunities into long-term reality, enterprises need to disconnect. 2021 will be the year most begin this journey. The first stage will be to stabilize their remote workforces. Then they can move to these likely next steps:This has been occurring for the past several years, but the pace will accelerate. The more difficult workloads will be placed in the queue for relocation to cloud or MSPs.Fueled by a strong economy, the growth of data centers over the past few years has led to the identification of vulnerabilities and a new understanding of the risk of owning and maintaining your own hardware and software. Why own or lease office space you won’t use? Commercial space will not be completely abandoned, but liability costs will likely force hotel-type options to share offices and conference rooms.It’s one thing to have the tools and the infrastructure to support a functional remote workforce, it’s another thing to embrace them. Culture needs to change and will take longer than one year. Major changes include how managers measure employee performance and work processes. Staff should also take the initiative to continuously improve these practices. Each company has its own demons to exorcise. The larger the enterprise, the more difficult the journey to a disconnected enterprise. Large or small, every journey begins with the first step. It’s time to lace up your hiking boots.", "pub_date": "2020-08-21"},
{"title": "Google CAMP helps you modernize your cloud apps", "overview": "The Google Cloud App Modernization Program offers best practices and tools for developing and delivering better apps faster in the Google Cloud", "image_url": null, "url": "https://www.infoworld.com/article/3572925/google-camp-helps-you-modernize-your-cloud-apps.html", "body": "Google has launched a cloud application modernization effort, which the company said leverages its experience in driving application delivery at speed and scale.Unveiled August 25, the Google Cloud App Modernization Program (CAMP) Is intended to help large enterprises modernize application development and delivery and improve speed.Also on InfoWorld: Cloud tech certifications count more than degrees nowFacets of Google CAMP include:Solutions, recommendations, and best practices for application modernization. The application lifecycle is covered from writing code to running and securing applications. Practices include driving alignment between developers and operators, lean product development, and technical practices such as implementing loosely coupled architectures and continuous testing.Tailored modernization advice gained through a data-driven assessment. Regardless of whether a developer is building a Kubernetes, serverless, or mainframe application, the assessment shows where to start a modernization effort, identifies priorities and how to maximize ROI. Bottlenecks are found, as well.An extensible platform for writing code and running, securing, and operating applications. Existing Google Cloud Platform services are extended to help run legacy and new applications.Anthos, Google’s hybrid and multi-cloud modernization platform. Google on August 25 announced hybrid AI capabilities for Anthos, including general availability of Speech-to-Text On Prem and introduced Anthos attached clusters, for managing Kubernetes clusters.Google CAMP is based on the company’s experience in high-speed application delivery, with the company deploying 12 million builds and 650 million tests daily, along with processing 2.5 exabytes of logs monthly and parsing more than 14 quadrillion monitoring metrics. The program also reflects six years of devops research and assessment into practices to drive high performance, Google said. Google Cloud can be accessed at cloud.google.com.", "pub_date": "2020-08-31"},
{"title": "Dealing with sovereign data in the cloud", "overview": "International regulations make dealing with cloud-based data a nightmare. These fundamental concepts can help keep you from running afoul of data sovereignty laws    ", "image_url": null, "url": "https://www.infoworld.com/article/3573568/dealing-with-sovereign-data-in-the-cloud.html", "body": "It’s Friday, and you’re about to close your laptop and go to happy hour, when you get an urgent e-mail stating that due to data being illegally transported out of the country, the company has been fined $250,000.  What happened?Also on InfoWorld: Cloud tech certifications count more than degrees nowAs bad luck would have it, your cloud provider backs up the database containing sovereign data to a storage system outside of the country. This is done for a good reason: Relocating data for business continuity and disaster recovery purposes to another region reduces the risk of data loss if the primary region is down. Of course, this is not the fault of the cloud provider. This is a common configuration error that occurs primarily because the cloudops team does not understand issues with laws and regulations around data. The database administrator may not have been aware it was happening. Lack of training led to this problem and the quarter million slap in the face.   Data sovereignty is more of a legal issue than a technical one. The idea is that data is subject to the laws of the nation where it’s collected and exists. Laws vary from country to country, but the most common governance you’ll see is not allowing some types of data to leave the country at any time. Other regulations enforce encryption and how the data is handled and by whom. These were pretty easy rules to follow when we had dedicated data centers in each country, but the use of public clouds that have regions and points-of-presence all over the world complicates things. Misconfigurations, lack of understanding, and just general screw-ups lead to fines, impacts to reputations, and, in some cases, disallowing the use of cloud computing altogether.   Some best practices are emerging to deal with data sovereignty in the cloud. Data governance systems are worth their weight in gold. When dealing with regulations that are bound to data, these systems will keep you out of trouble since they won’t allow humans to violate data policies that are set to reflect the law of the land where the data resides. Training is another critical point. Most of the data sovereignty issues can be traced to human error. Everyone handing the data should be knowledgeable on the regulations. Many countries mandate this.Take advantage of security systems that are purpose-built to deal with data sovereignty issues. Identity-based security systems can deal with special security needs based on the identity of data, encrypt the data per regulations, and also ensure that it’s not transmitted out of the country or stolen in other ways.  There’s no real magic bullet here. As countries get more particular about how their data is managed and the pervasive use of international public clouds continues, more issues are bound to arise. Enterprises are well advised to be proactive here, or else things can go sideways quickly.  ", "pub_date": "2020-09-01"},
{"title": "The distributed cloud era has arrived ", "overview": "The rise of distributed clouds and app-to-app networking dictates the use of cloud-native network and security infrastructure ", "image_url": null, "url": "https://www.infoworld.com/article/3573776/the-distributed-cloud-era-has-arrived.html", "body": "The term “cloud” has been evolving ever since the term was first used in the early 1990s. One could argue that Cloud 1.0 was really nothing more than a euphemism for hosted services. Hosted services gave companies the ability to run critical apps off their premises in a highly secure, predictable environment. This value proposition continued with the future rise of services like AWS and Microsoft Azure, where businesses would “lift and shift” legacy apps and drop them into a cloud. Cloud 2.0 gave rise to web-optimized apps. In Cloud 2.0 the apps were truly built for the cloud and spawned companies that made the cloud their primary compute platform. However, this cloud strategy revolved around a single cloud provider and traditional monolithic app architectures. Even companies that used multiple clouds built app A on one cloud, app B on another, etc. In this case, multicloud was actually multiple clouds being used as discrete, independent infrastructure entities.Also on InfoWorld: Cloud tech certifications count more than degrees nowWe have now entered the Cloud 3.0 era, which can be thought of as multicloud on steroids. The rise of microservices and containers has allowed app developers to build apps by accessing services from multiple cloud providers. Most modern, cloud-native apps are being built this way. Edge computing is on the horizon, which will create more locations for app developers to extend access to data and app services. This is the concept of the distributed cloud, where the cloud is no longer a single location, but a set of distributed resources.Distributed cloud changes app deliveryThe evolution of the cloud – and cloud-native apps as a result – has had a profound impact on the networking and security services required to connect app components to other app components and to connect apps to users. With Cloud 1.0, IT professionals used physical appliances such as load balancers or application delivery controllers and web app firewalls. These were installed in the same data centers that hosted the app infrastructure. With Cloud 2.0, these functions were virtualized and installed as a cloud resource. The network and app architectures were largely the same, but the infrastructure shifted to cloud-resident virtual appliances.>>With distributed cloud (Cloud 3.0), app components (e.g. microservices) are modular and reside in containers across multiple clusters. This creates significant deployment and operational challenges for DevOps teams and IT professionals. The dynamic and distributed nature of containerized workloads and microservices can’t be supported by the physical or even virtual infrastructure traditionally used for monolithic apps, which rely on centralized control and visibility, as opposed to the dynamic operational model needed for highly distributed clusters and workloads. A containerized workload can be spun up and deprecated in a matter of minutes, even seconds. This means the supporting network and security infrastructure like load balancers, web app firewalls, and API gateways need to be spun up and down just as quickly. Meeting the operational challenges of distributed cloudThis week cloud-native app infrastructure provider, Volterra, announced the latest release of its VoltMesh service, which addresses many of the operational challenges associated with deploying and operating modern apps in a distributed cloud model. The company offers a wide range of what would typically be known as “application delivery services” from the cloud, made available as a single SaaS-based offering.Instead of having to deploy multiple virtual appliances per cluster, across multiple clusters, VoltMesh offers an integrated service stack that can be easily deployed across a distributed cluster with centralized management, end-to-end visibility, and policy control. By leveraging the speed and ubiquity of a SaaS-based service, DevOps can speed up the deployment of distributed cloud-native applications and simplify ongoing operations, while developers can achieve better code integration and agility as they have greater freedom of where and how to develop. VoltMesh can be deployed in clusters in every major cloud provider as well as private clouds and edge sites.  Volterra also offers its own application delivery network (ADN) to improve the performance and security of cloud-native apps – hosting them directly on Volterra’s private network and closer to end users.The cloud-native infrastructure approach Volterra is taking is for more than just speed though, as there are major security implications. As app developers shift to building cloud-native apps on microservices, new threats are emerging from embedded and unseen APIs. The number of APIs per app has exploded, creating more intra-application traffic. Security approaches like segmentation, even micro-segmentation, operate at the network layer – so they’re blind at the API layer. This means DevOps and DevSecOps teams must shift the focus of their zero-trust model from the network to the API layer, including the ability to “see” all APIs and then enforce policies on them.Also on InfoWorld: 6 ways Alibaba Cloud challenges AWS, Azure, and GCPAs part of this release, VoltMesh has unveiled its new API auto-discovery capability that can automatically find all APIs in an application through the use of its machine learning engine. It then automatically applies policies to whitelist only the APIs that are required and validated, rendering any unneeded or unsafe ones ineffective. This shrinks the attack surface significantly, increasing protection and compliance without delaying app release cycles.The concept of multicloud (or multiple clouds) plus monolithic apps is rapidly giving way to distributed cloud plus distributed apps, and this will change the way we provision networking and security infrastructure for them. Traditional application delivery infrastructure needs to evolve to be cloud-native and distributed if DevOps and NetOps teams hope to keep up with the agility of developers building modern apps today.", "pub_date": "2020-09-03"},
{"title": "Higher ed needs a course in cloud computing", "overview": "To survive in a post-pandemic world, most higher education institutions need cloud computing to enable agile learning models", "image_url": null, "url": "https://www.infoworld.com/article/3573751/higher-ed-needs-a-course-in-cloud-computing.html", "body": "Four-year colleges may face up to a 20 percent reduction in fall enrollment, according to SimpsonScarborough, a higher education research and marketing company. Its findings are based on surveys of more than 2,000 college-bound high school seniors and current college students. According to the Chronicle of Higher Education, nine percent of institutions plan to operate only online during the fall semester. The remainder will take a hybrid approach, with some classes offered online, while other classes will operate both online and in-person. Also on InfoWorld: Cloud tech certifications count more than degrees nowSome of the realities that colleges and universities now face include:A large percentage of students who won’t start or continue their higher education until the pandemic is largely behind us, as reflected in the study cited above.The movement to remote learning means that colleges will have much more direct competition. The higher education systems will be viewed as a commodity, with students able to select the college with the best results, not the one nearest to home.Students will demand reduced tuition because they will not use the college facilities, dormitories, or even interact directly with professors.The most appealing higher education choices will be those that deliver a better digital experience for the student, not those that have more traditional attributes.How does this relate to cloud computing technology as a tool to save many of these colleges and universities? Clearly, it’s the fastest path to a digitally enabled future. However, the more important role of cloud is its ability to leverage existing educational delivery systems that are purpose-built to quickly enable an institution to move into remote learning.For example, look at the movement to cloud-based student information systems. These systems register students for classes, document grades, deal with transcripts, and record student tests and other assessment scores. They can also build student schedules, track attendance, and manage many other student-related data needs. If this seems like something the brick and mortar institutions already have in place, that’s a sound assumption. However, they don’t always have the capabilities to deal with remote learning systems, including integration with content, usage, and tracking student progress. Without this integration, it’s unlikely that colleges will be successful in their move to digital learning. This is not about existing universities keeping up with the new world during and after the pandemic. This is more about the ability to take advantage of a technology (specifically, cloud computing) as a force multiplier to provide a better education per dollar spent. Here are a couple of emerging opportunities in this arena:Machine learning-based systems can monitor and enhance the learning process for each student. They can provide deep analysis of learning activities, change learning paths on the fly to better accommodate the needs of the student, and dynamically monitor student well-being.Actual changes to courses, authors, and instructors has always dragged far behind feedback. The larger issues are how to deal with the more systemic changes that need to occur at the same time. Institutions need to consider culture, the way the institution sells itself, the differing student personas, and—what might be a new factor for some institutions or instructors—students who never set foot on the campus because they are 100 percent remote. The end goal is to use technology to meet students’ learning expectations. The most innovative institutions will go well beyond expectations to improve the educational experience as compared to traditional educational operations just last year. This paradigm shift has been on higher education’s horizon for decades. Remote learning has always been around. Thirty years ago, personal computers made it easier.  Five to ten years ago, cloud made it more feasible. The pandemic simply pushed this shift to the top of the to-do lists within most higher education institutions.", "pub_date": "2020-09-04"},
{"title": "How AI will improve API security", "overview": "By using AI models to continuously inspect all API activity, we can fill in the cracks left by policy-based API protections", "image_url": null, "url": "https://www.infoworld.com/article/3516595/how-ai-will-improve-api-security.html", "body": "APIs have become the crown jewels of organizations’ digital transformation initiatives, empowering employees, partners, customers, and other stakeholders to access applications, data, and business functionality across their digital ecosystem. So, it’s no wonder that hackers have increased their waves of attacks against these critical enterprise assets.Unfortunately, it looks like the problem will only worsen. Gartner has predicted that, “By 2022, API abuses will be the most-frequent attack vector resulting in data breaches for enterprise web applications.”Also on InfoWorld: AI, machine learning, and deep learning: Everything you need to knowMany enterprises have responded by implementing API management solutions that provide mechanisms, such as authentication, authorization, and throttling. These are must-have capabilities for controlling who accesses APIs across the API ecosystem—and how often. However, in building their internal and external API strategies, organizations also need to address the growth of more sophisticated attacks on APIs by implementing dynamic, artificial intelligence (AI) driven security.This article examines API management and security tools that organizations should incorporate to ensure security, integrity, and availability across their API ecosystems.Rule-based and policy-based security measuresRule-based and policy-based security checks, which can be performed in a static or dynamic manner, are mandatory parts of any API management solution. API gateways serve as the main entry point for API access and therefore typically handle policy enforcement by inspecting incoming requests against policies and rules related to security, rate limits, throttling, etc. Let’s look closer at some static and dynamic security checks to see the additional value they bring.Static security checks do not depend on the request volume or any previous request data, since they usually validate message data against a predefined set of rules or policies. Different static security scans are performed in gateways to block SQL injection, cohesive parsing attacks, entity expansion attacks, and schema poisoning, among others.Meanwhile, static policy checks can be applied to payload scanning, header inspection, and access patterns, among others. For example, SQL injection is a common type of attack performed using payloads. If a user sends a JSON (JavaScript Object Notation) payload, the API gateway can validate this particular request against predefined JSON schema. The gateway also can limit the number of elements or other attributes in content as required to protect against harmful data or text patterns within messages.Dynamic security checks, in contrast to static security scans, are always checking against something that varies over time. Usually this involves validating request data with decisions made using existing data. Examples of dynamic checks include access token validation, anomaly detection, and throttling. These dynamic checks depend heavily on the data volume being sent to the gateway. Sometimes these dynamic checks occur outside the API gateway, and then the decisions are communicated to the gateway. Let’s look at a couple examples.Throttling and rate limiting are important for reducing the impact of attacks, because whenever attackers get access to APIs, the first thing they do is read as much data as possible. Throttling API requests — i.e., limiting access to the data — requires that we keep a count of incoming requests within a specific time window. If a request count exceeds the allocated amount at that time, the gateway can block API calls. With rate limiting, we can limit the concurrent access allowed for a given service.Authentication helps API gateways to identify each user who invokes an API uniquely. Available API gateway solutions generally support basic authentication, OAuth 2.0, JWT (JSON Web Token) security, and certificate-based security. Some gateways also provide an authentication layer on top of that for additional fine-grained permission validation, which is usually based on XACML (eXtensible Access Control Markup Language) style policy definition languages. This is important when an API contains multiple resources that need different levels of access control for each resource.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesLimitations of traditional API securityPolicy-based approaches around authentication, authorization, rate limiting, and throttling are effective tools, but they still leave cracks through which hackers can exploit APIs. Notably, API gateways front multiple web services, and the APIs they manage are frequently loaded with a high number of sessions. Even if we analyzed all those sessions using policies and processes, it would be difficult for a gateway to inspect every request without additional computation power.Additionally, each API has its own access pattern. So, a legitimate access pattern for one API could indicate malicious activity for a different API. For example, when someone buys items through an online shopping application, they will conduct multiple searches before making the purchase. So, a single user sending 10 to 20 requests to a search API within a short period of time can be a legitimate access pattern for a search API. However, if the same user sends multiple requests to the buying API, the access pattern could indicate malicious activity, such as a hacker trying to withdraw as much as possible using a stolen credit card. Therefore, each API access pattern needs to be analyzed separately to determine the correct response.Yet another factor is that significant numbers of attacks happen internally. Here, users with valid credentials and access to systems utilize their ability to attack those systems. Policy-based authentication and authorization capabilities are not designed to prevent these kinds of attacks. Even if we could apply more rules and policies to an API gateway to protect against the attacks described here, the additional overhead on the API gateway would be unacceptable. Enterprises cannot afford to frustrate genuine users by asking them to bear the processing delays of their API gateways. Instead, gateways need to process valid requests without blocking or slowing user API calls.The case for adding an AI security layerTo fill the cracks left by policy-based API protections, modern security teams need artificial intelligence-based API security that can detect and respond to dynamic attacks and the unique vulnerabilities of each API. By applying AI models to continuously inspect and report on all API activity, enterprises could automatically discover anomalous API activity and threats across API infrastructures that traditional methods miss.Even in cases where standard security measures are able to detect anomalies and risks, it can take months to make the discoveries. By contrast, using pre-built models based on user access patterns, an AI-driven security layer would make it possible to detect some attacks in near real time.Importantly, AI engines usually run outside of API gateways and communicate their decisions to them. Because the API gateway does not have to expend resources to process these requests, the addition of AI-security typically does not impact runtime performance.Integrating policy-based and AI-driven API securityWhen adding AI-powered security to an API management implementation, there will be a security enforcement point and a decision point. Typically, these units are independent due to the high computational power required, but the latency should not be allowed to affect their efficiency.The API gateway intercepts API requests and applies various policies. Linked to it is the security enforcement point, which describes the attributes of each request (API call) to the decision point, requests a security decision, and then enforces that decision in the gateway. The decision point, powered by AI, continuously learns the behavior of each API access pattern, detects anomalous behaviors, and flags different attributes of the request.Also on InfoWorld: What AI can really do for your business (and what it can’t)There should be an option to add policies to the decision point as needed and invoke these policies—which may vary from API to API—during the learning period. Any policies should be defined by the security team once the potential vulnerabilities of each API they plan to expose are thoroughly understood. However, even without support from external policies, adaptive, AI-powered decision point and enforcement point technology will eventually learn and prevent some of the complex attacks that we cannot detect with policies.Another advantage of having two separate security enforcement point and decision point components is the ability to integrate with existing API management solutions. A simple user interface enhancement and customized extension could integrate the security enforcement point to the API publisher and gateway. From the UI, the API publisher could choose whether to enable AI security for the published API, along with any special policies that needed. The extended security enforcement point would publish the request attributes to the decision point and restrict access to the API according to the decision point’s response.However, publishing events to the decision point and restricting access based on its response will take time and depend heavily on the network. Therefore, it is best implemented asynchronously with the help of a caching mechanism. This will affect the accuracy a bit, but when considering the efficiency of the gateway, adding an AI security layer will minimally contribute to the overall latency.AI-driven security layer challengesOf course, benefits don’t come without costs. While an AI-driven security layer offers an additional level of API protection, it presents some challenges that security teams will need to address.. The additional AI security layer adds some overhead to the message flow. So, mediation solutions should be smart enough to handle information gathering and publishing outside the main mediation flow.. A high volume of false positives will require additional review by security professionals. However, with some advanced AI algorithms, we can reduce the number of false positives triggered.. People feel uncomfortable when they don’t understand how a decision was made. Dashboards and alerts can help users to visualize the factors behind a decision. For example, if an alert clearly states that a user was blocked for accessing the system at an abnormal rate of 1,000-plus times within a minute, people can understand and trust the system’s decision.. Most AI and machine learning solutions rely on massive volumes of data, which is often sensitive and personal. As a result, these solutions could become prone to data breaches and identity theft. Complying with the European Union GDPR (General Data Protection Regulation) helps to mitigate this risk but doesn’t eliminate it entirely.. The most powerful AI systems are trained through supervised learning, which requires labeled data that is organized to make it understandable by machines. But labeled data has limits, and the future automated creation of increasingly difficult algorithms will only exacerbate the problem.. An AI system’s effectiveness depends on the data it is trained on. Too often, bad data is associated with ethnic, communal, gender, or racial biases, which can affect crucial decisions about individual users.Keep up with hot topics in software development with InfoWorld’s App Dev Report newsletterGiven the critical role of APIs in enterprises today, they increasingly are becoming targets for hackers and malicious users. Policy-based mechanisms, such as authentication, authorization, payload scanning, schema validation, throttling, and rate limiting, are baseline requirements for implementing a successful API security strategy. However, only by adding AI models to continuously inspect and report on all API activity will enterprises be protected against the most sophisticated security attacks emerging today.WSO2", "pub_date": "2020-01-29"},
{"title": "JetBrains taps machine learning for full-line code completion", "overview": "IntelliJ IDE's 2020 roadmap also features collaborative editing and lightweight text editing", "image_url": null, "url": "https://www.infoworld.com/article/3518452/jetbrains-taps-machine-learning-for-full-line-code-completion.html", "body": "JetBrains has laid out a 2020 roadmap for IntelliJ IDEA and its IntelliJ-based IDEs. The promised new capabilities range from additional machine learning-driven code completion to collaborative editing.The company said the new machine learning-based code completion capabilities would make better use of the context for ranking completion suggestions and generate completion variants that go beyond a single identifier to provide full-line completion. Considered a major area of investment, full-line completion may take a while to appear in the product.Also on InfoWorld: The new features in Java 14Choosing your Java IDEJetBrains already had been exploring the use of machine learning for code completion, and some results of that research have made their way into products. IntelliJ now uses machine learning to improve the ranking of completion variants, and language plug-ins tag each produced completion variant with different attributes. IntelliJ also uses machine learning to determine which attributes contribute to item ranking so the most-relevant items are at the top of the list.In addition to machine learning based code completion, JetBrains cited a multitude of improvements to IntellIj for 2020, subject to change. These include:Collaborative editing support. Users would connect their IDEs to a primary system as “thin clients,” which would not require direct source code access. Each user would have their own state, with a set of open files, caret position, a completion variants list, and other capabilities. Expanded use of the IDE as a lightweight text editor. A dedicated mode for editing non-project files also is being developed.Two modes of integration with Git. Developers would be able to switch between a new UI that supports the staging area but not changelists, and the current UI based on changelists. Combining the two does not seem feasible.Easier onboarding and environment setup and configuration. The system would take care of installing Git, the Java Development Kit, and so forth. Deeper cloud integration.A redesigned project model to remove current limitations such as lack of support for arbitrary mixing of projects of different types. Benefits would include faster project opening and smoother synchronization with Maven and Gradle.Improved indexing performance as well as making indexing less-disruptive. Users also would be notified about indexing anomalies.A redesign of the read/write locks thread model to tackle the issue of UI freezes.More discoverable refactorings during autodetection. One example is adding the possibility to detect changes in the declaration of a method and adjust usage accordingly.Support for loading and unloading most plug-ins without a restart. The intent is to have an IDE that right-sizes itself for each project. Spring projects, for example, would only be loaded with plug-ins that use Spring.The addition of Code Vision capabilities for displaying rich contextual information in the code editor. This capability already has been featured in JetBrain’s Rider IDE for .NET.Localization of IntelliJ-based IDEs in Asian markets, with initial support for Chinese Simplified and support for Korean and Japanese to follow.", "pub_date": "2020-01-29"},
{"title": "Keeping your enterprise AI expertise up to speed", "overview": "AI is such a significant force, you can’t afford not to stay on top of the ever-changing developments", "image_url": null, "url": "https://www.infoworld.com/article/3518475/keeping-your-enterprise-ai-expertise-up-to-speed.html", "body": "Artificial intelligence (AI) could not be a more strategic enterprise technology. As we move into the ’20s, the most disruptive business applications will be those that incorporate machine learning, deep learning, and other forms of AI.AI has become the brain driving cloud-native enterprise applications. Developers everywhere are embedding AI microservices to imbue cloud applications with data-driven machine learning intelligence. Increasingly, there is no substitute for the sophisticated AI that performs high-speed inferencing on sensor-sourced data and on data acquired from applications, clouds, hub gateways, and other online resources.How data analysis, AI, and IoT will shape the post-pandemic ‘new normal’Staying abreast of AI trends, technologies, and applications is fundamental to success in modern business, even if you’re not a data scientist or machine learning specialist. Companies that innovate with AI will dominate their industries for decades to come.Build a top-notch enterprise AI competencyConsidering how ubiquitous and dynamic the AI industry has become, keeping your enterprise AI expertise up to snuff can be a challenge. If nothing else, you should make sure that your company implements a far-reaching program of AI skills, processes, tools, platforms, and methodologies that covers the following major points:: Assemble dedicated teams of data scientists and other developers to build, train, deploy, and manage AI applications as a standardized operational process across all business functions.: Deploy an integrated, open, and trusted platform for data science, machine learning, data engineering, and application building across the multicloud.: Combine hybrid data from on-premises platforms and public clouds when building, training, deploying, and managing machine learning, deep learning, and other AI models.: Deploy a fast, scalable, hybrid data environment to manage both data at rest and data in motion for myriad AI workloads.: Adopt cloud-based AI devops tools that incorporate popular modeling frameworks, automate model management and hyperparameter tuning, accelerate AI workloads across distributed GPUs and other compute nodes, and enable developers to access pretrained models from libraries across public clouds, private clouds, and on-premises systems.: Distribute and scale AI inferencing, training, modeling, and data preparation workloads across public clouds, private clouds, and on-premises systems.: Adopt robust tools for data integration, security, governance, lifecycle management, devops, and orchestration across all AI initiatives, projects, applications, and workloads.Also on InfoWorld: Artificial intelligence predictions for 2020Keep up with enterprise AI trendsWrapping your head around AI’s complexities is a never-ending task. As an industry analyst, I do this for a living, and even I often feel like I’m simply treading water.The AI industry is evolving so fast that all of us—enterprises, teams, individuals—need a mental map of trends in both business and technical topics. What follows is a cheat sheet of top AI trends that I’ve compiled from the last two years’ worth of my own year-end predictions, all of which—near as I can tell—are still in full force:: AI regulations are coming fast in most modern countries and every industry. Enterprise chief legal officers are mandating end-to-end AI transparency. AI risk-mitigation controls are becoming standard patterns available in data science pipeline tools. AI data science team workbenches are enabling downstream reproducibility. AI deepfakery is working its magic—benign and otherwise—more deeply into our lives.: GPUs are dominating AI acceleration. GPUs are expanding their footprint in immersive AI applications. AI systems-on-chip are dominating the hardware-accelerator wares.: AI development frameworks are becoming interchangeable within an open industry ecosystem. More data scientists are buying certified high-performance AI algorithms, trained models, and training data from online marketplaces. AI modeling frameworks are converging on a two-horse race.: SaaS-based AI solutions are reducing enterprise demand for data scientists. More labeling of AI training data is being automated through on-demand cloud services. Kubernetes-orchestrated containers are becoming integral to the AI pipeline. Reinforcement learning is becoming a mainstream AI approach. Client-side training is moving toward the AI mainstream. Blockchain is feeling its way into the AI ecosystem.: Dominant AI development frameworks are being re-engineered for superior cloud-to-edge performance. AI benchmarking frameworks are crystallizing and gaining enterprise adoption. Industry-standard AI benchmarks are becoming a competitive battlefront.: AI is automating AI developers’ core modeling functions. Automated end-to-end AI devops pipelines are becoming standard practice. Enterprise AI is shifting toward continual real-world experimentation. AI is becoming an industrialized operational business function. AI is driving closed-loop IT operations management.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesFor enterprise professionals, your strategy for staying prepared and abreast of all of this should include participation in AI industry and professional confabs taking place this year, such as the Nvidia GPU Technology Conference. Also, going back to school to earn an AI certification would be a good idea. And it couldn’t hurt to engage with friendly AI industry analysts for an expert’s reality check on all this.", "pub_date": "2020-01-30"},
{"title": "Interested in machine learning? Better learn PyTorch", "overview": "Don’t look now, but easy, straightforward PyTorch has become the hottest product in data science", "image_url": null, "url": "https://www.infoworld.com/article/3518453/interested-in-machine-learning-better-learn-pytorch.html", "body": "Building on the rampant popularity of Python was always going to be a good idea for the Facebook-born PyTorch, an open source machine learning framework. Just how good of an idea, however, few could have guessed. That’s because no matter how many things you get right when launching an open source project (great docs, solid technical foundation, etc.), there is always an element of luck to a project’s success.Well, consider PyTorch lucky, then. Or blessed. Or something. Because it’s booming and, if analyst Thomas Dinsmore is to be believed, “By the end of [2020] PyTorch will have more active contributors than TensorFlow.” More contributors and more adoption? That’s a big jump for a rival to TensorFlow, long considered the industry default since its public release in 2015.Also on InfoWorld: Artificial intelligence predictions for 2020Wild and crazy adoptionAs detailed in OpenHub, TensorFlow and PyTorch are running neck-and-neck in terms of 12-month contributor totals: TensorFlow (906) and PyTorch (900). This represents huge progress by the PyTorch community, given TensorFlow’s head start, and is reflected in the growth in PyTorch’s user community, as reflected in Jeff Hale’s analysis of job posting sites for data scientist roles:To be clear, this analysis reflects  growth or decline over the past year. The TensorFlow user community is still much larger than PyTorch’s, though in academics PyTorch has gone from distant minority to overwhelming majority almost overnight. All things considered, it’s not hard to see PyTorch quickly bridging the gap at this pace.Particularly given PyTorch’s comparative advantages. Did I mention Python?Lowering the bar to data scienceAs Serdar Yegulalp wrote back in 2017 at the launch of PyTorch, “A chief advantage to PyTorch is that it lives in and allows the developer to plug into the vast ecosystem of Python libraries and software. Python programmers are also encouraged to use the styles they’re familiar with, rather than write code specifically meant to be a wrapper for an external C/C++ library.” This means that PyTorch has always had the advantage of approachability. The documentation is excellent and there’s a healthy community of developers happy to help out.This advantage is further accentuated by PyTorch’s computational graph setup. As Savan Visalpara explains:TensorFlow is ‘Define-and-Run,’ whereas PyTorch is ‘Define-by-Run.’ In [a] Define-and-Run framework, one would define conditions and iterations in the graph structure then run it. In [a] Define-by-Run [framework, the] graph structure is defined on-the-fly during forward computation, [which is a more] natural way of coding.Dhiraj Kumar concurs, arguing that such a dynamic model allows data scientists to “fully see each and every computation and know exactly what is going on.”To be sure, with the release of TensorFlow 2.0, Google has made TensorFlow “eager by default.” As Martin Heller explains, “Eager execution means that TensorFlow code runs when it is defined, as opposed to adding nodes and edges to a graph to be run in a session later, which was TensorFlow’s original mode.”Also on InfoWorld: PyTorch vs. TensorFlow: How to chooseWhile this sounds great for TensorFlow because it helps the framework compete better with PyTorch in terms of ease of use, “In enabling Eager mode by default, TensorFlow forces a choice onto their users — use eager execution for ease of use and require a rewrite for deployment, or don’t use eager execution at all.While this is the same situation that PyTorch is in, the opt-in nature of PyTorch’s TorchScript is likely to be more palatable than TensorFlow’s ‘Eager by default,’” warns Horace He. TensorFlow Eager mode also suffers from performance issues, though we’d expect these to improve over time.In sum, while the market still leans heavily on TensorFlow, PyTorch’s easy-to-learn, straightforward-to-use approach that ties into the world’s most popular programming language for data science is proving a winner. Although academia has been fastest to embrace PyTorch, we should expect to see ever-increasing adoption with the enterprise set, too.", "pub_date": "2020-01-30"},
{"title": "A brief history of artificial intelligence", "overview": "Despite huge advances in machine learning models, AI challenges remain much the same today as 60 years ago", "image_url": null, "url": "https://www.infoworld.com/article/3527209/a-brief-history-of-artificial-intelligence.html", "body": "In the early days of artificial intelligence, computer scientists attempted to recreate aspects of the human mind in the computer. This is the type of intelligence that is the stuff of science fiction—machines that think, more or less, like us. This type of intelligence is called, unsurprisingly, intelligibility. A computer with intelligibility can be used to explore how we reason, learn, judge, perceive, and execute mental actions.Early research on intelligibility focused on modeling parts of the real world and the mind (from the realm of cognitive scientists) in the computer. It is remarkable when you consider that these experiments took place nearly 60 years ago.Also on InfoWorld: Artificial intelligence predictions for 2020Early models of intelligence focused on deductive reasoning to arrive at conclusions. One of the earliest and best known A.I. programs of this type was the Logic Theorist, written in 1956 to mimic the problem-solving skills of a human being. The Logic Theorist soon proved 38 of the first 52 theorems in chapter two of the , actually improving one theorem in the process. For the first time, it was clearly demonstrated that a machine could perform tasks that, until this point, were considered to require intelligence and creativity.Soon research turned toward a different type of thinking, inductive reasoning. Inductive reasoning is what a scientist uses when examining data and trying to come up with a hypothesis to explain it. To study inductive reasoning, researchers created a cognitive model based on the scientists working in a NASA laboratory, helping them to identify organic molecules using their knowledge of organic chemistry. The Dendral program was the first real example of the second feature of artificial intelligence, , a set of techniques or algorithms to accomplish an inductive reasoning task, in this case molecule identification.Dendral was unique because it also included the first knowledge base, a set of if/then rules that captured the knowledge of the scientists, to use alongside the cognitive model. This form of knowledge would later be called an . Having both kinds of “intelligence” available in a single program allowed computer scientists to ask, “What makes certain scientists so much better than others? Do they have superior cognitive skills, or greater knowledge?”By the late 1960’s the answer was clear. The performance of Dendral was almost completely a function of the amount and quality of knowledge obtained from the experts. The cognitive model was only weakly related to improvements in performance.This realization led to a major paradigm shift in the artificial intelligence community. Knowledge engineering emerged as a discipline to model specific domains of human expertise using expert systems. And the expert systems they created often exceeded the performance of any single human decision maker. This remarkable success sparked great enthusiasm for expert systems within the artificial intelligence community, the military, industry, investors, and the popular press.As expert systems became commercially successful, researchers turned their attention to techniques for modeling these systems and making them more flexible across problem domains. It was during this period that object-oriented design and hierarchical ontologies were developed by the AI community and adopted by other parts of the computer community. Today hierarchical ontologies are at the heart of knowledge graphs, which have seen a resurgence in recent years.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesAs researchers settled on a form of knowledge representation known as “production rules,” a form of first order predicate logic, they discovered that the systems could learn automatically; i.e., the systems coud write or rewrite the rules themselves to improve performance based on additional data. Dendral was modified and given the ability to learn the rules of mass spectrometry based on the empirical data from experiments.As good as these expert systems were, they did have limitations. They were generally restricted to a particular problem domain, and could not distinguish from multiple plausible alternatives or utilize knowledge about structure or statistical correlation. To address some of these issues, researchers added certainty factors—numerical values that indicated how likely a particular fact is true.The start of the second paradigm shift in AI occurred when researchers realized that certainty factors could be wrapped into statistical models. Statistics and Bayesian inference could be used to model domain expertise from the empirical data. From this point forward, artificial intelligence would be increasingly dominated by machine learning.There is a problem, though. Although machine learning techniques such as random forest, neural networks, or GBTs (gradient boosted trees) produce accurate results, they are nearly impenetrable black boxes. Without intelligible output, machine learning models are less useful than traditional models in several respects. For example, with a traditional AI model, a practitioner might ask:Why did the model make this mistake?Is the model biased?Can we demonstrate regulatory compliance?Why does the model disagree with a domain expert?The lack of intelligibility has training implications as well. When a model breaks, and cannot explain why, it makes it more difficult to fix. Add more examples? What kind of examples? Although there are some simple trade-offs we can make in the interim, such as accepting less accurate predictions in exchange for intelligibility, the ability to explain machine learning models has emerged as one of the next big milestones to be achieved in AI.Keep up with advances in machine learning, AI, and big data analytics with InfoWorld’s Machine Learning and Analytics Report newsletterThey say that history repeats itself. Early AI research, like that of today, focused on modeling human reasoning and cognitive models. The three main issues facing early AI researchers—knowledge, explanation, and flexibility—also remain central to contemporary discussions of machine learning systems.Knowledge now takes the form of data, and the need for flexibility can be seen in the brittleness of neural networks, where slight perturbations of data produce dramatically different results. Explainability too has emerged as a top priority for AI researchers. It is somewhat ironic how, 60 years later, we have moved from trying to replicate human thinking to asking the machines how they think.", "pub_date": "2020-02-13"},
{"title": "Can you put your trust in AIops?", "overview": "As ops tools morph into AIops, they bring greater value to more complex deployments—if users will let them", "image_url": null, "url": "https://www.infoworld.com/article/3526444/the-state-of-aiops.html", "body": "AIops (artificial intelligence for IT operations) is one of those cool buzzwords that is actually part of another buzzword: cloudops (cloud operations), which is a part of the mother of all buzzwords: cloud computing.The concept of AIops and the tool category of AIops are really the maturation of operational tools in general. Most of those in the traditional ops tools space, at least in the past few years, bolted an AI engine onto a tool and called it AIops.Also on InfoWorld: Artificial intelligence predictions for 2020Some purpose-built AIops tool startups out there are leveraging AI from the jump. All are worth a look as you select AIops tools; however, there are no mainstream brands. The objective was and is obvious. Since most of these tools have been data gathering tools and analytics tools from the beginning, adding AI allows them to learn from that data rather than just externalize issues with the services under management. In some cases, they can correct issues using preprogrammed routines, such as restarting a server or blocking an IP address that seems to be attacking one of your servers.Now that we’re a few years into this paradigm and its technology offerings, we’re starting to note some patterns—some good, and some not so good. Let’s explore both. As far as what’s working, AIops tools in many instances are ops tools in their fourth, fifth, or sixth generations. Moreover, most of them have had public cloud management in mind for a while and are able to bridge the gap between on-premises legacy system management and managing applications and services in the public clouds.They are capable tools for managing and monitoring cloud, multicloud, legacy, and even IoT and edge-based systems. This ability to support complex system heterogeneity is really the true value of the ops tools, and why they are important to those implementing cloud or noncloud systems. Keep up with the latest developments in cloud computing with InfoWorld’s Cloud Computing Report newsletterThe downside seems to be that most users are not taking advantage of the AI subsystems in the tools, so you may be paying for a feature you’re not using. I don’t view this as the fault of the tool provider; for the most part, this relates to how the tools are installed, set up, and used by the traditional and cloudops teams. This is due to a lack of training in some cases, or a lack of valid use cases in the current set of systems—cloud and not cloud—under management. Clearly, AIops is going to be part of most cloud-based deployments. Just as clearly, the more complex those deployments, such as multicloud, the more value they will bring.", "pub_date": "2020-02-14"},
{"title": "RStudio Conference 2020 videos you don’t want to miss", "overview": "You can watch dozens of must-see RStudio Conference videos online. Don't know where to start? Let us help", "image_url": null, "url": "https://www.infoworld.com/article/3528303/rstudio-conference-2020-videos-you-dont-want-to-miss.html", "body": "With dozens of RStudio Conference videos now available online, it’s hard to know where to begin. I hope this look at some of my favorites will help get you started!Error messages in RI could probably watch Jenny Bryan teach data  (oh,  looks interesting, maybe  want to try typing in a thousand rows . . . . ). But in this keynote, she tackles a much more compelling topic: dealing with errors in R. There’s a lot of useful advice here, which she shares in an engaging, relatable way. One takeaway: Try the equivalent of a reboot—restart your R session! (I’ve been doing that much more often since returning from the conference.) Video: Object of type ‘closure’ is not subsettable.Also on InfoWorld: Artificial intelligence predictions for 2020New features in RStudioWondering what features are coming to the next version of RStudio desktop? RStudio’s Jonathan McPherson outlined several, including modern-era spell check (at last), better cloud usability on iOS, and more screen-reader accessibility for visually impaired users—something that also improves keyboard navigation for all users. Video: RStudio 1.3 Sneak Preview. State of the tidyverseRStudio Chief Scientist Hadley Wickham reviewed last year’s highlights from the tidyverse and this year’s plans for further development, but he was also fairly forthright in discussing some recent missteps.In particular, he acknowledged that the initial rollout of “tidy evaluation” launched with a somewhat difficult-to-master syntax and an unreasonable expectation that users would want to learn the detailed computing theory behind it. It turned out that many users didn’t care about the mechanics behind incorporating the tidyverse into their own custom functions; they just wanted to write their code. Since then, tidy eval syntax has been changed to more understandable  double braces.Wickham also outlined how tidyverse package authors will help users better understand the lifecycle of older functions and if/how some functions may be deprecated. Video: State of the tidyverse. Styled text with ggtextClaus Wilke gave an overview of the ggtext package in this fast-paced presentation, showing how to customize ggplot visualizations with colored text, images on axes, and more. He also explained the package’s current limits. Video: Spruce up your ggplot2 visualizations with formatted text. Below, you can also watch my Do More With R tutorial on one way to use ggtext: adding color to ggplot text. (Or read the companion article.)What you didn’t know about R’s scales packageI’ve used scales package functions such as  or  to add commas or dollar signs to a vector of numbers, but I never really explored the package further. Turns out that was my loss. At this presentation, data scientist Dana Seidel showed that scales does a lot more than format numbers. One tip: The  function lets you easily see how various colors and palettes look. Video: The little package that could: Taking visualizations to the next level with the scales package.Result from running .Customizing Shiny apps and R MarkdownShiny creator and RStudio CTO Joe Cheng demo’d bootstraplib, a new package for customizing the look of Shiny apps without having to hunt through and tweak complex CSS. The bootstraplib package lets you change Bootstrap defaults within an R script, without having to write HTML and CSS. (Bootstrap is the open source HTML/CSS/JavaScript framework used by Shiny and many other Web projects.) You can also use bootstraplib in non-Shiny R Markdown documents. Video: Styling Shiny apps with Sass and Bootstrap 4. Better spaghetti plots using brolgar in RA spaghetti plot that makes sense, using the brolgar R package. What do you get when you have a load of items plotted over time? That data type is known as longitudinal, and visualizing it can often end up looking like a pile of spaghetti. To help solve this problem, Nicholas Tierney at Monash University created the brolgar package (watch the presentation if you’re wondering why that name) to summarize, visualize, and otherwise understand such data. Video: Making better spaghetti (plots): Exploring the individuals in longitudinal data with the brolgar package. Dataviz best (and worst) practicesThis wasn’t R-specific, but University of Pennsylvania dataviz specialist Will Chase gave an engaging, opinionated talk on how to “take your charts from drab to fab.” One tip: “White space is like garlic — take as much as you think you need and triple it.” Video: The Glamour of Graphics.From Will Chase’s The Glamour of Graphics presentation.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesR Markdown to its limitsThere is a lot more one can do with R Markdown than I thought. And the fun-as-well-as-educational Teacup Giraffe site pushes the limits. In addition to enjoying a look at the Teacup Giraffe website, this presentation by neuroscience Ph.D. student Desiree De Leon includes some basic advice for improving your own R Markdown documents. Video: Of Teacups, Giraffes, & R Markdown.And speaking of getting more out of markdown, RStudio’s Yihui Xie had a separate talk showing how to generate many more file types than just HTML or PDF from an R Markdown document. Video: One R Markdown Document, Fourteen Demos. 3D visualizations in RI’d been resisting razzle-dazzle around the rayshader package for awhile. Did I  need to turn ggplots into 3D visualizations and animate them? But I’m glad I went to author Tyler Morgan-Wall’s presentation, because the package  pretty cool—even if I’m not sure yet how I’d use it in my own work.3D raytracer version of a 2D ggplot.Morgan-Wall showed how to turn a conventional graphic into a 3D visualization and animation with very little code. He also showed some recent package improvements that make some graphics more visually striking. In this case, seeing the animated examples is a lot better than trying to read about them. If you’re at all interested in this package, it’s worth watching the presentation. Video: 3D ggplots with rayshader. Accelerating analytics in RThe Apache Arrow project is a multi-language standard for in-memory data aimed at interoperability and high performance. Arrow has been implemented in R with the arrow package. Ursa Labs Engineering Director Neal Richardson outlined the status of Arrow in R, including the ability to query a directory of files using dplyr syntax without having to load that data into memory, as well as some upcoming features. Video: Accelerating analytics with Apache Arrow. list-columns in data.tableIf you’ve seen the heated discussions on social media, you might think that tidyverse and data.table are in two opposing camps. But while each has its fans, there are an increasing number of people who use both. Utah State Research Assistant Professor Tyson S. Barrett is one, and he brought data.table to RStudio Conference with a talk on using complex list-columns with data.table  tidyverse functions. One interesting tip: If you’re joining a complex data set, nesting all of the columns you’re  joining on can help prevent errors.from juliesquid for openscapes, illustrated by allison_horst(CC BY 4.0)Barrett also mentioned his tidyfast package, which has a streamlined “translation” of data.table code to tidyverse-like functions. (It’s similar to dtplyr but doesn’t use “lazy” data sets.) Unfortunately, this session video occasionally obscures some of the code and graphs being shown. If you watch this one, I suggest looking at the slides separately; they’re available at Barrett’s website. Video: List-columns in data.table: Reducing the cognitive & computational burden of complex data.Not familiar with data.table? Check out my Do More With R 5-minute intro below. Bonus: RStudio Conference 2020 lightning talksThere were a lot of interesting lightning talks, but a few stood out in part for the cool websites and packages being demo’d as well as the presentations themselves.RStudio intern Maya Gans showed a drag-and-drop interface for tidyverse tasks such as transforming, summarizing, and plotting data. It’s an interesting way to teach tidyverse concepts before students have to learn actual code. Video: TidyBlocks: using the language of the tidyverse in a blocks-based interface. Website: TidyBlocks.tech.The still-experimental livecode package lets you live code a demo and have it appear on attendees’ own systems in near real time. University of Edinburgh lecturer Colin Rundel explains why you’d want to do that and how it works. Video: `livecode`: Broadcast your live coding sessions from and to RStudio.Data science for software engineers: Busting software myths with R featured a website designed to teach statistics to software engineering students with relevant problems like: Does test-driven software improve quality? Does sleep deprivation make programmers more or less effective? When will that project be finished? Yim Register showed a bit of the site, but you can also check out all the lessons at Data Science for Software Engineers. Keep up with advances in machine learning, AI, and big data analytics with InfoWorld’s Machine Learning and Analytics Report newsletterBonus: RStudio Conference 2020 keynotesRStudio founder and CEO J.J. Allaire discussed the state of open source software, how it’s possible to fund open source efforts, and the company’s move to become a certified benefit corporation. Video (presentation only, not subsequent Q&A): Open Source Software for Data Science.Every time I see Martin Wattenberg and Fernanda Viegas speak, I leave feeling grateful that I’ve got a job that lets me peek into the work and thoughts of some supersmart people. Co-leaders of Google Brain’s PAIR (People+AI Research), the two discussed a number of their projects on topics like understanding algorithm bias. Several of the projects they discussed are available to the public. Video: Data, visualization, and designing. A final note: With multiple tracks going on at once, I missed a lot of excellent talks. I also attended other good ones that didn’t make the list because I didn’t want this article to get too long. For example, how Associated Press uses R (Larry Fenn), hitting R a million times a day at T-Mobile (Heather Nolis and Jacqueline Nolis), and tuning models with the tune and workflow packages (Max Kuhn). You can find all of the available videos here: https://resources.rstudio.com/rstudio-conf-2020.InfoWorld’s Do More With R video tutorials", "pub_date": "2020-02-21"},
{"title": "5 reasons to choose PyTorch for deep learning", "overview": "TensorFlow still has certain advantages, but a stronger case can be made for PyTorch every day", "image_url": null, "url": "https://www.infoworld.com/article/3528780/5-reasons-to-choose-pytorch-for-deep-learning.html", "body": "PyTorch is definitely the flavor of the moment, especially with the recent 1.3 and 1.4 releases bringing a host of performance improvements and more developer-friendly support for mobile platforms. But why should you choose to use PyTorch instead of other frameworks like MXNet, Chainer, or TensorFlow? Let’s look into five reasons that add up to a strong case for PyTorch.Before we get started, a plea to TensorFlow users who are already typing furious tweets and emails even before I begin: Yes, there are also plenty of reasons to choose TensorFlow over PyTorch, especially if you’re targeting mobile or web platforms. This isn’t intended to be a list of reasons that “TensorFlow sucks” and “PyTorch is brilliant,” but a set of reasons that together make PyTorch the framework I turn to first. TensorFlow is great in its own ways, I admit, so please hold off on the flames.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesPyTorch is PythonOne of the primary reasons that people choose PyTorch is that the code they look at is fairly simple to understand; the framework is designed and assembled to work with Python instead of often pushing up against it. Your models and layers are simply Python classes, and so is everything else: optimizers, data loaders, loss functions, transformations, and so on.Due to the eager execution mode that PyTorch operates under, rather than the static execution graph of traditional TensorFlow (yes, TensorFlow 2.0 does offer eager execution, but it’s a touch clunky at times) it’s very easy to reason about your custom PyTorch classes, and you can dig into debugging with TensorBoard or standard Python techniques all the way from  statements to generating flame graphs from stack trace samples. This all adds up to a very friendly welcome to those coming into deep learning from other data science frameworks such as Pandas or Scikit-learn.PyTorch also has the plus of a stable API that has only had one major change from the early releases to version 1.3 (that being the change of Variables to Tensors). While this is undoubtedly due to its young age, it does mean that the vast majority of PyTorch code you’ll see in the wild is recognizable and understandable no matter what version it was written for.PyTorch comes ready to use While the “batteries included” philosophy is definitely not exclusive to PyTorch, it’s remarkably easy to get up and running with PyTorch. Using PyTorch Hub, you can get a pre-trained ResNet-50 model with just one line of code:And PyTorch Hub is unified across domains, making it a one-stop shop for architectures for working with text and audio as well as vision.As well as models, PyTorch comes with a long list of, yes, loss functions and optimizers, like you’d expect, but also easy-to-use ways of loading in data and chaining built-in transformations. It’s also rather straightforward to build your own loaders or transforms. Because everything is Python, it’s simply a matter of implementing a standard class interface.One little note of caution is that a  of the batteries that are included with PyTorch have been very biased towards vision problems (found in the torchvision package), with some of the text and audio support being more rudimentary. I’m happy to report that in the post-1.0 era, the torchtext and torchaudio packages are being improved upon considerably.Also on InfoWorld: The best software development, cloud computing, data analytics, and machine learning productsPyTorch rules researchPyTorch is heaven for researchers, and you can see this in its use in papers at all major deep learning conferences. In 2018, PyTorch was growing fast, but in 2019, it has become the framework of choice at CVPR, ICLR, and ICML, among others. The reason for this wholehearted embrace is definitely linked to our first reason above: PyTorch is Python.Experimenting with new concepts is much easier when creating new custom components is a simple, stable subclass of a standard Python class. And the flexibility offered means that if you want to write a layer that sends parameter information to TensorBoard, ElasticSearch, or an Amazon S3 bucket... you can just do it. Want to pull in esoteric libraries and use them inline with network training or an odd new attempt at a training loop? PyTorch is not going to stand in your way.One thing holding PyTorch back a little has been the lack of a clear path from research to production. Indeed, TensorFlow still rules the roost for production usage, no matter how much PyTorch has taken over research. But with PyTorch 1.3 and the expansion of TorchScript, it has become easy to use Python annotations that use the JIT engine to compile research code into a graph representation, with resulting speedups and easy export to a C++ runtime. And these days, integrating PyTorch with Seldon Core and Kubeflow is supported, allowing for production deployments on Kubernetes that are almost (not quite) as simple as with TensorFlow.PyTorch makes learning deep learning easyThere are dozens of deep learning courses out there, but for my money the fast.ai course is the best — and it’s free! While the first year of the course leaned heavily on Keras, the fast.ai team — Jeremy Howard, Rachel Thomas, and Sylvain Gugger — switched to PyTorch in the second iteration of the course and haven’t looked back. (Though to be fair, they are bullish on Swift for TensorFlow.)In the most recent version of the course, you’ll discover how to achieve state-of-the-art results on tasks such as classification, segmentation, and predictions in text and vision domains, along with learning all about GANs and a host of tricks and insights that even hardened experts will find illuminating.While the fast.ai course uses fast.ai’s own library that provides further abstractions on top of PyTorch (making it even easier to get to grips with deep learning), the course also delves deep into the fundamentals, building a PyTorch-like library from scratch, which will give you a thorough understanding of how the internals of PyTorch actually work. The fast.ai team even manages to fix some bugs in mainline PyTorch along the way. Also on InfoWorld: Artificial intelligence predictions for 2020PyTorch has a great communityFinally, the PyTorch community is a wonderful thing. The main website at pytorch.org has both great documentation that is kept in good sync with the PyTorch releases and an excellent set of tutorials that cover everything from an hour blitz of PyTorch’s main features to deeper dives on how to extend the library with custom C++ operators. While the tutorials could use a little more standardization around things like training/validation/test splits and training loops, they are an invaluable resource, especially when a new feature is introduced.Beyond the official documentation, the Discourse-based forum at discuss.pytorch.org is an amazing resource where you can easily find yourself talking to and being helped out by core PyTorch developers. With over fifteen hundred posts a week, it’s a friendly and active community. And while discussion is more focused on fast.ai’s own library, the similar forums over at forums.fast.ai is another great community (with lots of crossover) that is eager to help newcomers in a non-gatekeeping manner, which sadly is a problem in many arenas of deep learning discussion.PyTorch today and tomorrowThere you have it—five reasons to use PyTorch. As I said at the beginning, not all of these are exclusive to PyTorch versus competitors, but the combination of all of these reasons makes PyTorch my deep learning framework of choice. There are definitely areas where PyTorch is currently deficient — e.g., in mobile, with sparse networks, and easy quantizing of models, just to pick three out of the hat. But given the high speed of development, PyTorch will be a much stronger performer in these areas by year’s end.A couple of further examples just to finish us out. First, PyTorch Elastic — introduced as an experimental feature in December — extends PyTorch’s existing distributed training packages to provide for more robust training of large-scale models. As the name suggests, it does so by running on multiple machines with elasticity, allowing nodes to drop in and out of the training job at any time without causing the entire job to come crashing to a halt.Also on InfoWorld: What is CUDA? Parallel programming for GPUsSecond, OpenAI has announced it is adopting PyTorch as its primary development framework. This is a major win for PyTorch, as it indicates that the creators of GPT-2 — a state-of-the-art language model for question answering, machine translation, reading comprehension, and summarization — believe that PyTorch offers them a more productive environment than TensorFlow for iterating over their ideas. Coming in the wake of Preferred Networks putting its deep learning framework Chainer into maintenance mode and moving to PyTorch, OpenAI’s decision highlights how far PyTorch has come in the past two years and strongly suggests that PyTorch will continue to improve and gain users in the years to come. If these big players in the AI world prefer to use PyTorch, then it’s probably good for the rest of us too.", "pub_date": "2020-02-24"},
{"title": "Use Azure Cognitive Services to automate forms processing", "overview": "Form Recognizer brings unsupervised machine learning to paper document processing, and it’s a snap to  build into your applications", "image_url": null, "url": "https://www.infoworld.com/article/3528784/use-azure-cognitive-services-to-automate-forms-processing.html", "body": "Microsoft’s Cognitive Services, powered by machine learning, are an easy way to add artificial intelligence to your apps, offering pay-as-you-go access to a selection of useful algorithms. Unlike many other web services, they’re continuously evolving, improving as they ingest more and more labeled data.That’s an important difference between machine learning and other, more familiar, algorithms. As Microsoft improves its training and models, the scope of the services continues to get better, along with responsiveness and accuracy. Some can even take advantage of a process called Transfer Learning, where training a model with one set of data improves its performance with another.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesContinuous improvement isn’t the only benefit of the research work Microsoft puts into its Cognitive Services. Cognitive Services operationalize that research, delivering new tools and services as research moves from the lab into commercial products. What matters here is the transition between preview and general availability, as Azure and Microsoft Research work together to take what was pure research and turn it into tools you can include in your applications.Microsoft has been able to containerize some of its Cognitive Services for use on Azure’s Edge servers and on any other platform that supports Docker. Instead of pushing data to the cloud over low-bandwidth links, you can process data locally, as part of an IoT Hub instance, sending only the information that matters to other applications or to administrators.Introducing Form RecognizerOne of the more interesting new services currently in preview is Form Recognizer. As organizations work their way through digital transformations, it’s important to bring paper documents and forms into new business processes. Traditional scanning and optical character recognition go some way to digitizing documents, but they miss the semistructured nature of forms, scanning all the data on the page.Form Recognizer takes a more nuanced approach to working with form data, using machine learning to parse the structure of a form and then extract the information. By building a model of the structure of a form, you can use that model to build a semantically tagged output, with key/value pairs and tables that can then be used to populate structured stores, either using SQL or NoSQL document databases. All you need are a handful of forms to use as training data, allowing you to build a labeled data set that can be used to tune the Form Recognizer model to work with your files.As Form Recognizer is an API, you can incorporate it in new and existing business processes, replacing manual data capture processes while flagging exceptions that may need human intervention. You can even use Form Recognizer in conjunction with Power Platform tools such as Power BI to deliver business insights from what would have been paper-only data.Also on InfoWorld: Microsoft Azure cloud migration: 3 success storiesTraining a Form Recognizer model One of the interesting aspects of Form Recognizer is that the underlying model uses unsupervised learning. There’s no need to label the training data. The system recognizes the form elements and generates the appropriate data structures for your form data. Although that’s an easier way to train a system, you do have the option of using labeled data to get more accurate and faster results.A key element of the training process is the layout API. This gives the model a structure for the layout of a form, with labels for the various fields. Using the data from this and from labeled training forms, you can quickly define the output data structures and ensure that your code is ready to work with the service.Building labeled samples for training requires a local application, available as a Docker container with a Web UI. You can download it from Microsoft and run the container on Windows, MacOS, or Linux, if you have Docker installed. There’s even the option to run the container with Azure Kubernetes Service (AKS) or on any other Kubernetes infrastructure. Form images are stored in an Azure Blob, and the local recognizer will OCR the forms, making them ready for you to label the various form elements that you want to extract using Form Recognizer. You only need five or six sample forms to train the model.Once trained, you have a custom Form Recognizer model with its own model ID and an accuracy score. If you want to improve the model, add more sample data. The resulting model can be tested using the training tool on documents that haven’t been part of your training set. You’ll be presented with a view of the source document with bounding boxes for recognized data and a confidence level for each element. It’s important to note that Form Recognizer can’t work with all form elements; at the moment there’s no support for check boxes or for complex tables.Also on InfoWorld: Artificial intelligence predictions for 2020Using Form Recognizer in your appsBuilding an application around Form Recognizer is relatively easy. If you’re not using a language with a supported SDK there’s a REST API that can take your form images and extract the data. The service currently supports most common image formats: JPC, PNG, PDF, and TIFF.The API is relatively simple; it uses POST to upload and analyze the form contents, with a GET to bring back the result. Images are sent as part of a JSON object with the POST, or as a standard file stream. Once the job has been loaded, a standard HTTP 2020 response returns the result ID that will hold the analysis results. You can then make a call to the service with the result ID. If the form has been processed, the results will be delivered in a JSON object that can be parsed, delivering the form key/value pairs and any result tables.Like all the Cognitive Services, the results have a confidence level. You can use this to direct some forms for manual checks, otherwise delivering the result data into your line of business applications, either storing the data for future use or using it to drive a business process.One useful feature of Form Recognizer (and one that clearly builds on Microsoft’s own requirements for its expense system) is a prebuilt model that works with common U.S. receipt formats. You can use it to capture and feed receipt data into your own expenses workflow, using a phone camera to capture receipt data on the go. Workers will be able to generate expense reports from their phones without having to spend time entering data into web forms; the Form Recognizer tools will capture the necessary data and, together with user information and device locations, update records automatically.Keep up with the latest developments in software development, cloud computing, data analytics, and machine learning with the InfoWorld Daily newsletterGetting started with Form Recognizer is relatively simple, and with a generous limit of 500 free pages per month, you should be able to see quickly if it works for you. Once up and running, it should provide a useful bridge between pen and paper and the digital world, using photographs or scans to quickly bring form content into your business processes. With the quality of modern phone cameras and their support for computational photography, it’s possible to make form recognition a simple plug-in that takes a photo and uploads it to your recognizer, saving a local copy for your records.Form Recognizer is a tool that quickly shows the benefit of machine learning, with a model that’s designed to work flexibly in a relatively closed domain. Applying Azure’s Cognitive Services to specific business problems makes a lot of sense. Handling a paper-to-digital transition is one of those problems that has long been a blocker to improving business processes. Using machine learning to reduce the cost and time needed to deliver digitization is a win for most businesses, especially if it means that we can use the cameras in our pocket rather than expensive scanners and unreliable OCR software.", "pub_date": "2020-02-25"},
{"title": "Are you ready for multicloud? A checklist", "overview": "For many organizations, the use of multiple public clouds is inevitable. Here’s how to make your multicloud strategic instead of accidental. ", "image_url": null, "url": "https://www.infoworld.com/article/3584433/are-you-ready-for-multicloud-a-checklist.html", "body": "Perhaps you deployed your first cloud-native Node.js application to Amazon Web Services and then received a new assignment to port over several legacy .NET applications to a public cloud. Should you try Amazon Lightsail as a first step, or should you review Microsoft Azure’s options for .NET developers?MulticloudAre you ready for multicloud? A checklist (InfoWorld)5 challenges every multicloud strategy must address (CIO)How to manage multiple cloud collaboration tools in a WFH world (Computerworld)Building stronger multicloud security: 3 key elements (CSO)Multicloud management: Challenges for technology, people, processes (Network World)Or maybe your team has applications running on Azure that need to securely connect to machine learning models deployed by the data science team on Google Cloud Platform. It’s easy to conceive scenarios where development and data science teams end up exploring, prototyping, and deploying applications, databases, microservices, and machine learning models to multiple public clouds.Also on InfoWorld: When hybrid multicloud has technical advantagesFor larger enterprises, supporting multiple clouds is almost inevitable, due to the herculean level of governance required to channel all development, data science, and shadow IT efforts to a single public cloud. Even so, there are several reasons why global businesses and larger enterprises determine that “being multicloud” is strategically important.I reached out to several experienced IT leaders active on Twitter through social chats like IDGTechTalk and CIOChat to get their perspectives on whether and how organizations should support multiple clouds. I’ve included their opinions and insights in this checklist on multicloud readiness. Are you ready?Ease into multicloud complexities IT leaders know the complexities of setting up secure and robust cloud infrastructures. Naturally, these complexities multiply when you combine multiple clouds. You should strive to avoid dealing with them all at once. Operating across multiple clouds is complex because of the required governance, technical expertise, and integrations. As Sarbjeet Johal, an independent technology strategist, puts it, “Nobody gets up in the morning and says we are going to do multicloud today. They just fall into it, mainly due to organizational silos. Multicloud is as easy as 1-2-3... said no one ever!”Joanne Friedman, Ph.D. and CEO of Connektedminds, suggests that IT teams leverage their primary cloud provider wherever possible, rather than hunt for new or better capabilities in a second provider. “Accept that there is no one size fits all in public clouds, and only 40% to 60% of what is required can be found with one provider,” she advises. Other IT leaders share pragmatic viewpoints on how multiclouds evolve and how to navigate initial complexities. Travis Campbell, a big data consultant, offers this insight into where the multicloud journey begins:Companies doing ‘multicloud’ but really treating it as a single cloud by each line of business are a special case here. For example, finance may have applications on cloud X, while engineering is deploying to cloud Y, and there’s no cross-pollination of work and data. It’s multicloud without hard problems.So, in the case of organizations already operating multiple clouds independently for different purposes, an inevitable next step would be application integrations, data integrations, or service orchestrations across those clouds. This is where those hard problems begin. It’s best to proceed slowly and cautiously. Make the case for a multicloud architectureOther IT leaders chimed in with similar sentiments. Specifically, many noted that supporting multicloud isn’t easy today, and that IT leaders should identify strong business rationale before endorsing a multicloud architecture. I asked them to identify some of the reasons why they might seek multicloud architectures.Mark Thiele, CEO and Founder of Edgevana, outlines a number of strategic benefits to seek from multicloud architectures. “I would target multicloud when it provides my customers with one or more of significant price value, speed to market, a unique technical capability that drives better value, innovation, and performance improvements,” he says. Mike D. Kail, Executive Technologist at Palo Alto Strategy Group, agrees. “The main reason [to consider multicloud] is when a cloud service provider offers a service that is far and above better than the one used in your initial deployments,” he says. “An example would be TensorFlow for artificial intelligence and machine learning.”Ed Featherston, Distinguished Technologist at HPE, seconds that view, and provides some operational guidance:Many organizations I work with select one platform as their main focus, and other platforms based on business benefit for specific needs and solutions. IT leaders must define how they approach multicloud from a strategy, support, and process perspective. When other platforms are brought in, there is a framework and structure to provide consistency in their approach. Don’t let it be because of “cloud sprawl,” as that always results in disaster.Chris Ibbitson, Chief Technologist of Financial Services at HPE, says that enterprises seek agility in public and private clouds. “Whether it is with public cloud providers, or in a hybrid cloud model, utilizing a mix of private cloud capabilities and multiple public cloud providers is focused on the agility and speed of delivering change,” he says. “While most organizations have already adopted some form of hybrid cloud, the focus is now turning to multicloud.”So IT might bring in a second cloud service provider to support a specific business or technical need. In these cases, IT leaders should define which businesses and use cases each cloud will serve and which services and technologies each cloud will provide. Also on InfoWorld: 6 ways Alibaba Cloud challenges AWS, Azure, and GCPJohal and others shared some additional reasons:Large enterprises seek to avoid being reliant on a single cloud service provider, especially if they expect to negotiate enterprise-specific service levels and pricing.Data sovereignty and compliance often require storing data in the country of residence, specific requirements on encryption and data security, or even specificity on acceptable cloud service providers.Organizations seeking mergers and acquisitions often target multiple cloud service providers and support models to enable easy onboarding and to simplify support structures.IT requires specific technical capabilities, especially at scale, where there might be strategic business advantages in deploying to one cloud service provider versus another.When IT elects to self-manage a commercially bought application that runs on a public cloud that’s different from the public cloud IT has adopted as their development standard. Industry-specific and other niche applications may provide the structure and support models on a single cloud service provider.There are also places where hybrid and multicloud architectures offer technical advantages, especially for edge computing, human safety applications, and real-time analytics.Review cloud support models before going multicloudWhile businesses may have some rationale to support a multicloud architecture, IT leaders still strongly suggest performing financial analyses and reviewing your cloud operating models.Kail suggests answering these questions, “Are the benefits going to be greater than the complexities of deploying and operating a multicloud architecture? Are there financial implications, both short and long term?”Steven Kaplan, Vice President of Customer Success Finance at Nutanix, agrees. He suggests, “As with any significant IT decision, it’s imperative to do a comprehensive financial analysis not only to select the most strategically optimal solution but to provide both the justification and financial baseline for moving forward.”When there is justification, IT must consider extending their service models and expertise from one cloud to multicloud. Featherston notes that cloud is more about people, process, and culture than it is about the technology. “Organizations should try to get through those dramatic transformations in their organization for one platform first to lay the groundwork and provide the framework to move forward on other platforms,” he says. “Basically, get one cloud right first, then branch out.”Thiele offers several specific practices that should be in place before expanding multicloud. “The ability to coordinate common processes around security, deployment, visibility, and resource management, among other things, is vital. Having well-defined devops and devsecops processes and tools enable both improved efficiency and more consistent security policy.”Friedman’s recommendation is to start with infrastructure as code tools. “They support a form of infrastructure automation and configuration,” she notes. “While they don’t address the entire scope of cloud governance, having programmable and version-controlled infrastructure is important.”Embrace devops to support multicloud architecturesI agree with Thiele and Friedman that robust devsecops practices—especially around CI/CD for deploying applications, infrastructure as code for provisioning and configuring infrastructure, and monitoring capabilities including AIops—are critical to both implementing and supporting multicloud architectures.But what I learned from this group of IT leaders is that not just any devops technology or devops practices will do, as some are better suited for multicloud strategies than others. One approach is to steer away from the proprietary tools of cloud service providers, such as AWS CloudFormation, Azure Resource Manager, or Google Cloud Deployment Manager, even as some are providing multicloud support. Enterprise IT groups should also expand their devops culture from a deployment-frequency focus to include deployment agility.Kail has a specific recommendation. “Infrastructure-as-code should be table stakes to mitigate configuration drift, and of course security needs to be architected from day one,” he notes. “Tools such as Terraform certainly help. Security solutions that can span multicloud are also paramount.”Multiple IT leaders recommended Terraform for multicloud architectures because it’s a declarative, agentless, masterless provisioning tool. One recommended architecture pairs Terraform with Packer, Docker, and Kubernetes to support provisioning, server templating, and orchestration.However, selecting a multicloud-enabling tool doesn’t mean the implementation supports multiple clouds. Featherston recommends Terraform, but with a disclaimer. “The one caveat I have is making sure clients understand, it’s a common language to use on multiple platforms, but the code to build AWS does not build Azure,” he explains. “The benefit is that the team has a common language to code in, but the tradeoff is that not all platform features may be available.”Beyond infrastructure as code, Friedman recommends the following tools, practices, and governance to meet multicloud requirements:Automation and orchestration for both applications and individual virtual machinesSecurity including identity management and data protection/encryptionPolicy governance and compliance including audits and SLA metricsPerformance monitoring of both the infrastructure (i.e. compute instances, storage, networks) and applicationsCost management through resource optimization and billing estimatesTechnology companies are also selling multicloud enabling technologies. For example, HPE GreenLake Central provides visibility into cost and compliance across different cloud providers, and Google Cloud Anthos enables a consistent development and operations experience for hybrid and multicloud environments.Also on InfoWorld: Which multicloud architecture will win out?Prepare for a multicloud futureDespite all of the complexities today in adopting multiple public clouds, the consensus among IT leaders is that most enterprises will support multicloud architectures, and even integrate applications across clouds. We can use history as a lesson, as enterprises had to support Linux and Windows, .NET and Java, and Oracle and Microsoft SQL databases, to name just a few examples—despite all of the rationale behind sticking to one platform.Some final thoughts from these leaders:“I expect we’ll see an evolution of multicloud over the next three to five years, whereby more workloads are designed and abstracted away from the underlying cloud provider by default. The focus will be on the advanced capabilities different cloud providers can enable, such as artificial intelligence, machine learning, and analytics.” – Chris Ibbitson”Solve for a single cloud first, then determine if multicloud is right for you.” – Mike D. Kail“Multicloud architectures are hard and expensive to maintain. Be strategic about single cloud providers and tactical about multicloud undertakings. Avoid doing multicloud at the infrastructure layer for the same workloads at almost all costs.” – Sarbjeet Johal“Opt for the paths of least resistance, focus on security first, then automation and orchestration inculcated through policy.” – Joanne FriedmanTravis Campbell sums it up well:I think the people being successful at this are doing lowest common denominator stuff and using the barest minimum on each provider with their layer baked on top to handle the abstraction. Build once, run anywhere is the golden ticket we’re all seeking. We’re still a ways out.Thanks to these contributors for sharing their experience and insight. You can find all of them on Twitter: @efeatherston, @ibbitsc, @joannefriedman, @mdkail, @mthiele10,  @ROIdude, and @sarbjeetjohal. ", "pub_date": "2020-12-14"},
{"title": "The multicloud challenge: Building the future everywhere", "overview": "At a time when procuring on-prem infrastructure and personnel seems more daunting than ever, organizations are increasingly turning to multiple clouds to provide just the array of functionality they need. ", "image_url": null, "url": "https://www.infoworld.com/article/3600216/the-multicloud-challenge-building-the-future-everywhere.html", "body": "Fear of the cloud has evaporated. Instead, most companies now use at least several public clouds, from AWS to Azure to Salesforce to Slack. Hence the ascendance of the term “multicloud,” which now encompasses not just the management of IaaS and SaaS clouds, but also private clouds of virtualized on-prem resources.MulticloudAre you ready for multicloud? A checklist (InfoWorld)5 challenges every multicloud strategy must address (CIO)How to manage multiple cloud collaboration tools in a WFH world (Computerworld)Building stronger multicloud security: 3 key elements (CSO)Multicloud management: Challenges for technology, people, processes (Network World)The low barrier to entry of the cloud has been both a blessing and a curse. The ability to simply open a cloud account and start using an application or building one has delivered unprecedented agility. But it also makes it easy for stakeholders to go off in their own directions, sometimes with too little regard for cost or security risks.Multicloud’s problem is as old as IT: the problem of governance. For some that’s an ugly word, because it smacks of a bureaucracy that stands squarely in the way of getting things done, as in: Fill out your request in triplicate and you’ll get a couple of cloud VMs in six weeks if you’re lucky. But few would advocate anarchy, either – you don’t want developers running around building cloud applications on a whim using, say, pricey AI/ML services and live customer data.In a recent CIO Think Tank, Thomas Sweet, vice president of IT solutions at GM Financial, introduced a well-chosen phrase: “minimum viable governance.” Instead of pummeling people with prohibitions or elaborate approval processes, give them lightweight cloud “guardrails” to prevent duplicate efforts or poor cloud security. Couple those with cost ceilings and a catalog of pre-approved cloud services, and developers or enterprising LoB managers have the freedom they need to experiment and innovate.In particular, the big three IaaS clouds – AWS, Google Cloud Platform, and Microsoft Azure – provide environments where innovation can flourish, in part because they’re cauldrons of emerging technology, from serverless computing to AR/VR app dev platforms. For many organizations, “multicloud” really refers to adopting two or more big-three IaaS clouds, mainly because the second or third cloud offers a new or better cloud service others lack. Wrapping guardrails around that multiplicity is an endless governance challenge.But that’s where the future is pointing: Toward a world where we assemble hundreds of cloud services from multiple providers into the applications we and our customers need, iterating and innovating as well go. This collection of articles from InfoWorld, CIO, Computerworld, CSO, and Network World explains how forward-looking organizations are moving toward that goal and the lessons they’re learning along the way.", "pub_date": "2020-12-14"},
{"title": "Elevating SaaS to its rightful place in the enterprise", "overview": "Most enterprises don’t consider SaaS as much as they should in their cloud journey. That’s a huge mistake.  ", "image_url": null, "url": "https://www.infoworld.com/article/3600903/elevating-saas-to-its-rightful-place-in-the-enterprise.html", "body": "According to Gartner, SaaS (software as a service) remains the largest sector of the cloud computing market and is expected to grow to $117.7 billion in 2021 (a 16 percent increase). This is pretty impressive growth, which largely has been driven by the pandemic and the need for SaaS systems to support remote work. SaaS has long been the red-headed stepchild of the cloud computing world. Most don’t consider SaaS as part of the cloud, focusing instead on IaaS providers, including AWS, Google, and Microsoft. This is largely because the SaaS world is widely distributed, with more than 5,000 SaaS applications out there, from bail bonds management to full-blown ERP systems on demand. Also on InfoWorld: PaaS, CaaS, or FaaS? How to chooseSalesforce.com is certainly well known, and table stakes for most enterprises, but by doing the math you’ll quickly determine that most of the SaaS market is smaller, more tactical business systems on demand. These systems are more cost efficient than having them sit on servers in your data center.    There are some areas where enterprises can improve their use of SaaS within their larger cloud computing strategy. A few suggestions: Plan out and build mechanisms for moving data in and between SaaS systems and other enterprises systems, either on-premises or within IaaS clouds. Extend your larger security strategy, such as the use of IAM (identity and access management), to SaaS systems. I’m seeing a great deal of vulnerabilities in SaaS systems as security becomes an afterthought, taking a back seat to time to market. We use SaaS systems because they bring prebuilt business processes that we don’t have to create in net-new applications. However, these are not optimized unless integrated with other application processes housed on-premises or in cloud-based systems. Process orchestration layers are handy here.SaaS was really the first part of the cloud computing market that emerged in the late 90s and proved that cloud was a viable and cost-effective alternative to application ownership. That said, it’s not getting the respect it deserves, and that will get you into trouble quickly.  ", "pub_date": "2020-12-15"},
{"title": "Go local with Azure Logic Apps development", "overview": "Microsoft’s low-code business process automation tools add container runtimes and a local development environment.", "image_url": null, "url": "https://www.infoworld.com/article/3601348/go-local-with-azure-logic-apps-development.html", "body": "Low- and no-code development have become key for digital transformation, giving information workers the tools to build the apps they need. There’s a significant app gap, one that under-resourced development teams are increasingly unable to fill. So giving tools to end-users to write code makes a lot of sense, as it lets them scratch their itches, building process automations that allow them to concentrate on the more complex parts of their tasks.Microsoft has been rolling out enhancements to its Power Platform process automation suite, with a focus on working with its line-of-business Dynamics 365 suite. But the Power Platform isn’t the only low-code environment in Microsoft’s developer toolbox; Azure also offers a process automation workflow tool in the shape of its serverless Logic Apps.Also on InfoWorld: 7 low-code platforms developers should knowBusiness process automation in AzureAlthough Logic Apps is often thought of as a no-code tool, it’s better understood as an alternative to the venerable BizTalk business automation tool. When you build a Logic App, you’re providing basic rules for linking more than one application with a network of components, providing the rules and structure needed for message routing.With Logic Apps, events from one application can be processed and routed to another using connectors to translate them into the appropriate format for an API. The systems being connected don’t need to be event driven; the connectors handle translation from event to write to RPC call to, well, however the application interacts with the world—even down to remotely driving a green-screen user interface.It’s very much a developer-focused tool, helping fill the other app gap: the space between applications that weren’t intended to be used together, but when connected fulfil vital business functions. Using a serverless process automation tool helps solve a lot of problems; you don’t need to provision hardware, and you can use it to build hybrid applications that bridge SaaS platforms and on-premises code.At first sight Logic Apps looks very like Power Apps, with a similar graphical editor for process flows. A recent update added a new design tool that helps visualize more complex workflows, as the application being built using Logic Apps uses more and more features, building out workflow rules that could be hard to display. Microsoft will continue to build on the new Logic Apps layout engine, adding features that come with the new Logic Apps runtime.Writing Logic Apps in codeYou don’t need to use a graphical tool to build a Logic App; Microsoft provides a JSON-based declarative programming language for building flows between connectors, with a set of Visual Studio and Visual Studio Code tools. The underlying technology is Microsoft’s Workflow Definition Language (WDL), which gives you the tools to build a workflow that links two more connectors using declarative statements.The WDL is defined with a schema, which needs to be linked at the start of the JSON workflow file. This defines the version being used so the appropriate workflow engine is called. Your file will then define actions, triggers, and outputs. Triggers are the inputs to a workflow, with as many as 10 supported in a WDL application. That’s an advantage over the graphical designer, which lets you have only one trigger per workflow.Switching to code can help build much more complex rule sets, allowing you to mix multiple inputs or have flows that are driven by conditional operations; for example, only processing an event on input 1 if inputs 2 and 3 both have specific values. Similarly, you can drive as many as 10 different outputs, through as many as 250 actions. Other options include parameters that are used within a workflow; these need to be defined before runtime.Although there are issues with the visual designer in Visual Studio Code, it can help you create initial connections and visualize the flow of your WDL applications. Think of it as a visual linter, a way to quickly show problems and indicate dead ends and loops in your code. Building rule sets this way is relatively easy, but complex flows are best visualized before you try them out. You can use static results to build mocks into a flow, either for testing or to manage specific input or error cases.Working with WDL in Visual Studio CodeWDL is a declarative language so it has the option of including expressions that are evaluated at run time. This approach lets you use values from parameter arrays; for example, if you have an array that contains lists of products and prices as an array of key/value pairs, you can use price in a flow by referencing the product in an expression.As WDL is the output of the visual design tool in either Visual Studio or in the Azure Portal, you can use it to create an initial scaffold for your application and then edit it in Visual Studio Code to go beyond the limits of the visual tool. As part of the same project you can now create your own custom connectors or extend existing ones, linking Logic Apps to your own applications or to partner APIs.The Visual Studio Code Logic Apps extension installs from the Marketplace as part of the Azure extension group, and you can access it directly from the Azure menu in the Code sidebar. Once signed into Azure, you can attach a new Logic App to an Azure account, either as an individual app or as part of a larger project. You will need to either create a new resource group for your Logic App or add it to an existing one.Once you’ve chosen a region and named your app, you’re presented with a blank logic app and the JSON skeleton for its code. Usefully you’re not limited to running Logic Apps in Azure, as the runtime is containerized and can be hosted in your own Kubernetes environments or used as a local development environment on Windows, macOS, and Linux. Creating a local development environment requires opening a new local project from the Visual Studio Code Logic Apps extension, connecting it to an Azure resource group, and creating a Logic App.A welcome expansionMicrosoft’s expansion of its Logic Apps platform is welcome, as workflow and process automation are important integration tools that allow you to link applications with minimal code. Adding it to both Visual Studio and Visual Studio Code makes it easier to include in projects, mixing serverless Azure Functions, local container runtimes, and your own custom connectors to build complex rules-based workflows with minimal code.As Logic Apps gains more functionality with new runtimes and new development tools, it’s going to become more of an alternative to BizTalk, especially for organizations that are taking a hybrid approach to cloud development and using containers to move workloads between cloud and on-premises.With Kubernetes seeing an increased role at the edge of the network, it will be possible to build and deploy process automation as close as possible to information sources, using containers for both Logic Apps and Functions, as well as for your own connectors. Logic Apps’ ease of development will be a bonus and, like Power Automate, should go some way to reducing the app gap, if not filling it.", "pub_date": "2020-12-15"},
{"title": "When AIops tools outsmart you", "overview": "AIops is leading us to the operational promised land for cloud computing, but we have to be smart enough to know how to follow", "image_url": null, "url": "https://www.infoworld.com/article/3575867/when-aiops-tools-outsmart-you.html", "body": "Our ability to augment technology with artificial intelligence and machine learning does not seem to have limits. We now have AI-powered analytics, smart Internet of Things, AI at the edge, and of course AIops tools.At their essence, AIops tools do smart automations. These include self-healing, proactive maintenance, even working with security and governance systems to coordinate actions, such as identifying a performance issue as a breach.Also on InfoWorld: How to choose a cloud machine learning platformWe need to consider discovery as well, or the capability of gathering data ongoing and leveraging that data to train the knowledge engine. This allows the knowledgebases to become savvier. Greater knowledge about how the systems under management behave or are likely to behave creates a better capability of predicting issues and being proactive around fixes and reporting. Some of the other advantages of AIops automation:Removing the humans from cloudops processes, only alerting them when things require manual intervention. This means fewer operational personnel and lower costs.Automatic generation of trouble tickets and direct interaction with support operations, removing all manual and nonautomated processes.Finding the root cause of an issue and fixing it, either through automated or manual mechanisms (self-healing).  Some of the advantages of AIops discovery:Integrating AIops with other enterprise tools, such as devops, governance, and security operations.Looking for trends that allow the operational team to be proactive, as covered above.Examining huge amount of data from the resources under management, and providing meaningful summaries, which allows for automated action based on summary data.AIops is powerful technology. What are some of the hindrances to taking full advantage of AIops and the power of the tools? The quick answer is the humans. I’m finding that AIOps tools are not being used or considered, mostly due to shortsighted budget issues. If they are being used, they are not leveraged in optimal ways.    Although it would be easy to blame the IT organizations themselves, the larger issue is the lack of a critical mass of best practices of the right way to use AIops. Even some of the providers are pushing their own customers in the wrong directions, and I’m spending a lot of time these days attempting to course correct.    The core issue is the complexity of the AIops tools themselves—ironic considering that they are supposed to combat operational complexities of cloud computing. The difficulty in how to configure the tools properly is systemic.  What are the best practices that are being ignored or misunderstood? I have a few to share this time, but more in the future: The people using AIops tools don’t have a holistic understanding of what all of the systems, applications, and databases mean. No coordination across tool silos could actually lead to more vulnerabilities.  . These complex tools require that you understand the workings of AI engines, the correct use of automation, and, most importantly, the correct way to test these tools.You would hate to have your own AIops solution be smarter than you. The best way to avoid that is to try not to be dumb—just saying.", "pub_date": "2020-09-25"},
{"title": "Where to find free and open data sets on the web", "overview": "From government, health, and finance data to weather, baseball, and Star Trek, countless collections of free data are available to scratch your analytical itch", "image_url": null, "url": "https://www.infoworld.com/article/3574979/where-to-find-free-and-open-data-sets-on-the-web.html", "body": "Bosses love to hear the word “free.” Everyone wants to get something for nothing. The good news is that there’s a burgeoning collection of free data available for the taking. Some of it might even be useful for your project or your career.What’s the catch? Sometimes there’s no catch at all. Many of the sources below come from government agencies. Once they’re done collecting the information, it often costs them very little to share it openly with everyone. Technically it’s not free because you’re paying for it on April 15th. But the good news is that your project budget won’t feel the pinch.Also on InfoWorld: How to choose a cloud machine learning platformOther data collections are a subtle form of advertising. All of the major cloud companies host various collections of open data sets. You don’t need to use their cloud servers, but the performance will be that much better when the bits are stored in the same data center. The cloud companies could be purchasing 30-second spots on the Super Bowl, but this form of advertising is a better strategy for everyone.The one danger with working with cost-free data is that the boss will assume that it’s also trouble-free. Many times the data will require a bit more work on your part. Perhaps the government agency that collected it liked to use its own peculiar format. Perhaps the data needs to be re-aggregated for your needs. There’s a good chance you’re going to need to write a bit of code to get it to work.Some of the data projects function like open source software and work best when everyone contributes their own small part. I have a weather station in my backyard hooked up to the Personal Weather Station network that gathers data from close to a quarter million different citizen scientists. Participation is essential, but you’ll be able to leverage the work of everyone else at the same time. If your work is going to help build these projects, be prepared to pull your weight with project management.The good news is that the barriers to entry are small. You don’t need to ask permission and you don’t need to beg forgiveness. Here are N different corners of the web to just start downloading and exploring.Data.govThe General Services Agency (GSA) maintains Data.gov, a big list of data sets the US government shares openly. As of this writing, there are 210,756 entries, many from the agencies that specialize in support of commerce (maritime, agriculture, energy). There are no secrets from classified agencies, though, and nothing from Area 51.Kaggle Some of the data sources are not much more than a file repository. Kaggle is more of a cult. They've started with more than 50,000 different data sets and then added the basic tools (Jupyter notebooks) for making sense of them. There are already 400,000 different public notebooks that other data scientists have shared that analyze the data underneath. On top of that, Kaggle has added some online courses on using everything and mixed in some competitions with real cash prizes.For instance, Cornell’s Laboratory of Ornithology is offering $25,000 to the best classifiers for birdsong, or what they call “bird vocalizations.” The Open Vaccine initiative will award $25,000 to the best models for predicting RNA degradation that will affect the COVID-19 vaccine. There is plenty of serious work to be found among the CSV or JSON files, but if you grow tired you can also have some fun. One data collection, for instance, is filled with lines scraped from all of the Star Trek episodes from the six major series. FiveThirtyEightThe FiveThirtyEight website is devoted to reporting stories with the support of a rich collection of data. When they can, they also share these data sets for you to do your own research. There are past records of their predictions for the major sports leagues, explorations about social attitudes like surveys of men asking what it means to be a man, and, of course, endless polls about upcoming political votes.UNICEFThe UN agency responsible for helping raise healthy children around the world shares a wide variety of data sets that are useful to anyone with the same goals. The big picture can be found in marquee data sets like The State of the World’s Children 2019 Statistical Tables for those who want to track the change numerically. A more focused visualization can be discovered in tables that explore how iodized salt affects disease or the success of primary education.Financial dataOhio State’s library keeps a web page current with pointers to some of the biggest collections of economic and financial data. There are historical records of US data sets and also some data collected by the World Bank. Some require an academic account and some are free to the public.BaseballAmerica’s sport is blessed by some fans who are adept enough with computers to develop extensive collections of data about the players and the results of their games. Sean Lahman’s database, for instance, contains complete batting and pitching statistics from 1871 through 2019. There are also tables of other details like fielding statistics, managerial changes, and World Series results that may not be complete, but might as well be for the modern era, which in major league baseball begins with the 20th century.Also on InfoWorld: The 6 best programming languages for AI developmentProject Retrosheet was started to assemble play-by-play summaries of all major league games whenever possible, and it is now complete through 1974. If you happen to have access to a scorecard from an earlier game, check the “most wanted” list to see if you can fill in a hole. Chadwick Baseball Bureau maintains a GitHub repo for the data if you prefer.The Society for American Baseball Research maintains a list of other sources including offerings from commercial entities like FanGraphs, Baseball Reference, and Major League Baseball itself.GoogleIf you’re just looking for a particular data set, Google Dataset Search lets you search the entire web for data sets using keywords. The results can be filtered by license, data format, and the time since the last update. Some of the most intriguing data sets are also included in Google’s public data directory, which not only lists the sources but offers some interactive dashboards. The World Bank, for instance, charts fertility versus life expectancy and you can track how this changes over the years with a slider.Amazon Web ServicesAWS users who want data stored in S3 buckets can turn to the Repository of Open Data on AWS, or RODA. There’s wide variety in the thousands of data sets but the highlights tend to be the data sets from sources with which AWS is openly collaborating like the Space Telescope Institute (stars), NOAA (NEXRAD weather radar imagery), and Common Crawl (more than 25 billion web pages). There are several good examples to help you get started analyzing the data using, of course, AWS services like Lambda or Comprehend.MicrosoftMicrosoft also has a number of data sets on Azure. City planners can look for insight in the records from the New York CIty taxi board, which tracks all fares. Economists and traders can look at price records for commodities for insight on inflation and economic changes. All are ready to be analyzed by Microsoft’s machine learning tools.FacebookSome of what we store on Facebook is private because we make it so. Some is shared with friends. Some content is completely open. Facebook supports research on the so-called “Facebook graph” with their Graph API. It’s not the same as downloading the entire data set, but it can be useful for some queries. Just remember that not everyone uses the same privacy settings, so you might not see every person or every post.YelpThe website known for reviews of restaurants, bars, and other public accommodations shares a great deal of the information in a public data set that you can study. There are more than eight million reviews of more than 200,000 establishments just waiting for you or your AI to parse them. They are a good source for training data for natural language processing and machine learning.Open Data KitThe bits distributed by the Open Data Kit community and its JavaScript-based cousin ODK-X aren’t data per se. They’re software designed to support scientists and researchers who are creating the data sets. The code lets you create a user interface that simplifies data collection by the front-line researchers and then begins the classification and cleaning workflow. The tools are used by a diverse group of organizations supporting field research including the World Mosquito Project and the Red Cross. How data analysis, AI, and IoT will shape the post-pandemic ‘new normal’Web scrapingNot all data reside in easily accessible databases with APIs. An enormous volume of information is embedded in web pages and the data needs to be pried out of them with some clever tools. This so-called web scraping is still a pretty good method, but it can have legal limitations. Some sites ban it in their terms of service and others watch for too many requests from one user and then either cut off the user or slow down the responses.Tools like Puppeteer make it simpler to spin up one (or many!) headless versions of a web browser, download a web page, extract the right data, and do it again and again. There are now headless versions for most major browsers, thanks to the software testing community that needs to automate the testing process. Web scraping may not always be appropriate, but when it is it can be the fastest way to get the data you need. Nothing is more open than the open web.", "pub_date": "2020-09-30"},
{"title": "Microsoft’s innovative new tools for the ‘new normal’", "overview": "Support for social distancing, remote collaboration, immersive reality, and biosensing are top announcements at Ignite 2020", "image_url": null, "url": "https://www.infoworld.com/article/3584139/microsofts-innovative-new-tools-for-the-new-normal.html", "body": "Everybody who survives 2020 will look back on it as a year that stood outside the normal flow of our lives. Tech vendors have had to adapt to the chaotic storm of events that has turned global society upside down.In addition to the FAANG vendors (Facebook, Amazon, Apple, Netflix, Google), Microsoft will remain one of the dominant figures in the eventual post-pandemic phase of our lives. This week at its now virtual-only annual Ignite conference, the Redmond, Washington-based cloud powerhouse cemented its position as an innovation leader.How data analysis, AI, and IoT will shape the post-pandemic ‘new normal’We could easily focus on Microsoft’s numerous tweaks to its Azure cloud portfolio and Office productivity tools, but these are less interesting than the new and enhanced solutions that are positioning the company for life after COVID-19.In this coming social order, the most pivotal technologies will be those that enable greater social distancing, more productive remote collaboration, more fluidly immersive reality, and comprehensive biosensing.Here’s my dissection of the chief Ignite announcements that address each of these trends.Social distancingSocial distancing increasingly depends on tools for keeping people out of range of those who might spread infection. Contactless technologies are fast becoming most people’s interface for everything from payment to self-service interactions with all enterprise apps and devices.Recognizing how central this is to the future, Microsoft this week announced contactless support in its Teams solution. Later this year, Microsoft Teams Rooms devices—such as the new Surface Hub 2S—will support new contactless meetings capabilities within physical conference rooms. Contactless support will be built into the Rooms remote app and will be available for Teams through Microsoft Cortana voice assistance.In addition, Microsoft announced Teams Panels, a new category of mountable devices that use information from other wirelessly connected Teams devices, such as people-counting cameras. The Panels devices can show room capacity information, notify meeting participants of room scheduling and occupancy, and assist with wayfinding around physical offices. They will also be able to provide concrete safety guidance from the meeting facility administrator.Better support for people-counting cameras was also in Microsoft’s announcements this week. It rolled out a new spatial analysis feature of the computer vision service within its Azure Cognitive Service portfolio. For social distancing purposes, this feature combines images from multiple cameras and employs AI algorithms to count people in a space, measure how long they’ve been there, determine the distance between them, and meter their queue wait times. For retail, security, and other apps, it can also meter foot traffic in a space and determine when people are trespassing in forbidden zones.Remote collaborationWorking from home has become essential to keep our economy, society, and even political system from screeching to a halt. It’s no secret that usage of Microsoft Teams and other remote collaboration solutions has skyrocketed during this crisis, along with distance learning, remote medicine, and other technologies that replace in-person meetings of all sorts.Recognizing that it has a pivotal platform for this new era, Microsoft this week announced many enhancements to Teams that bridge the productivity gap between remote workers and those who may choose to operate out of traditional multiperson offices, now or in the future. These new features, some of which will be available later this year, include:Support for as many as 1,000 participants per meeting with the full meeting experience and scale to support 20,000 participants in a view-only mode with live captions.Automate attendee registration through e-mail and generate a postmeeting reporting dashboard to analyze attendee engagement.Automatically generate a postmeeting recap with the recording, transcript, chat, and shared files, then store them in the meeting’s Outlook calendar event entry and in Microsoft 365.Split participants into smaller groups, thereby facilitating brainstorming sessions and workgroup discussions. Also allow presenters to switch between breakout rooms, make announcements to all rooms, and close them to return everyone to the main meeting.Transpose a presenter’s video feed onto the foreground of the slide they’re showing.Place all participants in a shared background no matter where they are, while defining default backgrounds (such as auditoriums, conference rooms, or coffee shops) for all meeting attendees.Automatically scale and center meeting participants’ videos in their virtual seats, regardless of how close or far they are from their camera.Provide a streamlined Calling view that shows contacts, voicemail, and calling history at once, while enabling single-click initiation and return of calls.Immersive realityRemote work tends to lack the hands-on, interactive support that we’ve come to expect from on-site technical and administrative support personnel. As enterprises retool their internal processes to support a post-pandemic workforce, they’re exploring a growing range of immersive reality hardware, software, and services to ensure seamless support across distributed personnel.Recognizing that remote users need immersive reality to achieve full productivity in an increasingly virtual world, Microsoft made three key announcements at Ignite:Expansion of global availability of HoloLens 2, its MR (mixed-reality) headsets: These blend augmented reality’s dynamically superimposed captioning with virtual reality’s simulated environments.Private preview of a new MR service called Azure Object Anchors: This enables HoloLens headsets to recognize an object in the real world and dynamically map relevant instructions or visuals onto it. It does so automatically without requiring users or MR developers to have any expert knowledge or barcodes to align physical and virtual objects.Launch of Dynamics 365 Remote Assist: People in two different physical locations can collaborate and solve problems in a shared MR environment.Taken together, these solutions enable, for example, a headset-wearing car owner to be guided by an automaker-developed MR program in performing a simple repair job at home.Comprehensive biosensingLife after COVID-19 will see growing adoption of biosensors to monitor the presence and spread of pathogens that threaten lives and health. We’ll see deployment of ubiquitous sensors to detect viral pathogens in the air, water, soil, surfaces, and human and animal tissues. We’ll also see them internetworked through IoT (the Internet of Things).Addressing this imperative head-on, Microsoft announced Premonition, an early warning system for disease outbreaks such as, but not limited to, COVID-19. Available in coming weeks through an early access program, Premonition is designed to detect pandemics and other disease outbreaks early so that they can be prevented, contained, and otherwise mitigated before they spin out of control. The service automates the capture, collection, aggregation, and analysis of data on potential outbreaks.Behind the scenes, Premonition leverages a a global network of IoT-connected sensing platforms to feed supercomputers with real-time genomic data recovered from insects and other environmental sources. Running on Azure, it uses robotics to autonomously collect environmental samples and scan the recovered genomes for biological threats.Over the past five years, Microsoft has tested its technologies in various habitats worldwide, ranging from the sands of the Florida Keys to the deep forests of Tanzania. To date, it has scanned more than 80 trillion base-pairs of environmentally recovered genomic material worldwide for biological threats.Anticipating the growing need for governments to maintain counter-contagion nerve centers, Microsoft has worked with Harris County, Texas (where Houston is located), for the past four years on the initative now called Premonition. In conjunction with Premonition’s public launch, Microsoft has announced that Harris County is expanding its deployment to support continuous biological situational awareness. The county’s goal is to enable health officials to monitor and forecast outbreaks in real time on a map so that they can better plan early interventions to protect the public.The announcements that weren’t madeMicrosoft has the strongest portfolio for helping enterprise customers shift operations toward a future in which more work is done from employees’ homes and the practical distinction between physical and virtual team spaces vanishes.Such productivity will depend on tools such as those Microsoft announced at Ignite for social distancing, remote collaboration, immersive reality, and comprehensive biosensing. But there were a few notable gaps in Microsoft’s strategy, if we consider purely the announcements at Ignite 2020.For starters, Microsoft did not make any robotics-focused announcements. That was unfortunate, if we consider that the market for automated disinfection robots continues to heat up with new entrants. Before long, many offices and public spaces will be equipped with biosensors and robotic cleansing platforms that allow workers and customers to reoccupy them.Also, Microsoft did not indicate whether it plans to package its Premonition biosensing technology for on-premise deployment in enterprise facilities, or even for hybrid deployments that span the Azure public cloud and enterprise private clouds. This was a noteworthy gap in its Premonition story, especially since Microsoft made a big to-do about adding more devices and services to its Azure Stack hybrid-cloud portfolio.Also on InfoWorld: How to choose a cloud machine learning platformNeither was there any discussion of whether Microsoft plans to provide customers of Premonition with quarantine orchestration tools to guide more orderly lockdown, quarantine, and reopening of physical facilities in the face of disease outbreaks.That latter capability will almost certainly become a standard feature of enterprise facility management solutions everywhere as we move more deeply into the new decade.", "pub_date": "2020-10-01"},
{"title": "Build custom models with Azure Machine Learning Designer", "overview": "Now anyone can be a model designer with assistive tooling and a new automated ML development service. All you need is data.", "image_url": null, "url": "https://www.infoworld.com/article/3586551/build-custom-models-with-azure-machine-learning-designer.html", "body": "Machine learning is an important part of modern application development, replacing much of what used to be done using a complex series of rules engines, and expanding coverage to a much wider set of problems. Services like Azure’s Cognitive Services provide prebuilt, pretrained models that support many common use cases, but many more need custom model development.Going custom with MLHow do we go about building custom machine learning models? You can start at one end using statistical analysis languages like R to build and validate models, where you’ve already got a feel for the underlying structure of your data, or you can work with the linear algebra features of Python’s Anaconda suite. Similarly, tools such as PyTorch and TensorFlow can help construct more complex models, taking advantage of neural nets and deep learning while still integrating with familiar languages and platforms.Also on InfoWorld: The best free data science courses during quarantineThat’s all good if you’ve got a team of data scientists and mathematicians able to build, test, and (most importantly) validate their models. With machine learning expertise hard to find, what’s needed are tools to help guide developers through the process of creating the models that businesses need. In practice, most machine learning models fall into two types: the first identifies similar data, the second identifies outlying data.We might use the first type of app to identify specific items on a conveyor belt or have the second look for issues in data from a series of industrial sensors. Scenarios like these aren’t particularly complex, but they still require building a validated model, ensuring that it can identify what you’re looking for and find the signal in the data, not amplify assumptions or respond to noise.Introducing Azure Machine Learning DesignerAzure provides various tools for this, alongside its prebuilt, pretrained, customizable models. One, Azure Machine Learning Designer, lets you work with your existing data with a set of visual design tools and drag-and-drop controls.You don’t need to write code to build your model, though there’s the option to bring in custom R or Python where necessary. It’s a replacement for the original ML Studio tool, adding deeper integration into Azure’s machine learning SDKs and with support for more than CPU-based models, offering GPU-powered machine learning and automated model training and tuning.To get started with Azure Machine Learning Designer open the Azure Machine Learning site and log in with an Azure account. Begin by connecting to a subscription and creating a workspace for your models. The setup wizard asks you to specify whether the resulting models have a public or private end point and whether you’re going to be working with sensitive data before choosing how keys are managed. Sensitive data will be processed in what Azure defines as a “high business impact workspace,” which reduces the amount of diagnostic data collected by Microsoft and adds extra levels of encryption.Configuring a machine learning workspaceOnce you’ve walked through the wizard, Azure checks your settings before creating your ML workspace. Usefully it offers you an ARM template so you can automate the creation process in future, providing a framework for scripts that business analysts can use from an internal portal to reduce the load on your Azure administrators. Deploying the resources needed to create a workspace can take time, so be prepared to wait a while before you can start building any models.Your workspace contains tools for developing and managing machine learning models, from design and training to managing compute and storage. It also helps you label existing data, increasing the value of your training data set. You’re likely to want to start with the three main options: working with the Azure ML Python SDK in a Jupyter-style notebook, using Azure ML’s automated training tools, or the low-code drag-and-drop Designer surface. IDGAzure Machine Learning Studio offers multiple ways to use your data to create ML models.Using Azure ML Designer to create a modelThe Designer is the quickest way to start with custom machine learning, as it gives you access to a set of prebuilt modules that can be chained together to make a machine learning API that’s ready for use in your code. Begin by creating a canvas for your ML pipeline, setting up the compute target for your pipeline. Compute targets can be set for the entire model, or for individual modules within the pipeline, allowing you to tune performance appropriately.It’s best to think of your model’s compute resources as serverless compute, which scales up and down as necessary. When you’re not using it, it will scale down to zero and can take as long as five minutes to spin up again. This might impact application operations, so ensure that it’s available before running applications that depend on it. You will need to consider the resources needed to train a model when choosing a compute target. Complex models can take advantage of Azure’s GPU support, with support for most of Azure’s compute options (depending on your available quota).Once you’ve set up your training compute resources, pick a training data set. This can be your own data or one of Microsoft’s samples. Custom data sets can be constructed from local files, from data already stored on Azure, from the Web, or from registered open data sets (which are often government information).Using data in Azure ML DesignerTools in the Designer allow you to explore the data sets you’re using, so you can be sure you have the right source for the model you’re trying to build. With a data source on the canvas, you can start dragging in modules and connecting them to process your training data; for example, removing columns that don’t contain enough data or cleaning up missing data. This drag-and-connect process is very like working with low-code tools, such as those in the Power Platform. What differs here is that you have the option of using your own modules.Once data has been processed, you can start to choose the modules you want to train your model. Microsoft provides a set of common algorithms, as well as tools for splitting data sets for training and testing. The resulting models can be scored using another module once you run them through training. Scores are passed to an evaluation module so you can see how well your algorithm operated. You do need some statistical knowledge to interpret the results so you can understand the types of errors that are generated, though in practice the smaller the error value, the better. You don’t need to use the prepared algorithms, as you can bring in your own Python and R code.A trained and tested model can be quickly converted into an inferencing pipeline, ready for use in your applications. This adds input and output REST API end points to your model, ready for use in your code. The resulting model is then deployed to an AKS inferencing cluster as a ready-to-use container.Let Azure do it all for you: Automated Machine LearningIn many cases you don’t even need to do that much development. Microsoft recently released an Automated ML option, based on work done at Microsoft Research. Here you start with an Azure-accessible data set, which must be tabular data. It’s intended for three types of model: classification, regression, and forecasts. Once you provide data and choose a type of model, the tool will automatically generate a schema from the data that you can use to toggle specific data fields on and off, building an experiment that is then run to build and test a model.Automated ML will create and rank several models, which you can investigate to determine which is the best for your problem. Once you’ve found the model you want, you can quickly add input and output stages and deploy it as a service, ready for use in tools like Power BI.With machine learning an increasingly important predictive tool across many different types of business problem, Azure Machine Learning Designer can bring it a much wider audience. If you’ve got data, you can build both analytical and predictive models, with minimal data science expertise. With the new Automated ML service, it’s easy to go from data to service to no-code analytics.", "pub_date": "2020-10-20"},
{"title": null, "overview": null, "image_url": null, "url": "https://www.infoworld.com/author/Simon-Bisson/", "body": "", "pub_date": null},
{"title": null, "overview": null, "image_url": null, "url": "https://www.infoworld.com/blog/enterprise-microsoft/", "body": "", "pub_date": null}
][
{"title": "Reconciling political beliefs with career ambitions", "overview": "Data professionals face personal dilemmas as AI contributes to ethical issues such as reduced privacy, algorithmic bias, and advanced military weapons. ", "image_url": "https://images.idgesg.net/images/article/2020/09/red_flag_warning_button_alert_danger_disaster_by_matejmo_gettyimages-644328002_2400x1600-100858026-large.jpg", "url": "https://www.infoworld.com/article/3599208/reconciling-political-beliefs-with-career-ambitions.html", "body": "Today’s political environment is increasingly hostile to data management as a profession. If you’ve made your career in machine learning, data mining, predictive modeling, or related fields, these controversies may have you second-guessing your decision to pursue this line of work. The principal flashpoints center on issues of data privacy, algorithmic bias, and AI weaponization.Prioritizing privacy over data-driven marketingData management professionals face growing scrutiny over privacy violations, surveillance, and other intrusive impacts of the applications they’re responsible for building and managing.Also on InfoWorld: Will work from home be permanent? Many developers like the ideaOne of the most disturbing new themes is the ideological framing of how technology enables “surveillance capitalism.” This term, coined by Harvard Business School professor Shoshana Zuboff, essentially stigmatizes the collection, ownership, processing, and use of customers’ PII (personally identifiable information). This notion regards any enterprise use of customer PII—such as microsegmentation, contextual offer targeting, and cross-channel ad optimization—as a form of monitoring and control.Concerns over data-driven CRM aren’t limited to academics. It's clear from recent congressional hearings that this perspective aligns with popular opinion regarding the practices of Google, Facebook, Amazon, and other 21 century digital businesses. The public is increasingly uneasy about privacy encroachment, as big brands compete to see who can acquire the most comprehensive range of intrusive data about every aspect of our lives, including our inner thoughts, sentiments, and predilections.Consequently, many data professionals are facing a crossroads in their careers. On the one hand, their employers have built successful businesses fueled by predictive targeting, one-to-one personalization and multichannel engagement. On the other hand, data professionals are feeling more squeamish about the depth to which they use customer PII to fuel AI-driven CRM programs, such as predictive personality profiling, next-best-action targeting, or real-time behavioral pricing.None of this is particularly strange, sinister, or shameful. These practices are central to how business is done these days. If you have ideological misgivings about these or other data-driven CRM methodologies, you’ll probably never work in enterprise data management or modern marketing. If you refuse to work on a program simply because it implements these modern customer-engagement methodologies, you won’t get much sympathy from your employers and, in fact, they are likely to show you the door.But that doesn’t mean you have to stand idly by while your employer runs amok with customer data. You can become your company’s foremost data-privacy advocate, for example. If nothing else, you can make sure your firm complies rigorously with the European Union’s General Data Protection Regulation and similar privacy laws elsewhere.Ridding our lives of data-driven algorithmic biasesData has been on the front lines in recent culture wars due to accusations of racial, gender, and other forms of socioeconomic bias perpetrated in whole or in part through algorithms.Algorithmic biases have become a hot-button issue in global society, a trend that has spurred many jurisdictions and organizations to institute a greater degree of algorithmic accountability in AI practices. Data scientists who’ve long been trained to eliminate biases from their work now find their practices under growing scrutiny from government, legal, regulatory, and other circles.Eliminating bias in the data and algorithms that drive AI requires constant vigilance on the part of not only data scientists but up and down the corporate ranks. As Black Lives Matter and similar protests have pointed out, data-driven algorithms can embed serious biases that harm demographic groups (racial, gender, age, religious, ethnic, or national origin) in various real-world contexts.Much of the recent controversy surrounding algorithmic biases has focused on AI-driven facial recognition software. Biases in facial recognition applications are especially worrisome if used to direct predictive policing programs or potential abuse by law enforcement in urban areas with many disadvantaged minority groups.Many AI solution vendors have seen an extensive grassroots effort among their own employees to take a strong stand against police abuses of facial recognition. In June as the Black Lives Matter protests heated up, employees at Amazon Web Services called on the firm to sever its police contracts. More than 250 Microsoft employees published an open letter demanding that the company end its work with police departments.This is an AI application domain in which practitioners will have to take their lumps as biases continue to surface, and associated legal and regulatory penalties follow closely behind. So far there is little consensus on viable frameworks for regulating uses, deployments, and management of facial recognition programsNevertheless, AI practitioners know that the opportunities in facial recognition are too numerous and lucrative to forgo indefinitely. Embedding facial recognition into iPhones and other devices will ensure that this technology is a key tool in everybody’s personal tech portfolio. More businesses are incorporating facial recognition into internal and customer-facing applications for biometric authentication, image/video autotagging, query by image, and other valuable uses. Social distancing has made many people more receptive to facial recognition as a contactless option for strong authentication to many device-level and online services.Indeed, a recent Cap Gemini global survey found that adoption of facial recognition is likely to continue growing among large businesses in every sector and region, even as the pandemic recedes.Bias isn’t an issue that can be fixed once and for all. Where decision-making algorithms are concerned, organizations must always make biases as transparent as possible and attempt to eliminate any that perpetuate unfair societal outcomes. In addition, ongoing auditing of AI biases—not just in facial recognition but in all other socially impactful application domains—must become a standard task in AI devops workflows.­­All of this raises the possibility that more data scientists will have to decide whether to participate in such projects and to what extent. Many may take the limited, near-term option of opting out of contributing to law enforcement applications of facial recognition. As a longer-term sustainable approach, data scientists who aren’t ideologically opposed to facial recognition will need to redouble efforts to eliminate biases that are baked into these models and the facial-image data sets that train them.Opting out of AI-driven weapons developmentData is central to modern warfare. Data-driven algorithms, especially deep learning, give weapons systems the capability of seeing, hearing, sensing, and adjusting real-time strategies far better and faster than most humans.AI is the future of warfare and of defenses against algorithmic weapon systems. Future battles will almost certainly have casualty counts that are staggering and lopsided, especially when one side’s arsenal is almost entirely composed of autonomous weapons systems equipped with phalanxes of 3D cameras, millimeter-wave radar, biochemical detectors, and other ambient sensors.If you’re a career-minded data scientist, you’ll be tempted to lend your talents to military projects, which tend to be the most exciting, cutting-edge, R&D-driven initiatives in AI. The shortage of highly qualified AI professionals practically ensures that if you have what it takes, you’ll fetch a high salary with numerous perks. Also, the amount of VC money flowing into startups in this sector ensures that many data scientists who’ve cut their teeth on AI-centric weapon programs will become quite wealthy and powerful.AI professionals are highly ambivalent about participating in military projects. In a recent survey cited here, U.S. AI specialists reported more favorable than unfavorable attitudes about working with the Department of Defense, though a large plurality are neutral on the topic. Respondents reported being more favorably inclined to accepting DoD grants for basic research than applied research. In the survey, AI professionals’ most cited reason for taking DoD grants was to work on “interesting problems.” “Discomfort with how DoD will use the work” was the most frequently cited downside. Approximately three-quarters of those surveyed had negative attitudes about battlefield applications of AI.AI professionals may think that hitching their career to a commercial cloud provider such as Amazon Web Services, Microsoft, Google, or IBM will help them avoid pressure to work on military projects. That’s just not so. Many tech firms have been applying their innovations to DoD projects for several years. In fact, Big Tech continues to cultivate close ties with the U.S. military, as shown in this recent study.If you think you can limit your contributions to defensive and back-office AI applications in the military—per the “ground rules” that former Google CEO Eric Schmidt proposed a few years ago—you’re in for a rude awakening. No such rules for commercial AI vendors’ military engagement can realistically stop the underlying approaches from being used in weapons systems for offensive purposes. In fact, the likelihood of that possibility is heightened by the fact that many military projects’ underlying AI technologies, including open-source modeling software and unclassified image data, are available freely.In the unlikely scenario that all AI companies walk away from projects with the United States’ and other nations’ military establishments, that would still leave an opportunity for universities and nonprofit research centers to pick up the work. Considering how much money the military is likely to funnel into such contracts, this could easily reverse the brain drain that’s causing the best and brightest AI researchers to leave academia and seek their fortunes in the private sector.None of this means you can’t opt out of participating in the cyber-industrial complex that’s sprung up around militarized AI. If you’re morally opposed to this sort of work, you can spend your entire data career without ever having to compromise your principles. AI specialists can find plenty of humanitarian or other unobjectionable uses for their talents.Alternately, you might consider working on technological countermeasures designed to neutralize an adversary’s AI-powered weaponry. One of the most promising new professional opportunity is in building AI-driven counterdrone defenses. In recent years, militaries all over the world have deployed drones successfully as a fast-strike, low-cost, AI-driven alternative to conventional warfare tactics such as armored vehicles and fighter jets. For example, Azerbaijan used drones successfully in a recent war against Armenia, deploying the miniature unmanned aerial vehicles to destroy tanks and other armored fighting vehicles.Counterdrone defenses are a hot focus of R&D and startup activity. Many such projects use AI to automate detecting drones, pinpointing locations, and predicting likely flight paths of drones. Many use AI to automate classification of approaching drones by model, operator, and threat profile, taking care to minimize false positive and false negatives. They also rely on machine learning to automatically trigger security alerts and activate the mechanisms to physically destroy, disable, distract, or otherwise neutralize weaponized drones.Finding a middle roadIn a politically polarized cultural landscape, data professionals may find it difficult to keep their leanings under wraps. They may also have pangs of conscience that deter them from engaging in projects whose objectives contravene their deep convictions.Before you opt out of some otherwise objectionable datacentric project, consider whether you can contribute to implementing effective controls such as privacy protection mechanisms, debiasing processes, and automated countermeasures that mitigate the more objectionable aspects. That could allow you, on some level, to reconcile your political convictions with your professional ambitions.In the process, you would be doing your part to make the world a better place.", "pub_date": "2020-12-03"},
{"title": "Today’s data science roles won’t exist in 10 years", "overview": "AutoML is poised to turn developers into data scientists — and vice versa. Here’s how AutoML will radically change data science for the better. ", "image_url": "https://images.idgesg.net/images/article/2018/10/ai_machine-learning_analytics_data-scientist-100777422-large.jpg", "url": "https://www.infoworld.com/article/3596894/todays-data-science-roles-wont-exist-in-10-years.html", "body": "In the coming decade, the data scientist role as we know it will look very different than it does today. But don’t worry, no one is predicting lost jobs, just  jobs.Data scientists will be fine — according to the Bureau of Labor Statistics, the role is still projected to grow at a higher than average clip through 2029. But advancements in technology will be the impetus for a huge shift in a data scientist’s responsibilities and in the way businesses approach analytics as a whole. And AutoML tools, which help automate the machine learning pipeline from raw data to a usable model, will lead this revolution.Also on InfoWorld: How to choose a cloud machine learning platformIn 10 years, data scientists will have entirely different sets of skills and tools, but their function will remain the same: to serve as confident and competent technology guides that can make sense of complex data to solve business problems.AutoML democratizes data scienceUntil recently, machine learning algorithms and processes were almost exclusively the domain of more traditional data science roles—those with formal education and advanced degrees, or working for large technology corporations. Data scientists have played an invaluable role in every part of the machine learning development spectrum. But in time, their role will become more collaborative and strategic. With tools like AutoML to automate some of their more academic skills, data scientists can focus on guiding organizations toward solutions to business problems via data.In many ways, this is because AutoML democratizes the effort of putting machine learning into practice. Vendors from startups to cloud hyperscalers have launched solutions easy enough for developers to use and experiment on without a large educational or experiential barrier to entry. Similarly, some AutoML applications are intuitive and simple enough that non-technical workers can try their hands at creating solutions to problems in their own departments—creating a “citizen data scientist” of sorts within organizations.In order to explore the possibilities these types of tools unlock for both developers and data scientists, we first have to understand the current state of data science as it relates to machine learning development. It’s easiest to understand when placed on a maturity scale.Smaller organizations and businesses with more traditional roles in charge of digital transformation (i.e.,  classically trained data scientists) typically fall on this end of this scale. Right now, they are the biggest customers for out-of-the-box machine learning applications, which are more geared toward an audience unfamiliar with the intricacies of machine learning.These turnkey applications tend to be easy to implement, and relatively cheap and easy to deploy. For smaller companies with a very specific process to automate or improve, there are likely several viable options on the market. The low barrier to entry makes these applications perfect for data scientists wading into machine learning for the first time. Because some of the applications are so intuitive, they even allow non-technical employees a chance to experiment with automation and advanced data capabilities—potentially introducing a valuable sandbox into an organization.This class of machine learning applications is notoriously inflexible. While they can be easy to implement, they aren’t easily customized. As such, certain levels of accuracy may be impossible for certain applications. Additionally, these applications can be severely limited by their reliance on pretrained models and data.Examples of these applications include Amazon Comprehend, Amazon Lex, and Amazon Forecast from Amazon Web Services and Azure Speech Services and Azure Language Understanding (LUIS) from Microsoft Azure. These tools are often sufficient enough for burgeoning data scientists to take the first steps in machine learning and usher their organizations further down the maturity spectrum.How data analysis, AI, and IoT will shape the post-pandemic ‘new normal’Customizable solutions with AutoMLOrganizations with large yet relatively common data sets—think customer transaction data or marketing email metrics—need more flexibility when using machine learning to solve problems. Enter AutoML. AutoML takes the steps of a manual machine learning workflow (data discovery, exploratory data analysis, hyperparameter tuning, etc.) and condenses them into a configurable stack. AutoML applications allow more experiments to be run on data in a larger space. But the real superpower of AutoML is the accessibility — custom configurations can be built and inputs can be refined relatively easily. What’s more, AutoML isn’t made exclusively with data scientists as an audience. Developers can also easily tinker within the sandbox to bring machine learning elements into their own products or projects.While it comes close, AutoML’s limitations mean accuracy in outputs will be difficult to perfect. Because of this, degree-holding, card carrying data scientists often look down upon applications built with the help of AutoML — even if the result is accurate enough to solve the problem at hand.Examples of these applications include Amazon SageMaker AutoPilot or Google Cloud AutoML. Data scientists a decade from now will undoubtedly need to be familiar with tools like these. Like a developer who is proficient in multiple programming languages, data scientists will need to have proficiency with multiple AutoML environments in order to be considered top talent.“Hand-rolled” and homegrown machine learning solutionsThe largest enterprise-scale businesses and Fortune 500 companies are where most of the advanced and proprietary machine learning applications are currently being developed. Data scientists at these organizations are part of large teams perfecting machine learning algorithms using troves of historical company data, and building these applications from the ground up. Custom applications like these are only possible with considerable resources and talent, which is why the payoff and risks are so great.Like any application built from scratch, custom machine learning is “state-of-the-art” and is built based on a deep understanding of the problem at hand. It’s also more accurate — if only by small margins — than AutoML and out-of-the-box machine learning solutions.Getting a custom machine learning application to reach certain accuracy thresholds can be extremely difficult, and often requires heavy lifting by teams of data scientists. Additionally, custom machine learning options are the most time-consuming and most expensive to develop.An example of a hand-rolled machine learning solution is starting with a blank Jupyter notebook, manually importing data, and then conducting each step from exploratory data analysis through model tuning by hand. This is often achieved by writing custom code using open source machine learning frameworks such as Scikit-learn, TensorFlow, PyTorch, and many others. This approach requires a high degree of both experience and intuition, but can produce results that often outperform both turnkey machine learning services and AutoML.Also on InfoWorld: The best free data science courses during quarantineTools like AutoML will shift data science roles and responsibilities over the next 10 years. AutoML takes the burden of developing machine learning from scratch off of data scientists, and instead puts the possibilities of machine learning technology directly in the hands of other problem solvers. With time freed up to focus on what they know—the data and the inputs themselves — data scientists a decade from now will serve as even more valuable guides for their organizations.Rackspace—newtechforum@infoworld.com", "pub_date": "2020-11-18"},
{"title": "Weaponizing the cloud to fight addiction", "overview": "We’re seeing a rise in overdose deaths in the U.S. during the pandemic. Perhaps it’s time to leverage cloud technology to address this problem. ", "image_url": "https://images.techhive.com/images/article/2014/03/168822677-100250248-large.jpg", "url": "https://www.infoworld.com/article/3597988/weaponizing-the-cloud-to-fight-addiction.html", "body": "According to the CDC, opioids were involved in 46,802 overdose deaths in 2018 (69.5 percent of all drug overdose deaths). For those of you living in the United States, this is old news.As the pandemic stretches on, deaths in the U.S. from opioids and other habit-forming drugs such as alcohol, are likely to rise in 2020. We’re at a point where most health organizations are deeply concerned.We could reduce the number of deaths by helping addicts in better ways. The combinations of cloud, artificial intelligence, and IoT (Internet of Things) working together could replace rehab clinics as the preferred way to overcome dangerous or unhealthy addictions.Core to this approach is the notion of habit tracking. Or, better put, monitoring behaviors over a long period of time to determine if you have a problem. Then you find a way to resolve it on your own using technology. Also on InfoWorld: Cloud tech certifications count more than degrees nowA virtual breathalyzer is in development to combat alcoholism, for example. Cloud-connected sensors in devices that we carry, such as smartphones or fitness bands, are able to detect someone’s level of intoxication based on walking patterns. Automobiles could be prevented from starting until the intoxication subsides. These behaviors are turned into patterns that can be spotted with machine learning systems, and the problem diagnosed. From there certain automated processes are triggered, such as not allowing driving, or more importantly, kicking off treatment processes that could prevent at least 10 to 20 percent of addiction-related deaths.These systems would not be possible without cloud computing and related services, such as AI and IoT. Unless the cost is low and the back-end processing powerful, they won’t be effective. Moreover, humans have to carry or wear these devices, so the user experience has to be compelling, as well.This is really a matter of leveraging technology that is becoming a commodity (cloud and IoT) to solve another problem. Most of us know an addict or perhaps are one ourselves. We have the potential to do a lot of good with a very small amount of cost and technology.", "pub_date": "2020-11-20"},
{"title": "Review: DataRobot aces automated machine learning", "overview": "DataRobot’s end-to-end AutoML suite not only speeds up the creation of accurate models, but can combine time series, images, geographic information, tabular data, and text in a single model", "image_url": "https://images.idgesg.net/images/article/2018/06/digital-twins_ai_robots_mirror-image_duo_pair_toy-robots-100760560-large.jpg", "url": "https://www.infoworld.com/article/3596593/review-datarobot-aces-automated-machine-learning.html", "body": "Data science is nothing if not tedious, in ordinary practice. The initial tedium consists of finding data relevant to the problem you’re trying to model, cleaning it, and finding or constructing a good set of features. The next tedium is a matter of attempting to train every possible machine learning and deep learning model to your data, and picking the best few to tune.Then you need to understand the models well enough to explain them; this is especially important when the model will be helping to make life-altering decisions, and when decisions may be reviewed by regulators. Finally, you need to deploy the best model (usually the one with the best accuracy and acceptable prediction time), monitor it in production, and improve (retrain) the model as the data drifts over time.Read the InfoWorld review: Google Cloud AI lights up machine learningAutoML, i.e. automated machine learning, can speed up these processes dramatically, sometimes from months to hours, and can also lower the human requirements from experienced Ph.D. data scientists to less-skilled data scientists and even business analysts. DataRobot was one of the earliest vendors of AutoML solutions, although they often call it Enterprise AI and typically bundle the software with consulting from a trained data scientist. DataRobot didn’t cover the whole machine learning lifecycle initially, but over the years they have acquired other companies and integrated their products to fill in the gaps.As shown in the listing below, DataRobot has divided the AutoML process into 10 steps. While DataRobot claims to be the only vendor to cover all 10 steps, other vendors might beg to differ, or offer their own services plus one or more third-party services as a “best of breed” system. Competitors to DataRobot include (in alphabetical order) AWS, Google (plus Trifacta for data preparation), H2O.ai, IBM, MathWorks, Microsoft, and SAS.The 10 steps of automated machine learning, according to DataRobot: Data identificationData preparationFeature engineeringAlgorithm diversityAlgorithm selectionTraining and tuningHead-to-head model competitionsHuman-friendly insightsEasy deploymentModel monitoring and managementDataRobot platform overviewAs you can see in the slide below, the DataRobot platform tries to address the needs of a variety of personas, automate the entire machine learning lifecycle, deal with the issues of model explainability and governance, deal with all kinds of data, and deploy pretty much anywhere. It mostly succeeds.DataRobot helps data engineers with its AI Catalog and Paxata data prep. It helps data scientists primarily with its AutoML and automated time series, but also with its more advanced options for models and its Trusted AI. It helps business analysts with its easy-to-use interface. And it helps software developers with its ability to integrate machine learning models with production systems. DevOps and IT benefit from DataRobot MLOps (acquired in 2019 from ParallelM), and risk and compliance officers can benefit from its Trusted AI. Business users and executives benefit from better and faster model building and from data-driven decision making.End-to-end automation speeds up the entire machine learning process and also tends to produce better models. By quickly training many models in parallel and using a large library of models, DataRobot can sometimes find a much better model than skilled data scientists training one model at a time. A quote from an associate professor of information management on one of DataRobot’s web pages essentially says that DataRobot AutoML managed to find a model in one hour(!) that outperformed (by a factor of two!) the best model a skilled grad student was able to train in a few months, because the student had missed a class of algorithms that worked well for the data. Your mileage may vary, of course.In the row marked multimodal in the diagram below, there are five icons. At first they confused me, so I asked what they mean. Essentially, DataRobot has models that can handle time series, images, geographic information, tabular data, and text. The surprising bit is that it can combine all of those data types in a single model.DataRobot offers you a choice of deployment locations. It will run on a Linux server or Linux cluster on-premises, in a cloud VPC, in a hybrid cloud, or in a fully managed cloud. It supports Amazon Web Services, Microsoft Azure, or Google Cloud Platform, as well as Hadoop and Kubernetes.DataRobot platform diagram. Several of the features were added to the platform through acquisitions, including data preparation and MLOps.Paxata data prepDataRobot acquired self-service data preparation company Paxata in December 2019. Paxata is now integrated with DataRobot’s AI Catalog and feels like part of the DataRobot product, although you can still buy it as a standalone product if you wish.Paxata has three functions. First, it allows you to import datasets. Second, it lets you explore, clean, combine, and condition the data. And third, it allows you to publish prepared data as an AnswerSet. Each step you perform in Paxata creates a version, so that you can always continue to work on the data.Also on InfoWorld: MLOps: The rise of machine learning operationsData cleaning in Paxata includes standardizing values, removing duplicates, finding and fixing errors, and more. You can shape your data using tools such as pivot, transpose, group by, and more.The screenshot below shows a real estate dataset that has a dozen Paxata processing steps. It starts with a house price tabular dataset; then it adds exterior and interior images, removes unnecessary columns and bad rows, and adds ZIP code geospatial information. This screenshot is from the House Listings demo.Paxata allows the user to construct AnswerSets from datasets one step at a time. The Paxata tools all have a GUI, although the Compute tool lets the user enter simple formulas or build advanced formulas using columns and functions.DataRobot automated machine learningBasically, DataRobot AutoML works by going through a couple of exploratory data analysis (EDA) phases, identifying informative features, engineering new features (especially from  types), then trying a lot of models with small amounts of data.EDA phase 1 runs on up to 500MB of your dataset and provides summary statistics, as well as checking for outliers, inliers, excess zeroes, and disguised missing values. When you select a target and hit run, DataRobot “searches through millions of possible combinations of algorithms, preprocessing steps, features, transformations, and tuning parameters. It then uses supervised learning algorithms to analyze the data and identify (apparent) predictive relationships.”DataRobot autopilot mode starts with 16% of the data for all appropriate models, 32% of the data for the top 16 models, and 64% of the data for the top eight models. All results are displayed on the leaderboard. Quick mode runs a subset of models on 32% and 64% of the data. Manual mode gives you full control over which models to execute, including specific models from the repository.DataRobot AutoML in action. The models being trained are at the right, along with the percentage of the data being used for training each model.DataRobot time-aware modelingDataRobot can do two kinds of time-aware modeling if you have date/time features in your dataset. You should use out-of-time validation (OTV) when your data is time-relevant but you are not forecasting (instead, you are predicting the target value on each individual row). Use OTV if you have single event data, such as patient intake or loan defaults.You can use time series when you want to forecast multiple future values of the target (for example, predicting sales for each day next week). Use time series to extrapolate future values in a continuous sequence.In general, it has been difficult for machine learning models to outperform traditional statistical models for time series prediction, such as ARIMA. DataRobot’s time series functionality works by encoding time-sensitive components as features that can contribute to ordinary machine learning models. It adds columns to each row for examples of predicting different distances into the future, and columns of lagged features and rolling statistics for predicting that new distance.Values over time graph for time-related data. This helps to determine trends, weekly patterns, and seasonal patterns.DataRobot Visual AIIn April 2020 DataRobot added image processing to its arsenal. Visual AI allows you to build binary and multi-class classification and regression models with images. You can use it to build completely new image-based models or to add images as new features to existing models. Also on InfoWorld: 10 MLops platforms to manage the machine learning lifecycleVisual AI uses pre-trained neural networks, and three new models: Neural Network Visualizer, Image Embeddings, and Activation Maps. As always, DataRobot can combine its models for different field types, so classified images can add accuracy to models that also use numeric, text, and geospatial data. For example, an image of a kitchen that is modern and spacious and has new-looking, high-end appliances might result in a home-pricing model increasing its estimate of the sale price.There is no need to provision GPUs for Visual AI. Unlike the process of training image models from scratch, Visual AI’s pre-trained neural networks work fine on CPUs, and don’t even take very long.This multi-class confusion matrix for image classification shows a fairly clean separation, with most of the predictions true positives or true negatives.The color overlays in these home exterior images from the House Listings demo highlight the features the model factored into its sale price predictions. These factors were combined with other fields, such as square footage and number of bedrooms.DataRobot Trusted AIIt’s easy for an AI model to go off track, and there are numerous examples of what not to do in the literature. Contributing factors include outliers in the training data, training data that isn’t representative of the real distribution, features that are dependent on other features, too many missing feature values, and features that leak the target value into the training.DataRobot has guardrails to detect these conditions. You can fix them in the AutoML phase, or preferably in the data prep phase. Guardrails let you trust the model more, but they are not infallible.Humble AI rules allow DataRobot to detect out of range or uncertain predictions as they happen, as part of the MLOps deployment. For example, a home value of $100 million in Cleveland is unheard-of; a prediction in that range is most likely a mistake. For another example, a predicted probability of 0.5 may indicate uncertainty. There are three ways of responding when humility rules fire: Do nothing but keep track, so that you can later refine the model using more data; override the prediction with a “safe” value; or return an error.Too many machine learning models lack explainability; they are nothing more than black boxes. That’s often especially true of AutoML. DataRobot, however, goes to great lengths to explain its models. The diagram that follows is fairly simple, as neural network models go, but you can see the strategy of processing text and categorical variables in separate branches and then feeding the results into a neural network.Blueprint for an AutoML model. This model processes categorical variables using one-hot encoding and text variables using word-grams, then factors them all into a Keras Slim Residual neural network classifier. You can drill into any box to see the parameters and get a link to the relevant documentation.DataRobot MLOpsOnce you have built a good model you can deploy it as a prediction service. That isn’t the end of the story, however. Over time, conditions change. We can see an example in the graphs below. Based on these results, some of the data that flows into the model — elementary school locations — needs to be updated, and then the model needs to be retrained and redeployed.Looking at the feature drift from MLOps tells you when conditions change that affect the model’s predictions. Here we see that a new elementary school has opened, which typically raises the value of nearby homes.Overall, DataRobot now has an end-to-end AutoML suite that takes you from data gathering through model building to deployment, monitoring, and management. DataRobot has paid attention to the pitfalls in AI model building and provided ways to mitigate many of them. Overall, I rate DataRobot very good, and a worthy competitor to Google, AWS, Microsoft, and H2O.ai. I haven’t reviewed the machine learning offerings from IBM, MathWorks, or SAS recently enough to rate them.Also on InfoWorld: How to choose a cloud machine learning platformI was surprised and impressed to discover that DataRobot can run on CPUs without accelerators and produce models in a few hours, even when building neural network models that include image classification. That may give it a slight edge over the four competitors I mentioned for AutoML, because GPUs and TPUs are not cheap.", "pub_date": "2020-12-02"},
{"title": "How Nvidia’s Arm acquisition will drive AI to every edge", "overview": "Filling a gap in their offerings, Nvidia hopes to gain money and market share with Arm's extensive reach ", "image_url": "https://images.idgesg.net/images/article/2017/07/keys-to_access_solutions_rubic-cube_puzzle-100728331-large.jpg", "url": "https://www.infoworld.com/article/3575396/how-nvidias-arm-acquisition-will-drive-ai-to-every-edge.html", "body": "Nvidia is sitting pretty in AI (artificial intelligence) right now. For the next few years, most AI systems will continue to be trained on Nvidia GPUs and specialized hardware and cloud services that incorporate these processors.However, Nvidia has been frustrated in its attempts to become a dominant provider of AI chips for deployment into smartphones, embedded systems, and other edge devices. To address that strategic gap, Nvidia this past week announced that it is acquiring processor architecture firm Arm Holdings from SoftBank Group and the SoftBank Vision Fund.Also on InfoWorld: What is CUDA? Parallel programming for GPUsOnce the acquisition closes in the expected 18 months, Nvidia will retain Arm’s name, brand identity, management team, and base of operations in Cambridge, United Kingdom. It will also expand Arm’s Cambridge-based research and development facility, while establishing an Nvidia research facility, developer training facilities, and startup incubator at the site. Arm will operate as an Nvidia division.This is truly a landmark deal. Nvidia will almost certainly integrate its GPU technology into the core smartphone, IoT, and embedded chip architectures designed by Arm, thereby driving its AI technologies ubiquitously to edge devices everywhere.What follows are the principal respects in which Nvidia will benefit from acquiring Arm.Bolstering Nvidia’s bottom lineNvidia is picking up Arm for a $40 billion consideration of cash and shares, which makes it far larger than the $7 billion acquisition of Mellanox that closed in April.Arm comes into Nvidia as a significant cash cow from which its new parent will almost certainly fund ambitious new projects and fill-in acquisitions going forward. Nvidia’s acquisition of Arm is expected to be accretive to the acquirer’s bottom line. Once the deal closes, Arm will start contributing its considerable profits to its new parent’s own net income immediately. Nvidia’s income statement will gain millions of dollars in Arm's annual licensing fees and billions of dollars in royalty fees.The transaction, which is subject to the usual closing conditions and regulatory approvals, represents a middling return on SoftBank’s $32 billion outlay when it took Arm private in 2016. Nvidia’s bargain on the deal stems largely from the fact that SoftBank had run into a cash crunch after losing billions of dollars due to the pandemic and bad bets on Uber and WeWork. SoftBank will acquire an ownership stake in Nvidia of no more than 10 percent, effectively making it one of Nvidia’s largest shareholders.Giving Nvidia a new competitive leverNvidia’s Arm acquisition comes at a time when Intel’s next generation of chips has encountered major delays. Nvidia will be able to leverage the Arm acquisition to contend with Intel across a wide range of mobile, edge, embedded, gaming, and IoT end points. Arm provides the basic architectures for the low-power central processor chips within smartphones and tablets from such licensees as Apple, Samsung, and Qualcomm.Through licensee Apple, Arm-based processors will replace Intel processors in the next generation of Mac computers. The deal will give Nvidia a better shot at displacing Imagination Technologies as Apple’s GPU supplier for its iOS devices. And it will enable Nvidia to serve chip makers who are adapting Arm designs to work in servers and PCs, which have long been Intel’s stronghold.Boosting Nvidia’s processor market positionNvidia is rapidly becoming the dominant vendor of processors for cloud-to-edge deployments of AI. Even before the Arm acquisition, Nvidia was racking up record sales, saw its share price double this year, and overtook Intel as the most valuable U.S. semiconductor company. At the same time, Intel continued to stumble in its efforts to bring to market a credible GPU alternative to Nvidia’s flagship offerings.Nvidia’s Arm acquisition will boldly advance its position in the smartphone market, as well as in the markets for embedded, IoT, and other edge devices, which are all segments from which Nvidia has largely been absent. By contrast, Arm boasts a near-monopoly in providing IP (intellectual property) for mobile device chip architectures. Arm currently licenses designs for third-party microprocessors that power approximately 90 percent of the world’s smartphones and in many other types of edge and mobile devices. Arm’s energy-efficient designs have been used to create 160 billion chips that are manufactured and sold by more than 1,000 licensees.Diversifying Nvidia’s solution and technology portfolioNvidia is acquiring a firm with a complementary technology portfolio, business model, and go-to-market approach.Nvidia doesn’t design CPUs, which are the core of Arm’s chip IP. Nvidia doesn’t license IP to semiconductor companies, which is Arm’s principal business model. And Nvidia doesn’t compete in the mobility market, which is where Arm’s primary licensees operate.Also, Nvidia doesn't own any chip fabrication plants, but outsources production of its chip designs to specialized foundries. Arm, by contrast, doesn’t outsource its chip designs at all, but instead licenses its IP to other vendors that fabricate them, either in their own facilities or outsourced.Once the acquisition is finalized, Nvidia plans to build an Arm-powered supercomputer to support AI R&D at Arm’s Cambridge location. Nvidia also plans to expand Arm’s IP licensing portfolio with Nvidia technologies, especially the latter’s market-leading GPUs.The converged firms will be able to address cloud-to-edge opportunities that combine Nvidia’s AI solutions with Arm’s vast array of licensees. Even before this latest deal, SoftBank had driven Arm’s diversification into new opportunities to license its IP into partnerships in the data center, automotive, IoT, and network processing markets. Arm had already announced that it was designing its Pelion software IP for a growing range of low-power, high-performance AI apps running on edge devices.It’s unclear where Arm’s AI investments will land in the converged firm’s strategies and solution portfolio. To support its ambitions in the AI arena, Arm already leverages IP from its recent acquisitions of Stream Technologies and Treasure Data. These technologies support ingestion, storage, and management of data to be used in building and training machine learning (ML) models that can execute transparently across CPUs, GPUs, and neural network processing units. Arm has also been investing in tools that allow ML models to be dynamically updated on edge devices and also to support secure, distributed, cross-node ML computations.It's important to note that Arm’s processor designs also serve as the basis for Amazon Web Services’ Graviton2 processors and for Fujitsu's A64FX processors that are used in the latter’s Fugaku supercomputer. It remains to be seen whether these Arm licensees—whose respective AI solution portfolios directly compete with Nvidia—will adopt any add-on AI core tech.Nevertheless, even if such licensees balk at adopting Nvidia’s AI, we can expect the combined Nvidia-Arm to offer more of its own chips (GPUs, CPUs, etc.) to power AI in “big-core” data center, high-performance computing, and supercomputer deployments. We should also expect Arm to aggressively offer such IP to its vast ecosystem of licensees.Locking down a strategic supplier for NvidiaNvidia, in acquiring Arm, will be securing its future access to Arm’s processor technology, while keeping it out of the hands of competitors. If this deal is approved, many semiconductor companies that would otherwise have simply been Nvidia’s rivals will also become its customers.Nevertheless, Nvidia has announced its intention to continue Arm’s open licensing ecosystem, a pledge that will be absolutely necessary in order to secure the necessary regulatory approvals. Nvidia has pledged to continue Arm’s policy of customer neutrality, licensing IP to companies who may compete with Nvidia in GPU, AI, and other product segments.Extending Nvidia’s market reach and scaleLast but not least, Nvidia is acquiring a vendor with a much larger market reach and scale than its own.For starters, the deal will expand Nvidia’s reach in the development community from the current 2 million to more than 15 million. More significantly, Nvidia shipped about 100 million chips in the past year, while Arm’s more than 1,000 technology partner licensees shipped more than 22 billion (with a “b”) chips last year and more than 180 billion to date.Facing oppositionNvidia’s pledge to continue Arm’s open licensing and customer neutrality will be a key element for running the gauntlet of regulatory challenges that are sure to ensue.China looked long and hard at the recently approved Mellanox acquisition and may prove difficult to placate, especially considering strains in its geopolitical posture vis-à-vis the United States.Nvidia’s assurance that Arm’s site and team will remain intact in England will prove essential in securing that country's approval. However, political forces at work in the United Kingdom might cause the regulatory approvals to be difficult to secure.Seemingly to prevent the matter from being a regulatory distraction during the approval process, Nvidia’s Arm acquisition will not include Arm‘s two IoT Services Group software businesses. SoftBank had previous announced a plan to spin off the two businesses into new SoftBank-owned stand-alone entities. But nearly three weeks ago, Arm said it was halting the spin-off plans.The biggest potential challenge for a combined Nvidia-Arm, even after the deal clears all regulatory hurdles, will be whether Arm licensees in the microprocessor industry will be comfortable sourcing this key technology from a competitor. Arm’s reputation as the “Switzerland of the semiconductor industry” is at stake. Nvidia rivals—such as AMD—may seek out alternative sources if they perceive that the deal gives CEO Jensen Huang’s firm an unfair advantage in the battle to bring AI to edge devices.", "pub_date": "2020-09-17"},
{"title": "Anti-adversarial machine learning defenses start to take root", "overview": "Adversarial attacks are one of the greatest threats to the integrity of the emerging AI-centric economy.", "image_url": "https://images.idgesg.net/images/article/2020/07/hacker_hacking_laptop_security_threat_attack_by_undefined_undefined_gettyimages-962366210_2400x1600_cio_cw-100852252-large.jpg", "url": "https://www.infoworld.com/article/3596596/anti-adversarial-machine-learning-defenses-start-to-take-root.html", "body": "Much of the anti-adversarial research has been on the potential for minute, largely undetectable alterations to images (researchers generally refer to these as “noise perturbations”) that cause AI’s machine learning (ML) algorithms to misidentify or misclassify the images. Adversarial tampering can be extremely subtle and hard to detect, even all the way down to pixel-level subliminals. If an attacker can introduce nearly invisible alterations to image, video, speech, or other data for the purpose of fooling AI-powered classification tools, it will be difficult to trust this otherwise sophisticated technology to do its job effectively.Growing threat to deployed AI appsThis is no idle threat. Eliciting false algorithmic inferences can cause an AI-based app to make incorrect decisions, such as when a self-driving vehicle misreads a traffic sign and then turns the wrong way or, in a worst-case scenario, crashes into a building, vehicle, or pedestrian. Though the research literature focuses on simulated adversarial ML attacks that were conducted in controlled laboratory environments, general knowledge that these attack vectors are available will almost certainly cause terrorists, criminals, or mischievous parties to exploit them.Also on InfoWorld: How to choose a cloud machine learning platformAlthough high-profile adversarial attacks did not appear to impact the ML that powered this year’s U.S. presidential campaign, we cannot deny the potential for these in future electoral cycles. Throughout this pandemic-wracked year, adversarial attacks on ML platforms have continued to intensify in other sectors of our lives.This year, the National Vulnerability Database (part of the U.S. National Institute for Science and Technology) issued its first Common Vulnerabilities and Exposures report  for an ML component in a commercial system. Also, the Software Engineering Institute’s CERT Coordination Center issued its first vuln note flagging the extent to which many operational ML systems are vulnerable to arbitrary misclassification attacks.Late last year, Gartner predicted that during the next two years 30 percent of all cyberattacks on AI apps would use adversarial tactics. Sadly, it would be premature to say that anti-adversarial best practices are taking hold within the AI community. A recent industry survey by Microsoft found that few industry practitioners are taking the threat of adversarial machine learning seriously at this point or using tools that can mitigate the risks of such attacks.Even if it were possible to identify adversarial attacks in progress, targeted organizations would find it challenging to respond to these assaults in all their dizzying diversity. And there’s no saying whether ad-hoc responses to new threats will coalesce into a pre-emptive anti-adversarial AI “hardening” strategy anytime soon.Anti-adversarial ML security methodologies As these attacks surface in greater numbers, AI professionals will clamor for a consensus methodology for detecting and dealing with adversarial risks.An important milestone in adversarial defenses took place recently. Microsoft, MITRE, and 11 other organizations released an Adversarial ML Threat Matrix. This is an open, extensible framework structured like MITRE’s widely adopted ATT&CK framework that helps security analysts classify the most common adversarial tactics that have been used to disrupt and deceive ML systems.Developed in conjunction with Carnegie Mellon and other leading research universities, the framework presents techniques for monitoring an organization’s ML systems to detect whether such attacks are in progress or have already taken place. It lists vulnerabilities and adversary behaviors that are effective against production ML systems. It also provides case studies describing how well-known attacks such as the Microsoft Tay poisoning and the Proofpoint evasion attack can be analyzed using this framework.As discussed in the framework, there are four principal adversarial tactics for compromising ML apps.involves unauthorized recovery of a functionally equivalent ML model by iteratively querying the model with arbitrary inputs. The attacker can infer and generate a high-fidelity offline copy of the model to guide further attacks to the deployed production ML model. occurs when attackers iteratively introduce arbitrary inputs, such as subtle pixel-level changes to images. The changes are practically undetectable to human senses but cause vulnerable ML models to classify the images or other doctored content incorrectly.involves unauthorized recovery of the predictive features that were used to build an ML model. It enables attackers to launch inferences that compromise the private data that was used in training the model. means training data has been contaminated in order to surreptitiously produce specific unauthorized inferences when arbitrary input data is introduced to the poisoned ML model in runtime.Taken individually or combined in diverse ways, these tactics could enable an attacker to surreptitiously “reprogram” an AI app or steal precious intellectual property (data and ML models). All are potential tools for perpetrating fraud, espionage, or sabotage against applications, databases, and other online systems with ML algorithms at their heart.Fruitful anti-adversarial ML tools and tacticsAnti-adversarial tactics must be rooted deeply in the ML development pipeline, leveraging code repositories, CI/CD (continuous integration/continuous delivery), and other devops infrastructure and tools.Grounding their recommendations in devsecops and traditional application security practices, the framework’s authors call for a multipronged anti-adversarial methodology that includes critical countermeasures. would reduce exploitable adversarial vulnerabilities in ML programs and enable other engineers to audit source code. In addition, security-compliance code examples in popular ML frameworks would contribute to the spread of adversarially hardened ML apps. So far TensorFlow is the only ML framework that provides consolidated guidance around traditional software attacks and links to tools for testing against adversarial attacks. The framework’s authors recommend exploring whether containerizing ML apps can help to quarantine uncompromised ML systems from the impact of adversarially impacted ML systems. help detect potential adversarial weaknesses in ML apps as coded or when the apps execute particular code paths. ML tools such as cleverhans, secml, and IBM’s Adversarial Robustness Toolbox support varying degrees of static and dynamic ML code testing. The Adversarial ML Threat Matrix’s publishers call for such tools to be integrated with full-featured ML development toolkits to support fine-grained code assessment before ML apps are committed to the code repository. They also recommend integration of dynamic code-analysis tools for adversarial ML into CI/CD pipelines. This latter recommendation would support automation of adversarial ML testing in production ML apps. support runtime detection of adversarial and other anomalous processes being executed on ML systems. The matrix’s publishers call for ML platforms to use these tools to monitor, at the very least, for attacks listed in the curated repository. This would enable tracing adversarial attacks back to their sources and exporting anomalous event logs to security incident and event management systems. They propose that detection methods be written into a format that facilitates easy sharing among security analysts. They also recommend that the adversarial ML research community register adversarial vulnerabilities in a trackable system like the National Vulnerability Database in order to alert impacted vendors, users, and other stakeholders.A growing knowledgebaseThe new anti-adversarial framework’s authors provide access through their GitHub repo to what they call a “curated repository of attacks.” Every attack documented in this searchable resource has a description of the adversarial technique, the type of advanced persistent threat that has been observed to use the tactic, recommendations for detecting it, and references to publications that provide further insight.As they become aware of new adversarial ML attack vectors, AI and security professionals should register those in this repository. This way the initiative can keep pace with the growing range of threats to the integrity, security, and reliability of deployed ML apps.Going forward, AI application developers and security analysts should also:Assume the possibility of adversarial attacks on all in-production ML applications.Perform adversarial threat assessments prior to writing or deploying vulnerable code.Generate adversarial examples as a standard risk-mitigation activity in the AI training pipeline.Test AI apps against a wide range of adversarial inputs to determine the robustness of their inferences.Reuse adversarial-defense knowledge, such as that provided by the new Adversarial ML Threat Matrix, to improve AI resilience against bogus input examples.Update ongoing adversarial attack defenses throughout the lifecycle of deployed AI models.Ensure data scientists have sophisticated anti-adversarial methodologies to guide them in applying these practices throughout the AI development and operationalization lifecycle.For further information on the new Adversarial ML Threat Matrix, check out the initiative’s GitHub repository, MITRE’s announcement, and the Carnegie Mellon SEI/CERT’s blog post. Other useful resources for security analysts to develop their own anti-adversarial strategies include Microsoft’s taxonomy of ML failure modes, threat modeling guidance specifically for ML systems, and the security development lifecycle bug bar to systematically triage attacks on ML systems.", "pub_date": "2020-11-19"},
{"title": "10 MLops platforms to manage the machine learning lifecycle", "overview": "Machine learning lifecycle management systems rank and track your experiments over time, and sometimes integrate with deployment and monitoring", "image_url": "https://images.idgesg.net/images/article/2019/11/ai_artificial_intelligence_man_graphic_interface_data_by_art24hr_gettyimages_1157378096-100817765-large.jpg", "url": "https://www.infoworld.com/article/3572442/10-mlops-platforms-to-manage-the-machine-learning-lifecycle.html", "body": "For most professional software developers, using application lifecycle management (ALM) is a given. Data scientists, many of whom do not have a software development background, often have  used lifecycle management for their machine learning models. That’s a problem that’s much easier to fix now than it was a few years ago, thanks to the advent of “MLops” environments and frameworks that support machine learning lifecycle management.Also on InfoWorld: How to choose a cloud machine learning platformWhat is machine learning lifecycle management?The easy answer to this question would be that machine learning lifecycle management is the same as ALM, but that would also be wrong. That’s because the lifecycle of a machine learning model is different from the software development lifecycle (SDLC) in a number of ways.To begin with, software developers more or less know what they are trying to build before they write the code. There may be a fixed overall specification (waterfall model) or not (agile development), but at any given moment a software developer is trying to build, test, and debug a feature that can be described. Software developers can also write tests that make sure that the feature behaves as designed.By contrast, a data scientist builds models by doing experiments in which an optimization algorithm tries to find the best set of weights to explain a dataset. There are many kinds of models, and currently the only way to determine which is best is to try them all. There are also several possible criteria for model “goodness,” and no real equivalent to software tests.Unfortunately, some of the best models (deep neural networks, for example) take a long time to train, which is why accelerators such as GPUs, TPUs, and FPGAs have become important to data science. In addition, a great deal of effort often goes into cleaning the data and engineering the best set of features from the original observations, in order to make the models work as well as possible.Keeping track of hundreds of experiments and dozens of feature sets isn’t easy, even when you are using a fixed dataset. In real life, it’s even worse: Data often drifts over time, so the model needs to be tuned periodically.There are several different paradigms for the machine learning lifecycle. Often, they start with ideation, continue with data acquisition and exploratory data analysis, move from there to R&D (those hundreds of experiments) and validation, and finally to deployment and monitoring. Monitoring may periodically send you back to step one to try different models and features or to update your training dataset. In fact, any of the steps in the lifecycle can send you back to an earlier step.Machine learning lifecycle management systems try to rank and keep track of all your experiments over time. In the most useful implementations, the management system also integrates with deployment and monitoring.Machine learning lifecycle management productsWe’ve identified several cloud platforms and frameworks for managing the machine learning lifecycle. These currently include Algorithmia, Amazon SageMaker, Azure Machine Learning, Domino Data Lab, the Google Cloud AI Platform, HPE Ezmeral ML Ops, Metaflow, MLflow, Paperspace, and Seldon.Algorithmia can connect to, deploy, manage, and scale your machine learning portfolio. Depending on which plan you choose, Algorithmia can run on its own cloud, on your premises, on VMware, or on a public cloud. It can maintain models in its own Git repository or on GitHub. It manages model versioning automatically, can implement pipelining, and can run and scale models on-demand (serverless) using CPUs and GPUs. Algorithmia provides a keyworded library of models (see screenshot below) in addition to hosting your models. It does not currently offer much support for model training.Amazon SageMaker is Amazon’s fully managed integrated environment for machine learning and deep learning. It includes a Studio environment that combines Jupyter notebooks with experiment management and tracking (see screenshot below), a model debugger, an “autopilot” for users without machine learning knowledge, batch transforms, a model monitor, and deployment with elastic inference.Azure Machine Learning is a cloud-based environment that you can use to train, deploy, automate, manage, and track machine learning models. It can be used for any kind of machine learning, from classical machine learning to deep learning, and both supervised learning and unsupervised learning.Azure Machine Learning supports writing Python or R code as well as providing a drag-and-drop visual designer and an AutoML option. You can build, train, and track highly accurate machine learning and deep-learning models in an Azure Machine Learning Workspace, whether you train on your local machine or in the Azure cloud.Azure Machine Learning interoperates with popular open source tools, such as PyTorch, TensorFlow, Scikit-learn, Git, and the MLflow platform to manage the machine learning lifecycle. It also has its own open source MLOps environment, shown in the screenshot below.Also on InfoWorld: How dataops improves data, analytics, and machine learningThe Domino Data Science platform automates devops for data science, so you can spend more time doing research and test more ideas faster. Automatic tracking of work enables reproducibility, reusability, and collaboration. Domino lets you use your favorite tools on the infrastructure of your choice (by default, AWS), track experiments, reproduce and compare results (see screenshot below), and find, discuss, and re-use work in one place.The Google Cloud AI Platform includes a variety of functions that support machine learning lifecycle management: an overall dashboard, the AI Hub (see screenshot below), data labeling, notebooks, jobs, workflow orchestration (currently in a pre-release state), and models. Once you have a model you like, you can deploy it to make predictions.The notebooks are integrated with Google Colab, where you can run them for free. The AI Hub includes a number of public resources including Kubeflow pipelines, notebooks, services, TensorFlow modules, VM images, trained models, and technical guides. Public data resources are available for image, text, audio, video, and other types of data.HPE Ezmeral ML Ops offers operational machine learning at enterprise scale using containers. It supports the machine learning lifecycle from sandbox experimentation with machine learning and deep learning frameworks, to model training on containerized distributed clusters, to deploying and tracking models in production. You can run the HPE Ezmeral ML Ops software on-premises on any infrastructure, on multiple public clouds (including AWS, Azure, and GCP), or in a hybrid model.Metaflow is a Python-friendly, code-based workflow system specialized for machine learning lifecycle management. It dispenses with the graphical user interfaces you see in most of the other products listed here, in favor of decorators such as , as shown in the code excerpt below. Metaflow helps you to design your workflow as a directed acyclic graph (DAG), run it at scale, and deploy it to production. It versions and tracks all your experiments and data automatically. Metaflow was recently open-sourced by Netflix and AWS. It can integrate with Amazon SageMaker, Python-based machine learning and deep learning libraries, and big data systems.MLflow is an open source machine learning lifecycle management platform from Databricks, still currently in Alpha. There is also a hosted MLflow service. MLflow has three components, covering tracking, projects, and models.MLflow tracking lets you record (using API calls) and query experiments: code, data, config, and results. It has a web interface (shown in the screenshot below) for queries.MLflow projects provide a format for packaging data science code in a reusable and reproducible way, based primarily on conventions. In addition, the Projects component includes an API and command-line tools for running projects, making it possible to chain together projects into workflows.MLflow models use a standard format for packaging machine learning models that can be used in a variety of downstream tools — for example, real-time serving through a REST API or batch inference on Apache Spark. The format defines a convention that lets you save a model in different “flavors” that can be understood by different downstream tools.Paperspace Gradientº is a suite of tools for exploring data, training neural networks, and building production-grade machine learning pipelines. It has a cloud-hosted web UI for managing your projects, data, users, and account; a CLI for executing jobs from Windows, Mac, or Linux; and an SDK to programmatically interact with the Gradientº platform.Gradientº organizes your machine learning work into projects, which are collections of experiments, jobs, artifacts, and models. Projects can optionally be integrated with a GitHub repo via the GradientCI GitHub app. Gradientº supports Jupyter and JupyterLab notebooks.Also on InfoWorld: Applying devops in data science and machine learningExperiments (see screenshot below) are designed for executing code (such as training a deep neural network) on a CPU and optional GPU without managing any infrastructure. Experiments are used to create and start either a single job or multiple jobs (e.g. for a hyperparameter search or distributed training). Jobs are a made up of a collection of code, data, and a container that are packaged together and remotely executed. Paperspace experiments can generate machine learning models, which can be interpreted and stored in the Gradient Model Repository.Paperspace Core can manage virtual machines with CPUs and optionally GPUs, running in Paperspace’s own cloud or on AWS. Gradientº jobs can run on these VMs.  Seldon Core is an open-source platform for rapidly deploying machine learning models on Kubernetes. Seldon Deploy is an enterprise subscription service that allows you to work in any language or framework, in the cloud or on-prem, to deploy models at scale. Seldon Alibi is an open-source Python library enabling black-box machine learning model inspection and interpretation.Deep learning vs. machine learning: Understand the differencesWhat is machine learning? Intelligence derived from dataWhat is deep learning? Algorithms that mimic the human brainMachine learning algorithms explainedWhat is natural language processing? AI for speech and textAutomated machine learning or AutoML explainedSupervised learning explainedSemi-supervised learning explainedUnsupervised learning explainedReinforcement learning explainedKaggle: Where data scientists learn and competeWhat is CUDA? Parallel processing for GPUsHow to choose a cloud machine learning platformDeeplearning4j: Deep learning and ETL for the JVMReview: Amazon SageMaker plays catch-upTensorFlow 2 review: Easier machine learningReview: Google Cloud AutoML is truly automated machine learningReview: MXNet deep learning shines with GluonPyTorch review: A deep learning framework built for speedReview: Keras sails through deep learning", "pub_date": "2020-09-21"},
{"title": "14 open source tools to make the most of machine learning", "overview": "Tap the predictive power of machine learning with these diverse, easy-to-implement libraries and frameworks", "image_url": "https://images.techhive.com/images/article/2016/08/machine-learning-cloud-ai-artifical-intelligence-3-100678132-large.jpg", "url": "https://www.infoworld.com/article/3575420/14-open-source-tools-to-make-the-most-of-machine-learning.html", "body": "Spam filtering, face recognition, recommendation engines — when you have a large data set on which you’d like to perform predictive analysis or pattern recognition, machine learning is the way to go. The proliferation of free open source software has made machine learning easier to implement both on single machines and at scale, and in most popular programming languages. These open source tools include libraries for the likes of Python, R, C++, Java, Scala, Clojure, JavaScript, and Go.Also on InfoWorld: How to choose a cloud machine learning platformApache MahoutApache Mahout provides a way to build environments for hosting machine learning applications that can be scaled quickly and efficiently to meet demand. Mahout works mainly with another well-known Apache project, Spark, and was originally devised to work with Hadoop for the sake of running distributed applications, but has been extended to work with other distributed back ends like Flink and H2O.Mahout uses a domain specific language in Scala. Version 0.14 is a major internal refactor of the project, based on Apache Spark 2.4.3 as its default.ComposeCompose, by Innovation Labs, targets a common issue with machine learning models: labeling raw data, which can be a slow and tedious process, but without which a machine learning model can’t deliver useful results. Compose lets you write in Python a set of labeling functions for your data, so labeling can be done as programmatically as possible. Various transformations and thresholds can be set on your data to make the labeling process easier, such as placing data in bins based on discrete values or quantiles.Core ML ToolsApple’s Core ML framework lets you integrate machine learning models into apps, but uses its own distinct learning model format. The good news is you don’t have to pretrain models in the Core ML format to use them; you can convert models from just about every commonly used machine learning framework into Core ML with Core ML Tools.Core ML Tools runs as a Python package, so it integrates with the wealth of Python machine learning libraries and tools. Models from TensorFlow, PyTorch, Keras, Caffe, ONNX, Scikit-learn, LibSVM, and XGBoost can all be converted. Neural network models can also be optimized for size by using post-training quantization (e.g., to a small bit depth that’s still accurate). CortexCortex provides a convenient way to serve predictions from machine learning models using Python and TensorFlow, PyTorch, Scikit-learn, and other models. Most Cortex packages consist of only a few files — your core Python logic, a cortex.yaml file that describes what models to use and what kinds of compute resources to allocate, and a requirements.txt file to install any needed Python requirements. The whole package is deployed as a Docker container to AWS or another Docker-compatible hosting system. Compute resources are allocated in a way that echoes the definitions used in Kubernetes for same, and you can use GPUs or Amazon Inferentia ASICs to speed serving.Also on InfoWorld: The best free data science courses during quarantineFeaturetoolsFeature engineering, or feature creation, involves taking the data used to train a machine learning model and producing, typically by hand, a transformed and aggregated version of the data that’s more useful for the sake of training the model. Featuretools gives you functions for doing this by way of high-level Python objects built by synthesizing data in dataframes, and can do this for data extracted from one or multiple dataframes. Featuretools also provides common primitives for the synthesis operations (e.g., , to provide time elapsed between instances of time-stamped data), so you don’t have to roll those on your own.GoLearnGoLearn, a machine learning library for Google’s Go language, was created with the twin goals of simplicity and customizability, according to developer Stephen Whitworth. The simplicity lies in the way data is loaded and handled in the library, which is patterned after SciPy and R. The customizability lies in how some of the data structures can be easily extended in an application. Whitworth has also created a Go wrapper for the Vowpal Wabbit library, one of the libraries found in the Shogun toolbox.GradioOne common challenge when building machine learning applications is building a robust and easily customized UI for the model training and prediction-serving mechanisms. Gradio provides tools for creating web-based UIs that allow you to interact with your models in real time. Several included sample projects, such as input interfaces to the Inception V3 image classifier or the MNIST handwriting-recognition model, give you an idea of how you can use Gradio with your own projects.H2OH2O, now in its third major revision, provides a whole platform for in-memory machine learning, from training to serving predictions. H2O’s algorithms are geared for business processes—fraud or trend predictions, for instance—rather than, say, image analysis. H2O can interact in a stand-alone fashion with HDFS stores, on top of YARN, in MapReduce, or directly in an Amazon EC2 instance.Hadoop mavens can use Java to interact with H2O, but the framework also provides bindings for Python, R, and Scala, allowing you to interact with all of the libraries available on those platforms as well. You can also fall back to REST calls as a way to integrate H2O into most any pipeline. OryxOryx, courtesy of the creators of the Cloudera Hadoop distribution, uses Apache Spark and Apache Kafka to run machine learning models on real-time data. Oryx provides a way to build projects that require decisions in the moment, like recommendation engines or live anomaly detection, that are informed by both new and historical data. Version 2.0 is a near-complete redesign of the project, with its components loosely coupled in a lambda architecture. New algorithms, and new abstractions for those algorithms (e.g., for hyperparameter selection), can be added at any time.PyTorch LightningWhen a powerful project becomes popular, it’s often complemented by third-party projects that make it easier to use. PyTorch Lightning provides an organizational wrapper for PyTorch, so that you can focus on the code that matters instead of writing boilerplate for each project.Lightning projects use a class-based structure, so each common step for a PyTorch project is encapsulated in a class method. The training and validation loops are semi-automated, so you only need to provide your logic for each step. It’s also easier to set up the training results in multiple GPUs or different hardware mixes, because the instructions and object references for doing so are centralized. Also on InfoWorld: 5 reasons to choose PyTorch for deep learningScikit-learnPython has become a go-to programming language for math, science, and statistics due to its ease of adoption and the breadth of libraries available for nearly any application. Scikit-learn leverages this breadth by building on top of several existing Python packages—NumPy, SciPy, and Matplotlib—for math and science work. The resulting libraries can be used for interactive “workbench” applications or embedded into other software and reused. The kit is available under a BSD license, so it’s fully open and reusable.ShogunShogun is one of the longest-lived projects in this collection. It was created in 1999 and written in C++, but can be used with Java, Python, C#, Ruby, R, Lua, Octave, and Matlab. The latest major version, 6.0.0, adds native support for Microsoft Windows and the Scala language.Though popular and wide-ranging, Shogun has competition. Another C++-based machine learning library, Mlpack, has been around only since 2011, but professes to be faster and easier to work with (by way of a more integral API set) than competing libraries. Spark MLlibThe machine learning library for Apache Spark and Apache Hadoop, MLlib boasts many common algorithms and useful data types, designed to run at speed and scale. Although Java is the primary language for working in MLlib, Python users can connect MLlib with the NumPy library, Scala users can write code against MLlib, and R users can plug into Spark as of version 1.5. Version 3 of MLlib focuses on using Spark’s DataFrame API (as opposed to the older RDD API), and provides many new classification and evaluation functions.Another project, MLbase, builds on top of MLlib to make it easier to derive results. Rather than write code, users make queries by way of a declarative language à la SQL. Also on InfoWorld: 10 MLops platforms to manage the machine learning lifecycleWekaWeka, created by the Machine Learning Group at the University of Waikato, is billed as “machine learning without programming.” It’s a GUI workbench that empowers data wranglers to assemble machine learning pipelines, train models, and run predictions without having to write code. Weka works directly with R, Apache Spark, and Python, the latter by way of a direct wrapper or through interfaces for common numerical libraries like NumPy, Pandas, SciPy, and Scikit-learn. Weka’s big advantage is that it provides browsable, friendly interfaces for every aspect of your job including package management, preprocessing, classification, and visualization.", "pub_date": "2020-09-23"},
{"title": "When AIops tools outsmart you", "overview": "AIops is leading us to the operational promised land for cloud computing, but we have to be smart enough to know how to follow", "image_url": "https://images.techhive.com/images/article/2017/04/5_fumbling_dumb_mistake-100719968-large.jpg", "url": "https://www.infoworld.com/article/3575867/when-aiops-tools-outsmart-you.html", "body": "Our ability to augment technology with artificial intelligence and machine learning does not seem to have limits. We now have AI-powered analytics, smart Internet of Things, AI at the edge, and of course AIops tools.At their essence, AIops tools do smart automations. These include self-healing, proactive maintenance, even working with security and governance systems to coordinate actions, such as identifying a performance issue as a breach.Also on InfoWorld: How to choose a cloud machine learning platformWe need to consider discovery as well, or the capability of gathering data ongoing and leveraging that data to train the knowledge engine. This allows the knowledgebases to become savvier. Greater knowledge about how the systems under management behave or are likely to behave creates a better capability of predicting issues and being proactive around fixes and reporting. Some of the other advantages of AIops automation:Removing the humans from cloudops processes, only alerting them when things require manual intervention. This means fewer operational personnel and lower costs.Automatic generation of trouble tickets and direct interaction with support operations, removing all manual and nonautomated processes.Finding the root cause of an issue and fixing it, either through automated or manual mechanisms (self-healing).  Some of the advantages of AIops discovery:Integrating AIops with other enterprise tools, such as devops, governance, and security operations.Looking for trends that allow the operational team to be proactive, as covered above.Examining huge amount of data from the resources under management, and providing meaningful summaries, which allows for automated action based on summary data.AIops is powerful technology. What are some of the hindrances to taking full advantage of AIops and the power of the tools? The quick answer is the humans. I’m finding that AIOps tools are not being used or considered, mostly due to shortsighted budget issues. If they are being used, they are not leveraged in optimal ways.    Although it would be easy to blame the IT organizations themselves, the larger issue is the lack of a critical mass of best practices of the right way to use AIops. Even some of the providers are pushing their own customers in the wrong directions, and I’m spending a lot of time these days attempting to course correct.    The core issue is the complexity of the AIops tools themselves—ironic considering that they are supposed to combat operational complexities of cloud computing. The difficulty in how to configure the tools properly is systemic.  What are the best practices that are being ignored or misunderstood? I have a few to share this time, but more in the future: The people using AIops tools don’t have a holistic understanding of what all of the systems, applications, and databases mean. No coordination across tool silos could actually lead to more vulnerabilities.  . These complex tools require that you understand the workings of AI engines, the correct use of automation, and, most importantly, the correct way to test these tools.You would hate to have your own AIops solution be smarter than you. The best way to avoid that is to try not to be dumb—just saying.", "pub_date": "2020-09-25"},
{"title": "Where to find free and open data sets on the web", "overview": "From government, health, and finance data to weather, baseball, and Star Trek, countless collections of free data are available to scratch your analytical itch", "image_url": "https://images.techhive.com/images/article/2014/04/dna-research-science-medical-advancement-101334656-100264916-large.jpg", "url": "https://www.infoworld.com/article/3574979/where-to-find-free-and-open-data-sets-on-the-web.html", "body": "Bosses love to hear the word “free.” Everyone wants to get something for nothing. The good news is that there’s a burgeoning collection of free data available for the taking. Some of it might even be useful for your project or your career.What’s the catch? Sometimes there’s no catch at all. Many of the sources below come from government agencies. Once they’re done collecting the information, it often costs them very little to share it openly with everyone. Technically it’s not free because you’re paying for it on April 15th. But the good news is that your project budget won’t feel the pinch.Also on InfoWorld: How to choose a cloud machine learning platformOther data collections are a subtle form of advertising. All of the major cloud companies host various collections of open data sets. You don’t need to use their cloud servers, but the performance will be that much better when the bits are stored in the same data center. The cloud companies could be purchasing 30-second spots on the Super Bowl, but this form of advertising is a better strategy for everyone.The one danger with working with cost-free data is that the boss will assume that it’s also trouble-free. Many times the data will require a bit more work on your part. Perhaps the government agency that collected it liked to use its own peculiar format. Perhaps the data needs to be re-aggregated for your needs. There’s a good chance you’re going to need to write a bit of code to get it to work.Some of the data projects function like open source software and work best when everyone contributes their own small part. I have a weather station in my backyard hooked up to the Personal Weather Station network that gathers data from close to a quarter million different citizen scientists. Participation is essential, but you’ll be able to leverage the work of everyone else at the same time. If your work is going to help build these projects, be prepared to pull your weight with project management.The good news is that the barriers to entry are small. You don’t need to ask permission and you don’t need to beg forgiveness. Here are N different corners of the web to just start downloading and exploring.Data.govThe General Services Agency (GSA) maintains Data.gov, a big list of data sets the US government shares openly. As of this writing, there are 210,756 entries, many from the agencies that specialize in support of commerce (maritime, agriculture, energy). There are no secrets from classified agencies, though, and nothing from Area 51.Kaggle Some of the data sources are not much more than a file repository. Kaggle is more of a cult. They've started with more than 50,000 different data sets and then added the basic tools (Jupyter notebooks) for making sense of them. There are already 400,000 different public notebooks that other data scientists have shared that analyze the data underneath. On top of that, Kaggle has added some online courses on using everything and mixed in some competitions with real cash prizes.For instance, Cornell’s Laboratory of Ornithology is offering $25,000 to the best classifiers for birdsong, or what they call “bird vocalizations.” The Open Vaccine initiative will award $25,000 to the best models for predicting RNA degradation that will affect the COVID-19 vaccine. There is plenty of serious work to be found among the CSV or JSON files, but if you grow tired you can also have some fun. One data collection, for instance, is filled with lines scraped from all of the Star Trek episodes from the six major series. FiveThirtyEightThe FiveThirtyEight website is devoted to reporting stories with the support of a rich collection of data. When they can, they also share these data sets for you to do your own research. There are past records of their predictions for the major sports leagues, explorations about social attitudes like surveys of men asking what it means to be a man, and, of course, endless polls about upcoming political votes.UNICEFThe UN agency responsible for helping raise healthy children around the world shares a wide variety of data sets that are useful to anyone with the same goals. The big picture can be found in marquee data sets like The State of the World’s Children 2019 Statistical Tables for those who want to track the change numerically. A more focused visualization can be discovered in tables that explore how iodized salt affects disease or the success of primary education.Financial dataOhio State’s library keeps a web page current with pointers to some of the biggest collections of economic and financial data. There are historical records of US data sets and also some data collected by the World Bank. Some require an academic account and some are free to the public.BaseballAmerica’s sport is blessed by some fans who are adept enough with computers to develop extensive collections of data about the players and the results of their games. Sean Lahman’s database, for instance, contains complete batting and pitching statistics from 1871 through 2019. There are also tables of other details like fielding statistics, managerial changes, and World Series results that may not be complete, but might as well be for the modern era, which in major league baseball begins with the 20th century.Also on InfoWorld: The 6 best programming languages for AI developmentProject Retrosheet was started to assemble play-by-play summaries of all major league games whenever possible, and it is now complete through 1974. If you happen to have access to a scorecard from an earlier game, check the “most wanted” list to see if you can fill in a hole. Chadwick Baseball Bureau maintains a GitHub repo for the data if you prefer.The Society for American Baseball Research maintains a list of other sources including offerings from commercial entities like FanGraphs, Baseball Reference, and Major League Baseball itself.GoogleIf you’re just looking for a particular data set, Google Dataset Search lets you search the entire web for data sets using keywords. The results can be filtered by license, data format, and the time since the last update. Some of the most intriguing data sets are also included in Google’s public data directory, which not only lists the sources but offers some interactive dashboards. The World Bank, for instance, charts fertility versus life expectancy and you can track how this changes over the years with a slider.Amazon Web ServicesAWS users who want data stored in S3 buckets can turn to the Repository of Open Data on AWS, or RODA. There’s wide variety in the thousands of data sets but the highlights tend to be the data sets from sources with which AWS is openly collaborating like the Space Telescope Institute (stars), NOAA (NEXRAD weather radar imagery), and Common Crawl (more than 25 billion web pages). There are several good examples to help you get started analyzing the data using, of course, AWS services like Lambda or Comprehend.MicrosoftMicrosoft also has a number of data sets on Azure. City planners can look for insight in the records from the New York CIty taxi board, which tracks all fares. Economists and traders can look at price records for commodities for insight on inflation and economic changes. All are ready to be analyzed by Microsoft’s machine learning tools.FacebookSome of what we store on Facebook is private because we make it so. Some is shared with friends. Some content is completely open. Facebook supports research on the so-called “Facebook graph” with their Graph API. It’s not the same as downloading the entire data set, but it can be useful for some queries. Just remember that not everyone uses the same privacy settings, so you might not see every person or every post.YelpThe website known for reviews of restaurants, bars, and other public accommodations shares a great deal of the information in a public data set that you can study. There are more than eight million reviews of more than 200,000 establishments just waiting for you or your AI to parse them. They are a good source for training data for natural language processing and machine learning.Open Data KitThe bits distributed by the Open Data Kit community and its JavaScript-based cousin ODK-X aren’t data per se. They’re software designed to support scientists and researchers who are creating the data sets. The code lets you create a user interface that simplifies data collection by the front-line researchers and then begins the classification and cleaning workflow. The tools are used by a diverse group of organizations supporting field research including the World Mosquito Project and the Red Cross. How data analysis, AI, and IoT will shape the post-pandemic ‘new normal’Web scrapingNot all data reside in easily accessible databases with APIs. An enormous volume of information is embedded in web pages and the data needs to be pried out of them with some clever tools. This so-called web scraping is still a pretty good method, but it can have legal limitations. Some sites ban it in their terms of service and others watch for too many requests from one user and then either cut off the user or slow down the responses.Tools like Puppeteer make it simpler to spin up one (or many!) headless versions of a web browser, download a web page, extract the right data, and do it again and again. There are now headless versions for most major browsers, thanks to the software testing community that needs to automate the testing process. Web scraping may not always be appropriate, but when it is it can be the fastest way to get the data you need. Nothing is more open than the open web.", "pub_date": "2020-09-30"},
{"title": "Microsoft’s innovative new tools for the ‘new normal’", "overview": "Support for social distancing, remote collaboration, immersive reality, and biosensing are top announcements at Ignite 2020", "image_url": "https://images.idgesg.net/images/article/2020/08/protective_face_mask_with_world_map_print_surrounded_by_covid-19_coronavirus_cell_morphology_infection_outbreak_pandemic_by_domin_domin_gettyimages-1227206818_cw_cio_2400x1600-100854886-large.jpg", "url": "https://www.infoworld.com/article/3584139/microsofts-innovative-new-tools-for-the-new-normal.html", "body": "Everybody who survives 2020 will look back on it as a year that stood outside the normal flow of our lives. Tech vendors have had to adapt to the chaotic storm of events that has turned global society upside down.In addition to the FAANG vendors (Facebook, Amazon, Apple, Netflix, Google), Microsoft will remain one of the dominant figures in the eventual post-pandemic phase of our lives. This week at its now virtual-only annual Ignite conference, the Redmond, Washington-based cloud powerhouse cemented its position as an innovation leader.How data analysis, AI, and IoT will shape the post-pandemic ‘new normal’We could easily focus on Microsoft’s numerous tweaks to its Azure cloud portfolio and Office productivity tools, but these are less interesting than the new and enhanced solutions that are positioning the company for life after COVID-19.In this coming social order, the most pivotal technologies will be those that enable greater social distancing, more productive remote collaboration, more fluidly immersive reality, and comprehensive biosensing.Here’s my dissection of the chief Ignite announcements that address each of these trends.Social distancingSocial distancing increasingly depends on tools for keeping people out of range of those who might spread infection. Contactless technologies are fast becoming most people’s interface for everything from payment to self-service interactions with all enterprise apps and devices.Recognizing how central this is to the future, Microsoft this week announced contactless support in its Teams solution. Later this year, Microsoft Teams Rooms devices—such as the new Surface Hub 2S—will support new contactless meetings capabilities within physical conference rooms. Contactless support will be built into the Rooms remote app and will be available for Teams through Microsoft Cortana voice assistance.In addition, Microsoft announced Teams Panels, a new category of mountable devices that use information from other wirelessly connected Teams devices, such as people-counting cameras. The Panels devices can show room capacity information, notify meeting participants of room scheduling and occupancy, and assist with wayfinding around physical offices. They will also be able to provide concrete safety guidance from the meeting facility administrator.Better support for people-counting cameras was also in Microsoft’s announcements this week. It rolled out a new spatial analysis feature of the computer vision service within its Azure Cognitive Service portfolio. For social distancing purposes, this feature combines images from multiple cameras and employs AI algorithms to count people in a space, measure how long they’ve been there, determine the distance between them, and meter their queue wait times. For retail, security, and other apps, it can also meter foot traffic in a space and determine when people are trespassing in forbidden zones.Remote collaborationWorking from home has become essential to keep our economy, society, and even political system from screeching to a halt. It’s no secret that usage of Microsoft Teams and other remote collaboration solutions has skyrocketed during this crisis, along with distance learning, remote medicine, and other technologies that replace in-person meetings of all sorts.Recognizing that it has a pivotal platform for this new era, Microsoft this week announced many enhancements to Teams that bridge the productivity gap between remote workers and those who may choose to operate out of traditional multiperson offices, now or in the future. These new features, some of which will be available later this year, include:Support for as many as 1,000 participants per meeting with the full meeting experience and scale to support 20,000 participants in a view-only mode with live captions.Automate attendee registration through e-mail and generate a postmeeting reporting dashboard to analyze attendee engagement.Automatically generate a postmeeting recap with the recording, transcript, chat, and shared files, then store them in the meeting’s Outlook calendar event entry and in Microsoft 365.Split participants into smaller groups, thereby facilitating brainstorming sessions and workgroup discussions. Also allow presenters to switch between breakout rooms, make announcements to all rooms, and close them to return everyone to the main meeting.Transpose a presenter’s video feed onto the foreground of the slide they’re showing.Place all participants in a shared background no matter where they are, while defining default backgrounds (such as auditoriums, conference rooms, or coffee shops) for all meeting attendees.Automatically scale and center meeting participants’ videos in their virtual seats, regardless of how close or far they are from their camera.Provide a streamlined Calling view that shows contacts, voicemail, and calling history at once, while enabling single-click initiation and return of calls.Immersive realityRemote work tends to lack the hands-on, interactive support that we’ve come to expect from on-site technical and administrative support personnel. As enterprises retool their internal processes to support a post-pandemic workforce, they’re exploring a growing range of immersive reality hardware, software, and services to ensure seamless support across distributed personnel.Recognizing that remote users need immersive reality to achieve full productivity in an increasingly virtual world, Microsoft made three key announcements at Ignite:Expansion of global availability of HoloLens 2, its MR (mixed-reality) headsets: These blend augmented reality’s dynamically superimposed captioning with virtual reality’s simulated environments.Private preview of a new MR service called Azure Object Anchors: This enables HoloLens headsets to recognize an object in the real world and dynamically map relevant instructions or visuals onto it. It does so automatically without requiring users or MR developers to have any expert knowledge or barcodes to align physical and virtual objects.Launch of Dynamics 365 Remote Assist: People in two different physical locations can collaborate and solve problems in a shared MR environment.Taken together, these solutions enable, for example, a headset-wearing car owner to be guided by an automaker-developed MR program in performing a simple repair job at home.Comprehensive biosensingLife after COVID-19 will see growing adoption of biosensors to monitor the presence and spread of pathogens that threaten lives and health. We’ll see deployment of ubiquitous sensors to detect viral pathogens in the air, water, soil, surfaces, and human and animal tissues. We’ll also see them internetworked through IoT (the Internet of Things).Addressing this imperative head-on, Microsoft announced Premonition, an early warning system for disease outbreaks such as, but not limited to, COVID-19. Available in coming weeks through an early access program, Premonition is designed to detect pandemics and other disease outbreaks early so that they can be prevented, contained, and otherwise mitigated before they spin out of control. The service automates the capture, collection, aggregation, and analysis of data on potential outbreaks.Behind the scenes, Premonition leverages a a global network of IoT-connected sensing platforms to feed supercomputers with real-time genomic data recovered from insects and other environmental sources. Running on Azure, it uses robotics to autonomously collect environmental samples and scan the recovered genomes for biological threats.Over the past five years, Microsoft has tested its technologies in various habitats worldwide, ranging from the sands of the Florida Keys to the deep forests of Tanzania. To date, it has scanned more than 80 trillion base-pairs of environmentally recovered genomic material worldwide for biological threats.Anticipating the growing need for governments to maintain counter-contagion nerve centers, Microsoft has worked with Harris County, Texas (where Houston is located), for the past four years on the initative now called Premonition. In conjunction with Premonition’s public launch, Microsoft has announced that Harris County is expanding its deployment to support continuous biological situational awareness. The county’s goal is to enable health officials to monitor and forecast outbreaks in real time on a map so that they can better plan early interventions to protect the public.The announcements that weren’t madeMicrosoft has the strongest portfolio for helping enterprise customers shift operations toward a future in which more work is done from employees’ homes and the practical distinction between physical and virtual team spaces vanishes.Such productivity will depend on tools such as those Microsoft announced at Ignite for social distancing, remote collaboration, immersive reality, and comprehensive biosensing. But there were a few notable gaps in Microsoft’s strategy, if we consider purely the announcements at Ignite 2020.For starters, Microsoft did not make any robotics-focused announcements. That was unfortunate, if we consider that the market for automated disinfection robots continues to heat up with new entrants. Before long, many offices and public spaces will be equipped with biosensors and robotic cleansing platforms that allow workers and customers to reoccupy them.Also, Microsoft did not indicate whether it plans to package its Premonition biosensing technology for on-premise deployment in enterprise facilities, or even for hybrid deployments that span the Azure public cloud and enterprise private clouds. This was a noteworthy gap in its Premonition story, especially since Microsoft made a big to-do about adding more devices and services to its Azure Stack hybrid-cloud portfolio.Also on InfoWorld: How to choose a cloud machine learning platformNeither was there any discussion of whether Microsoft plans to provide customers of Premonition with quarantine orchestration tools to guide more orderly lockdown, quarantine, and reopening of physical facilities in the face of disease outbreaks.That latter capability will almost certainly become a standard feature of enterprise facility management solutions everywhere as we move more deeply into the new decade.", "pub_date": "2020-10-01"},
{"title": "Build custom models with Azure Machine Learning Designer", "overview": "Now anyone can be a model designer with assistive tooling and a new automated ML development service. All you need is data.", "image_url": "https://images.idgesg.net/images/article/2020/07/machine-learning-and-mlops-hpe-ezmeral-softwaretg-100851051-large.jpg", "url": "https://www.infoworld.com/article/3586551/build-custom-models-with-azure-machine-learning-designer.html", "body": "Machine learning is an important part of modern application development, replacing much of what used to be done using a complex series of rules engines, and expanding coverage to a much wider set of problems. Services like Azure’s Cognitive Services provide prebuilt, pretrained models that support many common use cases, but many more need custom model development.Going custom with MLHow do we go about building custom machine learning models? You can start at one end using statistical analysis languages like R to build and validate models, where you’ve already got a feel for the underlying structure of your data, or you can work with the linear algebra features of Python’s Anaconda suite. Similarly, tools such as PyTorch and TensorFlow can help construct more complex models, taking advantage of neural nets and deep learning while still integrating with familiar languages and platforms.Also on InfoWorld: The best free data science courses during quarantineThat’s all good if you’ve got a team of data scientists and mathematicians able to build, test, and (most importantly) validate their models. With machine learning expertise hard to find, what’s needed are tools to help guide developers through the process of creating the models that businesses need. In practice, most machine learning models fall into two types: the first identifies similar data, the second identifies outlying data.We might use the first type of app to identify specific items on a conveyor belt or have the second look for issues in data from a series of industrial sensors. Scenarios like these aren’t particularly complex, but they still require building a validated model, ensuring that it can identify what you’re looking for and find the signal in the data, not amplify assumptions or respond to noise.Introducing Azure Machine Learning DesignerAzure provides various tools for this, alongside its prebuilt, pretrained, customizable models. One, Azure Machine Learning Designer, lets you work with your existing data with a set of visual design tools and drag-and-drop controls.You don’t need to write code to build your model, though there’s the option to bring in custom R or Python where necessary. It’s a replacement for the original ML Studio tool, adding deeper integration into Azure’s machine learning SDKs and with support for more than CPU-based models, offering GPU-powered machine learning and automated model training and tuning.To get started with Azure Machine Learning Designer open the Azure Machine Learning site and log in with an Azure account. Begin by connecting to a subscription and creating a workspace for your models. The setup wizard asks you to specify whether the resulting models have a public or private end point and whether you’re going to be working with sensitive data before choosing how keys are managed. Sensitive data will be processed in what Azure defines as a “high business impact workspace,” which reduces the amount of diagnostic data collected by Microsoft and adds extra levels of encryption.Configuring a machine learning workspaceOnce you’ve walked through the wizard, Azure checks your settings before creating your ML workspace. Usefully it offers you an ARM template so you can automate the creation process in future, providing a framework for scripts that business analysts can use from an internal portal to reduce the load on your Azure administrators. Deploying the resources needed to create a workspace can take time, so be prepared to wait a while before you can start building any models.Your workspace contains tools for developing and managing machine learning models, from design and training to managing compute and storage. It also helps you label existing data, increasing the value of your training data set. You’re likely to want to start with the three main options: working with the Azure ML Python SDK in a Jupyter-style notebook, using Azure ML’s automated training tools, or the low-code drag-and-drop Designer surface. IDGAzure Machine Learning Studio offers multiple ways to use your data to create ML models.Using Azure ML Designer to create a modelThe Designer is the quickest way to start with custom machine learning, as it gives you access to a set of prebuilt modules that can be chained together to make a machine learning API that’s ready for use in your code. Begin by creating a canvas for your ML pipeline, setting up the compute target for your pipeline. Compute targets can be set for the entire model, or for individual modules within the pipeline, allowing you to tune performance appropriately.It’s best to think of your model’s compute resources as serverless compute, which scales up and down as necessary. When you’re not using it, it will scale down to zero and can take as long as five minutes to spin up again. This might impact application operations, so ensure that it’s available before running applications that depend on it. You will need to consider the resources needed to train a model when choosing a compute target. Complex models can take advantage of Azure’s GPU support, with support for most of Azure’s compute options (depending on your available quota).Once you’ve set up your training compute resources, pick a training data set. This can be your own data or one of Microsoft’s samples. Custom data sets can be constructed from local files, from data already stored on Azure, from the Web, or from registered open data sets (which are often government information).Using data in Azure ML DesignerTools in the Designer allow you to explore the data sets you’re using, so you can be sure you have the right source for the model you’re trying to build. With a data source on the canvas, you can start dragging in modules and connecting them to process your training data; for example, removing columns that don’t contain enough data or cleaning up missing data. This drag-and-connect process is very like working with low-code tools, such as those in the Power Platform. What differs here is that you have the option of using your own modules.Once data has been processed, you can start to choose the modules you want to train your model. Microsoft provides a set of common algorithms, as well as tools for splitting data sets for training and testing. The resulting models can be scored using another module once you run them through training. Scores are passed to an evaluation module so you can see how well your algorithm operated. You do need some statistical knowledge to interpret the results so you can understand the types of errors that are generated, though in practice the smaller the error value, the better. You don’t need to use the prepared algorithms, as you can bring in your own Python and R code.A trained and tested model can be quickly converted into an inferencing pipeline, ready for use in your applications. This adds input and output REST API end points to your model, ready for use in your code. The resulting model is then deployed to an AKS inferencing cluster as a ready-to-use container.Let Azure do it all for you: Automated Machine LearningIn many cases you don’t even need to do that much development. Microsoft recently released an Automated ML option, based on work done at Microsoft Research. Here you start with an Azure-accessible data set, which must be tabular data. It’s intended for three types of model: classification, regression, and forecasts. Once you provide data and choose a type of model, the tool will automatically generate a schema from the data that you can use to toggle specific data fields on and off, building an experiment that is then run to build and test a model.Automated ML will create and rank several models, which you can investigate to determine which is the best for your problem. Once you’ve found the model you want, you can quickly add input and output stages and deploy it as a service, ready for use in tools like Power BI.With machine learning an increasingly important predictive tool across many different types of business problem, Azure Machine Learning Designer can bring it a much wider audience. If you’ve got data, you can build both analytical and predictive models, with minimal data science expertise. With the new Automated ML service, it’s easy to go from data to service to no-code analytics.", "pub_date": "2020-10-20"},
{"title": "Why enterprises are turning from TensorFlow to PyTorch", "overview": "The deep learning framework PyTorch has infiltrated the enterprise thanks to its relative ease of use. Three companies tell us why they chose PyTorch over Google’s renowned TensorFlow framework.", "image_url": "https://images.idgesg.net/images/article/2018/10/artificial-intelligence_brain_machine-learning_digital-transformation_world-networking-100777429-large.jpg", "url": "https://www.infoworld.com/article/3597904/why-enterprises-are-turning-from-tensorflow-to-pytorch.html", "body": "A subcategory of machine learning, deep learning uses multi-layered neural networks to automate historically difficult machine tasks—such as image recognition, natural language processing (NLP), and machine translation—at scale.TensorFlow, which emerged out of Google in 2015, has been the most popular open source deep learning framework for both research and business. But PyTorch, which emerged out of Facebook in 2016, has quickly caught up, thanks to community-driven improvements in ease of use and deployment for a widening range of use cases.Also on InfoWorld: The best free online PyTorch courses and tutorialsPyTorch is seeing particularly strong adoption in the automotive industry—where it can be applied to pilot autonomous driving systems from the likes of Tesla and Lyft Level 5. The framework also is being used for content classification and recommendation in media companies and to help support robots in industrial applications.Joe Spisak, product lead for artificial intelligence at Facebook AI, told InfoWorld that although he has been pleased by the increase in enterprise adoption of PyTorch, there’s still much work to be done to gain wider industry adoption.“The next wave of adoption will come with enabling lifecycle management, MLOps, and Kubeflow pipelines and the community around that,” he said. “For those early in the journey, the tools are pretty good, using managed services and some open source with something like SageMaker at AWS or Azure ML to get started.”Disney: Identifying animated faces in moviesSince 2012, engineers and data scientists at the media giant Disney have been building what the company calls the Content Genome, a knowledge graph that pulls together content metadata to power machine learning-based search and personalization applications across Disney’s massive content library.“This metadata improves tools that are used by Disney storytellers to produce content; inspire iterative creativity in storytelling; power user experiences through recommendation engines, digital navigation and content discovery; and enable business intelligence,” wrote Disney developers Miquel Àngel Farré, Anthony Accardo, Marc Junyent, Monica Alfaro, and Cesc Guitart in a blog post in July.Before that could happen, Disney had to invest in a vast content annotation project, turning to its data scientists to train an automated tagging pipeline using deep learning models for image recognition to identify huge quantities of images of people, characters, and locations.Disney engineers started out by experimenting with various frameworks, including TensorFlow, but decided to consolidate around PyTorch in 2019. Engineers shifted from a conventional histogram of oriented gradients (HOG) feature descriptor and the popular support vector machines (SVM) model to a version of the object-detection architecture dubbed regions with convolutional neural networks (R-CNN). The latter was more conducive to handling the combinations of live action, animations, and visual effects common in Disney content.“It is difficult to define what is a face in a cartoon, so we shifted to deep learning methods using an object detector and used transfer learning,” Disney Research engineer Monica Alfaro explained to InfoWorld. After just a few thousand faces were processed, the new model was already broadly identifying faces in all three use cases. It went into production in January 2020.“We are using just one model now for the three types of faces and that is great to run for a Marvel movie like Avengers, where it needs to recognize both Iron Man and Tony Stark, or any character wearing a mask,” she said.As the engineers are dealing with such high volumes of video data to train and run the model in parallel, they also wanted to run on expensive, high-performance GPUs when moving into production.The shift from CPUs allowed engineers to re-train and update models faster. It also sped up the distribution of results to various groups across Disney, cutting processing time down from roughly an hour for a feature-length movie, to getting results in between five to 10 minutes today.“The TensorFlow object detector brought memory issues in production and was difficult to update, whereas PyTorch had the same object detector and Faster-RCNN, so we started using PyTorch for everything,” Alfaro said.That switch from one framework to another was surprisingly simple for the engineering team too. “The change [to PyTorch] was easy because it is all built-in, you only plug some functions in and can start quick, so it’s not a steep learning curve,” Alfaro said.When they did meet any issues or bottlenecks, the vibrant PyTorch community was on hand to help.Blue River Technology: Weed-killing robotsBlue River Technology has designed a robot that uses a heady combination of digital wayfinding, integrated cameras, and computer vision to spray weeds with herbicide while leaving crops alone in near real time, helping farmers more efficiently conserve expensive and potentially environmentally damaging herbicides.The Sunnyvale, California-based company caught the eye of heavy equipment maker John Deere in 2017, when it was acquired for $305 million, with the aim to integrate the technology into its agricultural equipment.Blue River researchers experimented with various deep learning frameworks while trying to train computer vision models to recognize the difference between weeds and crops, a massive challenge when you are dealing with cotton plants, which bear an unfortunate resemblance to weeds.Highly-trained agronomists were drafted to conduct manual image labelling tasks and train a convolutional neural network (CNN) using PyTorch “to analyze each frame and produce a pixel-accurate map of where the crops and weeds are,” Chris Padwick, director of computer vision and machine learning at Blue River Technology, wrote in a blog post in August.“Like other companies, we tried Caffe, TensorFlow, and then PyTorch,” Padwick told InfoWorld. “It works pretty much out of the box for us. We have had no bug reports or a blocking bug at all. On distributed compute it really shines and is easier to use than TensorFlow, which for data parallelisms was pretty complicated.”Padwick says the popularity and simplicity of the PyTorch framework gives him an advantage when it comes to ramping up new hires quickly. That being said, Padwick dreams of a world where “people develop in whatever they are comfortable with. Some like Apache MXNet or Darknet or Caffe for research, but in production it has to be in a single language, and PyTorch has everything we need to be successful.”Datarock: Cloud-based image analysis for the mining industryFounded by a group of geoscientists, Australian startup Datarock is applying computer vision technology to the mining industry. More specifically, its deep learning models are helping geologists analyze drill core sample imagery faster than before.Typically, a geologist would pore over these samples centimeter by centimeter to assess mineralogy and structure, while engineers would look for physical features such as faults, fractures, and rock quality. This process is both slow and prone to human error.“A computer can see rocks like an engineer would,” Brenton Crawford, COO of Datarock told InfoWorld. “If you can see it in the image, we can train a model to analyze it as well as a human.”Similar to Blue River, Datarock uses a variant of the RCNN model in production, with researchers turning to data augmentation techniques to gather enough training data in the early stages.“Following the initial discovery period, the team set about combining techniques to create an image processing workflow for drill core imagery. This involved developing a series of deep learning models that could process raw images into a structured format and segment the important geological information,” the researchers wrote in a blog post.Using Datarock’s technology, clients can get results in half an hour, as opposed to the five or six hours it takes to log findings manually. This frees up geologists from the more laborious parts of their job, Crawford said. However, “when we automate things that are more difficult, we do get some pushback, and have to explain they are part of this system to train the models and get that feedback loop turning.”Like many companies training deep learning computer vision models, Datarock started with TensorFlow, but soon shifted to PyTorch.Also on InfoWorld: 5 reasons to choose PyTorch for deep learning“At the start we used TensorFlow and it would crash on us for mysterious reasons,” Duy Tin Truong, machine learning lead at Datarock told InfoWorld. “PyTorch and Detecton2 was released at that time and fitted well with our needs, so after some tests we saw it was easier to debug and work with and occupied less memory, so we converted,” he said.Datarock also reported a 4x improvement in inference performance from TensorFlow to PyTorch and Detectron2 when running the models on GPUs — and 3x on CPUs.Truong cited PyTorch’s growing community, well-designed interface, ease of use, and better debugging as reasons for the switch and noted that although “they are quite different from an interface point of view, if you know TensorFlow, it is quite easy to switch, especially if you know Python.”", "pub_date": "2020-12-02"},
{"title": "On-premises data warehouses are dead", "overview": "The writing's been on the wall for awhile now. Data analytics migration to the cloud is now underway and is making on-premises data warehouses obsolete.   ", "image_url": "https://images.techhive.com/images/article/2017/03/sign-1749093_1920-100714819-large.jpg", "url": "https://www.infoworld.com/article/3597971/on-premises-data-warehouses-are-dead.html", "body": "Global Market Insights estimates that cloud providers will host the majority of data warehousing loads by 2025. But don’t take their word for it. Gartner estimates that 30 percent of data warehousing workloads now run in the cloud and that this will grow to two-thirds by 2024. Just a few years ago in 2016 the figure was less than 7 percent, also according to Gartner.   None of this should be a surprise. Even the core data warehouse technology providers have seen this trend and are spending the majority of their R&D budgets to build solutions for public cloud providers. Moreover, the public cloud providers themselves have “company killing” products, such as AWS’s RedShift, a columnar database designed to compete with the larger enterprise data warehouse players. What is cloud computing? Everything you need to know nowwhat is IaaS (infrastructure as a service)what is PaaS (platform as a service)what is SaaS (software as a service)What is multicloud? The next step in cloud computingPast impediments to building data warehouses and data marts on public clouds included a perception that security was still an issue on public clouds. Also petabytes of data were difficult to move from on-premises systems, considering that they had to be physically moved with portable storage systems. Finally, in many instances those running data warehouses on-premises could not find analytics tools to leverage locally and did not want to change. The reality is that all these blocks have been removed. Most were removed well before the people building and maintaining data warehouses understood that public clouds were far ahead of most on-premises tools. Today the cloud has better security, performance, cost, and analytics.    The real killer of on-premises data warehouses has been the rise of artificial intelligence on the cloud and the ability to integrate AI with traditional data analytics. AI is not new, but the ability to pay next to nothing for data intelligence, collocated with your data on the public clouds is. AI is a game changer, considering that data warehouses are also a source of training data that could span decades and provide business insights not yet achievable. Another trend that has pushed many on-premises data warehouses to the cloud is the rising need to leverage transactional data directly for analytics. Data warehouses have been famous for just taking snapshots of transactional data and rolling it up into a data warehouse for analytics. This means that the information could be several weeks if not months old. More and more executives are asking for real-time dashboards that consider current data from transactional systems, such as sales order entry.    This means we’ll need to use transitional data using data abstraction layers to emulate analytical databases and bind them with AI systems to make the solution even more compelling. It should come as no surprise that this technology can be found from public cloud providers, either through native services or through their ecosystems and marketplaces. So, on-premises data warehousing is pretty much dead. It’s survived by cloud-based data analytics and database technology that is easily augmented by cheap AI and the ability to deal with data in more innovative ways, such as using transactional data.    The movement to the cloud does a few things. First, it allows enterprises to finally consolidate data on a centrally accessible platform. Second, the data is typically more secure in the cloud. Finally, those who need to leverage the data are no longer restricted to the limitations of on-premises technology. These should be the last three nails in the coffin.  ", "pub_date": "2020-11-24"},
{"title": "Oracle open-sources Java machine learning library", "overview": "Tribuo offers tools for building and deploying classification, clustering, and regression models in Java, along with interfaces to TensorFlow, XGBoost, and ONNX", "image_url": "https://images.idgesg.net/images/article/2020/07/machine-learning-and-mlops-hpe-ezmeral-softwaretg-100851051-large.jpg", "url": "https://www.infoworld.com/article/3574935/oracle-open-sources-java-machine-learning-library.html", "body": "Looking to meet enterprise needs in the machine learning space, Oracle is making its Tribuo Java machine learning library available free under an open source license.With Tribuo, Oracle aims to make it easier to build and deploy machine learning models in Java, similar to what already has happened with Python. Released under an Apache 2.0 license and developed by Oracle Labs, Tribuo is accessible from GitHub and Maven Central.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesTribuo provides standard machine learning functionality including algorithms for classification, clustering, anomaly detection, and regression. Tribuo also includes pipelines for loading and transforming data and provides a suite of evaluations for supported prediction tasks. Because Tribuo collects statistics on inputs, Tribuo can describe the range of each input, for example. It also names features, managing feature IDs and output IDs under the hood to avoid ID conflicts and confusion when chaining models, loading data, and featurizing inputs.A Tribuo model knows when it sees a feature for the first time, which is particularly useful when working with natural language processing. Models know what outputs are, with outputs being strongly typed. Developers do not need to wonder if a float is a probability, a regressed value, or a cluster ID. With Tribuo, each of these is a separate type; the model can describe types and ranges it knows about. Use of strongly typed inputs and outputs means Tribuo can track the model construction process, from the point data is loaded through train/test splits or dataset transformations to model training and evaluation. This tracking data is baked into all models and evaluations.The Tribuo provenance system can generate a configuration that rebuilds the training pipeline to reproduce the model or evaluation. Also, a tweaked model can be built on new data or hyperparameters. Thus users always know what a Tribuo model is, where it came from, and how to create it.Oracle sees Tribuo filling a gap in the marketplace for machine learning for enterprise applications. For example, whereas the Google-built TensorFlow library provides core algorithms for deep learning, Tribuo provides several machine learning algorithms, some of which are in TensorFlow and some of which are not, while also providing an interface to TensorFlow, said Oracle’s Adam Pocock, principal member of the Oracle Labs technical staff. And whereas the Apache Spark analytics engine is for large, distributed systems, Tribuo is for smaller computations that can fit on a single machine, Pocock said.Also on InfoWorld: How to choose a cloud machine learning platformIn addition to TensorFlow, Tribuo provides interfaces to XGBoost and the ONNX runtime, allowing models stored in the ONNX format or trained in TensorFlow and XGBoost to be deployed alongside native Tribuo models. Support for the ONNX model format allows deployment in Java of models trained using popular Python libraries such as PyTorch.Tribuo runs on Java 8 or later. Oracle accepts code contributions to Tribuo under the Oracle Contributor Agreement. Tribuo already has been used internally at Oracle in the Fusion Cloud ERP product for intelligent document recognition, for example.", "pub_date": "2020-09-15"},
{"title": "Nvidia pushes into a wider application ecosystem", "overview": "The BlueField DPU architecture and DOCA SDK provide a strategic platform for expanding the company's reach.", "image_url": "https://images.idgesg.net/images/article/2020/03/geforce-gtc-logo-nvidia-100834092-large.jpg", "url": "https://www.infoworld.com/article/3587769/nvidia-pushes-into-a-wider-application-ecosystem.html", "body": "Nvidia is extending its solution footprint far beyond artificial intelligence (AI) and gaming, venturing broadly across the entire computing ecosystem into mobility and the next-generation cloud data center.Nvidia’s ambitions in this regard are clear from its pending acquisition of Arm Technology and from CEO Jensen Huang’s positioning of the company as a “full-stack computing” provider. Demonstrating that he’s putting substantial R&D dollars behind this vision, at the virtual Nvidia GPU Technology Conference this month, Huang announced the rollout of the company’s new BlueField “data processing unit (DPU)” chip architecture.Also on InfoWorld: What is CUDA? Parallel programming for GPUsAccelerating diverse workloads through programmable CPU offloadStrategically, the BlueField DPU builds on two of Nvidia’s boldest recent acquisitions. The new hardware architecture runs on Arm’s CPU architecture. It also incorporates high-speed interconnect technology that Nvidia acquired recently with Mellanox.Marking the company’s evolution beyond a GPU-centric product architecture, Nvidia’s new DPU architecture is a high-performance, multicore SoC (system on chip). BlueField DPUs incorporate software-programmable data-processing engines that can accelerate a wide range of AI, networking, acceleration, virtualization, security, storage, and other enterprise workloads.As the foundation of server-based intelligent network interface controllers, DPUs offload workloads from CPUs while efficiently parsing, processing, and transferring high volumes of data at line speeds. In addition to their CPU-offload acceleration benefits, Nvidia’s DPUs can strengthen data center security because the Arm cores embedded within them provide an added level of isolation between security services and CPU-executed applications.Announced at this latest GTC were the following versions of this new DPU SoC family:: Due to be included in new systems from Nvidia server hardware partners in 2021, this architecture features all capabilities of the Nvidia Mellanox ConnectX-6 Dx SmartNIC. It incorporates programmable Arm cores, supports data transfer rates of 200Gbps, and provides hardware offloads to accelerate key data center tasks. It speeds up security, networking and storage tasks, including isolation, root trust, key management, RDMA/RDMA over Converged ethernet, GPUDirect, elastic block storage, and data compression. It includes a controller for managing high-performance back-end nonvolatile memory express storage, all-flash arrays and hyperconverged systems. A single BlueField-2 DPU can offload data center workloads from as many as 125 CPU cores, thereby freeing up cycles to process other enterprise applications.: Under development and due to become available in 2021, this adds an Nvidia Ampere architecture GPU to BlueField-2 for in-networking computing with CUDA and Nvidia AI. It includes all the key features of BlueField-2 and leverages Nvidia’s third-generation Tensor Cores for real-time AI-driven security analytics. It can identify abnormal traffic indicative of theft of confidential data. It can also encrypt traffic analytics at line rate and introspect traffic to identify malicious activity and automatically trigger security features and automated responses.Nvidia also announced that it will launch next-generation BlueField-3 and BlueField-3X DPUs in 2022, and BlueField-4X in 2023. In the latter generation, Nvidia will integrate the GPU and Arm cores at the silicon level. The company promised that BlueField-4 will boost the DPU’s processing speeds 1000 times beyond BlueField-2X and 600 times beyond BlueField-3X.Building a robust ecosystem around the DPU accelerator architectureAs it evolves its hardware platform into a DPU-centric architecture in support of new enterprise applications, Nvidia is also making sure that it fully integrates its BlueField/DOCA accelerators into the Arm partner ecosystem.Signaling that strategy at GTC, the vendor announced that it will help Arm partners go to market with full-stack solution platforms that consist of GPU-enabled as well as DPU-enabled networking, storage and security technologies. It has engaged Arm partners to create full-stack solutions for high-performance computing, cloud, edge and PC opportunities. Also, it is porting its AI and RTX engines to Arm, so that they address a much larger market than the x86 platforms on which Nvidia has traditionally run.Partners are essential to Nvidia’s plans to support a wider range of enterprise application workloads than just AI on its new DPU product family. Integral to Nvidia’s land-and-expand strategy is DOCA, a new data center infrastructure SoC architecture and software development kit.Currently available to early access partners only, the DOCA SDK enables developers to program applications on BlueField-accelerated data center infrastructure services. Developers can offload CPU workloads to BlueField DPUs. Consequently, this new offering builds out Nvidia’s enterprise developer tools, complementing the CUDA programming model that enables development of GPU-accelerated applications. In addition, the SDK is fully integrated into the Nvidia NGC catalog of containerized software, thereby encouraging third-party application providers to develop, certify, and distribute DPU-accelerated applications.Several leading software vendors (VMware, Red Hat, Canonical, and Check Point Software Technologies) announced plans at GTC to integrate their wares with the new DSP/DOCA acceleration architecture in the coming year. In addition, Nvidia announced that several leading server manufacturers, including Asus, Atos, Dell Technologies, Fujitsu, Gigabyte, H3C, Inspur, Lenovo, Quanta/QCT, and Supermicro, plan to integrate the DPU into their respective products in the same timeframe.Although there was no specific Arm tie-in to Huang’s announcement that Microsoft is adopting Nvidia AI on Azure to bring GPU-accelerated smart experiences to its cloud-based Microsoft Office experience, it would not be surprising if, in coming years, more of the mobile experience on this and other Office apps were accelerated locally by leveraging DPU-offload technology .Enabling Nvidia solutions to lessen their dependency on GPU-centric functionalityNvidia’s product teams are wasting no time to incorporate the DPUs’ CPU-offload acceleration into their solutions. Most notably, Huang announced that the Nvidia EGX AI edge-server platform is evolving to combine the Nvidia Ampere architecture GPU and BlueField-2 DPU on a single PCIe card.Although there was no specific BlueField DPU tie-in to Nvidia Jetson, the company’s Arm-based SoC for AI robotics, one should expect that the DOCA SDK will advance to support development of these applications, which are a hot growth field for Nvidia’s core platforms. It’s also a safe bet that the company will use its new hardware and SDK to accelerate its Omniverse platform for collaborative 3-D content production, its Jarvis platform for conversational AI, and its new Maxine platform for cloud-native, AI-accelerated video streaming.Maintaining momentumNvidia’s new BlueField DPU architecture and DOCA SDK provide a strategic platform for broadening its reach into enterprise, service provider, and consumer opportunities of all types.By enabling hardware-accelerated CPU-offload of diverse workloads, the DPU architecture provides Nvidia with a clear path for converging the new DOCA programming models with its CUDA AI development framework and NGC catalog of containerized cloud solutions. This will enable the company to provide both its own product teams and solution partners with the hardware and software platforms needed to accelerate a full range of application and infrastructure workloads from cloud to edge.As it awaits the eventual approval of its proposed acquisition of Arm Technology, Nvidia will need to prove this new architecture to its existing partner ecosystem. If DPU technology falls short of Nvidia’s aggressive performance promises, that deficiency could sour relations with Arm’s vast array of licensees, all of whom rely heavily on its CPU-based processor architecture and would benefit from more seamless integration with Nvidia’s market-leading AI technology.Clearly Nvidia cannot afford to lose momentum in the cloud-to-edge microprocessor wars just when it has begun to pull away from archrival and CPU-powerhouse Intel.", "pub_date": "2020-10-29"},
{"title": "Microsoft brings .NET dev to Apache Spark", "overview": ".NET for Apache Spark 1.0 provides high-performance .NET APIs to Apache Spark including Spark SQL, Spark Streaming, and MLlib ", "image_url": "https://images.idgesg.net/images/article/2018/08/spiral_sparks_steelwork_coil_spring_by_463529_cc0_via_pixabay_1200x800-100766904-large.jpg", "url": "https://www.infoworld.com/article/3587595/microsoft-brings-net-dev-to-apache-spark.html", "body": "Microsoft and the .NET Foundation have released version 1.0 of .NET for Apache Spark, an open source package that brings .NET development to the Spark analytics engine for large-scale data processing.Announced October 27, .NET for Apache Spark 1.0 has support for .NET applications targeting .NET Standard 2.0 or later. Users can access Spark DataFrame APIs, write Spark SQL, and create user-defined functions UDFs).Also on InfoWorld: How to choose a cloud machine learning platformThe .NET for Apache Spark framework is available on the .NET Foundation’s GitHub page or from NuGet. Other capabilities of .NET for Apache Spark 1.0 include:An API extension framework to add support for additional Spark libraries including Linux Foundation Delta Lake, Microsoft OSS Hyperspace, ML.NET, and Apache Spark MLlib functionality..NET for Apache Spark programs that are not UDFs show the same speed as Scala and PySpark-based non-UDF applications. If applications include UDFs, .NET for Apache Spark programs are at least as fast as PySpark programs or might be faster..NET for Apache Spark is built into Azure Synapse and Azure HDInsight. It also can be used in other Apache Spark cloud offerings including Azure Databricks.The first public version of the project was announced in April 2019. Driving the development of .NET for Apache Spark was increased demand for an easier way to build big data applications instead of having to learn Scala or Python. The project is operated under the .NET Foundation and has been filed as a Spark Project Improvement Proposal to be considered for inclusion in the Apache Spark project directly.Looking ahead, Microsoft is addressing obstacles including setting up prerequisites and dependencies and finding quality documentation, with examples such as community-contributed “ready-to-run” Docker images and updates to .NET for Apache Spark documentation. Another priority is supporting deployment options including integration with CI/CD devops pipelines and publishing jobs directly from Visual Studio.", "pub_date": "2020-10-29"},
{"title": "What to expect from AWS re:Invent 2020", "overview": "The world’s biggest cloud vendor has shifted its annual event online and spread it across three weeks. Here’s our guide to what to expect and how to navigate the event from your home office.", "image_url": "https://images.idgesg.net/images/article/2020/09/hand_holds_virtual_light_bulb_of_network_of_sparks_ideas_innovation_creativity_transformation_by_sutad_watthanakul_gettyimages-1007367156_2400x1600-100859288-large.jpg", "url": "https://www.infoworld.com/article/3598018/what-to-expect-from-aws-reinvent-2020.html", "body": "Cloud computing giant Amazon Web Services (AWS) has its biggest event of the year next week, with AWS re:Invent running online-only and free of charge for the first time, starting November 30 and closing December 18.This year the event will not be spread across various hotels on the Las Vegas strip, but rather across a three-week period online. This brings its own logistical challenges.Also on InfoWorld: Why AWS leads in the cloudThe event will kick off with a “Late Night with AWS” session on Monday night, followed by CEO Andy Jassy’s typical three hour keynote on Tuesday, December 1. This will be followed by the Thursday partner keynote. CTO Werner Vogels will give his technical keynote during the third week, on Tuesday, December 15. The other keynotes will focus on technical areas like machine learning and infrastructure.Aside from the keynotes there are various “leadership” sessions, breakouts, lounges, and “ask the expert” sessions across 50 content tracks and multiple language options. Except for the Late Night sessions there will be no content on Mondays or Fridays.Industry-specific newsIt’s safe to expect Jassy and his senior leadership team to continue its recent efforts to make AWS more accessible to business decision makers, while keeping its core developer audience excited and engaged.“AWS has missed a direct line of communication to the C-suite, not just IT leaders or the developer community, but a business conversation. How does AWS help a senior leader of a business react, respond, or transform? This is the time for AWS to be doing that, as technology and business have never been so entwined,” Nick McQuire, vice president for enterprise research at CCS Insight told InfoWorld.McQuire sees this manifesting around a set of announcements focused on specific industries, such as telecoms, media, and industry. AWS announced a major deal with Verizon at last year’s re:Invent but will look to maintain momentum as 5G networks start to come online.This industry-specific work will span the whole portfolio at AWS but specifically its efforts around the edge and hybrid cloud, where Outposts and Local Zones could see some updates as they see momentum across those key industries. We also wouldn’t be surprised to see more vertical-specific machine learning products, like the anti-fraud detection for financial services and various medical applications which were announced last year.AWS product announcementsIn terms of major product announcements, AWS is expected to bolster its multicloud management toolset, enabling customers to manage their workloads on rival clouds, as well as those with AWS and on-premises. An October report from The Information proposes that such a product is due to be announced at re:Invent. AWS did not deny the validity of the claims when asked by InfoWorld.AWS has long tried to convince the market that enterprises should shift all of their workloads to the public cloud—ideally theirs—but has since enabled more hybrid cloud options to run cloud-like workloads in on-prem or private clouds. Now it could be set to soften its stance further in face of market pressures.Whatever is announced should help AWS compete with products like Google Cloud’s Anthos multicloud management platform, or Microsoft’s Azure Arc, or IBM’s suite of options via its newly acquired Red Hat assets.In an interesting piece of timing, Google Cloud CEO Thomas Kurian recently published a blog post expounding the values of an “open cloud approach”—one that “ensures operational and technical consistency across public clouds or private data centers and effective management of infrastructure, applications, and data across the organization.”McQuire at CCS Insight is less convinced, however. “I am not sure if multicloud will necessarily be the headline,” he said.Instead, he expects AWS to build on its hybrid cloud options like AWS Outposts and Local Zones, as well as continuing to build out its managed container and Kubernetes services. That is “unless they can showcase a capability which clearly exceeds the competition,” he added.Attendees should also expect announcements around the Amazon’s work with custom silicon, building on last year’s announcement of an extended partnership with Arm to design the Graviton2 and a new EC2 Inf1 instance that features AWS’s own Inferentia chips, which are optimized for machine learning workloads and now run most of the company’s Alexa workloads.Also on InfoWorld: How to make the most of the AWS free tierAWS expert lounges and learningAWS re:Invent will be particularly difficult to navigate this year, even without having to keep track of the shuttle schedule. The session catalog is difficult to parse and there is no clear agenda to follow.For those worried about missing out on all that networking they normally get to do, AWS is setting up plenty of virtual lounges. These lounges will also have AWS technical experts on hand to break down technical presentations and answer questions.It wouldn’t be AWS re:Invent without some training and certifications, with AWS running its usual range of “Jams” and “GameDays” for hands-on learning.", "pub_date": "2020-11-24"},
{"title": "Apple releases TensorFlow fork with speedups for M1 Macs", "overview": "Apple says the M1-compiled version of TensorFlow delivers several times faster performance on a number of benchmarks, while running existing TensorFlow scripts as-is", "image_url": "https://images.techhive.com/images/article/2014/04/bolts-of-light-speeding-through-the-acceleration-tunnel-95535268-100264665-large.jpg", "url": "https://www.infoworld.com/article/3596904/apple-releases-tensorflow-fork-with-speedups-for-m1-macs.html", "body": "Apple has released its own fork of the TensorFlow 2.4 machine learning framework, specifically optimized for its newly released M1 processor.According to Apple, the M1-compiled version of TensorFlow delivers several times faster performance on a number of benchmarks, compared to the same jobs running on an Intel version of the same 2020 edition MacBook Pro.InfoWorld’s 2020 Technology of the Year Award winners: The best software development, cloud computing, data analytics, and machine learning products of the yearThe fork, available as open source, requires MacOS 11.0 or better, and provides accelerations on Macs running the new M1 processor.Existing TensorFlow scripts run as-is with the fork; they do not need to be reworked to take advantage of its performance gains. According to VentureBeat, Apple plans to contribute its changes to the main TensorFlow project, to serve as a basis for other optimizations.Apple’s revamp of TensorFlow is one of the first examples of how M1 Macs are intended to draw developers to the Mac platform. M1 chips in new Macs replace the use of the Intel x86 processor, but can run existing software compiled for the x86 by way of Apple’s Rosetta2 binary translation technology.However, Rosetta2-translated apps do incur a performance hit, with some benchmarks running as slowly as 59% of native speed. For performance-sensitive applications, it makes sense to compile them to run natively on the M1.", "pub_date": "2020-11-18"},
{"title": "Public clouds hit a wall in innovation", "overview": "As virtual conferences become the new normal, it’s easier to see that cloud is in a creative stall. ", "image_url": "https://images.techhive.com/images/article/2017/01/01_boring-100702252-large.jpg", "url": "https://www.infoworld.com/article/3600129/public-clouds-hit-a-wall-in-innovation.html", "body": "As it opens this week, AWS re:Invent is not taking place in Vegas but is virtual and free. Virtual events are a silver lining of the pandemic because they keep me off airplanes and eliminate seven miles of walking each day at the bigger public cloud conferences. Maybe I’m getting lazy in my old age, but the time that virtual events save seems to be more productive.Not to pick on AWS, but when we look at the announced innovations at public cloud events during the past year, few were game changers. Yes, most vendors will continue to move toward the intelligent edge, providing more points of presence, and they will continue to exploit artificial intelligence. However, these are mostly evolutionary steps rather than revolutionary ideas.Also on InfoWorld: Cloud tech certifications count more than degrees nowIt doesn’t matter if we’re talking about moving from containers to serverless containers or from relational databases to purpose-built cloud-based databases or from outdated to next-generation cloud security. We’re at a point with public clouds where incremental improvements to existing cloud services will provide much more value to cloud users than taking the technology forward by leaps and bounds.As a technology CTO many times over, I’ve watched all technologies go through this kind of stall as the need to grow the business replaces the need to create net-new innovations. As a business, it’s often the right thing to do at a particular point in time. A more risk-averse posture can generate incremental growth and remove the chance of failure that naturally comes at the cutting edge.So, what happened to the revolution? It’s clear that public cloud providers have not pursued all ideas and innovations that could occur in public cloud computing. The large hyperscale companies still have a broad, ambitious vision for their futures. Enterprise IT professionals who consume cloud technology still want forward-thinking technologies, such as when cloud storage first came on the scene. At the same time, they also respond positively to service improvements and net-new tactical cloud services.The lack of innovations, while understandable, means that we’re no longer pushing technology to its limits or experimenting with an open mind as to what’s possible. I’m sure many inventive projects are taking place behind the scenes—perhaps some of them game changers—but what’s been announced so far in 2020 does not rise to that standard.", "pub_date": "2020-12-04"},
{"title": "Steeltoe: Simplify building .NET cloud microservices", "overview": "Open source .NET tools help build and deploy distributed applications to Spring Cloud and Kubernetes.", "image_url": "https://images.idgesg.net/images/article/2018/08/miniature_figurines_circuit_board_microservices_hardware_strategy_by_alfexe_gettyimages-918050974_plus_abstract_binary_numbers_by_tonivaver_via_gettyimages-827840478_1200x800-100768065-large.jpg", "url": "https://www.infoworld.com/article/3598923/steeltoe-simplify-building-net-cloud-microservices.html", "body": "The .NET Foundation is the home for more than .NET. It’s the open source hub for languages and frameworks to help you build on top of the various .NET runtimes and compilers, with contributions from companies and individuals around the world.One of the more popular tools came out of Pivotal (now part of VMware again). Intended to help developers build better .NET microservices, Steeltoe serves as a bridge between .NET and Pivotal’s Spring Cloud and Cloud Foundry platforms, as well as to Kubernetes and other containers, with a set of libraries that speed up application development. Now that it’s open, it’s receiving code contributions from across the .NET community, including teams at Microsoft, and building on work done in Netflix’s open-source libraries. You can find its repository on GitHub.Also on InfoWorld: What are microservices? Your next software architectureMicroservices with SteeltoeBy building on familiar ways of working, Steeltoe can also be used as a springboard into .NET development from Java and other enterprise languages. You can take work you’ve done in Java on Spring Cloud and port it to .NET, or use Steeltoe connectors to mix different technologies so existing applications can be enhanced with .NET microservices. Project managers will find this approach helpful, as they can mix and match available resources as necessary without having to worry about compatibility issues.Steeltoe can create a cloud microservice using its initialization tool, adding providers for different cloud services and automatically configuring code for deployment. Configuration can be easily stored outside your code, so you can keep tokens and other important authentication details in secure services such as Hashicorp’s Vault without exposing them in code repositories.Its biggest advantages are its libraries, which prepackage useful cloud design patterns ready for use in your code. These include support for service discovery, with Eureka and Consul clients, along with distributed tracing to help debug code. Other key elements support messaging by working with network file shares and provide connectors to the cloud services you’re going to use. Cloud-native microservices need to be stateless, easy to compose, and well-defined, and Steeltoe’s libraries help build code that supports cloud-native design patterns without changing how you write your .NET Core applications.The latest release is Steeltoe 3.0. This moves support away from the .NET Framework to .NET Core, ready for .NET 5 and future .NET Core-based releases. It’s added support for other distributed application platforms, such as Kubernetes, and for messaging-based architectures. That has meant changes in package naming, so code will need some refactoring when you upgrade to the new releases.Using circuit breaker patternsOne useful Steeltoe feature is support for Circuit Breaker patterns using tools based on Netflix’s Hystrix. The original Hystrix is in maintenance mode, but the model it uses is still useful for making microservices fault tolerant, adding resilience to applications by providing a way to quickly lock down failures and prevent cascades.Circuit breakers are an important tool in distributed architectures where you have limited observability into remote or third-party services. That makes them useful where services are built and managed by different devops teams, and where you may be consuming a service that becomes unreliable when too many applications use it. A circuit breaker monitors all calls to a service from your code. If too many calls fail, it falls back to alternatives and raises an alert. This can trigger a restart or simply wait until the remote service is available again.Steeltoe makes it easy to add a circuit breaker wrapper around any service call. This runs in its own thread pool to help manage load on the target service. If a thread isn’t available because there are too many calls from your code or the circuit breaker is waiting while the remote service isn’t responding, the circuit breaker can call its fallback. Similarly calls can have a timeout, with all actions logged and displayed in their own dashboard so you can see how services are operating. These logs are a useful diagnostic, and you can use tools like the ELK stack or Azure’s Monitor to make them part of your devops processes.Deploying to Kubernetes with Project TyeIf you want to make .NET part of a wider cloud-native environment, Steeltoe works well with Microsoft’s Project Tye, bringing .NET microservices to Kubernetes (in particular Azure Kubernetes Service). Here you build services using Steeltoe libraries before adding them to containers using Project Tye. This automates the process of creating .NET containers and Kubernetes manifests, as well as finding other services and handling container dependencies. It’s a useful tool, installed using the .NET command line tools. Once installed it’s easy to go from app scaffolding to running code, with a dashboard to monitor running services and view logs. Multiple services can be added to a single .NET solution, which can then be run using a single tye call.More complex configurations are managed using Project Tye’s own YAML manifests, with application and per-service configurations. The tye.yaml file will help manage dependencies, working with Docker files and container registries to pull in services and deploy them as necessary. Some cases require preconfiguring and deploying external services, for example databases or caches.Building services with Steeltoe and bundling with Project Tye is a quick way to get started with cloud-native .NET development. Although Project Tye remains an experiment, it will be interesting to see if Microsoft continues working on it now that .NET 5 has shipped, as its planned integration with tools like Dapr would make it a useful set of extensions to any .NET development environment. From my point of view, integrating it into the .NET command line environment would make a lot of sense, doing for .NET much of what tools like Draft do for the wider Kubernetes ecosystem.A cloud-agnostic approach to distributed applications in .NETBuilding and running distributed applications shouldn’t be hard; they’re key to working in and across multiple clouds. By supporting Spring Cloud and Kubernetes, Steeltoe offers an agnostic approach to multicloud development. You can build .NET code using familiar tools and techniques with the Steeltoe libraries, before using its configuration tools to support delivering your code to your target platform.The result is code that can be quickly switched from one cloud to another, and from your data center to a managed platform. Modern .NET Core-based code is designed for microservices, and Steeltoe helps provide many of the features you’d otherwise need to write yourself. It’s well worth exploring the various projects hosted by the .NET Foundation; you’re likely to find something that helps you write better code more quickly.", "pub_date": "2020-12-01"},
{"title": "Tune cloud app performance and cost efficiency", "overview": "Think your cloud apps are fully optimized for performance and cost? Guess again. These three tricks can cut thousands of dollars off cloud service bills and reduce latency. ", "image_url": "https://images.techhive.com/images/article/2017/04/1280px-mgocd-14_final_tune_of_a_martin_guitar-100719228-large.jpeg", "url": "https://www.infoworld.com/article/3599334/tune-cloud-app-performance-and-cost-efficiency.html", "body": "The elastic capabilities of public cloud computing are both good and bad. We can provision all the resources and cloud architects to solve performance issues. But tossing money at the situation willy-nilly doesn’t fix the root causes of the problems, which are typically traceable to bad design or bad coding.However some basic tricks of the cloud application development and design trade should allow you to at least reduce the number of performance issues (and by extension, trim monthly cloud bills as well). Here are three of the top ways to improve performance and cost efficiency and not break the bank:Also on InfoWorld: Which multicloud architecture will win out? Although many report larger cloud bills when running serverless, that has not been my experience. Indeed, if you’re unwilling or not budgeted to go into the application code and database schema to fine-tune performance issues, this approach leaves the resource provisioning to the cloud providers (using automation). They know how to optimize their resources better than you do.The idea is that the serverless system won’t over- or underprovision, based on the real-time resource requirements of the application, as determined by the serverless system. Optimization of cloud resources will be the responsibility of the serverless platform itself; many will find that unoptimized applications and databases will better perform if refactored for serverless.If you’re thinking that this is just technological duct tape, you’re right. This assumes that you can’t or won’t redesign, recode, and redeploy the applications, but you can port to a serverless platform. The cost savings should pay for moving to serverless in the first month, but you should gather metrics before and after to make sure. It’s a fundamental architectural principle to store data as close to the application using it as possible. I’m still seeing a number of databases stored in different regions than the applications, databases stored on different clouds, even on-premises databases with applications running in the public clouds. The result is more latency, poorer performance, more outages, and higher compute and network traffic bills.Many of these design flaws were driven by concerns about security and compliance. Even if your data needs to stay in country, or even on-premises, make sure that the application instances that use that data exist close to it. You’ll be happier with the cloud bill, and your users will be much happier with the performance and resiliency. This is the most laborious and thus the most expensive of all three tricks, although the level of effort required has fallen in the past several years.Code scanners, application profilers, and applications testing platforms can reveal a great deal of information about how applications can be refactored to increase performance by as much as three-fold. Of course this means recoding the applications in places, including testing and redeployment, which makes this approach the most expensive. But it’s also the most effective. See how that works?Performance and optimization issues are like back pain—we’re all going to encounter it at one time in our lives. When you do, any of these tricks will be a good place to find relief.", "pub_date": "2020-12-01"},
{"title": "AWS re:Invent: AWS gets more hybrid, but not multicloud", "overview": "The cloud giant announced several new options for customers to ease deployment and management of container-based and serverless applications, but stopped short of easing compatibility with cloud rivals. ", "image_url": "https://images.idgesg.net/images/article/2020/12/screenshot-2020-12-01-at-18.52.40-100868972-large.jpg", "url": "https://www.infoworld.com/article/3599101/aws-reinvent-aws-gets-more-hybrid-but-not-multicloud.html", "body": "Amazon Web Services made a set of announcements during the first day of its AWS re:Invent conference this week aimed at helping customers ease the deployment and management of container-based and serverless applications both on premises and in the AWS cloud, but stopped short of explicitly making it easier to run alongside rival clouds.In this respect there were three major announcements from AWS CEO Andy Jassy’s virtual re:Invent keynote on Tuesday, December 1. The first two, Amazon EKS Anywhere and Amazon ECS Anywhere, are aimed at helping customers run containerized workloads seamlessly on premises and in the cloud.Also on InfoWorld: Why AWS leads in the cloudAmazon Elastic Kubernetes Service (EKS) is a managed Kubernetes service that uses the popular open source container orchestrator. Elastic Container Service (ECS) is a more proprietary, AWS-centric option for running containers.Jassy acknowledged that customers often use different flavors of these managed container services for different workloads and in different teams depending on their skill sets and unique requirements.With the Anywhere options, AWS is looking to make it easier to run EKS and ECS both on premises and in the cloud, while alleviating common management headaches by allowing developers to use the same APIs and cluster configurations for both types of workloads.Amazon's EKS Distro (EKS-D) is also being open sourced, allowing developers to maintain consistent Kubernetes deployments across environments, including bare metal and VMs. \"We’ve learned that customers want a consistent experience on-premises and in the cloud for migration purposes or to enable hybrid cloud setups,\" a blog post by Michael Hausenblas and Micah Hausler from AWS said.The third announcement in this space was the public preview of AWS Proton, a new service that allows developer teams to manage AWS infrastructure provisioning and code deployments for both serverless and container-based applications using a set of templates.These centrally managed templates will define and configure everything from cloud resources to the CI/CD pipeline for testing and deployment, with observability on top. Developers can choose from a set of Proton templates for simple deployment, with monitoring and alerts built-in. Proton also identifies downstream dependencies to alert the relevant teams of changes, upgrade requirements, and rollbacks. Proton will support on-premises workloads through EKS Anywhere and ECS Anywhere as they come online for customers.Hybrid, not multicloudTowards the end of his keynote Jassy reiterated his view that most companies will eventually run predominantly in the cloud, but it will take time to get there. Hence the need for hybrid capabilities—such as AWS Outposts, EKS and ECS Anywhere, and AWS Direct Connect—as a necessary on-ramp for enterprise customers.“We think of hybrid infrastructure as including the cloud alongside other edge nodes, including on-premises data centers. Customers want the same APIs, control plane, tools, and hardware they are used to using in AWS regions. Effectively they want us to distribute AWS to these various edge nodes,” Jassy said.Many enterprise customers want to run different workloads with multiple cloud providers depending on their specific needs. Further, many of these customers want to avoid becoming too dependent on any one cloud. For example, 37% of respondents to the IDG Cloud Computing Survey this year cited the desire to avoid vendor lock-in as one of their primary goals.Ahead of the event it was rumored that AWS would go further in launching a broader multicloud management option which would allow customers to manage Kubernetes workloads running on rival Google Cloud Platform and Microsoft Azure cloud infrastructure, much like Google Cloud is trying to do with Anthos and Microsoft with Azure Arc, or IBM’s suite of options via its newly acquired Red Hat assets.This didn’t happen on day one of re:Invent.“With the notable exception of fully embracing multicloud services, AWS is gradually becoming more flexible in supporting a wider range of customer requirements,” Nick McQuire, senior vice president at CCS Insight said after the keynote.That doesn’t mean EKS couldn’t be used to traverse various environments. As InfoWorld contributor and AWS employee Matt Asay tweeted, “Could you run EKS Distro on another cloud? Yep.”“Customers will be able to run EKS Anywhere on any customer-managed infrastructure. We are still working on the specific infrastructure types that we will support at GA,” Deepak Singh, AWS VP of compute services, told InfoWorld.Other major announcementsOver the three hours of Jassy’s keynote there were many other announcements, including those around databases, which also focused on customers’ desires for portability. AWS Glue Elastic Views was announced as a means for simple data replication across various data stores, while the open source Babelfish for Aurora PostgreSQL offers a way to run SQL Server applications on Aurora PostgreSQL.The machine learning platform Amazon SageMaker was enhanced with a new automated data wrangler feature and a feature store to make it easier to store and reuse features. Amazon SageMaker Pipelines was announced as a CI/CD solution for machine learning pipelines.Also on InfoWorld: How to make the most of the AWS free tierJassy also spent a lot of time focusing on the vendor’s continued work around more price performant custom silicon. He first reflected on the success of its Graviton2 and Inferentia chips announced last year, the latter of which are optimized for machine learning workloads and now run most of the company’s Alexa workloads.This year focused on price performance for machine learning workloads, with the announcement of Habana Gaudi-based EC2 instances, offering more economically viable processors for machine learning workloads and AWS Trainium as a cost-effective option for machine learning model training.Day one down, just 20 left to go.", "pub_date": "2020-12-01"},
{"title": "Oracle adds analytics to MySQL in the cloud", "overview": "Oracle MySQL Database Service with the MySQL Analytics Engine combines OLTP and OLAP in the same MySQL database in the Oracle Cloud", "image_url": "https://images.techhive.com/images/article/2017/05/45751138_d7f5589826_b-100722046-large.jpg", "url": "https://www.infoworld.com/article/3599668/oracle-adds-analytics-to-mysql-in-the-cloud.html", "body": "Oracle is combining OLTP (online transaction processing) and OLAP (online analytical processing) services in its open source MySQL database in the Oracle Cloud, via Oracle MySQL Database Service with the MySQL Analytics Engine.With this Oracle Cloud service being introduced December 2, users gain the ability to run sophisticated analytics against their operational MySQL database. The MySQL Analytics Engine is an in-memory analytics accelerator that can scale to thousands of Oracle Cloud Infrastructure (OCI) cores.Also on InfoWorld: The best open source software of 2020Positioned to compete with the Amazon Redshift cloud data warehouse, the Oracle MySQL Database Service with the MySQL Analytics Engine saves users from having to integrate with a separate analytics database. Database administrators and developers gain a unified platform for OLTP and OLAP workloads. OLAP has not been a specialty of MySQL in the past.Capabilities of the MySQL Analytics Engine include:In-memory, hybrid columnar processingInter-node and intra-node parallelism optimized for Oracle Cloud InfrastructureDistributed query processing algorithmsTo add the analytics capabilities, the MySQL optimizer was enhanced to support analytics while the parser itself was unaltered. Hence, the existing MySQL syntax continues to work as is. With the analytics not requiring maintenance or development of any indexes, data updates made by users are propogated to the analytics engine and available to be queried immediately.Supporting large volumes of data and complex queries, MySQL Database Service with MySQL Analytics Engine has use cases such as fraud detection or decision support. The signup for the service can be found at oracle.com. ", "pub_date": "2020-12-02"},
{"title": "Salesforce targets public clouds with Hyperforce", "overview": "The SaaS CRM vendor is taking the first steps toward moving customers out of its own data centers and into those of its public cloud partners.", "image_url": "https://images.techhive.com/images/article/2017/02/p1200610-100708713-large.jpg", "url": "https://www.infoworld.com/article/3599854/salesforce-targets-public-clouds-with-hyperforce.html", "body": "Salesforce has re-architected its underlying infrastructure to make all of its CRM solutions run in the public cloud as part of a program it calls Hyperforce.Hyperforce may not have been Salesforce’s headline announcement during this year’s Dreamforce—the CRM giant’s big annual conference—but for IT professionals tasked with managing CRM platforms, it is an important one.Also on InfoWorld: Why AWS leads in the cloudSalesforce has long had a strong relationship with infrastructure-as-a-service provider Amazon Web Services (AWS), and has also built relationships with Microsoft Azure, Alibaba Cloud, and Google Cloud Platform over the past few years.For customers who opt to switch to Hyperforce, Salesforce will shift your Sales Cloud, Service Cloud, Marketing Cloud, Commerce Cloud, Industries, and other CRM products, complete with existing customizations and security controls, to the public cloud according to specific data residency needs and other, as yet unclear factors.“Currently, Salesforce determines which cloud providers and specific services are used in each location,” a Salesforce spokesperson told InfoWorld.In short, Salesforce is looking to make it easier for customers to move their CRM to the public cloud, where they get access to greater flexibility and resiliency, as well as fine-grained data residency controls to comply with any local compliance or regulatory needs. Hyperforce is also backward compatible, so developers will not need to rearchitect any custom apps to run on Hyperforce.Hyperforce’s security architecture plugs into existing identity controls and it provides encryption at rest and in transit. Security compliance certifications will transfer over as standard.Salesforce COO Bret Taylor announced the new platform at Dreamforce, hailing it as the biggest change the SaaS company had made since its founding in 1999. Taylor also touted “partnerships with all the amazing public cloud vendors to enable you to work in every region you do business.”According to Salesforce’s 2020 annual report, the company currently provides “the majority of our services to our customers from infrastructure designed and operated by us but secured within third-party data center facilities. In combination with these third-party data center facilities, we also run our services on cloud computing platform partners who offer Infrastructure-as-a-Service, including servers, storage, databases, and networking.”In this context, Hyperforce looks like the first step toward more of the latter, reducing the vendor’s reliance on its own hardware investments.“Hyperforce is a long-term project. Salesforce will continue to maintain existing infrastructure and data centers for an extended period of time,” a spokesperson told InfoWorld.Jason Wong, VP and analyst on the App Design and Development team at Gartner sees the Hyperforce announcement as “the culmination of many years of architecture and infrastructure modernization that Salesforce was doing to enhance its underlying platform that powers its core CRM SaaS products,” he told InfoWorld via email.Also on InfoWorld: Amazon, Google, and Microsoft take their clouds to the edge“This allows Salesforce to reduce reliance on running certain products on its own data centers and potentially its infrastructure costs over time,” Wong added. “As Salesforce’s growth comes more from international markets, the flexibility to support regional cloud deployments and multicloud coverage across regions is an important part of Hyperforce.”Hyperforce is live today in India and Germany, with availability coming to 10 more countries next year. Customers who decide to move to Hyperforce currently will not pay more than their existing set up.", "pub_date": "2020-12-03"},
{"title": "Microsoft rolls out data governance service on Azure", "overview": "Azure Purview automates the discovery and cataloging of data on-premises, in the cloud, and in SaaS applications.", "image_url": "https://images.idgesg.net/images/article/2019/11/abstract_background_technology_data_binary_plotted_points_by_ivanastar_gettyimages_901412996-100817846-large.jpg", "url": "https://www.infoworld.com/article/3600228/microsoft-azure-rolls-out-data-governance-service.html", "body": "Microsoft on December 3 introduced a preview of a cloud-based data governance service, called Azure Purview, to help organizations understand and manage their data.Free until January 1, Azure Purview automates the discovery and cataloging of data on-premises, in the cloud, and in SaaS applications. Information needed to govern data and assess compliance risks is gathered.Also on InfoWorld: When hybrid multicloud has technical advantagesMicrosoft said the service enables the creation of a holistic, up-to-date map of the company’s entire data landscape. Billed as the next generation of the data catalog, Azure Purview purports to reduce costs on multiple fronts, including reducing efforts to discover and classify data while eliminating hidden and explicit costs of maintaining homegrown systems and Excel-based solutions.Capabilities of Azure Purview include:Automated data discovery.Sensitive data classification, with data labeled across SQL Server, Azure, Microsoft 365, and Power BI.End-to-end data lineage.Finding of valuable, trustworthy data by data consumers.Integration of data systems using Apache Atlas APIs.Semantic search and data discovery using business or technical terms.Creation of a map of data across hybrid sources, with metadata management provided.Extraction of metadata, lineage, and classifications from existing data stores.AI-based data classifiers help identify data exposures and out-of-compliance data by looking for personally identifiable information and sensitive data.In February, Azure Purview’s data discovery, classification, and mapping capabilities will be available for data managed by other storage providers. Azure Purview was announced concurrently with the general availability of Azure Synaptics Analytics, an analytics service leveraging data integration, data warehousing, and enterprise data analytics.", "pub_date": "2020-12-04"},
{"title": "Has serverless computing run out of gas?", "overview": "While some industry observers see signs that serverless computing has stalled, others believe the paradigm is just getting started. ", "image_url": "https://images.idgesg.net/images/article/2018/01/broken-down_red-mustang_in-trouble_roadside_hopeless-100747976-large.jpg", "url": "https://www.infoworld.com/article/3598049/has-serverless-computing-run-out-of-gas.html", "body": "Serverless computing offers the promise of applications executed as event-driven compute cycles, whereby your code runs only when triggered by a request. Subscribers to this paradigm can save money by paying only for compute time actually used instead of maintaining a virtual or physical server. Users also are spared from having to manage infrastructure.  AWS Lambda, Microsoft Azure Functions, Google Cloud Functions, and other serverless compute offerings have attracted a lot of attention in recent years. But the serverless revolution may be showing signs of stalling, with some shops not exactly falling all over themselves to adopt the paradigm.Also on InfoWorld: AWS vs. Azure vs. Google Cloud: Which free tier is best?In a report on cloud adoption published this spring, O’Reilly Media found that, among participants surveyed who had not adopted serverless, most—about 62 percent—had no plans to so so. “It certainly feels like serverless has lost its steam, based on the conversations we’re having,” said Mike Loukides, vice president of content strategy at O’Reilly Media. Loukides noted that serverless has fallen short of growth expectations. “Some of that is due to those technical issues that won’t go away,” he said. “Designing systems that can tolerate lots of latency is a big architectural challenge. But it’s pretty clear that a fair number are making it work.” Whether they have dealt with the architectural issues, ignored them, or have a use case where it doesn’t matter is an interesting question, Loukides said.Peder Ulander, product market lead for enterprise and developer audience at AWS, which offers the AWS Lambda serverless platform, disputes the notion of any kind of lull in the serverless realm. “We’ve been very happy with regard to the growth of our serverless business and seeing it becoming more and more strategic,” Ulander said. He added that serverless reduces overall costs and improves developer productivity. Hundreds of thousands of AWS customers are using AWS Lambda.Inquiries about serverless have increased this year at Gartner, analyst Arun Chandrasekaran said. There is a class of applications that serverless is extremely good at, particularly event-driven architectures, with serverless adding a substantial advantage, he noted. Highly ephemeral applications that live for a few minutes or a few seconds are also suitable for serverless.  But Chandrasekaran cautioned that new technologies like serverless often go through a hype cycle before customers realize what the technology actually can do. “In many cases, customers may be trying to use the technology beyond what it’s meant for,” he said.Also on InfoWorld: PaaS, CaaS, or FaaS? How to chooseAndrew Davidson, vice president of cloud products at MongoDB, concurred that serverless was going through a hype cycle but stressed it was still in its early days. “There’s plenty of people who haven’t even started using serverless,” Davidson said. Serverless has worked for some situations but not for others, he said. “It can be used to augment specific, targeted use cases, providing ancillary application services,” he said. MongoDB has a serverless service native to its MongoDB Atlas database service to extend the value of the database.Gartner’s Chandrasekaran believes challenges in the serverless ecosystem today include application debugging and testing, monitoring, and security. More maturity and tools are needed for this, he said. Serverless ecosystem vendors need to provide the same tooling in edge environments as in public cloud environments. AWS’s Ulander, meanwhile, expects that as serverless advances, there will be more integrations around containers or Kubernetes.", "pub_date": "2020-12-08"},
{"title": "Exploring Azure Spatial Analysis containers", "overview": "Use containers and Azure IoT Hub to bring Cognitive Services machine learning to your own servers on the edge.", "image_url": "https://images.idgesg.net/images/article/2020/02/rackspace-article-1200x800-100832023-large.png", "url": "https://www.infoworld.com/article/3600193/exploring-azure-spatial-analysis-containers.html", "body": "Azure’s Cognitive Services are a quick and easy way to add machine learning to many different types of applications. Available as REST APIs, they can be quickly hooked into your code using simple asynchronous calls from their own dedicated SDKs and libraries. It doesn’t matter what language or platform you’re building on, as long as your code can deliver HTTP calls and parse JSON documents.Not all applications have the luxury of a low-latency connection to Azure. That’s why Microsoft is rolling out an increasing number of its Cognitive Services as containers, for use on appropriate hardware that may only have intermittent connectivity. That often requires using systems with a relatively high-end GPU, as the underlying neural nets used by the ML inferencing models require a lot of compute. Even so, with devices like Intel’s NUC9 hardware with an Nvidia Tesla-series GPU, that can be very small indeed.Also on InfoWorld: Kubernetes meets the real world: 3 success storiesPackaging Azure Cognitive ServicesAt the heart of the Cognitive Services suite are Microsoft’s computer vision models. They manage everything from object recognition and image analysis to object detection and tagging to character and handwriting recognition. They’re useful tools that can form the basis of complex applications and feed into either serverless Azure Functions or into no-code Power Apps.Microsoft has taken some of its computer vision modules and packaged them in containers for use on low-latency edge hardware or where regulations require data to be held inside your own data center. That means you can use the OCR container to capture data from pharmacies securely or use the spatial analysis container to deliver a secure and safe work environment.With businesses struggling to manage social distancing and safe working conditions during the current pandemic, tools like the Cognitive Services spatial analysis are especially important. With existing camera networks or relatively low-cost devices, such as the Azure Kinect Camera, you can build systems that can identify people and show if they are working safely: keeping away from dangerous equipment, maintaining a safe separation, or being in a well-ventilated space. All you need is an RTSP (real time streaming protocol) stream from each camera you’re using and an Azure subscription.Setting up spatial analysisGetting started with the spatial analysis container is easy enough. It’s intended for use with Azure IoT Edge, which manages container deployment, and requires a server with at least one Nvidia Tesla GPU. Microsoft recommends its own Azure Stack Edge hardware, as this now offers a T4 GPU option. Using Azure Stack Edge reduces your capital expenditure, as the hardware and software is managed from Azure and billed through an Azure subscription. For test and development, a desktop is good enough; the recommended hardware is a fairly hefty workstation-class PC with 32GB of RAM and two Tesla T4 GPUs with 16GB of GPU RAM.Any system running the container needs to have Docker with Nvidia GPU support, Nvidia’s CUDA tool and multiprocess support, and the Azure IoT Edge runtime, all on Ubuntu 18.04. Your cameras need to deliver an H.264 encoded stream over RTSP at 15fps and 1080p.The spatial analytics container is currently in preview, so you do need approval from the Cognitive Services team before you access it in Microsoft’s container registry. The approval form will ask what type of application you’re building and how it’s intended to be used. Once you get approval you can install Azure IoT Hub, either directly on an Azure Stack Edge device via the Azure CLI or after installing the Azure IoT Edge on your Linux PC. Once that’s in place, you can connect it to an Azure-hosted IoT Hub using the connect strings you can generate from the Azure command line. This will install the container on your edge hardware.It’s a lot easier to get things going on Azure Stack Edge hardware. Here the container runs in a managed Kubernetes cluster, set up as part of the Azure Stack Edge compute tools from the Azure Portal. This will host your IoT Hub Edge compute, hosting the Azure IoT Edge runtime. Again the container is deployed using an Azure CLI command, installing it as a module that can be managed from your IoT Hub instance. You will be able to see the container’s status from the Azure portal.Once deployed and installed, you can start to use the spatial analysis container in your code, connecting to it using the Azure Event Hub SDK. Local code can work directly with the default end point; if you want to use data elsewhere in your network you will need to route messages to another end point or to Azure blob storage.Using spatial analysis to detect peopleOnce the spatial analysis container is running, it’s time to start exploring its features. These take in an RTSP stream and deliver JSON documents. There are several different operations: counting the people in a zone, spotting when people cross a designated line or edge of a polygon, and tracking violations of distance rules. All the operations can be carried out on either individual frames or on live video. Like most IoT Hub services, the spatial analytics container outputs are delivered as events.You will need to configure each operation before using it, giving it a URL for the camera RTSP feed, an indication of whether you’re using live or recorded data, and a JSON definition of the line or area you want to analyze. More than one zone can be set up for each camera field of view. By setting thresholds for counts, you can define the confidence level of the model before triggering an event. Once you have the count data, you can trigger an alert if, for example, too many people are in a space. This can help businesses manage space in accordance with regulations, changing thresholds as regulations change. Lines are treated the same way as zones, triggering counts when the line is crossed.Microsoft demonstrated Azure Cognitive Services to manage site safety at its BUILD event back in 2018. It’s taken a while for these tools to arrive in preview, but it’s easy to see how you can combine spatial analytics and other computer vision services to continuously monitor and protect workspaces. A robot plasma cutter in a dynamic safety zone can quickly be shut it down if a worker enters its operation area. Or maybe you’re watching for workers on high catwalks after a frost, where you want them to avoid slipping.You can provide a set of operations that are quick to configure and easy to manage using Event Hubs and Event Grids to route events to the appropriate application. Now you’re able to build applications that can work with multiple cameras and mix local and cloud operations. You could mix object recognition with line crossing; for example, to detect if someone is too close to a gas pump while smoking a cigarette.Using spatial analysis outputsOutputs are JSON documents with both event and source information, ready for further processing and logging. It’s sensible to keep logs of detected incidents in order to refine the thresholds for detection. If a model is too sensitive, you can ramp up the detection threshold to reduce false positives. Logging events can help with any regulatory issues for compliance with safety rules or privacy requirements.By only providing count or event data, Microsoft falls clearly on the side of privacy with its spatial analysis tools. There’s no identification; just an alert that too many people are in a space or someone has crossed a line. It’s a sensible compromise that lets you build essential safety tools while ensuring that no personal information moves from your network to Microsoft’s.Taking an event-driven approach simplifies building applications. You can use Event Grid to route messages where necessary, drive serverless Functions, or trigger low- and no-code process automations. Using Event Grid as a publish and subscribe backbone increases the number of applications that can work with spatial analysis events, distributing them as needed without having to build your own messaging architecture.", "pub_date": "2020-12-08"},
{"title": "Overcoming the core hurdle to edge computing adoption", "overview": "Enterprises are figuring out that edge computing comes with its own set of challenges. Here’s how to work through the most difficult. ", "image_url": "https://images.techhive.com/images/article/2014/03/177279837-100248627-large.jpg", "url": "https://www.infoworld.com/article/3600452/overcoming-the-core-hurdle-to-edge-computing-adoption.html", "body": "Edge computing is picking up steam. According to this recent report by Turbonomic (requires registration), nearly 50 percent of organizations use or plan to use edge computing in the next 18 months.For those of you watching this market, many existing development projects listed as “edge computing” barely qualify for the title. Still, considering the state of edge computing just a few years ago, this is a huge leap in growth.Also on InfoWorld: Amazon, Google, and Microsoft take their clouds to the edgeThe factors that drive enterprise movement to the edge include:Edge-based solutions in the public cloud. In essence, these are pared down, private cloud versions of public clouds, such as AWS Outpost and Microsoft Stack. They often serve as a jumping off point from legacy systems to public clouds—like a public cloud with training wheels.IoT-based projects. Data storage and compute that’s closer to the edge of the network and to the source of the data provides better performance because less data is sent back to the centralized public cloud server.Edge computing architectures. This architecture involves more substantial and traditional servers, such as traditional storage and compute servers housed in specific offices or branches. Consider a restaurant chain that needs to place storage and compute at all locations but also wants to use a centrally managed paradigm.What stops the forward progress? No surprise here: It’s managing complexity without added cost and risk. According to the Turbonomic report: “Complexity, at 39 percent, is overwhelmingly considered the leading barrier to edge computing becoming conventional.” Complexity is almost double the second-place and third-place barriers: security (23 percent) and technology limitations in network/bandwidth throughput (22 percent). If this survey had been done a few years ago, I suspect that security and technology limitations would have been in the top two spots. What happened? In short, actual edge computing projects took the place of conceptual ones, with as many as 20 to 30 percent failing outright due to the inability to manage complexity.It isn't easy to manage widely distributed systems. There are challenges around configuration management, patching and software updates, CI/CD (continuous integration/continuous delivery), acceptance testing, distributed data storage, and security operations within edge-based implementations. This list is only a fraction of the complexity issues that must be managed at the edge.For now, these problems are difficult but not impossible to manage. There are any number of AIops, governance, and configuration management tools for cloud computing; however, very few tools are focused at the edge.Why? It’s difficult to nail down a repeatable approach and a technology stack for edge solutions. Edge-based systems can include almost any hardware and software, with a wide range of capabilities and limitations. In contrast, developers can depend on the consistencies of public cloud platforms.Edge computing will need sound, repeatable approaches to mediate complexity, as well as tools that provide consistent ways to approach the problem. We are just not there yet.", "pub_date": "2020-12-08"},
{"title": "Using OPA for multicloud policy and process portability", "overview": "How Open Policy Agent allows developer teams to write and enforce consistent policy and authorization across multicloud and hybrid cloud environments", "image_url": "https://images.techhive.com/images/article/2015/10/cloud-security-ts-100622309-large.jpg", "url": "https://www.infoworld.com/article/3596599/using-opa-for-multicloud-policy-and-process-portability.html", "body": "As multicloud strategies become fully mainstream, companies and dev teams are having to figure out how to create consistent approaches among cloud environments. Multicloud, itself, is ubiquitous: Among companies in the cloud, a full 93% have multicloud strategies—meaning they use more than one public cloud vendor like Amazon Web Services, Google Cloud Platform, or Microsoft Azure. Furthermore, 87% or those companies have a hybrid cloud strategy, mixing public cloud and on-premises cloud environments.The primary reason that companies move to the cloud at all is to improve the performance, availability, scalability, and cost-effectiveness of compute, storage, network, and database functions. Then, organizations adopt a multicloud strategy largely to avoid vendor lock-in.OPA: A general-purpose policy engine for cloud-nativeUsing OPA to safeguard KubernetesUsing OPA for cloud-native app authorizationBut multicloud also presents a second alluring possibility, an extension of that original cloud-native logic: the ability to abstract cloud computing architectures so they can port automatically and seamlessly (if not just quickly) between cloud providers to maximize performance, availability, and cost savings—or at least maintain uptime if one cloud vendor happens to goes down. Cloud-agnostic platforms like Kubernetes, which run the same in any environment—whether that’s AWS, GCP, Azure, private cloud, or wherever—offer a tantalizing glimpse of how companies could achieve this kind of multicloud portability.But while elegant in theory, multicloud portability is complicated in practice. Dependencies like vendor-specific features, APIs, and difficult-to-port data lakes make true application and workload portability a complicated journey. In practice, multicloud portability only really works—and works well—when organizations achieve consistency across cloud environments. For that, businesses need a level of policy abstraction that works across said vendors, clouds, APIs, and so on—enabling them to easily port skills, people, and processes across the cloud-native business. While individual applications may not always port seamlessly between clouds, the organization’s overall approach should.Using OPA to create consistent policy and processes across cloudsOne of the tools that has become popular, precisely because it’s domain agnostic, is Open Policy Agent (OPA). Developed by Styra and donated to the Cloud Native Computing Foundation, OPA is an open-source policy engine that lets developer teams build, scale, and enforce consistent, context-aware policy and authorization across the cloud-native realm. Because OPA lets teams write and enforce policies across any number of environments, at any number of enforcement points—for cloud infrastructure, Kubernetes, microservices APIs, databases, service meshes, application authorization, and much more—it allows organizations to take a portable approach to policy enforcement across multicloud and hybrid cloud environments.Moreover, as a policy-as-code tool, OPA enables organizations to take the policies that are otherwise in company wikis and people’s heads and codify them into machine-processable policy libraries. Policy as code not only lets organizations automatically enforce policy in any number of clouds, but also shift left and inject policies upstream, closer to the development teams who are working across clouds, in order to catch and prevent security, operational, and compliance risk sooner.Pairing OPA with Terraform and KubernetesAs one example, many developers now use OPA in tandem with infrastructure-as-code (IaC) tools like Terraform and AWS CDK. Developers use IaC tools to make declarative changes to their vendor-hosted cloud infrastructure—describing the desired state of how they want their infrastructure configured, and letting Terraform figure out which changes need to be made. Developers then use OPA, a policy-as-code tool, to write policies that validate the changes that Terraform suggests and test for misconfigurations or other problems, before they are applied to production.At the same time, OPA can automatically approve routine infrastructure changes to cut down on the need for manual peer review (and the potential for human error that comes with it). This creates a vital safety net and sanity check for developers, and allows them to experiment risk-free with different configurations. While the cloud infrastructure itself is not portable between vendors, the approach is, by design.In a similar way, developers also use OPA to control, secure, and operationalize Kubernetes across clouds, and even across various Kubernetes distributions. Kubernetes has become a standard for deploying, scaling, and managing fleets of containerized applications. Just as Kubernetes is portable, so, too, are the OPA policies that you run on top of it.There are many Kubernetes use cases for OPA. One popular use case, for example, is to use OPA as a Kubernetes admission controller to ensure containers are deployed correctly, with appropriate configuration and permissions. Developers can also use OPA to control Kubernetes ingress and egress decisions, for example writing policies that prohibit ingresses with conflicting hostnames to ensure that applications never steal each other’s internet traffic. Most important for the multicloud cloud, perhaps, is the ability to ensure that each Kubernetes distribution, across clouds, is provably in compliance with enterprise-wide corporate security policies.Creating standard cloud-native building blocksBefore companies can port applications seamlessly across public clouds, they must first create standard building blocks for developers across every cloud-native environment. Along these lines, developers not only use OPA to create policy, but to automate the enforcement of security, compliance, and operations standards across the CI/CD pipeline. This enables repeatable scale for any multicloud deployment, while speeding development and reducing manual errors.Also on InfoWorld: Devops for remote engineers and distributed teamsOPA’s enabling of policy as code means that companies can use tools like Terraform for their public clouds and OPA for policy, Kubernetes for container management and OPA for policy, plus any number of microservices API and app authorization tools and OPA for policy, while running those same OPA policies in the CI/CI pipeline, or on the laptops of developers.In short, organizations need not waste any time reverse-engineering applications for multicloud portability. Instead, they can focus on building a repeatable process, using common skills, across the entire cloud-native stack.Open Policy AgentStyra—newtechforum@infoworld.com", "pub_date": "2020-12-09"},
{"title": "What’s new in Kubernetes 1.20", "overview": "The latest release of the container orchestration system deprecates the Docker runtime in favor of its own runtime interface", "image_url": "https://images.idgesg.net/images/article/2019/02/container-ship_storage_transport_colorful-containers_diversity_outsourcing-100787326-large.jpg", "url": "https://www.infoworld.com/article/3229359/whats-new-in-kubernetes.html", "body": "KubernetesThe Docker runtime is being deprecated. However, this doesn’t mean Docker images or Dockerfiles don’t work in Kubernetes anymore. It just means Kubernetes will now use its own Container Runtime Interface (CRI) product to execute containers instead of the Docker runtime. For most users this will have no significant impact—e.g., any existing Docker images will work fine. But some issues might result when dealing with runtime resource limits, logging configurations, or how GPUs and other special hardware interact with the runtime (something to note for those using Kubernetes for machine learning). The previous link provides details on how to migrate workloads, if needed, and what issues to be aware of.Volume snapshot operations are now stable. This allows volume snapshots—images of the state of a storage volume—to be used in production. Kubernetes applications that depend on highly specific state, such as images of database files, will be easier to build and maintain with this feature active.Kubectl Debug is now in beta, allowing common debug workflows to be conducted from within the kubectl command-line environment. API Priority and Fairness (APF) is now enabled by default, although still in beta. Incoming requests to kube-apiserver can be sorted by priority levels, so that the administrator can specify which requests should be satisfied most immediately.Process PID Limiting is now in general availability. This feature ensures that pods cannot exhaust the number of process IDs available on a Linux host, or interfere with other pods by using up too many processes.Kubernetes 1.17, released in December 2019, introduced the following key new features and revisions: Volume snapshots, introduced in alpha in Kubernetes 1.12, are now promoted to beta. This feature allows a volume in a cluster to be snapshotted at a given moment in time. Snapshots can be used to provision a new volume with data from the snapshot, or to roll back an existing volume to an earlier snapshotted version. Volume snapshots make it possible to perform elaborate data-versioned or code-versioning operations inside a cluster that weren’t previously possible.More of the “in-tree” (included by default) storage plug-ins are now being moved to the Container Storage Interface (CSI) infrastructure. This means less direct dependencies on those drivers for the core version of Kubernetes. However, a cluster has to be explicitly updated to support migrating the in-tree storage plug-ins, but a successful migration shouldn’t have any ill effects for a cluster.The cloud provider labels feature, originally introduced in beta back in Kubernetes 1.2, is now generally available. Nodes and volumes are labeled based on the cloud provider where the Kubernetes cluster runs, as a way to describe to the rest of Kubernetes how those nodes and volumes should be handled (e.g., by the scheduler). If you are using the earlier beta versions of the labels yourself, you should upgrade them to their new counterparts to avoid problems.Where to download KubernetesYou can download the Kubernetes source code from the releases page of its official GitHub repository. Kubernetes is also available by way of the upgrade process provided by the numerous vendors that supply Kubernetes distributions.In this 90-second video, learn about Kubernetes, the open-source system for automating containerized applications, from one of the technology’s inventors, Joe Beda, founder and CTO at Heptio.What’s new in Kubernetes 1.16Custom resource definitions (CRDs), the long-recommended mechanism for extending Kubernetes functionality introduced in Kubernetes 1.7, are now officially a generally available feature. CRDs have already been widely used by third parties. With the move to GA, many optional-but-recommended behaviors are now required by default to keep the APIs stable.Many changes have been made to how volumes are handled. Chief among them is moving the volume resizing API, found in the Container Storage Interface (CSI), to beta.Kubeadm now has alpha support for joining Windows worker nodes to an existing cluster. The long-term goal here is to make Windows and Linux nodes both first-class citizens in a cluster, instead of having only a partial set of behaviors for Windows.CSI plug-in support is now available in alpha for Windows nodes, so those systems can start using the same range of storage plug-ins as Linux nodes.A new feature, Endpoint Slices, allows for greater scaling of clusters and more flexibility in handling network addresses. Endpoint Slices are now available as an alpha test feature.The way metrics are handled continues a major overhaul with Kubernetes 1.16. Some metrics are being renamed or deprecated to bring them more in line with Prometheus. The plan is to remove all deprecated metrics by Kubernetes 1.17.Finally, Kubernetes 1.16 removes a number of deprecated API versions. What’s new in Kubernetes 1.15More features (currently in alpha and beta) for Custom Resource Definitions, or CRDs. CRDs in Kubernetes are the foundation of its extensibility technology, allowing Kubernetes instances to be customized without falling out of conformance with upstream Kubernetes standards. The new features include the ability to convert CRDs between versions (something long available for native resources), OpenAPI publishing for CRDs, default values for fields in OpenAPI-validated schemas for CRDs, and more.Native high availability (HA) in Kubernetes is now in beta. Setting up a cluster for HA still requires planning and forethought, but the long-term goal is to make HA possible without any third-party software.More plug-ins that manage volumes have been migrated to use the Container Storage Interface (CSI), a consistent way to manage storage for hosted containers. Among the new features introduced in alpha for CSI are volume cloning, so that new persistent volumes can be based on an existing one.Certificate management now automatically rotates certificates before expiration.A new framework for plug-ins that perform scheduling operations has entered alpha.What’s new in Kubernetes 1.14Microsoft Windows Server 2019 is now officially supported as a platform for running both Kubernetes worker nodes and container scheduling. This means entire Kubernetes clusters can run on Windows exclusively, rather than having a mix of Windows and Linux systems.The plugin mechanism for Kubectl, the default Kubernetes command-line tool, is now a stable feature, letting developers implement their own Kubectl subcommands as standalone binaries.Persistent local volumes are now a stable feature. This lets locally attached storage be used by Kubernetes for persistent volumes. Aside from offering better performance than using network-attached storage, it also makes it easier (and potentially cheaper) to stand up a cluster.Process ID limiting for Linux hosts is now a beta feature. This prevents any one pod from using up too many process IDs and thus causing resource exhaustion on the host.What’s new in Kubernetes 1.13\n\n\n\n\nCoreDNS\nWhat’s new in Kubernetes 1.12Released in late September 2018, Kubernetes 1.12 brings to general availability the Kubelet TLS Bootstrap. The Kubelet TLS Bootstrap allows a Kubelet, or the primary agent that runs on every Kubernetes node, to join a TLS-secured cluster automatically, by requesting a TLS client certificate through an API. By automating this process, Kubernetes allows clusters to be configured with higher security by default.Also new in Kubernetes 1.12 is support for Microsoft Azure’s virtual machine scale sets (VMSS), a way to set up a group of VMs that automatically ramp up or down on schedule or to meet demand. Kubernetes’s cluster-autoscaling feature now works with VMSS.Other new features in Kubernetes 1.12:Snapshot and restore functionality for volumes (alpha).Custom metrics for pod autoscaling (beta). This allows custom status conditions or other metrics to be used when scaling a pod—for instance, if resources that are specific to a given deployment of Kubernetes need to be tracked as part of the application’s management strategy.Vertical pod scaling (beta), which allows a pod’s resource limits to be varied across its lifetime, as a way to better manage pods that have a high cost associated with disposing of them. This is a long-standing item on many wish lists for Kubernetes, because it allows for strategies to deal with pods whose behaviors aren’t easy to manage under the current scheduling strategy.What’s new in Kubernetes 1.11Released in early July 2018, Kubernetes 1.11 adds IPVS, or IP Virtual Server, to provides high-performance cluster load balancing using an in-kernel technology that’s less complex than the  system normally used for such things. Eventually, Kubernetes will use IPVS as the default load balancer, but for now it’s opt-in.Custom resource definitions, billed as a way to make custom configuration changes to Kubernetes without breaking its standardizations, may now be versioned to allow for graceful transitions from one set of custom resources to another over time. Also new are ways to define “status” and “scale” subresources, which can integrate with monitoring and high-availability frameworks in a cluster.Other major changes include:CoreDNS, introduced in 1.10, is now available as a cluster DNS add-on, and is used by default in the  administration tool.Kubelet configuration changes can now be rolled out across a live cluster without first taking the cluster down.The Container Storage Interface (CSI) now supports raw block volumes, interoperates with the kubelet plugin registration system, and can pass secrets more readily to CSI plugins.There are many changes to storage, including online resizing of persistent volumes, the ability to specify a maximum volume count for a node, and better support for protecting storage objects from being removed when in use.What’s new in Kubernetes 1.10The March 2018 Kubernetes 1.10 production release includes the beta release of the Container Storage Interface (alpha as of Kubernetes 1.9) that promotes an easier way to add volume plug-ins to Kubernetes, something that previously required recompilng the Kubernetes binary. The Kubectl CLI, used to perform common maintenance and administrative tasks in Kubernetes, can now accept binary plug-ins that perform authentication against third-party services such as cloud providers and Active Directory.“Non-shared storage,” or the ability to mount local storage volumes as persistent Kubernetes volumes, is now also beta. The APIs for persistent volumes now have additional checks to make sure persistent volumes that are in use aren’t deleted. The native DNS provider in Kubernetes can now be swapped with CoreDNS, a CNCF-managed DNS project with a modular architecture, although the swap can only be accomplished when a Kubernetes cluster is first set up.The Kubernetes project is now also moving to an automated issue life-cycle management project, to ensure stale issues don’t stay open for too long.What’s new in Kubernetes 1.9Kubernetes 1.9 was released in December 2017.Promoted to beta in Kubernetes 1.8 and now in production release in Kubernetes 1.9, the Apps Workloads API provides ways to define workloads based on their behaviors, such as long-running apps that need persistent state.Version 1 of the Apps Workloads API brings four APIs to general availability:", "pub_date": "2020-12-09"},
{"title": "A foolproof process to move and improve cloud data", "overview": "Moral of the story: Data migration is never simple, cloud or not. ", "image_url": "https://images.idgesg.net/images/article/2019/01/checklist-hand-100784637-large.jpg", "url": "https://www.infoworld.com/article/3600792/a-foolproof-process-to-move-and-improve-cloud-data.html", "body": "I’m often asked, “How can I relocate data to the cloud, and improve the databases, applications, security, governance, and dataops as the migration occurs?” Everyone is looking for a shortcut or a magical tool that will migrate and automatically improve the state of the data. Sorry, that magic does not yet exist. In the meantime, a nonmagical migration process provides the best odds of success. Before we explore that process, I’ll mention a few things:  Also on InfoWorld: Microsoft Azure cloud migration: 3 success storiesFirst, cloud relocation does not use a waterfall approach. Certain tasks need to be completed to move on to the next tasks, but not all. These dependences will be readily apparent, but feel free to do any of the tasks below out of sequence. Second, to get this right the first time, follow the process outlined below with the correct mix of talent. You’ll need subject matter experts for databases, security, ops, governance, cloud-specific services, etc. Those people are difficult to find right now. Finally, this is a general approach. You will need to add or remove some items. For instance, if you’re a health care company, you need to deal with more compliance and governance issues around the use, migration, and deployment of data. With all that said, here’s the process:Assess the “as is” state of the data, including models (object, relational, in memory, special purpose, or other), metadata, application coupling, and requirements (security, governance, business continuity/disaster recovery, and management). Tagging starts here. Look for opportunities to reduce redundancy and increase efficiency. This can be as impactful as moving from one model to another (relational to object) which requires a great deal of application refactoring, normalization of all data schemas, defining a single source of truth, etc. You need to consider security, governance, and data ops as well, which are redundant to everything listed here, just to be clear.Define the “to be” state with the changes and requirements defined above. One of the paths I recommend is the development of a CDM (common metadata model). A CDM, at its essence, provides a single source of truth for most and sometimes all of the data that exists in an enterprise. It’s made up of many different databases that may use different database models, such as relational and object, and many different structures or schemas. However, it appears to all who use the CDM as a single, unified, abstract database that, when asked a question, provides a common and consistent answer. Define a migration and implementation plan, focusing on the target cloud platforms. The devil is in the details, and some changes still need to be made in flight, but minor ones.Create a staging and testing platform for applications and databases. This may also include CI/CD (continuous integration/continuous delivery) links. Moving forward, they should be maintained by devsecops teams, as well as DBAs. Make a plan for that maintenance.Test deployment on the staging and testing platforms to determine performance, security, governance, fit-to-purpose, etc. Repeat for each application and database. Testing will help determine cost of ops and provide the data to project those costs for the next several years. Now that real cost metrics exist, this is easy. Projections also help avoid sticker shock when you get your first cloud bill. Implement cost governance. Define ops planning, including monitoring and management approaches, playbooks, and tools. Take advantage of abstraction and automation to remove humans from the ops processes as much as possible. Begin phased deployments, starting with the smallest and least important databases and applications, progressing to the biggest and most important. Try to deploy with flexible deadlines. Don’t worry, you’ll get better at it as you learn. Rushing this portion of the process is where failure typically occurs because important tasks are tossed out in favor of meeting an arbitrary deadline.Execute acceptance testing after each phase.Begin dataops.Take a vacation.On average, this process takes three weeks for each database. If you have 100 databases to migrate, realistically speaking, it will take about 42 to 52 weeks to complete. The move-and-improve processes are not magical or automatic, but they can be baked into the migration. Good luck.", "pub_date": "2020-12-11"},
{"title": "Are you ready for multicloud? A checklist", "overview": "For many organizations, the use of multiple public clouds is inevitable. Here’s how to make your multicloud strategic instead of accidental. ", "image_url": "https://images.idgesg.net/images/article/2020/12/ifw_ts_virtual_checkmark_in_digital_system_standards_quality_control_certification_certificates_by_vertigo3d_gettyimages_2400x1600-100870140-large.jpg", "url": "https://www.infoworld.com/article/3584433/are-you-ready-for-multicloud-a-checklist.html", "body": "Perhaps you deployed your first cloud-native Node.js application to Amazon Web Services and then received a new assignment to port over several legacy .NET applications to a public cloud. Should you try Amazon Lightsail as a first step, or should you review Microsoft Azure’s options for .NET developers?MulticloudAre you ready for multicloud? A checklist (InfoWorld)5 challenges every multicloud strategy must address (CIO)How to manage multiple cloud collaboration tools in a WFH world (Computerworld)Building stronger multicloud security: 3 key elements (CSO)Multicloud management: Challenges for technology, people, processes (Network World)Or maybe your team has applications running on Azure that need to securely connect to machine learning models deployed by the data science team on Google Cloud Platform. It’s easy to conceive scenarios where development and data science teams end up exploring, prototyping, and deploying applications, databases, microservices, and machine learning models to multiple public clouds.Also on InfoWorld: When hybrid multicloud has technical advantagesFor larger enterprises, supporting multiple clouds is almost inevitable, due to the herculean level of governance required to channel all development, data science, and shadow IT efforts to a single public cloud. Even so, there are several reasons why global businesses and larger enterprises determine that “being multicloud” is strategically important.I reached out to several experienced IT leaders active on Twitter through social chats like IDGTechTalk and CIOChat to get their perspectives on whether and how organizations should support multiple clouds. I’ve included their opinions and insights in this checklist on multicloud readiness. Are you ready?Ease into multicloud complexities IT leaders know the complexities of setting up secure and robust cloud infrastructures. Naturally, these complexities multiply when you combine multiple clouds. You should strive to avoid dealing with them all at once. Operating across multiple clouds is complex because of the required governance, technical expertise, and integrations. As Sarbjeet Johal, an independent technology strategist, puts it, “Nobody gets up in the morning and says we are going to do multicloud today. They just fall into it, mainly due to organizational silos. Multicloud is as easy as 1-2-3... said no one ever!”Joanne Friedman, Ph.D. and CEO of Connektedminds, suggests that IT teams leverage their primary cloud provider wherever possible, rather than hunt for new or better capabilities in a second provider. “Accept that there is no one size fits all in public clouds, and only 40% to 60% of what is required can be found with one provider,” she advises. Other IT leaders share pragmatic viewpoints on how multiclouds evolve and how to navigate initial complexities. Travis Campbell, a big data consultant, offers this insight into where the multicloud journey begins:Companies doing ‘multicloud’ but really treating it as a single cloud by each line of business are a special case here. For example, finance may have applications on cloud X, while engineering is deploying to cloud Y, and there’s no cross-pollination of work and data. It’s multicloud without hard problems.So, in the case of organizations already operating multiple clouds independently for different purposes, an inevitable next step would be application integrations, data integrations, or service orchestrations across those clouds. This is where those hard problems begin. It’s best to proceed slowly and cautiously. Make the case for a multicloud architectureOther IT leaders chimed in with similar sentiments. Specifically, many noted that supporting multicloud isn’t easy today, and that IT leaders should identify strong business rationale before endorsing a multicloud architecture. I asked them to identify some of the reasons why they might seek multicloud architectures.Mark Thiele, CEO and Founder of Edgevana, outlines a number of strategic benefits to seek from multicloud architectures. “I would target multicloud when it provides my customers with one or more of significant price value, speed to market, a unique technical capability that drives better value, innovation, and performance improvements,” he says. Mike D. Kail, Executive Technologist at Palo Alto Strategy Group, agrees. “The main reason [to consider multicloud] is when a cloud service provider offers a service that is far and above better than the one used in your initial deployments,” he says. “An example would be TensorFlow for artificial intelligence and machine learning.”Ed Featherston, Distinguished Technologist at HPE, seconds that view, and provides some operational guidance:Many organizations I work with select one platform as their main focus, and other platforms based on business benefit for specific needs and solutions. IT leaders must define how they approach multicloud from a strategy, support, and process perspective. When other platforms are brought in, there is a framework and structure to provide consistency in their approach. Don’t let it be because of “cloud sprawl,” as that always results in disaster.Chris Ibbitson, Chief Technologist of Financial Services at HPE, says that enterprises seek agility in public and private clouds. “Whether it is with public cloud providers, or in a hybrid cloud model, utilizing a mix of private cloud capabilities and multiple public cloud providers is focused on the agility and speed of delivering change,” he says. “While most organizations have already adopted some form of hybrid cloud, the focus is now turning to multicloud.”So IT might bring in a second cloud service provider to support a specific business or technical need. In these cases, IT leaders should define which businesses and use cases each cloud will serve and which services and technologies each cloud will provide. Also on InfoWorld: 6 ways Alibaba Cloud challenges AWS, Azure, and GCPJohal and others shared some additional reasons:Large enterprises seek to avoid being reliant on a single cloud service provider, especially if they expect to negotiate enterprise-specific service levels and pricing.Data sovereignty and compliance often require storing data in the country of residence, specific requirements on encryption and data security, or even specificity on acceptable cloud service providers.Organizations seeking mergers and acquisitions often target multiple cloud service providers and support models to enable easy onboarding and to simplify support structures.IT requires specific technical capabilities, especially at scale, where there might be strategic business advantages in deploying to one cloud service provider versus another.When IT elects to self-manage a commercially bought application that runs on a public cloud that’s different from the public cloud IT has adopted as their development standard. Industry-specific and other niche applications may provide the structure and support models on a single cloud service provider.There are also places where hybrid and multicloud architectures offer technical advantages, especially for edge computing, human safety applications, and real-time analytics.Review cloud support models before going multicloudWhile businesses may have some rationale to support a multicloud architecture, IT leaders still strongly suggest performing financial analyses and reviewing your cloud operating models.Kail suggests answering these questions, “Are the benefits going to be greater than the complexities of deploying and operating a multicloud architecture? Are there financial implications, both short and long term?”Steven Kaplan, Vice President of Customer Success Finance at Nutanix, agrees. He suggests, “As with any significant IT decision, it’s imperative to do a comprehensive financial analysis not only to select the most strategically optimal solution but to provide both the justification and financial baseline for moving forward.”When there is justification, IT must consider extending their service models and expertise from one cloud to multicloud. Featherston notes that cloud is more about people, process, and culture than it is about the technology. “Organizations should try to get through those dramatic transformations in their organization for one platform first to lay the groundwork and provide the framework to move forward on other platforms,” he says. “Basically, get one cloud right first, then branch out.”Thiele offers several specific practices that should be in place before expanding multicloud. “The ability to coordinate common processes around security, deployment, visibility, and resource management, among other things, is vital. Having well-defined devops and devsecops processes and tools enable both improved efficiency and more consistent security policy.”Friedman’s recommendation is to start with infrastructure as code tools. “They support a form of infrastructure automation and configuration,” she notes. “While they don’t address the entire scope of cloud governance, having programmable and version-controlled infrastructure is important.”Embrace devops to support multicloud architecturesI agree with Thiele and Friedman that robust devsecops practices—especially around CI/CD for deploying applications, infrastructure as code for provisioning and configuring infrastructure, and monitoring capabilities including AIops—are critical to both implementing and supporting multicloud architectures.But what I learned from this group of IT leaders is that not just any devops technology or devops practices will do, as some are better suited for multicloud strategies than others. One approach is to steer away from the proprietary tools of cloud service providers, such as AWS CloudFormation, Azure Resource Manager, or Google Cloud Deployment Manager, even as some are providing multicloud support. Enterprise IT groups should also expand their devops culture from a deployment-frequency focus to include deployment agility.Kail has a specific recommendation. “Infrastructure-as-code should be table stakes to mitigate configuration drift, and of course security needs to be architected from day one,” he notes. “Tools such as Terraform certainly help. Security solutions that can span multicloud are also paramount.”Multiple IT leaders recommended Terraform for multicloud architectures because it’s a declarative, agentless, masterless provisioning tool. One recommended architecture pairs Terraform with Packer, Docker, and Kubernetes to support provisioning, server templating, and orchestration.However, selecting a multicloud-enabling tool doesn’t mean the implementation supports multiple clouds. Featherston recommends Terraform, but with a disclaimer. “The one caveat I have is making sure clients understand, it’s a common language to use on multiple platforms, but the code to build AWS does not build Azure,” he explains. “The benefit is that the team has a common language to code in, but the tradeoff is that not all platform features may be available.”Beyond infrastructure as code, Friedman recommends the following tools, practices, and governance to meet multicloud requirements:Automation and orchestration for both applications and individual virtual machinesSecurity including identity management and data protection/encryptionPolicy governance and compliance including audits and SLA metricsPerformance monitoring of both the infrastructure (i.e. compute instances, storage, networks) and applicationsCost management through resource optimization and billing estimatesTechnology companies are also selling multicloud enabling technologies. For example, HPE GreenLake Central provides visibility into cost and compliance across different cloud providers, and Google Cloud Anthos enables a consistent development and operations experience for hybrid and multicloud environments.Also on InfoWorld: Which multicloud architecture will win out?Prepare for a multicloud futureDespite all of the complexities today in adopting multiple public clouds, the consensus among IT leaders is that most enterprises will support multicloud architectures, and even integrate applications across clouds. We can use history as a lesson, as enterprises had to support Linux and Windows, .NET and Java, and Oracle and Microsoft SQL databases, to name just a few examples—despite all of the rationale behind sticking to one platform.Some final thoughts from these leaders:“I expect we’ll see an evolution of multicloud over the next three to five years, whereby more workloads are designed and abstracted away from the underlying cloud provider by default. The focus will be on the advanced capabilities different cloud providers can enable, such as artificial intelligence, machine learning, and analytics.” – Chris Ibbitson”Solve for a single cloud first, then determine if multicloud is right for you.” – Mike D. Kail“Multicloud architectures are hard and expensive to maintain. Be strategic about single cloud providers and tactical about multicloud undertakings. Avoid doing multicloud at the infrastructure layer for the same workloads at almost all costs.” – Sarbjeet Johal“Opt for the paths of least resistance, focus on security first, then automation and orchestration inculcated through policy.” – Joanne FriedmanTravis Campbell sums it up well:I think the people being successful at this are doing lowest common denominator stuff and using the barest minimum on each provider with their layer baked on top to handle the abstraction. Build once, run anywhere is the golden ticket we’re all seeking. We’re still a ways out.Thanks to these contributors for sharing their experience and insight. You can find all of them on Twitter: @efeatherston, @ibbitsc, @joannefriedman, @mdkail, @mthiele10,  @ROIdude, and @sarbjeetjohal. ", "pub_date": "2020-12-14"},
{"title": "The multicloud challenge: Building the future everywhere", "overview": "At a time when procuring on-prem infrastructure and personnel seems more daunting than ever, organizations are increasingly turning to multiple clouds to provide just the array of functionality they need. ", "image_url": "https://images.idgesg.net/images/article/2020/12/intro_ts_binoculars_by_marco_piunti_gettyimages-610855316_2400x1600-100870141-large.jpg", "url": "https://www.infoworld.com/article/3600216/the-multicloud-challenge-building-the-future-everywhere.html", "body": "Fear of the cloud has evaporated. Instead, most companies now use at least several public clouds, from AWS to Azure to Salesforce to Slack. Hence the ascendance of the term “multicloud,” which now encompasses not just the management of IaaS and SaaS clouds, but also private clouds of virtualized on-prem resources.MulticloudAre you ready for multicloud? A checklist (InfoWorld)5 challenges every multicloud strategy must address (CIO)How to manage multiple cloud collaboration tools in a WFH world (Computerworld)Building stronger multicloud security: 3 key elements (CSO)Multicloud management: Challenges for technology, people, processes (Network World)The low barrier to entry of the cloud has been both a blessing and a curse. The ability to simply open a cloud account and start using an application or building one has delivered unprecedented agility. But it also makes it easy for stakeholders to go off in their own directions, sometimes with too little regard for cost or security risks.Multicloud’s problem is as old as IT: the problem of governance. For some that’s an ugly word, because it smacks of a bureaucracy that stands squarely in the way of getting things done, as in: Fill out your request in triplicate and you’ll get a couple of cloud VMs in six weeks if you’re lucky. But few would advocate anarchy, either – you don’t want developers running around building cloud applications on a whim using, say, pricey AI/ML services and live customer data.In a recent CIO Think Tank, Thomas Sweet, vice president of IT solutions at GM Financial, introduced a well-chosen phrase: “minimum viable governance.” Instead of pummeling people with prohibitions or elaborate approval processes, give them lightweight cloud “guardrails” to prevent duplicate efforts or poor cloud security. Couple those with cost ceilings and a catalog of pre-approved cloud services, and developers or enterprising LoB managers have the freedom they need to experiment and innovate.In particular, the big three IaaS clouds – AWS, Google Cloud Platform, and Microsoft Azure – provide environments where innovation can flourish, in part because they’re cauldrons of emerging technology, from serverless computing to AR/VR app dev platforms. For many organizations, “multicloud” really refers to adopting two or more big-three IaaS clouds, mainly because the second or third cloud offers a new or better cloud service others lack. Wrapping guardrails around that multiplicity is an endless governance challenge.But that’s where the future is pointing: Toward a world where we assemble hundreds of cloud services from multiple providers into the applications we and our customers need, iterating and innovating as well go. This collection of articles from InfoWorld, CIO, Computerworld, CSO, and Network World explains how forward-looking organizations are moving toward that goal and the lessons they’re learning along the way.", "pub_date": "2020-12-14"},
{"title": "Elevating SaaS to its rightful place in the enterprise", "overview": "Most enterprises don’t consider SaaS as much as they should in their cloud journey. That’s a huge mistake.  ", "image_url": "https://images.techhive.com/images/article/2017/03/saas-100712114-large.jpg", "url": "https://www.infoworld.com/article/3600903/elevating-saas-to-its-rightful-place-in-the-enterprise.html", "body": "According to Gartner, SaaS (software as a service) remains the largest sector of the cloud computing market and is expected to grow to $117.7 billion in 2021 (a 16 percent increase). This is pretty impressive growth, which largely has been driven by the pandemic and the need for SaaS systems to support remote work. SaaS has long been the red-headed stepchild of the cloud computing world. Most don’t consider SaaS as part of the cloud, focusing instead on IaaS providers, including AWS, Google, and Microsoft. This is largely because the SaaS world is widely distributed, with more than 5,000 SaaS applications out there, from bail bonds management to full-blown ERP systems on demand. Also on InfoWorld: PaaS, CaaS, or FaaS? How to chooseSalesforce.com is certainly well known, and table stakes for most enterprises, but by doing the math you’ll quickly determine that most of the SaaS market is smaller, more tactical business systems on demand. These systems are more cost efficient than having them sit on servers in your data center.    There are some areas where enterprises can improve their use of SaaS within their larger cloud computing strategy. A few suggestions: Plan out and build mechanisms for moving data in and between SaaS systems and other enterprises systems, either on-premises or within IaaS clouds. Extend your larger security strategy, such as the use of IAM (identity and access management), to SaaS systems. I’m seeing a great deal of vulnerabilities in SaaS systems as security becomes an afterthought, taking a back seat to time to market. We use SaaS systems because they bring prebuilt business processes that we don’t have to create in net-new applications. However, these are not optimized unless integrated with other application processes housed on-premises or in cloud-based systems. Process orchestration layers are handy here.SaaS was really the first part of the cloud computing market that emerged in the late 90s and proved that cloud was a viable and cost-effective alternative to application ownership. That said, it’s not getting the respect it deserves, and that will get you into trouble quickly.  ", "pub_date": "2020-12-15"},
{"title": "Go local with Azure Logic Apps development", "overview": "Microsoft’s low-code business process automation tools add container runtimes and a local development environment.", "image_url": "https://images.idgesg.net/images/article/2020/09/interconnecting_gears_process_automation_machinery_mechanism_efficiency_by_4x-image_gettyimages-1179710658_2400x1600-100858597-large.jpg", "url": "https://www.infoworld.com/article/3601348/go-local-with-azure-logic-apps-development.html", "body": "Low- and no-code development have become key for digital transformation, giving information workers the tools to build the apps they need. There’s a significant app gap, one that under-resourced development teams are increasingly unable to fill. So giving tools to end-users to write code makes a lot of sense, as it lets them scratch their itches, building process automations that allow them to concentrate on the more complex parts of their tasks.Microsoft has been rolling out enhancements to its Power Platform process automation suite, with a focus on working with its line-of-business Dynamics 365 suite. But the Power Platform isn’t the only low-code environment in Microsoft’s developer toolbox; Azure also offers a process automation workflow tool in the shape of its serverless Logic Apps.Also on InfoWorld: 7 low-code platforms developers should knowBusiness process automation in AzureAlthough Logic Apps is often thought of as a no-code tool, it’s better understood as an alternative to the venerable BizTalk business automation tool. When you build a Logic App, you’re providing basic rules for linking more than one application with a network of components, providing the rules and structure needed for message routing.With Logic Apps, events from one application can be processed and routed to another using connectors to translate them into the appropriate format for an API. The systems being connected don’t need to be event driven; the connectors handle translation from event to write to RPC call to, well, however the application interacts with the world—even down to remotely driving a green-screen user interface.It’s very much a developer-focused tool, helping fill the other app gap: the space between applications that weren’t intended to be used together, but when connected fulfil vital business functions. Using a serverless process automation tool helps solve a lot of problems; you don’t need to provision hardware, and you can use it to build hybrid applications that bridge SaaS platforms and on-premises code.At first sight Logic Apps looks very like Power Apps, with a similar graphical editor for process flows. A recent update added a new design tool that helps visualize more complex workflows, as the application being built using Logic Apps uses more and more features, building out workflow rules that could be hard to display. Microsoft will continue to build on the new Logic Apps layout engine, adding features that come with the new Logic Apps runtime.Writing Logic Apps in codeYou don’t need to use a graphical tool to build a Logic App; Microsoft provides a JSON-based declarative programming language for building flows between connectors, with a set of Visual Studio and Visual Studio Code tools. The underlying technology is Microsoft’s Workflow Definition Language (WDL), which gives you the tools to build a workflow that links two more connectors using declarative statements.The WDL is defined with a schema, which needs to be linked at the start of the JSON workflow file. This defines the version being used so the appropriate workflow engine is called. Your file will then define actions, triggers, and outputs. Triggers are the inputs to a workflow, with as many as 10 supported in a WDL application. That’s an advantage over the graphical designer, which lets you have only one trigger per workflow.Switching to code can help build much more complex rule sets, allowing you to mix multiple inputs or have flows that are driven by conditional operations; for example, only processing an event on input 1 if inputs 2 and 3 both have specific values. Similarly, you can drive as many as 10 different outputs, through as many as 250 actions. Other options include parameters that are used within a workflow; these need to be defined before runtime.Although there are issues with the visual designer in Visual Studio Code, it can help you create initial connections and visualize the flow of your WDL applications. Think of it as a visual linter, a way to quickly show problems and indicate dead ends and loops in your code. Building rule sets this way is relatively easy, but complex flows are best visualized before you try them out. You can use static results to build mocks into a flow, either for testing or to manage specific input or error cases.Working with WDL in Visual Studio CodeWDL is a declarative language so it has the option of including expressions that are evaluated at run time. This approach lets you use values from parameter arrays; for example, if you have an array that contains lists of products and prices as an array of key/value pairs, you can use price in a flow by referencing the product in an expression.As WDL is the output of the visual design tool in either Visual Studio or in the Azure Portal, you can use it to create an initial scaffold for your application and then edit it in Visual Studio Code to go beyond the limits of the visual tool. As part of the same project you can now create your own custom connectors or extend existing ones, linking Logic Apps to your own applications or to partner APIs.The Visual Studio Code Logic Apps extension installs from the Marketplace as part of the Azure extension group, and you can access it directly from the Azure menu in the Code sidebar. Once signed into Azure, you can attach a new Logic App to an Azure account, either as an individual app or as part of a larger project. You will need to either create a new resource group for your Logic App or add it to an existing one.Once you’ve chosen a region and named your app, you’re presented with a blank logic app and the JSON skeleton for its code. Usefully you’re not limited to running Logic Apps in Azure, as the runtime is containerized and can be hosted in your own Kubernetes environments or used as a local development environment on Windows, macOS, and Linux. Creating a local development environment requires opening a new local project from the Visual Studio Code Logic Apps extension, connecting it to an Azure resource group, and creating a Logic App.A welcome expansionMicrosoft’s expansion of its Logic Apps platform is welcome, as workflow and process automation are important integration tools that allow you to link applications with minimal code. Adding it to both Visual Studio and Visual Studio Code makes it easier to include in projects, mixing serverless Azure Functions, local container runtimes, and your own custom connectors to build complex rules-based workflows with minimal code.As Logic Apps gains more functionality with new runtimes and new development tools, it’s going to become more of an alternative to BizTalk, especially for organizations that are taking a hybrid approach to cloud development and using containers to move workloads between cloud and on-premises.With Kubernetes seeing an increased role at the edge of the network, it will be possible to build and deploy process automation as close as possible to information sources, using containers for both Logic Apps and Functions, as well as for your own connectors. Logic Apps’ ease of development will be a bonus and, like Power Automate, should go some way to reducing the app gap, if not filling it.", "pub_date": "2020-12-15"},
{"title": "Predictions for cloud computing in 2021", "overview": "Public cloud vendors will cement their dominance, but enterprises will look to hybrid clouds, PaaS, open-partner ecosystems, and augmented reality. ", "image_url": "https://images.techhive.com/images/article/2016/12/01_intro_prediction-100698470-large.jpg", "url": "https://www.infoworld.com/article/3601731/predictions-for-cloud-computing-in-2021.html", "body": "Cloud computing is the centerpiece of the world’s technical response to the COVID-19 crisis. Indeed, the leading public cloud providers were standout business successes in this most unusual of years. As businesses everywhere managed to keep the lights on by having personnel work from their homes, all of the principal cloud providers substantially grew their revenues and continued to deliver innovations at a blistering pace.With the novel coronavirus pandemic still hammering the global economy, here are my predictions for the enterprise cloud computing market in 2021.Also on InfoWorld: The state of cloud computing in 2020Cloud computing will be central to the postpandemic new normalIn the sad and fraught year now coming to a close, cloud services were a godsend for keeping the economy and our lives from grinding to a halt.In 2021 and beyond, everybody will continue to rely thoroughly on clouds (as well as on streaming, remote collaboration, smart sensors, and other cloud-reliant digital technologies) to emerge from a pandemic that is still grinding down on us remorselessly. Enterprise technology professionals will adjust their cloud strategies with one eye on COVID-19 trends and the other on their digital transformation initiatives. The tech vendors who stand to gain the most are those such as Amazon, Google, and Microsoft that provide full, cloud-to-edge ecosystems that enable seamless new normal lifestyles.Public clouds will grow even more dominantIn the past year, public clouds rode the pandemic to faster growth. According to IDC, enterprise cloud spending, both public and private, increased 34.4 percent from a year previous, while non-cloud IT spending declined by eight percent.In 2021 leading public cloud platforms—especially Amazon Web Services, Microsoft Azure, and Google Cloud Platform—will cement their dominance in the cloud market and expand their sway across many sectors of the global economy. AWS will retain its leading market share, though Microsoft, Google, and Alibaba will continue to close the gap. Revenue growth will remain explosive through mid-decade, according to Deloitte’s projections, never dipping below 30 percent annually. Global cloud spending will grow seven times faster than overall IT spending through this period. IDC forecasts that worldwide spending on public cloud services and infrastructure will nearly double, to around $500 billion, by 2023.Enterprises will mitigate cloud lock-in through hybrid and multicloud strategiesIn the past year, public clouds’ deepening dominance compelled traditional enterprise computing companies to set their strategic focus on hybrid and multiclouds. In 2021, enterprises will grow more uneasy with their reliance on the top tier providers. IT professionals will seek out hybrid and multicloud tools to reduce their risks of being locked in to specific providers. This is already a mainstream tactic considering that, per Flexera estimates, 93 percent of enterprises have a multicloud strategy and 87 percent have a hybrid cloud strategy.Going forward the growing maturity of hybrid/multicloud offerings from AWS, Microsoft, and Google will tempt enterprise cloud managers into increasing their spending with these providers. At the same time, private-cloud stalwarts IBM, Hewlett Packard Enterprise, Cisco, Dell EMC, VMware, and others will continue to beef up their hybrid/multicloud integrations with the dominant public cloud services in order to defend their enterprise IT market shares.Nevertheless this space is now a war of attrition. Hybrid-cloud appliances such as AWS Outposts and Google Anthos won’t significantly boost those vendors’ shares in their core public cloud market segments.Platform as a service will grow in public cloud revenue shareIn this year’s rapid shift to work-from-home arrangements, SaaS providers such as Oracle, SAP, and Salesforce provided an essential platform for continue business as usual in spite of the disruptions.As remote work remains a mainstream approach, SaaS providers of all sorts will be poised for runaway growth. In 2021 Gartner projects SaaS will remain the largest cloud market segment by revenues, growing to $117.7 billion by year end. However, PaaS-based application services will grow even faster, driven by enterprise customers’ increasing emphasis on cloud-native, containerized, and serverless cloud platforms.One of the biggest PaaS growth segments in 2021 will be multicloud serverless offerings from Vendia, Microsoft, Red Hat, and others. These solutions and associated low-code platforms will be essential ingredients in more enterprises’ application modernization, digital transformation, and business continuity strategies.Intelligent edge will become the principal cloud on-rampIn the year gone by, the rapid shift of most economic sectors to remote work triggered a boom in mobile devices, AI (artificial intelligence)-powered automation, autonomous robotics, and industrial IoT (Internet of Things) platforms.In 2021 public cloud providers will shift an increasing share of their workloads to intelligent-edge platforms to deliver the low latencies required by these applications. By this time next year, approximately 90 percent of industrial enterprises will use edge computing, according to Frost & Sullivan projections. As 5G is rolled out worldwide during the coming years, demand for cloud-to-edge applications will skyrocket. More of these workloads will involve edge-based, AI-driven processing of smart sensor data and Tiny ML (machine learning) workloads.In the coming year, IBM/Red Hat, Hewlett Packard Enterprise, and Microsoft will invest deeply in the strategic 5G/edge/AI platforms that they launched in 2020. Nvidia will leverage its Arm acquisition to deepen its portfolio of solutions to automate delivery of AI apps to cloud-to-edge and hybrid-cloud environments. This trend will also drive demand for software-defined wide area networks to the benefit of VMware, Cisco, Juniper, Arista, and other vendors in this segment.Under regulatory siege, Big Tech will emphasize open partner ecosystemsIn the year now ending, Amazon, Microsoft, Google, and other Big Tech companies found themselves fending off lawsuits, regulatory actions, and legislative pressure to be broken up. In 2021, these and other dominant cloud providers will slow down strategic acquisitions, which they’ve been accused of using to stop rivals in their tracks, while ramping up and boasting of their open partner ecosystems. Increasingly the big cloud providers will position themselves as back-end fulfillment agents within partner-led enterprise deals. They will downplay efforts to use their multicloud and hybrid cloud solutions as a lever to migrate enterprise workloads from legacy, on-premises platforms.Augmented reality will help train the employees needed to support cloud growth.Here is one last cloud-computing prediction, pertaining more to cloud professionals’ working lives than to the market for cloud services. As enterprises retool their cloud management processes, they will adopt a growing range of augmented reality tools to deliver training and guidance to cloud technical staff working from home.Like all businesses, cloud companies and their customers will need to ensure that their personnel are safe, healthy, and productive in distanced work environments. Innovative approaches such as augmented reality are the only way that Amazon can hope to deliver on its recently announced plan to train 29 million people worldwide to work in cloud computing. Even in normal times, Amazon would find it impractical to train this many people in person.Beyond keeping people safe, the cloud computing industry’s biggest challenge in 2021 will be to find, train, and equip enough skilled people to support customers’ digital transformation initiatives.", "pub_date": "2020-12-17"},
{"title": "The cloud’s role in elections grows more significant", "overview": "Most campaigns view polling data, logistics, and demographics as fundamental to winning an election. More likely, data science and cloud computing will win the day. ", "image_url": "https://images.idgesg.net/images/article/2019/09/cso_voting_elections_i_voted_sticker_by_parker_johnson_cc0_via_unsplash_abstract_data_encryption_secured_by_markus_spiske_cc0_via_unsplash_2400x1600-100812360-large.jpg", "url": "https://www.infoworld.com/article/3587342/the-clouds-role-in-elections-grows-more-significant.html", "body": "A year ago I wrote about the importance of leveraging cloud computing and data science to win elections, pointing to the 2020 campaigns, which come to a conclusion today. The basic assertion was that technology, focusing on the true meaning of data, would count more than traditional robocalls and door-to-door campaigning, since you’re able to understand more about the electorate, and thus target more effectively. The campaigns won’t let you talk to their data nerds, but they all have them. Data science approaches and the use of cloud computing allow the campaigns to come to some hidden conclusions that can boost their candidate's chances.Also on InfoWorld: How dataops improves data, analytics, and machine learningSophisticated calculations and analytics are able to derive patterns in the data that those viewing the data in traditional ways won’t see. For example, data science can find undecided voters who are likely to vote for a particular candidate and come up with a motivation for them to vote. It can tap into opposition to a local bill that really has nothing to do with the federal election, and use that issue in the messaging to get another 10 percent of undecided voters, with more than 80 percent of them voting for your candidate. Compare this precision versus a “spray-and-prey” approach where you blast out messaging and hope it hits a few undecided voters.     A human brain can’t find these types of patterns in the data. You need advanced analytics that use machine learning to find things you did not know existed. The ability to weaponize massive amounts of seemingly innocuous data is something that was nice to have back in 2012, but in 2020 it’s mandatory to put more odds in your favor. Modern campaigning is really about data wars and strategic targeting of voters rather than promoting ideas.In this world, ideas are dynamic; how the campaign messaging is viewed depends on who the campaign wants to view it. You may see a different message than your neighbor does. Deep analytics and AI have discerned that you and your neighbor are motivated by different issues, so you are each given different messages to drive your behaviors.  The result: The campaign with the best data science approaches and the ability to leverage cloud computing as a force multiplier will likely win. I’m not sure this what the Founding Fathers saw coming, but here it is.            ", "pub_date": "2020-11-03"},
{"title": "Review: Google Cloud AI lights up machine learning", "overview": "Google Cloud AI and Machine Learning Platform is missing some pieces, and much is still in beta, but its scope and quality are second to none. ", "image_url": "https://images.idgesg.net/images/article/2020/09/light_bulb_with_gears_and_network_of_sparks_outlining_a_virtual_brain_ideas_innovation_intelligence_creativity_transformation_by_urupong_gettyimages-1066422156_2400x1600-100859291-large.jpg", "url": "https://www.infoworld.com/article/3586836/review-google-cloud-ai-lights-up-machine-learning.html", "body": "Google has one of the largest machine learning stacks in the industry, currently centering on its Google Cloud AI and Machine Learning Platform. Google spun out TensorFlow as open source years ago, but TensorFlow is still the most mature and widely cited deep learning framework. Similarly, Google spun out Kubernetes as open source years ago, but it is still the dominant container management system.Google is one of the top sources of tools and infrastructure for developers, data scientists, and machine learning experts, but historically Google AI hasn’t been all that attractive to business analysts who lack serious data science or programming backgrounds. That’s starting to change.Also on InfoWorld: How to choose a cloud machine learning platformThe Google Cloud AI and Machine Learning Platform includes AI building blocks, the AI platform and accelerators, and AI solutions. The AI solutions are fairly new and aimed at business managers rather than data scientists. They may include consulting from Google or its partners.The AI building blocks, which are pre-trained but customizable, can be used without intimate knowledge of programming or data science. Nevertheless, they are often used by skilled data scientists for pragmatic reasons, essentially to get stuff done without extensive model training.The AI platform and accelerators are generally for serious data scientists, and require coding skill, knowledge of data preparation techniques, and lots of training time. I recommend going there only after trying the relevant building blocks.There are still some missing links in Google Cloud’s AI offerings, especially in data preparation. The closest thing Google Cloud has to a data import and conditioning service is the third-party Cloud Dataprep by Trifacta; I tried it a year ago and was underwhelmed. The feature engineering built into Cloud AutoML Tables is promising, however, and it would be useful to have that sort of service available for other scenarios.The seamy underside of AI has to do with ethics and responsibility (or the lack thereof), along with persistent model biases (often because of biased data used for training). Google published its AI Principles in 2018. It’s a work in progress, but it’s a basis for guidance as discussed in a recent blog post on Responsible AI.There is lots of competition in the AI market (over a dozen vendors), and lots of competition in the public cloud market (over half-a-dozen credible vendors). To do the comparisons justice, I’d have to write an article at least five times as long as this one, so as much as I hate leaving them out, I’ll have to omit most product comparisons. For the top obvious comparison, I can summarize: AWS does most of what Google does, and is also very good, but generally charges higher prices.Google Cloud’s AI building blocks don’t require much machine learning expertise, instead building on pre-trained models and automatic training. The AI Platform allows you to train and deploy your own machine learning and deep learning models.Google Cloud AI Building BlocksGoogle Cloud AI Building Blocks are easy-to-use components that you can incorporate into your own applications to add sight, language, conversation, and structured data. Many of the AI building blocks are pre-trained neural networks, but can be customized with transfer learning and neural network search if they don’t serve your needs out of the box. AutoML Tables is a little different, in that it automates the process a data scientist would use to find the best machine learning model for a tabular data set.The Google Cloud AutoML services provide customized deep neural networks for language pair translation, text classification, object detection, image classification, and video object classification and tracking. They require tagged data for training, but don’t require significant knowledge of deep learning, transfer learning, or programming.Google Cloud AutoML customizes Google’s battle-tested, high-accuracy deep neural networks for your tagged data. Rather than starting from scratch when training models from your data, AutoML implements automatic deep transfer learning (meaning that it starts from an existing deep neural network trained on other data) and neural architecture search (meaning that it finds the right combination of extra network layers) for language pair translation and the other services listed above.In each area, Google already has one or more pre-trained services based on deep neural networks and huge sets of labeled data. These may well work for your data unmodified, and you should test that to save yourself time and money. If they don’t do what you need, Google Cloud AutoML helps you to create a model that does, without requiring that you know how to perform transfer learning or how to design neural networks.Transfer learning offers two big advantages over training a neural network from scratch. First, it requires a lot less data for training, since most of the layers of the network are already well trained. Second, it trains a lot faster, since it’s only optimizing the final layers.While the Google Cloud AutoML services used to be presented together as a package, they are now listed with their base pre-trained services. What most other companies call AutoML is performed by Google Cloud AutoML Tables.Read the full review of Google Cloud AutoMLI tested an AutoML Vision custom flower classifier, which I trained in an hour from Google sample images, with a photo of tulips that I took at a nearby art museum.The usual data science process for many regression and classification problems is to create a table of data for training, clean and condition the data, perform feature engineering, and try to train all of the appropriate models on the transformed table, including a step to optimize the best models’ hyperparameters. Google Cloud AutoML Tables can perform this entire process automatically once you manually identify the target field.AutoML Tables automatically searches through Google’s model zoo for structured data to find the best model for your needs, ranging from linear/logistic regression models for simpler data sets to advanced deep, ensemble, and architecture-search methods for larger, more complex ones. It automates feature engineering on a wide range of tabular data primitives — such as numbers, classes, strings, timestamps, and lists — and helps you detect and take care of missing values, outliers, and other common data issues.Its codeless interface guides you through the full end-to-end machine learning lifecycle, making it easy for anyone on your team to build models and reliably incorporate them into broader applications. AutoML Tables provides extensive input data and model behavior explainability features, along with guardrails to prevent common mistakes. AutoML Tables is also available in API and notebook environments.AutoML Tables competes with Driverless AI and several other AutoML implementations and frameworks.Google Cloud AutoML Tables automates the whole pipeline for creating predictive models for tabular data, from feature engineering through deployment.At the Analyze phase of AutoML Tables, you can see the descriptive statistics of all your original features.The free Google Cloud Vision “Try the API” interface allowed me to drag a JPEG onto a web page and see the results. The child was smiling, so the “Joy” tag is correct. The algorithm didn’t quite recognize the paper Pilgrim hat.The Google Cloud Vision API is a pre-trained machine learning service for categorizing images and extracting various features. It can classify images into thousands of pre-trained categories, ranging from generic objects and animals found in the image (such as a cat), to general conditions (for example, dusk), to specific landmarks (Eiffel Tower, Grand Canyon), and identify general properties of the image, such as its dominant colors. It can isolate areas that are faces, then apply geometric (facial orientation and landmarks) and emotional analyses to the faces, although it does not recognize faces as belonging to specific people, except for celebrities (which requires a special usage license). Vision API uses OCR to detect text within images in more than 50 languages and various file types. It can also identify product logos, and detect adult, violent, and medical content.Read the full review of Google Cloud Machine Learning APIsThe Google Cloud Video Intelligence API automatically recognizes more than 20,000 objects, places, and actions in stored and streaming video. It also distinguishes scene changes and extracts rich metadata at the video, shot, or frame level. It additionally performs text detection and extraction using OCR, detects explicit content, automates closed captioning and subtitles, recognizes logos, and detects faces, persons, and poses.Google recommends the Video Intelligence API for extracting metadata to index, organize, and search your video content. It can transcribe videos and generate closed captions, as well as flag and filter inappropriate content, all more cost-effectively than human transcribers. Use cases include content moderation, content recommendations, media archives, and contextual advertisements.The Google Cloud Natural Language API finds entities, sentiment, syntax, and categories. Here we see syntax diagrams for two sentences from a Google press release.Natural language processing (NLP) is a big part of the “secret sauce” that makes input to Google Search and the Google Assistant work well. The Google Cloud Natural Language API exposes that same technology to your programs. It can perform syntax analysis (see the image below), entity extraction, sentiment analysis, and content classification, in 10 languages. You may specify the language if you know it; otherwise, the API will attempt to auto-detect the language. A separate API, currently available for early access on request, specializes in healthcare-related content.Read the full review of Google Cloud Machine Learning APIsThe Google Cloud Translation API can translate over a hundred language pairs, can auto-detect the source language if you don’t specify it, and comes in three flavors: Basic, Advanced, and Media Translation. The Advanced Translation API supports a glossary, batch translation, and the use of custom models. The Basic Translation API is essentially what is used by the consumer Google Translate interface. AutoML Translation allows you to train custom models using transfer learning.The Media Translation API translates content directly from audio (speech), either audio files or streams, in 12 languages, and automatically generates punctuation. There are separate models for video and phone call audio.Read the full review of Google Cloud Machine Learning APIs", "pub_date": "2020-11-09"},
{"title": null, "overview": null, "image_url": null, "url": "https://www.infoworld.com/author/Simon-Bisson/", "body": "", "pub_date": null},
{"title": "What are deepfakes? AI that deceives", "overview": "Deepfakes extend the idea of video compositing with deep learning to make someone appear to say or do something they didn’t really say or do", "image_url": "https://images.idgesg.net/images/article/2019/10/cso_a_virtual_face_constructed_of_binary_code_artificial_intelligence_digital_identity_deepfakes_by_thinkstock_2400x1600-100812617-large.jpg", "url": "https://www.infoworld.com/article/3574949/what-are-deepfakes-ai-that-deceives.html", "body": "Deepfakes are media — often video but sometimes audio — that were created, altered, or synthesized with the aid of deep learning to attempt to deceive some viewers or listeners into believing a false event or false message.The original example of a deepfake (by reddit user /u/deepfake) swapped the face of an actress onto the body of a porn performer in a video – which was, of course, completely unethical, although not initially illegal. Other deepfakes have changed what famous people were saying, or the language they were speaking.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesDeepfakes extend the idea of video (or movie) compositing, which has been done for decades. Significant video skills, time, and equipment go into video compositing; video deepfakes require much less skill, time (assuming you have GPUs), and equipment, although they are often unconvincing to careful observers.How to create deepfakesOriginally, deepfakes relied on autoencoders, a type of unsupervised neural network, and many still do. Some people have refined that technique using GANs (generative adversarial networks). Other machine learning methods have also been used for deepfakes, sometimes in combination with non-machine learning methods, with varying results.Essentially, autoencoders for deepfake faces in images run a two-step process. Step one is to use a neural network to extract a face from a source image and encode that into a set of features and possibly a mask, typically using several 2D convolution layers, a couple of dense layers, and a softmax layer. Step two is to use another neural network to decode the features, upscale the generated face, rotate and scale the face as needed, and apply the upscaled face to another image.Training an autoencoder for deepfake face generation requires a lot of images of the source and target faces from multiple points of view and in varied lighting conditions. Without a GPU, training can take weeks. With GPUs, it goes a lot faster.Generative adversarial networks can refine the results of autoencoders, for example, by pitting two neural networks against each other. The generative network tries to create examples that have the same statistics as the original, while the discriminative network tries to detect deviations from the original data distribution.Training GANs is a time-consuming iterative technique that greatly increases the cost in compute time over autoencoders. Currently, GANs are more appropriate for generating realistic single image frames of imaginary people (e.g. StyleGAN) than for creating deepfake videos. That could change as deep learning hardware becomes faster.How to detect deepfakesEarly in 2020, a consortium from AWS, Facebook, Microsoft, the Partnership on AI’s Media Integrity Steering Committee, and academics built the Deepfake Detection Challenge (DFDC), which ran on Kaggle for four months.The contest included two well-documented prototype solutions: an introduction, and a starter kit. The winning solution, by Selim Seferbekov, also has a fairly good writeup.The details of the solutions will make your eyes cross if you’re not into deep neural networks and image processing. Essentially, the winning solution did frame-by-frame face detection and extracted SSIM (Structural Similarity) index masks. The software extracted the detected faces plus a 30 percent margin, and used EfficientNet B7 pretrained on ImageNet for encoding (classification). The solution is now open source.Sadly, even the winning solution could only catch about two-thirds of the deepfakes in the DFDC test database.Deepfake creation and detection applicationsOne of the best open source video deepfake creation applications is currently Faceswap, which builds on the original deepfake algorithm. It took Ars Technica writer Tim Lee two weeks, using Faceswap, to create a deepfake that swapped the face of Lieutenant Commander Data (Brent Spiner) from  into a video of Mark Zuckerberg testifying before Congress. As is typical for deepfakes, the result doesn’t pass the sniff test for anyone with significant graphics sophistication. So, the state of the art for deepfakes still isn’t very good, with rare exceptions that depend more on the skill of the “artist” than the technology.That’s somewhat comforting, given that the winning DFDC detection solution isn’t very good, either. Meanwhile, Microsoft has announced, but has not released as of this writing, Microsoft Video Authenticator. Microsoft says that Video Authenticator can analyze a still photo or video to provide a percentage chance, or confidence score, that the media is artificially manipulated.Video Authenticator was tested against the DFDC dataset; Microsoft hasn’t yet reported how much better it is than Seferbekov’s winning Kaggle solution. It would be typical for an AI contest sponsor to build on and improve on the winning solutions from the contest.Facebook is also promising a deepfake detector, but plans to keep the source code closed. One problem with open-sourcing deepfake detectors such as Seferbekov’s is that deepfake generation developers can use the detector as the discriminator in a GAN to guarantee that the fake will pass that detector, eventually fueling an AI arms race between deepfake generators and deepfake detectors.On the audio front, Descript Overdub and Adobe’s demonstrated but as-yet-unreleased VoCo can make text-to-speech close to realistic. You train Overdub for about 10 minutes to create a synthetic version of your own voice; once trained, you can edit your voiceovers as text.A related technology is Google WaveNet. WaveNet-synthesized voices are more realistic than standard text-to-speech voices, although not quite at the level of natural voices, according to Google’s own testing. You’ve heard WaveNet voices if you have used voice output from Google Assistant, Google Search, or Google Translate recently.Deepfakes and non-consensual pornographyAs I mentioned earlier, the original deepfake swapped the face of an actress onto the body of a porn performer in a video. Reddit has since banned the /r/deepfake sub-Reddit that hosted that and other pornographic deepfakes, since most of the content was non-consensual pornography, which is now illegal, at least in some jurisdictions.Another sub-Reddit for -pornographic deepfakes still exists at /r/SFWdeepfakes. While the denizens of that sub-Reddit claim they’re doing good work, you’ll have to judge for yourself whether, say, seeing Joe Biden’s face badly faked into Rod Serling’s body has any value — and whether any of the deepfakes there pass the sniff test for credibility. In my opinion, some come close to selling themselves as real; most can charitably be described as crude.Banning /r/deepfake does not, of course, eliminate non-consensual pornography, which may have multiple motivations, including revenge porn, which is itself a crime in the US. Other sites that have banned non-consensual deepfakes include Gfycat, Twitter, Discord, Google, and Pornhub, and finally (after much foot-dragging) Facebook and Instagram.In California, individuals targeted by sexually explicit deepfake content made without their consent have a cause of action against the content’s creator. Also in California, the distribution of malicious deepfake audio or visual media targeting a candidate running for public office within 60 days of their election is prohibited. China requires that deepfakes be clearly labeled as such.Deepfakes in politicsMany other jurisdictions  laws against political deepfakes. That can be troubling, especially when high-quality deepfakes of political figures make it into wide distribution. Would a deepfake of Nancy Pelosi be worse than the conventionally slowed-down video of Pelosi manipulated to make it sound like she was slurring her words? It could be, if produced well. For example, see this video from CNN, which concentrates on deepfakes relevant to the 2020 presidential campaign.Deepfakes as excuses“It’s a deepfake” is also a possible excuse for politicians whose real, embarrassing videos have leaked out. That recently happened (or allegedly happened) in Malaysia when a gay sex tape was dismissed as a deepfake by the Minister of Economic Affairs, even though the other man shown in the tape swore it was real.On the flip side, the distribution of a probable amateur deepfake of the ailing President Ali Bongo of Gabon was a contributing factor to a subsequent military coup against Bongo. The deepfake video tipped off the military that something was wrong, even more than Bongo’s extended absence from the media.Also on InfoWorld: The best free data science courses during quarantineMore deepfake examplesA recent deepfake video of , the 1999 Smash Mouth classic, is an example of manipulating video (in this case, a mashup from popular movies) to fake lip synching. The creator, YouTube user ontyj, notes he “Got carried away testing out wav2lip and now this exists...” It’s amusing, although not convincing. Nevertheless, it demonstrates how much better faking lip motion has gotten. A few years ago, unnatural lip motion was usually a dead giveaway of a faked video.It could be worse. Have a look at this deepfake video of President Obama as the target and Jordan Peele as the driver. Now imagine that it didn’t include any context revealing it as fake, and included an incendiary call to action.Are you terrified yet?Deep learning vs. machine learning: Understand the differencesWhat is machine learning? Intelligence derived from dataWhat is deep learning? Algorithms that mimic the human brainMachine learning algorithms explainedAutomated machine learning or AutoML explainedSupervised learning explainedSemi-supervised learning explainedUnsupervised learning explainedReinforcement learning explainedWhat is computer vision? AI for images and videoWhat is face recognition? AI for Big BrotherWhat is natural language processing? AI for speech and textKaggle: Where data scientists learn and competeWhat is CUDA? Parallel processing for GPUs", "pub_date": "2020-09-15"},
{"title": "Algorithmia unveils MLops for teams", "overview": "Algorithmia Teams delivers machine learning operations and management for small groups as a pay-as-you-go cloud service", "image_url": "https://images.idgesg.net/images/article/2019/11/ai_artificial_intelligence_ml_machine_learning_abstract_face_by_kentoh_gettyimages_1042827860-100817767-large.jpg", "url": "https://www.infoworld.com/article/3572325/algorithmia-unveils-mlops-for-teams.html", "body": "Machine learning tools vendor Algorithmia has unveiled a version of its machine learning operations and management platform for small groups within organizations, called Algorithmia Teams.The cloud-hosted offering, for quickly delivering machine learning models into production, is intended to provide the performance of an enterprise-grade MLOps platform with the cost flexibility of a cloud-based service. A pay-as-you-go consumption model is leveraged to limit short-term financial exposure while enabling users to scale use of the platform as needed. Introduced August 25, Teams is intended to address a situation in which some companies have been constrained by the tools needed to manage AI/ML efforts as well as costs.There are Basic and Professional versions of Teams, with both offering secure workspaces for collaboration and connectors to external cloud data sources such as AWS, Microsoft Azure, and Google Cloud Platform. The Professional edition includes support, a dedicated Slack channel, and the option to purchase a custom language pack or data connector development. The Professional version costs $299 per month plus pay-as-you-go CPU/GPU consumption. The Basic version is pay-as-you-go CPU/GPU consumption with no monthly fee.Algorithmia, which was launched in 2014, maintains the infrastructure for Teams. You can sign up for Algorithmia Teams at the Algorithmia website.", "pub_date": "2020-08-27"},
{"title": "MLops: The rise of machine learning operations", "overview": "Once machine learning models make it to production, they still need updates and monitoring for drift. A team to manage ML operations makes good business sense ", "image_url": "https://images.idgesg.net/images/article/2020/07/teams_teamwork_collaboration_discussion_planning_meeting_communication_by_brauns_gettyimages-497812268_2400x1600-100853242-large.jpg", "url": "https://www.infoworld.com/article/3570716/mlops-the-rise-of-machine-learning-operations.html", "body": "As hard as it is for data scientists to tag data and develop accurate machine learning models, managing models in production can be even more daunting. Recognizing model drift, retraining models with updating data sets, improving performance, and maintaining the underlying technology platforms are all important data science practices. Without these disciplines, models can produce erroneous results that significantly impact business.Developing production-ready models is no easy feat. According to one machine learning study, 55 percent of companies had not deployed models into production, and 40 percent or more require more than 30 days to deploy one model. Success brings new challenges, and 41 percent of respondents acknowledge the difficulty of versioning machine learning models and reproducibility.How data analysis, AI, and IoT will shape the post-pandemic ‘new normal’The lesson here is that new obstacles emerge once machine learning models are deployed to production and used in business processes.Model management and operations were once challenges for the more advanced data science teams. Now tasks include monitoring production machine learning models for drift, automating the retraining of models, alerting when the drift is significant, and recognizing when models require upgrades. As more organizations invest in machine learning, there is a greater need to build awareness around model management and operations.The good news is platforms and libraries such as open source MLFlow and DVC, and commercial tools from Alteryx, Databricks, Dataiku, SAS, DataRobot, ModelOp, and others are making model management and operations easier for data science teams. The public cloud providers are also sharing practices such as implementing MLops with Azure Machine Learning.There are several similarities between model management and devops. Many refer to model management and operations as MLops and define it as the culture, practices, and technologies required to develop and maintain machine learning models.Understanding model management and operationsTo better understand model management and operations, consider the union of software development practices with scientific methods.As a software developer, you know that completing the version of an application and deploying it to production isn’t trivial. But an even greater challenge begins once the application reaches production. End-users expect regular enhancements, and the underlying infrastructure, platforms, and libraries require patching and maintenance.Now let’s shift to the scientific world where questions lead to multiple hypotheses and repetitive experimentation. You learned in science class to maintain a log of these experiments and track the journey of tweaking different variables from one experiment to the next. Experimentation leads to improved results, and documenting the journey helps convince peers that you’ve explored all the variables and that results are reproducible.Data scientists experimenting with machine learning models must incorporate disciplines from both software development and scientific research. Machine learning models are software code developed in languages such as Python and R, constructed with TensorFlow, PyTorch, or other machine learning libraries, run on platforms such as Apache Spark, and deployed to cloud infrastructure. The development and support of machine learning models require significant experimentation and optimization, and data scientists must prove the accuracy of their models.Like software development, machine learning models need ongoing maintenance and enhancements. Some of that comes from maintaining the code, libraries, platforms, and infrastructure, but data scientists must also be concerned about model drift. In simple terms, model drift occurs as new data becomes available, and the predictions, clusters, segmentations, and recommendations provided by machine learning models deviate from expected outcomes.Successful model management starts with developing optimal modelsI spoke with Alan Jacobson, chief data and analytics officer at Alteryx, about how organizations succeed and scale machine learning model development. “To simplify model development, the first challenge for most data scientists is ensuring strong problem formulation. Many complex business problems can be solved with very simple analytics, but this first requires structuring the problem in a way that data and analytics can help answer the question. Even when complex models are leveraged, the most difficult part of the process is typically structuring the data and ensuring the right inputs are being used are at the right quality levels.”I agree with Jacobson. Too many data and technology implementations start with poor or no problem statements and with inadequate time, tools, and subject matter expertise to ensure adequate data quality. Organizations must first start with asking smart questions about big data, investing in dataops, and then using agile methodologies in data science to iterate toward solutions.Monitoring machine learning models for model driftGetting a precise problem definition is critical for ongoing management and monitoring of models in production. Jacobson went on to explain, “Monitoring models is an important process, but doing it right takes a strong understanding of the goals and potential adverse effects that warrant watching. While most discuss monitoring model performance and change over time, what’s more important and challenging in this space is the analysis of unintended consequences.”One easy way to understand model drift and unintended consequences is to consider the impact of COVID-19 on machine learning models developed with training data from before the pandemic. Machine learning models based on human behaviors, natural language processing, consumer demand models, or fraud patterns have all been affected by changing behaviors during the pandemic that are messing with AI models.Technology providers are releasing new MLops capabilities as more organizations are getting value and maturing their data science programs. For example, SAS introduced a feature contribution index that helps data scientists evaluate models without a target variable. Cloudera recently announced an ML Monitoring Service that captures technical performance metrics and tracking model predictions.MLops also addresses automation and collaborationIn between developing a machine learning model and monitoring it in production are additional tools, processes, collaborations, and capabilities that enable data science practices to scale. Some of the automation and infrastructure practices are analogous to devops and include infrastructure as code and CI/CD (continuous integration/continuous deployment) for machine learning models. Others include developer capabilities such as versioning models with their underlying training data and searching the model repository.Also on InfoWorld: Applying devops in data science and machine learningThe more interesting aspects of MLops bring scientific methodology and collaboration to data science teams. For example, DataRobot enables a champion-challenger model that can run multiple experimental models in parallel to challenge the production version’s accuracy. SAS wants to help data scientists improve speed to markets and data quality. Alteryx recently introduced Analytics Hub to help collaboration and sharing between data science teams.All this shows that managing and scaling machine learning requires a lot more discipline and practice than simply asking a data scientist to code and test a random forest, k-means, or convolutional neural network in Python.", "pub_date": "2020-08-13"},
{"title": "Cloudops tool integration is more important than the tools themselves ", "overview": "Focus on the features and functions of cloud operations and monitoring tools and you could miss the much larger advantage", "image_url": "https://images.idgesg.net/images/article/2018/04/plane_in_a_forest_surprising_find_unexpected_unusual_out-of-place_david_kovalenko_cc0_via_unsplash_1200x800-100754668-large.jpg", "url": "https://www.infoworld.com/article/3570440/cloudops-tool-integration-is-more-important-than-the-tools-themselves.html", "body": "It’s 3:00 on a Tuesday, and your AIops tool messages that the corporate network is reaching a saturation point. It seems that one of the virtual cloud servers is spinning off a massive number of packets, hijacked by a rogue piece of software placed by a hacker the night before.You wish that the security operations tool would have picked up on this, but it was the general-purpose management and monitoring tool that saw the network traffic spiking out of threshold and sounded the alarm that drew attention to the breach. The offending server is quickly taken down; all is right with the world again. However, this could have gone much better.Also on InfoWorld: Application monitoring: What devops can do betterWhat’s missing is direct integration between the AIops tool and the security tool. Although they have different missions, they need each other. The security tool needs visibility into the behavior of all applications and infrastructure, considering that behaviors that are out of line with normal operations can often be tracked to security issues, such as DDoS attacks.  At the same time, the cloudops tool could play some role in automatically defending the cloud-based systems, such as attempting a restart or taking other corrective action so the issue does not result in an outage. The recovery could be reported back to the security tool, which would take further action, such as blocking the IP address that is the source of the DDoS attack.This example describes security and ops tools working together, but there is much value in other tool integration as well. Configuration management, testing, special-purpose monitoring such as edge computing and IoT, data governance, etc., can all benefit from working together to create common automation between tools. The smarter cloud management and monitoring players, especially those selling AIops tools, have largely gotten the tool integration religion. They are able to work and play well with other cloud tools to move towards a 1+1=3 kind of value driver. It’s the number one thing I look for these days, beyond the feature and function of each tool, but it’s still not high on the radar of most of the enterprises selecting cloud tools for the first time.  The reality is no magical tool does it all. They all have limited missions, which is a good thing, considering that they can be good at a few things versus being bad at many things. The strategy is to select the best-of-breed tools for each function (security, performance management, network management, application monitoring, data governance) and have those tools provide a common integration layer where events and information can be shared in a peer-to-peer method.  If this seems much more complex than just picking a single-purpose tool for a single purpose and not having to think about how they work together, you’re right. But, if this stuff were easy, it would not bring the value of cloud computing that enterprises have been promised.", "pub_date": "2020-08-18"},
{"title": "What is face recognition? AI for Big Brother", "overview": "Facial recognition is becoming more accurate, but some systems exhibit racial bias and some uses of the technology are controversial ", "image_url": "https://images.idgesg.net/images/article/2017/11/facial_recognition_system_identification_digital_id_security_scanning_thinkstock_858236252_3x3-100740902-large.jpg", "url": "https://www.infoworld.com/article/3573069/what-is-face-recognition-ai-for-big-brother.html", "body": "Can Big Brother identify your face from street-level CCTV surveillance and tell whether you’re happy, sad, or angry? Can that identification lead to your arrest on an outstanding warrant? What are the odds that the identification is incorrect, and really connects to someone else? Can you defeat the surveillance entirely using some trick?On the flip side, can you get into a vault protected by a camera and facial identification software by holding up a print of an authorized person’s face? What if you put on a 3-D mask of an authorized person’s face?Welcome to face recognition — and the spoofing of facial recognition.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesWhat is face recognition?Face recognition is a method for identifying an unknown person or authenticating the identity of a specific person from their face. It’s a branch of computer vision, but face recognition is specialized and comes with social baggage for some applications, as well as some vulnerabilities to spoofing.How does face recognition work?The early facial recognition algorithms (which are still in use today in improved and more automated form) rely on biometrics (such as the distance between eyes) to turn the measured facial features from a two-dimensional image into a set of numbers (a feature vector or template) that describes the face. The recognition process then compares these vectors to a database of known faces which have been mapped to features in the same way. One complication in this process is adjusting the faces to a normalized view to account for head rotation and tilt before extracting the metrics. This class of algorithms is called .Another approach to face recognition is to normalize and compress 2-D facial images, and to compare these with a database of similarly normalized and compressed images. This class of algorithms is called .Three-dimensional face recognition uses 3-D sensors to capture the facial image, or reconstructs the 3-D image from three 2-D tracking cameras pointed at different angles. 3-D face recognition can be considerably more accurate than 2-D recognition.Skin texture analysis maps the lines, patterns, and spots on a person’s face to another feature vector. Adding skin texture analysis to 2-D or 3-D face recognition can improve the recognition accuracy by 20 to 25 percent, especially in the cases of look-alikes and twins. You can also combine all the methods, and add in multi-spectral images (visible light and infrared), for even more accuracy.Face recognition has been improving year over year since the field began in 1964. On average, the error rate has reduced by half every two years.Face recognition vendor testsNIST, the US National Institute of Standards and Technology, has been performing tests of facial recognition algorithms, the Face Recognition Vendor Test (FRVT), since 2000. The image datasets used are mostly law enforcement mug shots, but also include in-the-wild still images, such as those found in Wikimedia, and low-resolution images from webcams.The FRVT algorithms are mostly submitted by commercial vendors. The year-over-year comparisons show major gains in performance and accuracy; according to the vendors, this is primarily because of the use of deep convolutional neural networks.Related NIST face recognition testing programs have studied demographic effects, detection of face morphing, identification of faces posted on social media, and identification of faces in video. A previous series of tests was conducted in the 1990s under a different moniker, Face Recognition Technology (FERET).NISTFacial identification miss rates across the false positive range, from NIST.IR.8271 (September 2019). Note that both axes are logarithmic.Face recognition applicationsFace recognition applications mostly fall into three major categories: security, health, and marketing/retail. Security includes law enforcement, and that class of facial recognition uses can be as benign as matching people to their passport photos faster and more accurately than humans can, and as creepy as the “Person of Interest” scenario where people are tracked via CCTV and compared to collated photo databases. Non-law-enforcement security includes common applications such as face unlock for mobile phones and access control for laboratories and vaults.Health applications of facial recognition include patient check-ins, real-time emotion detection, patient tracking within a facility, assessing pain levels in non-verbal patients, detecting certain diseases and conditions, staff identification, and facility security. Marketing and retail applications of face recognition include identification of loyalty program members, identification and tracking of known shoplifters, and recognizing people and their emotions for targeted product suggestions.Face recognition controversies, biases, and bansTo say that some of these applications are controversial would be an understatement. As a 2019 New York Times article discusses, facial recognition has swirled in controversy, from its use for stadium surveillance to racist software.Stadium surveillance? Face recognition was used at the 2001 Super Bowl: the software identified 19 people thought to be subjects of outstanding warrants, though none were arrested (not for lack of trying).Racist software? There have been several issues, starting with the 2009 face tracking software that could track whites but not Blacks, and continuing with the 2015 MIT study that showed that the facial recognition software of the time worked much better on white male faces than female and/or Black faces.These sorts of issues have led to outright bans of facial recognition software in specific places or for specific uses. In 2019, San Francisco became the first major American city to block police and other law enforcement agencies from using face recognition software; Microsoft called for federal regulations on facial recognition; and MIT showed that Amazon Rekognition had more trouble determining female gender than male gender from face images, as well as more trouble with Black female gender than white female gender.In June 2020, Microsoft announced that it will not sell and has not sold its face recognition software to the police; Amazon banned police from using Rekognition for a year; and IBM abandoned its facial recognition technology. Banning face recognition entirely will not be easy, however, given its wide adoption in iPhones (Face ID) and other devices, software, and technologies.Not all face recognition software suffers from the same biases. The 2019 NIST demographic effects study followed up on the MIT work and showed that algorithmic demographic bias varies widely among developers of face recognition software. Yes, there are demographic effects on the false match rate and false non-match rate of facial identification algorithms, but they can vary by several orders of magnitude from vendor to vendor, and they have been decreasing over time.Hacking face recognition, and anti-spoofing techniquesGiven the potential privacy threat from face recognition, and the attraction of getting access to high-value resources protected by face authentication, there have been many efforts to hack or spoof the technology. You can present a printed image of a face instead of a live face, or an image on a screen, or a 3-D printed mask, to pass authentication. For CCTV surveillance, you can play back a video. To avoid surveillance, you can try the “CV Dazzle” fabrics and make-up, and/or IR light emitters, to fool the software into not detecting your face.Of course, there are efforts to develop anti-spoofing techniques for all these attacks. To detect printed images, vendors use a liveness test, such as waiting for the subject to blink, or perform motion analysis, or use infrared to distinguish a live face from a printed image. Another approach is to perform micro-texture analysis, since human skin is optically different from prints and mask materials. The latest anti-spoofing techniques are mostly based on deep convolutional neural networks.This is an evolving field. There’s an arms war going on between attackers and anti-spoofing software, as well as academic research on the effectiveness of different attack and defense techniques.Face recognition vendorsAccording to the Electronic Frontier Foundation, MorphoTrust, a subsidiary of Idemia (formerly known as OT-Morpho or Safran), is one of the largest vendors of face recognition and other biometric identification technology in the United States. It has designed systems for state DMVs, federal and state law enforcement agencies, border control and airports (including TSA PreCheck), and the state department. Other common vendors include 3M, Cognitec, DataWorks Plus, Dynamic Imaging Systems, FaceFirst, and NEC Global.The NIST Face Recognition Vendor Test lists algorithms from many more vendors from all over the world. There are also several open source face recognition algorithms, of varying quality, and a few major cloud services that offer face recognition.Amazon Rekognition is an image and video analysis service that can identify objects, people, text, scenes, and activities, including facial analysis and custom labels. The Google Cloud Vision API is a pretrained image analysis service that can detect objects and faces, read printed and handwritten text, and build metadata into your image catalog. Google AutoML Vision allows you to train custom image models.The Azure Face API does face detection that perceives faces and attributes in an image, performs person identification that matches an individual in your private repository of up to 1 million people, and performs perceived emotion recognition. The Face API can run in the cloud or on the edge in containers.Face datasets for recognition trainingThere are dozens of face datasets available for downloading that can be used for recognition training. Not all face datasets are equal: They tend to vary in image size, number of people represented, number of images per person, conditions of images, and lighting. Law enforcement also has access to non-public face datasets, such as current mugshots and driver’s license images.Some of the larger face databases are Labeled Faces in the Wild, with ~13K unique people; FERET, used for the early NIST tests; the Mugshot database used in the ongoing NIST FRVT; the SCFace surveillance camera database, also available with facial landmarks; and Labeled Wikipedia Faces, with ~1.5K unique identities. Several of these databases contain multiple images per identity. This list from researcher Ethan Meyers offers some cogent advice on picking a face dataset for a specific purpose.In summary, facial recognition is improving, and vendors are learning to detect most spoofing, but some applications of the technology are controversial. The error rate for face recognition is halving every two years, according to NIST. Vendors have improved their anti-spoofing techniques by incorporating convolutional neural networks.Meanwhile, there are initiatives to ban the use of face recognition in surveillance, especially by police. Banning face recognition entirely would be difficult, however, given how widespread it has become.Deep learning vs. machine learning: Understand the differencesWhat is machine learning? Intelligence derived from dataWhat is deep learning? Algorithms that mimic the human brainMachine learning algorithms explainedAutomated machine learning or AutoML explainedSupervised learning explainedSemi-supervised learning explainedUnsupervised learning explainedReinforcement learning explainedWhat is computer vision? AI for images and videoWhat is face recognition? AI for Big BrotherWhat is natural language processing? AI for speech and textKaggle: Where data scientists learn and competeWhat is CUDA? Parallel processing for GPUs", "pub_date": "2020-09-03"},
{"title": "Using robotics and immersive technologies to support work-from-home employees", "overview": "The COVID-19 crisis has accelerated the long-term shift toward remote working as a standard business practice", "image_url": "https://images.idgesg.net/images/article/2020/07/remote_worker_working_from_home_self_employed_contractor_freelancer_digital_nomad_by_allensima_gettyimages-1151404751_2400x1600-100853245-large.jpg", "url": "https://www.infoworld.com/article/3572346/using-robotics-and-immersive-technologies-to-support-work-from-home-employees.html", "body": "Even when the pandemic wanes, the percentage of personnel working remotely will continue to grow. According to a recent enterprise survey by 451 Research, the emerging technology research unit of S&P Global Market Intelligence, 80 percent have implemented or expanded universal work-from-home policies, and 67 percent plan to keep at least some remote work policies in place long term or permanently.However, remote employees may find their organizations’ administrative and technical support capabilities unsatisfactory. Remote support tends to lack the hands-on, interactive, and immersive services that we’ve come to expect from on-site technical and administrative support personnel. As enterprises retool their internal processes to support a post-pandemic workforce, they should explore a growing range of immersive and robotics capabilities that can deliver high-quality support to remote, mobile, and other nontraditional worksites.10 professional organizations focused on diversity in techBeing Black in IT: 3 tech leaders share their storiesGender gapped: The state of gender diversity in ITMāori participation in ITIT snapshot: Ethnic diversity in the tech industryUsing robotics to automate remote supportChief among these immersive technologies are RPA (robotic process automation), EC (embodied cognition), AR (augmented reality), VR (virtual reality), and MR (mixed reality).RPA has already come to remote work environments. However, this software-centric approach is not the same as having a traditional human administrative assistant close at hand.In this regard, EC tools, which use sensor-driven artificial intelligence to anchor robotics in physical environments, may bridge the gap by powering traditional hardware-based robots as multifunctional robotic digital assistants. As discussed in this recent article, Facebook researchers are developing EC technology to power a “truly embodied robot” that, among other capabilities, “could check to see whether a door is locked…or retrieve a smartphone that’s ringing in an upstairs bedroom.” Automating physical tasks such as these might boost worker productivity by fending off distractions and interruptions in the remote worker’s day.Of course, smart robots would need to automatically sense the physical parameters of diverse home and other remote work environments. In that regard, Facebook’s R&D points to technological advances that will make that possible. Specifically, Facebook Research has just open sourced a new audio simulation platform called “SoundSpaces,” which trains robotic agents to navigate 3-D environments through AI-based processing of visual and acoustic sensor data.This technology, which the company describes as the “first audio-visual platform for embodied AI,” automatically creates high-fidelity, semantics-infused renderings of virtually any sound source from real-world sensed environments. It enables embodied AI agents to acquire multimodal sensory understandings and develop complex reasoning about objects and places. It runs on Facebook AI’s AI Habitat simulation platform and includes audio renderings from two sets of publicly available 3-D environments: Matterport3D and Replica Dataset.Using dynamic captioning to augment remote supportAR can support remote work environments in which elements of the physical setting are unfamiliar to the worker. It displays “augmented” descriptive captions in a 3-D format over an employee’s direct camera view of his or her physical environment.For support applications, AR might dynamically supplement the information in a worker’s remote physical environment with a dynamic display of descriptive labels and guidance. Wearable AR technology could automatically present remote workers with the information they need at the exact moment they need it.These captions may be prepackaged with the AR-based support application, or they may be automatically inferred from the physical environments using AI-based technologies such as Facebook’s SoundSpaces. For example, some AR-based training applications could display step-by-step instructions for complicated assembly work. This eliminates the need for workers to retrieve that information manually from desktop computer or paper manuals.If the captions need to help remote workers understand human behaviors and intentions (such as in caregiving contexts), technology such as this recent MIT project, may come in handy. Researchers at MIT’s Computer Science and Artificial Intelligence Laboratory have developed a system that can detect and caption the behaviors of people within a room from Wi-Fi and other RF signals. Researchers claim that their approach can observe people through walls and other obstructions, even in complete darkness, and it can learn to detect those people’s interactions with objects, such as a cup of water.Though this may give people the jitters on privacy grounds, it can be very useful in support contexts such as remotely detecting when disabled personnel need assistance or when a fully abled worker is still struggling to master a new physical work skill. The technology, called RF-Diary, can be set up to notify support personnel when the detected behaviors cross some threshold requiring remote intervention.Using virtualization to simulate remote supportRemote employees typically lack direct physical access to all or most of the physical business process in which they’re participating.Simulating the parts of the process that the remote employee can’t interact with directly is where VR comes in handy. It is well suited to support scenarios where important participants, tools, and activities are entirely missing from the remote employee’s immediate physical environment. It may involve avatars to represent unseen personnel and computer-generated imagery to stand in for end-to-end workflows or specific projects, tasks, or outcomes. VR has long been a staple of remote training in medical, engineering, aerospace, military, and other fields. It can provide simulated learning and support capabilities in situations in which remote personnel can cultivate skills within their normal work routines without needing to visit in-person training sites.Another promising use of VR is to enable head-office support staff to interactively visualize the physical environment of the remote worker. One emerging technology that might be useful in this regard is this Stanford-developed “neural holography” system for 3-D rendering. It incorporates AI that is trained with a camera-in-the-loop simulator to generate high-quality, immersive, 3-D visualizations in real time.Then there’s MR, which hybridizes AR and VR by seamlessly blending real and simulated work environments. It is well suited to supporting remote environments where some, but not all, important participants and activities are not always within a worker’s immediate field of vision. This may call for dynamic labeling of those aspects of the physical environment whose meaning is not entirely evident alongside simulation of those aspects that are missing from the environment.Military and police agencies are using MR training tools in a variety of simulated scenarios, including active shooter, domestic violence, and traffic stops. The blend of physical and virtual environments creates realistic training without endangering the lives of soldiers or police officers.What’s nextIn the coming three to five years, immersive technologies will become fundamental to delivering high-quality administrative and technical support to remote workers. The “bring your own device” revolution of the past 20 years will ensure that many future workers will be very comfortable with immersive technologies such as AR, VR, and MR in their professional lives.Using these immersive tools in conjunction with at-home robotics devices will enable enterprises to deliver 24x7 support to remote workers that’s almost as hands-on as what they might have received if they’d remained at their company’s traditional offices.", "pub_date": "2020-09-03"},
{"title": "How to use TensorFlow in your browser", "overview": "Take advantage of TensorFlow.js to develop and train machine learning models in JavaScript and deploy them in a browser or on Node.js", "image_url": "https://images.idgesg.net/images/article/2019/02/neural_network_by_ktsimage_gettyimages-891769302_2400x1600-100788348-large.jpg", "url": "https://www.infoworld.com/article/3570444/how-to-use-tensorflow-in-your-browser.html", "body": "While you can train simple neural networks with relatively small amounts of training data with TensorFlow, for deep neural networks with large training datasets you really need to use CUDA-capable Nvidia GPUs, or Google TPUs, or FPGAs for acceleration. The alternative has, until recently, been to train on clusters of CPUs for weeks.One of the innovations introduced with TensorFlow 2.0 is a JavaScript implementation, TensorFlow.js. I wouldn’t have expected that to improve training or inference speed, but it does, given its support for all GPUs (not just CUDA-capable GPUs) via the WebGL API.What is TensorFlow.js?TensorFlow.js is a library for developing and training machine learning models in JavaScript, and deploying them in a browser or on Node.js. You can use existing models, convert Python TensorFlow models, use transfer learning to retrain existing models with your own data, and develop models from scratch.TensorFlow.js back endsTensorFlow.js supports multiple back ends for execution, although only one can be active at a time. The TensorFlow.js Node.js environment supports using an installed build of Python/C TensorFlow as a back end, which may in turn use the machine’s available hardware acceleration, for example CUDA. There is also a JavaScript-based back end for Node.js, but its capabilities are limited.In the browser, TensorFlow.js has several back ends with different characteristics. The WebGL back end provides GPU support using WebGL textures for storage and WebGL shaders for execution, and can be up to 100x faster than the plain CPU back end. WebGL does not require CUDA, so it can take advantage of whatever GPU is present.The WebAssembly (WASM) TensorFlow.js back end for the browser uses the XNNPACK library for optimized CPU implementation of neural network operators. The WASM back end is generally much faster (10x to 30x) than the JavaScript CPU back end, but is usually slower than the WebGL back end except for very small models. Your mileage may vary, so test both the WASM and WebGL back ends for your own models on your own hardware.TensorFlow.js models and layersTensorFlow.js supports two APIs for building neural network models. One is the Layers API, which is essentially the same as the Keras API in TensorFlow 2. The other is the Core API, which is essentially direct manipulation of tensors.Like Keras, the TensorFlow.js Layers API has two ways to create a model: sequential and functional. The sequential API is a linear stack of layers, implemented with a layer list (as shown below) or with the  method:The functional API uses the  API and can create arbitrary DAG (directed acyclic graph) networks:The Core API can accomplish the same goals, with different code, and less of an intuitive tie to layers. The model below may look like basic tensor operations, but it creates the same network as the two previous formulations. Note the use of  and , which are both neural network operations, in the  function below.Pre-built TensorFlow.js modelsThere are over a dozen pre-built TensorFlow.js models documented, available in the repository, and hosted on NPM (for use in Node.js) and unpkg (for use in a browser). You can use these models as supplied or for transfer learning. With a little work, you can also use them as building blocks for other models.Several of these models use a device's camera in real time, for example handpose:Handpose can detect palms and track hand-skeleton fingers. The list below is a convenient index into most of the prepackaged TensorFlow.js models.Image classificationObject detectionBody segmentationPose estimationText toxicity detectionUniversal sentence encoderSpeech command recognitionKNN classifierSimple face detectionSemantic segmentationFace landmark detectionHand pose detectionNatural language question answeringWhat is ml5.js?ml5.js is an open source, friendly, high-level interface to TensorFlow.js developed primarily at NYU. ml5.js provides immediate access in the browser to pre-trained models for detecting human poses, generating text, styling an image with another, composing music, pitch detection, common English language word relationships, and much more. While TensorFlow.js is aimed primarily at data scientists and developers, ml5.js aims to support broader public understanding of machine learning and foster deeper engagement with ethical computing, responsible data collection, and accessibility and diversity of people and perspectives in technology and the arts.Most of the examples in ml5.js depend on TensorFlow.js models. They have been packaged as web pages that you can run as is, or edit, for example to use different images.PoseNet can perform real-time pose estimation in the browser, from images or a video feed. Demo: Iris classification with TensorFlow.jsThe famous Iris discrimination dataset, originated by R.A. Fisher in 1936 to illustrate linear discriminant analysis, is still used as a test case for statistical and machine learning classification methods. It uses four features, the length and width of the flower sepals and petals, to classify three species of Iris, with 50 samples of each species. (Fisher’s original paper was published in the , which says more about science in 1936 than it does about the data or the statistics.)If you perform cluster analysis on this data, two of the species will share one cluster, with the third (I. Setosa) in a separate cluster. On the other hand, principal component analysis can separate all three species fairly well.The TensorFlow.js sample fits the Iris data with two fully-connected (dense) neural network layers, as shown in the code extract below.As you can see in the screenshot below, this model does a decent job of classifying the three species. If you play around with the parameters, however, you’ll discover that some confusion between two of the species (the ones in the same cluster) reappears if you iterate for more than 40 epochs.Model training for the Iris dataset using a two-dense-layer neural network model.Converting Python TensorFlow models to JavaScriptPart of the TensorFlow.js repository contains a converter for saved TensorFlow and Keras models. It supports three formats: SavedModel (the default for TensorFlow), HDF5 (the default for Keras), and TensorFlow Hub. You can use the converter for saved models from the standard repositories, models you’ve trained yourself, and models you’ve found elsewhere.There are actually two steps to the conversion. The first step is to convert the existing model to model.json and binary weight files. The second step is to use an API to load the model into TensorFlow.js, either  for converted TensorFlow and TensorFlow Hub models, or  for converted Keras models.Using transfer learningTensorFlow.js supports transfer learning in essentially the same way as TensorFlow. The documentation supplies examples for customizing MobileNet for your own images and customizing a model for speech command recognition for your own sound classes. Essentially, what you are doing in each of these codelabs is adding a small custom classifier on top of the trained model, and training that.Overall, TensorFlow.js can do almost anything that TensorFlow can do. However, given that the target environments for TensorFlow.js (garden variety GPUs for gaming) typically have less in the way of GPU memory than the big Nvidia server GPUs typically used for TensorFlow deep learning training, you may have to reduce the size of your model to make it run in a browser. The conversion utility does some of this for you, but you may have to take out layers manually and reduce the batch sizes for your training.", "pub_date": "2020-09-02"},
{"title": "MLflow is now a Linux Foundation project", "overview": "Databricks framework for managing machine learning projects will go to an open governance model", "image_url": "https://images.idgesg.net/images/article/2019/11/ai_artificial_intelligence_ml_machine_learning_vector_by_kohb_gettyimages_1146634284-100817775-large.jpg", "url": "https://www.infoworld.com/article/3563344/mlflow-is-now-a-linux-foundation-project.html", "body": "Databricks, the company behind the commercial development of Apache Spark, is placing its machine learning lifecycle project MLflow under the stewardship of the Linux Foundation.MLflow provides a programmatic way to deal with all the pieces of a machine learning project through all its phases — construction, training, fine-tuning, deployment, management, and revision. It tracks and manages the the datasets, model instances, model parameters, and algorithms used in machine learning projects, so they can be versioned, stored in a central repository, and repackaged easily for reuse by other data scientists.InfoWorld review: Nvidia RAPIDS brings Python analytics to the GPUMLflow’s source is already available under the Apache 2.0 license, so this isn’t about open sourcing a previously proprietary project. Instead, it’s about giving the project “a vendor neutral home with an open governance model,” according to Databricks’s press release.Projects for managing entire machine learning pipelines have taken shape over the past couple of years, providing single overarching tools for governing what is typically a sprawling and complex process involving multiple moving parts. Among them is a Google project, Tensorflow Extended, but better known is its descendent project Kubeflow, which uses Kubernetes to manage machine learning pipelines.MLflow differs from Kubeflow in several key ways. For one, it doesn’t require Kubernetes as a component; it runs on local machines by way of simple Python scripts, or in Databricks’s hosted environment. And while Kubeflow focuses on TensorFlow and PyTorch as its learning systems, MLflow is agnostic — it can work with models from those frameworks and many others. ", "pub_date": "2020-06-25"},
{"title": "The move to dynamic distributed cloud architectures ", "overview": "Now that we’re moving things out to the edge, an emerging approach blows the door off the cloud-only and edge-only models.", "image_url": "https://images.techhive.com/images/article/2015/12/thinkstockphotos-451936193-100632468-large.jpg", "url": "https://www.infoworld.com/article/3587772/the-move-to-dynamic-distributed-cloud-architectures.html", "body": "Edge computing is getting a great deal of attention now, and for good reason. Cloud architecture requires that some processing be placed closest to the point of data consumption. Think computing systems in your car, industrial robots, and now full-blown connected mini clouds such as Microsoft’s Stack and AWS’s Outpost, certainly all examples of edge computing.The architectural approach to edge computing—and IoT (Internet of Things), for that matter—is the creation of edge computing replicants in the public clouds. You can think of these as clones of what exists on the edge computing device or platform, allowing you to sync changes and manage configurations on “the edge” centrally.The trouble with this model is that it’s static. The processing and data is tightly coupled to the public cloud or an edge platform. There is typically no movement of those processes and data stores, albeit data is transmitted and received. This is a classic distributed architecture.Also on InfoWorld: Amazon, Google, and Microsoft take their clouds to the edgeThe trouble with the classic approach is that sometimes the processing and I/O load requirements expand to 10 times the normal load. Edge devices are typically underpowered, considering that their mission is fairly well defined, and edge applications are created to match the amount of resources on the edge device or platform. However, as edge devices become more popular, we’re going to need to expand the load on these devices or they will more frequently hit an upward limit that they can’t handle.The answer is the dynamic migration of processing and data storage from an edge device to the public cloud. Considering that a replicant is already on the public cloud provider, that should be less of a problem. You will need to start syncing the data as well as the application and configuration, so at any movement one can take over for the other (active/active).The idea here is to keep things as simple as you can. Assuming that the edge device does not have the processing power needed for a specific use case, the processing shifts from the edge to the cloud. Here the amount of CPU and storage resources are almost unlimited, and processing should be able to scale, afterwards returning the processing to the edge device, with up-to-date, synced data.Some ask the logical question about just keeping the processing and the data on the cloud and not bothering with an edge device. Edge is an architectural pattern that is still needed, with processing and data storage placed closest to the point of origin. Dynamic distributed leverages centralized processing as needed, dynamically. It provides the architectural advantage of allowing scalability without the loss of needed edge functionality.A little something to add to your bag of cloud architecture tricks.", "pub_date": "2020-10-30"},
{"title": "What is computer vision? AI for images and video", "overview": "Computer vision systems are not only good enough to be useful, but in some cases more accurate than human vision", "image_url": "https://images.idgesg.net/images/article/2018/08/eye_circuits_system_artificial_intelligence_machine_learning_privacy_by_vijay_patel_gettyimages-936718998_1200x800-100768000-large.jpg", "url": "https://www.infoworld.com/article/3572553/what-is-computer-vision-ai-for-images-and-video.html", "body": "Computer vision identifies and often locates objects in digital images and videos. Since living organisms process images with their visual cortex, many researchers have taken the architecture of the mammalian visual cortex as a model for neural networks designed to perform image recognition. The biological research goes back to the 1950s.The progress in computer vision over the last 20 years has been absolutely remarkable. While not yet perfect, some computer vision systems achieve 99% accuracy, and others run decently on mobile devices.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesThe breakthrough in the neural network field for vision was Yann LeCun’s 1998 LeNet-5, a seven-level  for recognition of handwritten digits digitized in 32x32 pixel images. To analyze higher-resolution images, the LeNet-5 network would need to be expanded to more neurons and more layers.Today’s best image classification models can identify diverse catalogs of objects at HD resolution in color. In addition to pure deep neural networks (DNNs), people sometimes use hybrid vision models, which combine deep learning with classical machine-learning algorithms that perform specific sub-tasks.Other vision problems besides basic image classification have been solved with deep learning, including image classification with localization, object detection, object segmentation, image style transfer, image colorization, image reconstruction, image super-resolution, and image synthesis.How does computer vision work?Computer vision algorithms usually rely on convolutional neural networks, or CNNs. CNNs typically use convolutional, pooling, ReLU, fully connected, and loss layers to simulate a visual cortex.The convolutional layer basically takes the integrals of many small overlapping regions. The pooling layer performs a form of non-linear down-sampling. ReLU layers apply the non-saturating activation function .In a fully connected layer, the neurons have connections to all activations in the previous layer. A loss layer computes how the network training penalizes the deviation between the predicted and true labels, using a Softmax or cross-entropy loss for classification.Computer vision training datasetsThere are many public image datasets that are useful for training vision models. The simplest, and one of the oldest, is MNIST, which contains 70,000 handwritten digits in 10 classes, 60K for training and 10K for testing. MNIST is an easy dataset to model, even using a laptop with no acceleration hardware. CIFAR-10 and Fashion-MNIST are similar 10-class datasets. SVHN (street view house numbers) is a set of 600K images of real-world house numbers extracted from Google Street View.COCO is a larger-scale dataset for object detection, segmentation, and captioning, with 330K images in 80 object categories. ImageNet contains about 1.5 million images with bounding boxes and labels, illustrating about 100K phrases from WordNet. Open Images contains about nine million URLs to images, with about 5K labels.Google, Azure, and AWS all have their own vision models trained against very large image databases. You can use these as is, or run transfer learning to adapt these models to your own image datasets. You can also perform transfer learning using models based on ImageNet and Open Images. The advantages of transfer learning over building a model from scratch are that it is much faster (hours rather than weeks) and that it gives you a more accurate model. You’ll still need 1,000 images per label for the best results, although you can sometimes get away with as few as 10 images per label.Computer vision applicationsWhile computer vision isn’t perfect, it’s often good enough to be practical. A good example is vision in self-driving automobiles.Waymo, formerly the Google self-driving car project, claims tests on seven million miles of public roads and the ability to navigate safely in daily traffic. There has been at least one accident involving a Waymo van; the software was not believed to be at fault, according to police.Also on InfoWorld: The best free data science courses during quarantineTesla has three models of self-driving car. In 2018 a Tesla SUV in self-driving mode was involved in a fatal accident. The report on the accident said that the driver (who was killed) had his hands off the steering wheel despite multiple warnings from the console, and that neither the driver nor the software tried to brake to avoid hitting the concrete barrier. The software has since been upgraded to require rather than suggest that the driver’s hands be on the steering wheel.Amazon Go stores are checkout-free self-service retail stores where the in-store computer vision system detects when shoppers pick up or return stock items; shoppers are identified by and charged through an Android or iPhone app. When the Amazon Go software misses an item, the shopper can keep it for free; when the software falsely registers an item taken, the shopper can flag the item and get a refund for that charge.In healthcare, there are vision applications for classifying certain features in pathology slides, chest x-rays, and other medical imaging systems. A few of these have demonstrated value when compared to skilled human practitioners, some enough for regulatory approval. There’s also a real-time system for estimating patient blood loss in an operating or delivery room.There are useful vision applications for agriculture (agricultural robots, crop and soil monitoring, and predictive analytics), banking (fraud detection, document authentication, and remote deposits), and industrial monitoring (remote wells, site security, and work activity).There are also applications of computer vision that are controversial or even deprecated. One is face recognition, which when used by government can be an invasion of privacy, and which often has a training bias that tends to misidentify non-white faces. Another is deepfake generation, which is more than a little creepy when used for pornography or the creation of hoaxes and other fraudulent images.Computer vision frameworks and modelsMost deep learning frameworks have substantial support for computer vision, including Python-based frameworks TensorFlow (the leading choice for production), PyTorch (the leading choice for academic research), and MXNet (Amazon’s framework of choice). OpenCV is a specialized library for computer vision that leans toward real-time vision applications and takes advantage of MMX and SSE instructions when they are available; it also has support for acceleration using CUDA, OpenCL, OpenGL, and Vulkan.Amazon Rekognition is an image and video analysis service that can identify objects, people, text, scenes, and activities, including facial analysis and custom labels. The Google Cloud Vision API is a pretrained image analysis service that can detect objects and faces, read printed and handwritten text, and build metadata into your image catalog. Google AutoML Vision allows you to train custom image models. Both Amazon Rekognition Custom Labels and Google AutoML Vision perform transfer learning.The Microsoft Computer Vision API can identify objects from a catalog of 10,000, with labels in 25 languages. It also returns bounding boxes for identified objects. The Azure Face API does face detection that perceives faces and attributes in an image, person identification that matches an individual in your private repository of up to one million people, and perceived emotion recognition. The Face API can run in the cloud or on the edge in containers.IBM Watson Visual Recognition can classify images from a pre-trained model, allow you to train custom image models with transfer learning, perform object detection with object counting, and train for visual inspection. Watson Visual Recognition can run in the cloud, or on iOS devices using Core ML.The data analysis package Matlab can perform image recognition using machine learning and deep learning. It has an optional Computer Vision Toolbox and can integrate with OpenCV.Computer vision models have come a long way since LeNet-5, and they are mostly CNNs. Examples include AlexNet (2012), VGG16/OxfordNet (2014), GoogLeNet/InceptionV1 (2014), Resnet50 (2015), InceptionV3 (2016), and MobileNet (2017-2018). The MobileNet family of vision neural networks was designed with mobile devices in mind.The Apple Vision framework performs face and face landmark detection, text detection, barcode recognition, image registration, and general feature tracking. Vision also allows the use of custom Core ML models for tasks like classification or object detection. It runs on iOS and macOS. The Google ML Kit SDK has similar capabilities, and runs on Android and iOS devices. ML Kit additionally supports natural language APIs.As we’ve seen, computer vision systems have become good enough to be useful, and in some cases more accurate than human vision. Using transfer learning, customization of vision models has become practical for mere mortals: computer vision is no longer the exclusive domain of Ph.D.-level researchers.Deep learning vs. machine learning: Understand the differencesWhat is machine learning? Intelligence derived from dataWhat is deep learning? Algorithms that mimic the human brainMachine learning algorithms explainedWhat is natural language processing? AI for speech and textAutomated machine learning or AutoML explainedSupervised learning explainedSemi-supervised learning explainedUnsupervised learning explainedReinforcement learning explainedKaggle: Where data scientists learn and competeWhat is CUDA? Parallel processing for GPUsHow to choose a cloud machine learning platformDeeplearning4j: Deep learning and ETL for the JVMReview: Amazon SageMaker plays catch-upTensorFlow 2 review: Easier machine learningReview: Google Cloud AutoML is truly automated machine learningReview: MXNet deep learning shines with GluonPyTorch review: A deep learning framework built for speedReview: Keras sails through deep learning", "pub_date": "2020-08-27"},
{"title": null, "overview": null, "image_url": null, "url": "https://www.infoworld.com/blog/enterprise-microsoft/", "body": "", "pub_date": null},
{"title": "How to make the most of the Google Cloud free tier", "overview": "10 tips for stretching Google’s free services to the limit and pinning your cloud meter at zero.", "image_url": "https://images.idgesg.net/images/article/2019/12/cso_gauges_meters_performance_indicators_dashboard_measures_by_autophotography_cc0_via_pixabay_binary_by_garik_barseghyan_aka_insspirito_cc0_via_pixabay_2400x1600-100824624-large.jpg", "url": "https://www.infoworld.com/article/3585633/how-to-make-the-most-of-the-google-cloud-free-tier.html", "body": "The cloud computing industry loves to give away free samples and Google is no different from Amazon or Microsoft in this respect. The companies know that if you give the customers a free taste, they’ll come back when it’s time for a meal.Google offers two types of free. New customers get $300 to spend on any of the machines or services spread out among the 24 “cloud regions,” 73 “zones,” and 144 “network edge locations.” The money works pretty much everywhere in the Google cloud from raw compute power to any of several dozen different products like databases or map services.Also on InfoWorld: How to make the most of the AWS free tierBut even when that free money runs out, the free gifts continue. There are 24 different products that offer continuous free samples that are billed as “always free.” Even if you’ve been a customer for years, you can still experiment. Of course Google adds the caveat that the word “always” in this generous promise is “subject to change.” But until that day comes, the BigQuery database will answer one terabyte of queries each month and AutoML Translation will turn 500,000 characters from one language to another.Some developers use the free tier for what it’s intended to be: an opportunity to explore without begging their boss and their boss’s boss for a budget. Others work on a side hustle or a website for the neighborhood kids. When the load is small, it’s easy to innovate without dealing with a monthly bill.Some developers take this to the extreme. They try to stay in the free tier as long as possible. Perhaps it’s because they want to brag about their insanely low burn rate. Maybe it’s just a form of modern machismo. Maybe they’re low on cash.In any case, working this free angle as long as possible generally leads to lean and efficient web applications that do as much as possible with as little as possible. When the day comes that they leave the free tier, the monthly bills will stay small as the project scales, something that warms the heart of every CFO.Here are a few of the secrets for squeezing every last drop of goodness from Google’s free offering. Maybe you’re cheap. Maybe you’re just waiting to tell your boss until the awesomeness is completely realized. Maybe you’re just having fun and this is a goof. Whatever the case, there are many ways to save. Store only what’s necessaryThe free databases like Firestore and Cloud Storage are completely flexible tools that squirrel away key-value documents and objects respectively. Google Cloud’s always-free tier lets you store your first 1GB and 10GB in each product respectively. But the more details your app keeps, the faster the free gigabytes will run out. So quit saving information unless you absolutely need it. This means no obsessive collection of data just in case you need it for debugging later. No extra timestamps, no large cache full of data you’re keeping just to be ready.Compression is your friendThere are dozens of good pieces of code for adding a layer of compression to your clients. Instead of storing fat blocks of JSON, the client code can run the data through an algorithm like LZW or Gzip before sending it over the wire to your server instances, which store it without unpacking it. That means faster responses, fewer bandwidth issues, and less impact on your free monthly data storage quota. Be a bit careful because some very small data packets can get bigger when the overhead from compression is included.Go serverlessGoogle is more generous with their intermittent compute services that are billed per request. Cloud Run will boot up and run a stateless container that answers two million requests each month for free. Cloud Functions will fire up your function in response to another two million requests. That’s more than 100,000 different operations each day on average. So quit waiting and start writing your code to the serverless model.Note: Some architects will cringe at the idea of using two completely different services. It may save money but it will double the complexity of the application and that means it will be harder to maintain. That is a real danger, but often you can more or less duplicate the function-as-a-service structure of Cloud Functions inside your own container, making it possible to consolidate your code later if you plan for it.Also on InfoWorld: 13 ways Google Cloud beats AWSUse the App EngineGoogle’s App Engine remains one of the best ways to spin up a web application without fussing over all of the details of how to deploy or scale it. Almost everything is automated so it will deploy new instances if the load grows. The App Engine comes with 28 “instance hours” for each day—meaning that your basic app will run free for 24 hours per day and can even scale up for four hours if there’s a burst of demand.Consolidate service callsThere is some freedom to add extras if you’re careful. The limits on serverless invocations are on the number of individual requests not on the complexity. You can pack more action and more results into each exchange by bundling all of the data operations into one bigger packet. So you can offer silly gimmicks like stock quotes, but only if you slip the extra few bytes into the absolutely essential packets. Just keep in mind that Google counts the memory used and the compute time. Your functions can’t exceed 400,000 GB-seconds memory and 200,000 GHz-seconds of compute time.Use local storageThe modern web API offers a number of good places to store information. There’s the perfectly good, old-fashioned cookie that’s limited to four kilobytes. The Web Storage API is a document-based key-value system that will cache at least five megabytes of data and some browsers will keep 10 megabytes. The IndexedDB offers a richer set of features like database cursors and indices that will speed plowing through the data which is often stored without limits.The more data you store locally on your user’s machine, the less you need to use your precious server-side storage. This can also mean faster responses and much less bandwidth devoted to carrying endless copies of the data back to your server. There will be problems, though, when users switch devices because the data probably won’t be in sync. Just make sure the important details are consistent.Find the hidden bargainsGoogle maintains a helpful page that summarizes all of the “always free” products, but if you poke around you’ll find plenty of free services that don’t even make the list. Google Maps, for instance, offers “$200 free monthly usage.” Google Docs and a few of the other APIs are always free.Use G SuiteMany of the G Suite products including Docs, Sheets, and Drive are billed separately and the users either get them free with their GMail account or their business pays for them as a suite. Instead of creating an app with built-in reporting, just write the data to a spreadsheet and share that. The spreadsheets are powerful enough to include graphs and plots like any dashboard. If you build a web app, you’ll need to burn your compute and data quotas to handle the interactive requests. But if you just create a Google Doc for your report, you’re dumping most of the work on Google’s machine.Strip away gimmicksSome features of modern web applications are pretty superfluous. Does your bank application need stock quotes? Do you need to include local time or temperature? Do you need to embed the latest tweets or Instagram photos? No. Get rid of all of these extras because each one means another call to your server machines and that eats away at your free limits. The product design team may dream big, but you can tell them, “No!”Be careful with new optionsSome of the cooler tools for building artificial intelligence services for your stack offer good limits for experimenting. The AutoML Video service will let you train your machine learning model on video feeds for 40 hours each month, before charges kick in. The service for tabular data will grind your rows and rows of information on a node free for six hours. This gives you enough rope to experiment or build basic models, but watch out. It would be dangerous to automate the process so every user could trigger a big machine learning job.Keep costs in perspectiveIt’s easy to take this game to the extreme and turn your application’s architecture into a Rube Goldberg device just to save a bit more cash. It’s important to remember that the jump from free tier to paying customer is often a pretty tiny step in Google Cloud. While there are many free services on the Internet that jump from free to thousands of dollars with one click, Google’s services generally aren’t priced like that.Also on InfoWorld: Amazon, Google, and Microsoft take their clouds to the edgeAfter churning through two million free invocations of Cloud Functions, the next one is a whopping $0.0000004. That’s only 40 cents per million. If you dig around your sock drawer, you should be able to cover a few extra million with little trouble.The price schedule is generous enough that you’re not going to have a heart attack when you step out of the free zone. If your application needs a few extra million this or that, you’ll probably be able to cover it. The important lesson is that keeping the computational load low will translate to smaller bills and faster responses.", "pub_date": "2020-11-02"},
{"title": "Kaggle: Where data scientists learn and compete", "overview": "By hosting datasets, notebooks, and competitions, Kaggle helps data scientists discover how to build better machine learning models", "image_url": "https://images.idgesg.net/images/article/2019/11/ai_artificial_intelligence_ml_machine_learning_robot_touch_human_hand_by_kentoh_gettyimages_1060703206-100817766-large.jpg", "url": "https://www.infoworld.com/article/3564164/kaggle-where-data-scientists-learn-and-compete.html", "body": "Data science is typically more of an art than a science, despite the name. You start with dirty data and an old statistical predictive model and try to do better with machine learning. Nobody checks your work or tries to improve it: If your new model fits better than the old one, you adopt it and move on to the next problem. When the data starts drifting and the model stops working, you update the model from the new dataset.Doing data science in Kaggle is quite different. Kaggle is an online machine learning environment and community. It has standard datasets that hundreds or thousands of individuals or teams try to model, and there’s a leaderboard for each competition. Many contests offer cash prizes and status points, and people can refine their models until the contest closes, to improve their scores and climb the ladder. Tiny percentages often make the difference between winners and runners-up.Also on InfoWorld: The best free data science courses during quarantineKaggle is something that professional data scientists can play with in their spare time, and aspiring data scientists can use to learn how to build good machine learning models.What is Kaggle?Looked at more comprehensively, Kaggle is an online community for data scientists that offers machine learning competitions, datasets, notebooks, access to training accelerators, and education. Anthony Goldbloom (CEO) and Ben Hamner (CTO) founded Kaggle in 2010, and Google acquired the company in 2017.Kaggle competitions have improved the state of the machine learning art in several areas. One is mapping dark matter; another is HIV/AIDS research. Looking at the winners of Kaggle competitions, you’ll see lots of XGBoost models, some Random Forest models, and a few deep neural networks.Kaggle competitionsThere are five categories of Kaggle competition: Getting Started, Playground, Featured, Research, and Recruitment.Getting Started competitions are semi-permanent, and are meant to be used by new users just getting their foot in the door in the field of machine learning. They offer no prizes or points, but have ample tutorials. Getting Started competitions have two-month rolling leaderboards.Playground competitions are one step above Getting Started in difficulty. Prizes range from kudos to small cash prizes.Featured competitions are full-scale machine learning challenges that pose difficult prediction problems, generally with a commercial purpose. Featured competitions attract some of the most formidable experts and teams, and offer prize pools that can be as high as a million dollars. That might sound discouraging, but even if you don’t win one of these, you’ll learn from trying and from reading other people’s solutions, especially the high-ranked solutions.Research competitions involve problems that are more experimental than featured competition problems. They do not usually offer prizes or points due to their experimental nature.In Recruitment competitions, individuals compete to build machine learning models for corporation-curated challenges. At the competition’s close, interested participants can upload their resume for consideration by the host. The prize is (potentially) a job interview at the company or organization hosting the competition.There are several formats for competitions. In a standard Kaggle competition, users can access the complete datasets at the beginning of the competition, download the data, build models on the data locally or in Kaggle Notebooks (see below), generate a prediction file, then upload the predictions as a submission on Kaggle. Most competitions on Kaggle follow this format, but there are alternatives. A few competitions are divided into stages. Some are code competitions that must be submitted from within a Kaggle Notebook.Kaggle datasetsKaggle hosts over 35 thousand datasets. These are in a variety of publication formats, including comma-separated values (CSV) for tabular data, JSON for tree-like data, SQLite databases, ZIP and 7z archives (often used for image datasets), and BigQuery Datasets, which are multi-terabyte SQL datasets hosted on Google’s servers.InfoWorld review: Nvidia RAPIDS brings Python analytics to the GPUThere are several ways of finding Kaggle datasets. On the Kaggle home page you will find a listing of “hot” datasets and datasets uploaded by people you follow. On the Kaggle datasets page you will find a dataset list (initially ordered by “hottest” but with other ordering options) and a search filter. You can also use tags and tag pages to locate datasets, for example https://www.kaggle.com/tags/crime.You can create public and private datasets on Kaggle from your local machine, URLs, GitHub repositories, and Kaggle Notebook outputs. You can set a dataset created from a URL or GitHub repository to update periodically.At the moment, Kaggle has quite a few COVID-19 datasets, challenges, and notebooks. There have already been several community contributions to the effort to understand this disease and the virus that causes it.Kaggle NotebooksKaggle supports three types of notebook: scripts, RMarkdown scripts, and Jupyter Notebooks. Scripts are files that execute everything as code sequentially. You can write notebooks in R or Python. R coders and people submitting code for competitions often use scripts; Python coders and people doing exploratory data analysis tend to prefer Jupyter Notebooks.Notebooks of any stripe can optionally have free GPU (Nvidia Tesla P100) or TPU accelerators and may use Google Cloud Platform services, but there are quotas that apply, for example 30 hours of GPU and 30 hours of TPUs per week. Basically, don’t use a GPU or a TPU in a notebook unless you need to accelerate deep learning training. Using Google Cloud Platform services may incur charges to your Google Cloud Platform account if you exceed free tier allowances.You can add Kaggle datasets to Kaggle notebooks at any time. You can also add Competition datasets, but only if you accept the rules of the competition. If you wish, you can chain notebooks by adding the output of one notebook to the data of another notebook.Notebooks run in kernels, which are essentially Docker containers. You can save versions of your notebooks as you develop them.You can search for notebooks with a site keyword query and a filter on notebooks, or by browsing the Kaggle homepage. You can also use the Notebook listing; like datasets, the order of notebooks in the list is by “hotness” by default. Reading public notebooks is a good way to learn how people do data science.You can collaborate with others on a notebook multiple ways, depending on whether the notebook is public or private. If it is public, you can grant editing privileges to specific users (everyone can view). If it is private, you can grant viewing or editing privileges.Kaggle public APIIn addition to building and running interactive notebooks, you can interact with Kaggle using the Kaggle command line from your local machine, which calls the Kaggle public API. You can install the Kaggle CLI using the Python 3 installer , and authenticate your machine by downloading an API token from the Kaggle site.The Kaggle CLI and API can interact with competitions, datasets, and notebooks (kernels). The API is open source and is hosted on GitHub at https://github.com/Kaggle/kaggle-api. The README file there provides the full documentation for the command-line tool.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesKaggle community and educationKaggle hosts community discussion forums and micro-courses. Forum topics include Kaggle itself, getting started, feedback, Q&A, datasets, and micro-courses. Micro-courses cover skills relevant to data scientists in a few hours each: Python, machine learning, data visualization, Pandas, feature engineering, deep learning, SQL, geospatial analysis, and so on.All in all, Kaggle is very useful for learning data science and for competing with others on data science challenges. It’s also very useful as a repository for standard public datasets. It’s not, however, a replacement for paid cloud data science services or for doing your own analysis.", "pub_date": "2020-06-30"},
{"title": "Azure Cosmos DB goes serverless", "overview": "Microsoft adds new options to its cloud-scale database.", "image_url": "https://images.idgesg.net/images/article/2018/03/database_futuristic-technology-100752016-large.jpg", "url": "https://www.infoworld.com/article/3588095/azure-cosmos-db-goes-serverless.html", "body": "Azure’s Cosmos DB is one of the platform’s foundations, powering many of its key services. Designed from the ground up as a distributed database, it implements a set of different consistency models allowing you to trade off between performance and latency for your applications. Then there are its different models for working with data, from familiar NoSQL and SQL APIs, to support for Mongo DB’s API, to the Gremlin graph database query engine.There’s enough in Cosmos DB to support most common cloud development scenarios, giving you a consistent data platform that can share data on a global scale. Microsoft often describes it as a “planetary-scale database,” a fitting description.Also on InfoWorld: 13 ways Microsoft Azure beats AWSThe serverless alternative to provisioned throughputFor all the benefits, Cosmos DB does have some downsides; not least its cost. Although there is a relatively limited free option, running it at scale can be expensive, and you need to take that into account when building applications around it. Budgeting for Cosmos DB request units is a complex process that’s hard to get right the first time, especially when you factor in scaling, either manually or automatically.Microsoft has run a preview of a serverless option for Cosmos DB for a while now, based on its core SQL API. It’s an interesting alternative to the traditionally provisioned option. It only charges you when it runs a request and suspends your instance when there’s nothing happening. There will be additional latency in database operations, as your instance needs to spin up when it’s been suspended. Of course there’s a charge for storage, but that’s the same with any Azure database. The initial trial has now been expanded to all of the Cosmos DB APIs, with general availability not too far in the future.Adding a serverless option to Cosmos DB makes a lot of sense for many types of workloads where you’re getting requests in small numbers and in batches. For a small workload with an irregular pattern of operations, a consumption-based pricing model makes a lot of sense—and can save a considerable amount of money over the long term as there is no commitment to provisioned throughput.Costs are low: You pay $0.282 per serverless request unit, for as many as a million RUs in a billing cycle. If you need a more reliable server you can set up an availability zone, though this increases costs by 1.25x. That’s still a reasonable deal, and what you lose in predictability, you gain in lower costs. Storage costs remain the same for both manual and automatic provisioned throughput.Getting started with serverless Cosmos DBJumping in is easy enough. Like a standard Cosmos DB account, you’ll need to provision it to a subscription and add your serverless instance to a resource group. Next choose the API you plan to use for queries, and when asked to choose a capacity mode, pick serverless rather than provisioned throughput. Finally link it to a region, remembering that you can only use serverless in a single Azure region; there is no option for geo-redundancy. You won’t be able to use it with the free tier, either.Once your serverless instance is running you can use its APIs to load data and make queries. Like a standard instance of Cosmos DB, you can build JavaScript functions and triggers that run inside the database, as well as use its many different APIs to manage queries.Serverless Cosmos DB should move out of preview soon, and is adding support for all its APIs, even for its recent Cassandra API. As it’s a public preview, you can set it up and explore its operation straight from the Azure Portal. While in preview there’s no support for ARM or other infrastructure as code deployment tools, though there should be once the service is generally available. You can’t automate configuration and deployment, so you won’t be able to use it as part of a CI/CD (continuous integration/continuous delivery) pipeline for now, as deployments will need to be manual.Building code with serverless Cosmos DBOne place you should get a lot of value from serverless Cosmos DB is in parallel with Azure Functions. The two serverless environments work well together and are ideal for bursty, low-volume, event-driven applications. Serverless Cosmos DB can quickly ramp up from zero to 5,000 request units a second, so if you’re writing code that uses Functions to track error conditions or other alerts, it’s an option for quickly gathering and storing data.Microsoft recommends using it as part of a development environment where you’re capturing data about the requests your full-scale application needs. As provisioning requests units is something of a black art, a serverless implementation running with all your in-database code is a useful development tool. You can set up an operational environment, run your tests, capture the number of requests used, and then use that data to provision throughput for a production deployment.Understanding the serverless limitationsThere are limitations to using a serverless Cosmos DB account. Perhaps the most important is that you don’t get access to multiregion deployments, as serverless accounts only run across a single region. It’s a limitation that makes sense: Multiregion Cosmos DB implementations need multiple instances running at the same time for inter-region replication and consistency. If serverless instances only run when they’re processing requests, then there’s no guarantee another region will be online to handle replication. As a result, there are changes to the Cosmos DB service-level objective for serverless instances, with writes expected to be 30ms or less, and reads 10ms or less.The other key limitation is a maximum 5,000 request units per second. Again, that should be sufficient for most simple or development implementations, but it does require you to keep an eye on your applications and be ready to switch to a provisioned Cosmos DB instance if you regularly go over your limits. At the same time, each serverless container can only store 50GB of data and indexes. Microsoft provides tools in the Azure Portal to help monitor operations, as well as in Azure Monitor.Adding a serverless option to Cosmos DB answers many questions about cost. For low-usage scenarios where you don’t need global coverage, it should be your first choice. Shift to using a provisioned throughput instance only when you’re able to understand your application’s request pattern and can budget accordingly.", "pub_date": "2020-11-03"},
{"title": "Using OPA for cloud-native app authorization", "overview": "How companies like Netflix, Pinterest, Yelp, Chef, and Atlassian use OPA for ‘who-and what-can-do-what’ application policy", "image_url": "https://images.idgesg.net/images/article/2019/09/blue_hexagon_security_secure_network_by_gerd_altmann_cc0_via_pixabay_2400x1600-100811613-large.jpg", "url": "https://www.infoworld.com/article/3586149/using-opa-for-cloud-native-app-authorization.html", "body": "In the cloud-native space, microservice architectures and containers are reshaping the way that enterprises build and deploy applications. They function, in a word,  than traditional monolithic applications.Microservices are far more distributed and dynamic than their traditional counterparts. A single application might have tens or hundreds of microservices, leading potentially to thousands of separate OS-level processes, each with its own API, deployed across multiple data centers all over the world, and spun up and down dynamically. These architectural differences from monolithic applications cause challenges for developers, operations, and security alike.OPA: A general-purpose policy engine for cloud-nativeUsing OPA to safeguard KubernetesAmong the biggest security challenges is authorization: controlling what users and machines can and can’t do within the application. In the pre-cloud era, authorization was solved at the gateway level—at the point at which end-users interacted with the application’s API—and often the downstream requests were assumed to be authorized.For a microservice application running in the cloud, gateway-level authorization is far less secure. If an attacker compromises a single microservice component, they can run the APIs of all the microservices that make up that application, which gives them complete control over it and all of the data it protects. So while internal APIs and microservice architecture enable better time-to-market for the application, they also pose a substantial security risk and need to be protected by deploying authentication and authorization, just like for public APIs.Changing development strategyNot only are the architectural changes brought about by microservices substantial, but the way people develop microservice-based applications is changing radically too. Traditionally, companies had a few highly coordinated development teams who built the application in tandem, and who released software only after intense scrutiny. Today, numerous dev teams work independently on discrete, individual application components, and they release code whenever they see fit. No team is responsible for “the whole app.” The benefit of having numerous teams is that they can all work independently, and therefore deliver software more efficiently overall.However, having numerous teams also means that every team needs to maintain and enforce authorization policies for each of the services they are responsible for. The real challenge is that security, compliance, and operational teams need to understand these authorization policies; they’re responsible for them. And if you ask 10 teams to implement authorization for their service, you’ll end up with 10 different hardcoded implementations in different programming languages, with different enforcement guarantees and different ways of logging unauthorized access attempts.All of this wreaks havoc on the outside teams that need to modify, or even just review, these authorization policies and understand how they are being enforced. Unifying authorization for the applicationA growing number of organizations (e.g. Netflix, Pinterest, Yelp, Chef, Atlassian) are implementing authorization by providing a unified authorization framework for all development teams. That framework provides a single language or GUI for writing policy, a single way of distributing those policies to the services that need them, a single architecture for enforcing those policies, and a single system for logging decisions.A unified framework for authorization often means faster time-to-market, since developers have less code to write. More importantly, a unified framework guarantees that policies are decoupled from the software systems they govern. There are numerous benefits to this fact: Security and compliance teams can review the policies separately from their implementation; policies enforced in multiple places only need to be written once; policies are enforced consistently across services; decisions are logged consistently, enabling easier SIEM analysis; and policies can be updated dynamically to respond to security events.In the end, because authorization is a topic under the purview of security, operations, compliance, and developers, a unified framework provides a natural point at which all of those different teams can collaborate.Using OPA for application authorizationFor many developers, operations, security, and compliance teams, Open Policy Agent (OPA) has become a primary tool for implementing consistent, secure, and scalable authorization across an organization and across the cloud-native ecosystem. OPA is a domain-agnostic, general-purpose policy engine—a building block for creating and implementing consistent and flexible policies. Developers can plug OPA into different layers of the stack for numerous rules-based use cases, from safeguarding Kubernetes, to managing policy consistently across CICD pipelines and public clouds, to implementing in-app authorization.For this latter use case (today’s topic), OPA is growing quickly in popularity, because it lets companies write policies that are understandable across many different dev teams and translatable across each of a company’s applications and their components. While there are numerous aspects of authorization where OPA applies, here we will focus on three of the most popular: service-level authorization, end user authorization, and policy-suite authorization. OPA for service-level authorizationOne popular OPA use case is service-level authorization, or the “what-can-do-what” policies that determine what applications and their components can do and how they talk to each other—if microservice A can talk to microservice B, for instance. When you have dozens or hundreds of microservices components, it’s essential that you have clear, consistent, and scalable rules about which services can call which APIs. From a security perspective, it’s also critical to have policies that limit the ability of attackers to laterally traverse those APIs.In fact, this was an early OPA use case for Netflix, which provides a textbook example. From an operational perspective, security engineers at Netflix needed to ensure that developers could tightly control which applications and services, from the vast back-end Netflix ecosystem, could call the REST API endpoint on their own service. Without such rules, Netflix services might not scale correctly to maintain high availability, as traffic pathways could become heavily congested.Because speed was a critical requirement of the overall system, service-level authorization decisions needed to happen at a sub-millisecond timescale. Netflix also needed to ensure that a potential attacker couldn’t compromise one service and traverse to others, and they needed a way to hot patch  authorization policy across microservices to stop an attack in its tracks.Because OPA provides a consistent policy framework and decouples authorization decisions from the app itself, it provided Netflix with a way to not only let dev teams control service-level authorization for their own apps, but also to update and federate policies across their environment in real time.OPA for end-user authorizationAnother way that app developers use OPA is for end-user authorization. These policies make sure that users (living, breathing human beings) have the right levels of access in the application and can take only certain actions depending on the user’s permissions and dynamic context. For some applications the end-user is a customer, and for others the end-user is an employee. Sometimes the policies are extensions of the service-level authorization policies described earlier, but allow developers or operators to run microservice APIs if they are on-call. This style of policy makes decisions based on the user’s identity, the corporate groups the user is a member of, and possibly other information loaded dynamically into OPA.Other times OPA is used to implement authorization for the application itself. For example, in a banking application, the policy might stop someone from withdrawing $100 from an account they don’t own. In this case, OPA is a building block for developers who must either build this functionality themselves or use a policy that already exists.Using OPA lets developers future-proof their authorization implementation. While in the early days, authorization requirements were often limited, they usually changed over time. OPA’s flexibility ensures you can start with a simple (say role-based) model and grow to a more sophisticated (say attribute-based) model as the requirements evolve. OPA for product-suite authorizationA third popular way to use OPA is to unify authorization across a suite of different products, sometimes known as entitlements services in sectors like banking. Enterprises often have numerous products deployed across the organization, and they need a unified way of implementing authorization across this product suite. Typically, this need comes from one of two places. Either the company is moving their legacy products into the cloud and the on-premises authorization system can’t come with, or the organization wants to improve the user experience across the product suite.Also on InfoWorld: The best open source software of 2020In the latter case, when different products owned by the same organization have different ways of performing authentication and authorization, the user experience can be poor, because it gives the impression that many companies are delivering these products, or that the organization lacks cohesion and organization. In both of these cases, OPA provides companies with a unified way to implement authorization or entitlements across all of their cloud-native products.Regardless of how or where enterprises are building applications in cloud-native environments, there are numerous authorization problems they must overcome—from end-user authorization, to managing microservices APIs, to deploying consistent authorization policies across the organization. OPA, as an open-source tool, is emerging as a clear standard for helping organizations solve these problems—helping to reduce complexity and development time while increasing the scalability and security of development.Open Policy AgentStyra—newtechforum@infoworld.com", "pub_date": "2020-11-04"},
{"title": "Black Friday retailers must be effective, ready, and resilient", "overview": "It’s a matter of deploying the right personalization, search, and offers to maximize conversion rate while avoiding boom fizzle pop. ", "image_url": "https://images.idgesg.net/images/article/2018/02/crisis_survival_strategy_disaster_preparedness_readiness_recovery_continuity_plan_thinkstock_871259810-100749344-large.jpg", "url": "https://www.infoworld.com/article/3587780/black-friday-retailers-must-be-effective-ready-and-resilient.html", "body": "An unprecedented Black Friday is coming. Will consumers run masklessly back into stores and clean out the shelves? Only if you have a sale on Clorox toilet wand refills. Will they swarm your app or website? Maybe. As one data scientist put it, “All predictions are wrong. Some are lucky.”Also on InfoWorld: Black developers tell how the US tech industry could do betterSome retailers will likely continue to see diminished demand even on Black Friday—such as those specializing in men’s slacks. Most retailers have already seen their traffic move online. For this Black Friday, they need to be effective, ready, resilient, and cost-effective.Effectiveness is the conversion rate of visitors to buyers. Ready is measured by throughput and latency, even when lots of traffic comes at once. Resilient is handling whatever 2020 throws at it. Finally, in the cloud era, cost-effectiveness is eating the world. (I felt that eye roll.)Effective front pageThe front page needs to sell to be effective. Any website analytics will tell you the most visited page is the homepage. Analytics will also tell you the homepage has the highest bounce rate. For the front page to be useful, its content needs to be informed by context. are usually the easiest to convert. They did not come to your site through a pay-per-click ad, a search, or a referral. They came directly to your site because they know you. When they arrive at the homepage, it should not be just any promotion or set of items. It should be offers tailored specifically for that customer., the homepage should also be customized. AI, machine learning, or just plain old counting should calculate what other items were bought by people who visited that offer. If the offer is brand new (a cold start), use data from existing customers who clicked on that item., the page should also not be static. As the day wears on, the content should rotate based on what people are buying or searching. Additionally, the front page should make offers to these first-time shoppers to encourage them to identify themselves.Effective searchThe straight keyword search is dead. As a repeat customer, if I type “shoes,” the search should know that I do not mean heels. My customer profile and my past searches should inform my search results. I should have a way out of this personalization by being more specific, but if I type “shoes” I should see running shoes and flip flops, based on my past purchases.While the site may not know a first-time shopper’s preferences, the context should inform future searches. Meaning if I type “skis” and then “goggles” and then “boots” I should see snow boots and ski boots and not cowboy boots.There are plentiful tools for personalizing search and homepages. Some commerce platforms such as BigCommerce or Shopify have much of this built-in. Third-party vendors such as Coveo and Lucidworks offer solutions for this sort of signal capture and machine learning personalization. Effective offersJust for you, we are offering 20% off on de-icing equipment. Seriously, I have had vendors offer me de-icing equipment for all of those frozen winters we get here in Savannah, GA. Context and personalization make offers compelling. If I searched on something, clicked on it, and started to leave the site, the smart play is to offer me something similar or a discount on what I was viewing.If I have searched several related items such as a glass carboy, tubing, malt, and hops, as a result, perhaps I should be offered a bundle such as a homebrew starter kit or the most popular homebrew item.No matter what, offers should be personal (Who am I? What do I like?) and contextual (What else did I search on? Am I leaving, searching for more things, buying something?). Many tools do this kind of personalized offer, including the tools built into commerce suites. There are other third-party offerings such as those by Justuno or Privy.Ready and resilientIn 2020, retailers should use cloud computing, have an elastic architecture, and live on the edge. Most retailers have already moved their core infrastructure to the cloud. Ideally, this should either be a commerce platform or a combination of SaaS and cloud-native solutions. Content delivery networks (CDNs) like Akamai, Cloudflare, or Netlify have now existed for decades. Images, static content, and other resources can be easily cached at the edge of the network to reduce the load on your web server. Even five years ago, CDNs were the luxury of kings or at least the Fortune 500. Today, they are affordable for the rest of us, and there is no good reason not to use them for a modern website.The next step is to ensure the site’s search and database architecture are elastic and resilient. While most ecommerce platforms include search, these search tools may not scale when all personalization tools are enabled. It may be necessary to investigate other solutions. Those solutions should replicate and have some way to scale and to fail gracefully. In short, they should have a modern cluster architecture. There are still many ecommerce sites that use decades-old search technology, especially Endeca. These tools were not designed for modern traffic or the cloud.As for databases, retail is often “overserved.” A NoSQL database that can relax consistency will meet the needs of most retail use cases. Meaning you do not need to pay for a distributed transaction unless the specific use case requires it. Almost any of the NoSQL databases can replicate to multiple data centers and provide sufficient high availability.My recommendation is to that allows for quickly capturing low-latency signal data, retrieving product data, and running real-time analytical queries. There are cloud service provider offerings such as Microsoft’s CosmosDB and third-party vendor products such as Couchbase. Barring that, you could cobble a few different databases together, but that involves paying more for ETL and dealing with greater complexity.Cost-effectiveEven for retailers with more traffic, margin is strained, so no one wants to pay for capacity they are not using. The term “autoscaling” is overused and stretched out. Ask any vendor (be they database, search, or otherwise) and they will tell you that they can scale up. But what happens when you want to  the number of instances you are using? Can that happen without an outage or excessive performance degradation? A lot of technology goes into scaling up. A lot more goes into scaling down.The questions to ask are, “Can I easily remove nodes?” and “How long does it typically take to rebalance for my type of data and load?” You should also ask, “Can I turn off components that I am not using?” and “What is the idle cost of the software just running?”Boom fizzle popIt is 2020. Assume that anything can happen. Things seem to be going boom, fizzle, or pop every day. The stakes are higher this year. They require investing more imagination into “What could go wrong?” Multicloud is not just a buzzword. For global retailers, it is a necessity. There are geographies where a preferred cloud service provider does not reach or where there is only one data center. A global retailer needs alternatives. Nor can you assume your provider will never fail. While cloud providers fail rarely compared to the self-hosted data centers of old, and failures of multiple availability zones are rare, it does happen. Having it happen on Black Friday in 2020 could be catastrophic for some retailers. What if your whizbang personalization component or third-party service chokes? It is one thing if the homepage becomes more generic or does not show an offer on exit. It is a bigger problem if visitors see a 404 inside an iframe or get an hourglass or spinning disc. Also on InfoWorld: Beyond NoSQL: The case for distributed SQLKeep it togetherThe great news about 2020 is that it is almost over. Staying up and converting customers this Black Friday is not fundamentally different than in years past. If anything, there are more options. It is just a matter of deploying the right personalization, search, and offers to maximize conversion rate while ensuring an adequate cloud-native architecture that can handle multiple failures while scaling to meet demand.", "pub_date": "2020-11-04"},
{"title": "IBM adds code risk analyzer to cloud-based CI/CD", "overview": "IBM Cloud Continuous Delivery’s Code Risk Analyzer scans Python, Node.js, and Java source code in Git repositories for security and legal risks ", "image_url": "https://images.idgesg.net/images/article/2019/10/cso_eyeglasses_resting_on_a_field_of_binary_code_review_threat_assessment_examine_inspect_check_vulnerabilities_by_suebsiri_gettyimages-1135006376_2400x1600-100813842-large.jpg", "url": "https://www.infoworld.com/article/3588100/ibm-adds-code-risk-analyzer-to-cloud-based-cicd.html", "body": "Looking to bring security and compliance analytics to devops, IBM has added its Code Risk Analyzer capability to its IBM Cloud Continuous Delivery service.Code Risk Analyzer is described by IBM as a security measure that can be configured to run at the start of a developer’s code pipeline, analyzing and reviewing Git repositories to discover issues with open source code. The goal is to help application teams recognize cybersecurity threats, prioritize application security problems, and resolve security issues. IBM Cloud Continuous Delivery helps provision toolchains, automate tests and builds, and control software quality with analytics.DevSecOps: How to bring security into agile development and CI/CDIBM said that as cloud-native development practices such as microservices and containers change security and compliance processes, it is no longer feasible for centralized operations teams to manage application security and compliance. Developers need cloud-native capabilities such as Code Risk Analyzer to embed into existing workflows. Code Risk Analyzer helps developers ensure security and compliance in routine workflows.In developing Code Risk Analyzer, IBM surveyed source artifacts used by IT organizations in building and deploying applications and in provisioning and configuring Kubernetes infrastructure and cloud services. Existing cloud solutions provide limited security controls across the source code spectrum including vulnerability scanning of application manifests. Thus it is necessary to design a solution that encompasses security and compliance assessment across artifacts.Code Risk Analyzer scans Git-based source code repositories for Python, Node.js, and Java code and performs vulnerability checks, license management checks, and CIS (Center for Internet Security) compliance checks on deployment configurations and generating a “bill of materials” for all dependencies and their sources. Terraform files used to provision cloud services such as Cloud Object Store are scanned to find any security misconfigurations. IBM sought to anchor security controls in standards such as NIST or CIS and to flatten the learning curve while introducing users to new security practices. Developers are shielded from having to understand security definitions and policies, with actionable feedback provided.", "pub_date": "2020-11-04"},
{"title": "The unwavering optimism of Tim O'Reilly", "overview": null, "image_url": "https://images.idgesg.net/images/article/2020/07/timoreilly1600-100850744-large.jpg", "url": "https://www.infoworld.com/article/3564824/the-unwavering-optimism-of-tim-oreilly.html", "body": "For better or worse, Tim O’Reilly has become known as something of an oracle for the technology industry in his forty-year career as a technical publisher, author and venture capitalist, credited with coining terms like Open Source and Web 2.0.Today, O'Reilly finds himself in the interesting position of being both a techno-optimist – for instance, about how artificial intelligence could augment human workers and help solve existential problems like climate change – while also being a fierce critic of the new power centres technology has created, particularly in Silicon Valley.\"I totally think that there is a massive opportunity for us to augment humans to do things, we need the machines,\" O'Reilly told InfoWorld last week, from his home in Oakland, California.With the world facing a rapidly ageing population, and the pressing need to prevent climate catastrophe, \"we'll be lucky if the AI and the robots arrive in time, quite honestly,\" he says.\"There are such enormous challenges facing our society. Inequity and inequality is a huge part of it. But for me, one of the really big ones is climate change,\" he says. \"We have to solve this problem or we're all toast. We're going to need every bit of ingenuity to do that. I think it will become the focus of innovation.\"That change in focus could also lead to an enormous raft of new jobs, he argues – provided the planet shifts away from fossil fuels, and what he describes as the \"Ponzi scheme\" of startup valuations.O’Reilly stops short of pushing for the sweeping radicalism of \"a new socialism\", but he insists that \"we have to design this system for human flourishing.”But what does that look like? How do we reskill the workforce to focus on this new class of problems, while ensuring the spoils are spread evenly, and not concentrated in the hands of big tech companies? Or entrepreneurs like Elon Musk, whom O'Reilly admires.Short of telling people to \"learn to code\", O'Reilly sees a new set of literacies being required if the workforce of the future is to take advantage of the oncoming \"augmentation\" that intelligent systems could enable.\"I think the golden age of the last couple of decades where you can become a programmer and you'll get a job... is sort of over,\" O'Reilly says. \"Programming is now more like being able to read and write. You just have to be able to do it to be able to get the most out of the tools and the environments that you're presented with, whatever they are.\"\"Every working scientist today is a programmer,\" he adds. \"Programming can make a journalist more successful, programming can make a marketer more successful, programming can make a salesperson more successful, programming can make an HR person more successful. Having technical literacy is on the same level as being good at reading, writing, and speaking.\"O'Reilly isn't blind to the trade-offs that society has made for the convenience that certain technologies bring. How does he maintain such a sunny disposition when it comes to the potential of technology in the face of growing inequality, the erosion of privacy, and the disinformation crisis that Silicon Valley has wrought?\"It's quite clear that we're now really aware of the enormous risks of these technologies, the risks for abuse,\" he says, adding that he doesn't believe government should be singled out to solve all of these issues.Although O'Reilly recognises that Congress recently announcing that it will legislate to regulate facial recognition technology is a step in the right direction, he notes that it's not nearly comprehensive enough to truly mitigate the risks. \"We're not really getting to the root of our engagement with the question of what is the governance structure for technologies that are really changing our society,\" he says. Complex problems require complex solutions. Take the recent exodus of advertising revenue from Facebook, where brands such as Unilever and Ben and Jerry's have pulled their marketing dollars from the social network over its policies surrounding hate speech.O'Reilly argues that Facebook is only doing what it is designed to do and has been thus far rewarded by the market for doing: attract as many eyeballs as possible and sell ads against that attention using algorithms.\"If you understand how algorithmic systems work, you realise they are curatorial systems, they represent choices,\" O'Reilly says. \"We need to have a completely different conversation about it. So too with facial recognition, it's on a continuum with all kinds of other technologies that take away people's privacy. On that continuum are things that people like and embrace and want, and things that they don't want.\"There is no silver bullet to solve these issues, but there are some steps that could be taken to realign the priorities of technology companies with those of society at large.\"Until we build ethical principles more broadly into our company governance – which things like the B Corp movement have tried to do – we have to take this as a comprehensive problem, with comprehensive solutions,\" O'Reilly says.As a long-time exponent of the power of open source, where does this community fit in to O'Reilly's vision for technology to help solve society's biggest problems?\"Open source is really challenged in this world, it's not going to be the same thing that it was in the PC era,\" he says.Tracing open source back to its roots, there have always been a plethora of opinions around what open source truly means, from the Free Software Foundation's definition, to the computer scientists at UC Berkley, or the MIT X Window System, which O'Reilly is most closely aligned with.The central idea here is that all code should be openly available to be modified and copied, with the overall aim being to push forward the state of the art.\"If you look at where open source is really thriving it is in areas like science, where there's not that desire to make a lot of money off of this, they just want other people to be able to use this and benefit from it,\" he says.\"That's why, for example, very early on in the open source discussion, I was saying data is going to be the new source of lock-in, we shouldn't be so focused on source code,\" he adds. \"If we had focused a lot more on issues of what it means when somebody controls the data, when somebody controls the algorithms which shape what data people see? That's where the open source discussion needs to be now.\"", "pub_date": "2020-07-03"},
{"title": "Learn PyTorch: The best free online courses and tutorials", "overview": "Look no further than these excellent free resources to master the development of deep learning models using PyTorch", "image_url": "https://images.techhive.com/images/article/2014/04/abstract-fire-rays-100152558-100264577-large.jpg", "url": "https://www.infoworld.com/article/3563527/learn-pytorch-the-best-free-online-courses-and-tutorials.html", "body": "Deep learning continues to be one of the hottest fields in computing, and while Google’s TensorFlow remains the most popular framework in absolute numbers, Facebook’s PyTorch has quickly earned a reputation for being easier to grasp and use.PyTorch has taken the world of deep learning research by storm, outstripping TensorFlow as the implementation framework of choice in submitted papers for AI conferences in the past two years. With recent improvements for producing optimized models and deploying them to production, PyTorch is definitely a framework ready for use in industry as well as R&D labs.Also on InfoWorld: 5 reasons to choose PyTorch for deep learningBut how to get started? You’ll find plenty of books and paid resources available for learning PyTorch, of course. But there are also plenty of resources on the Internet that will help you get to grips with the framework — for absolutely nothing. Plus, some of the free resources are of even higher quality than what you can pay for. Let’s take a look at what is on offer.PyTorch.org tutorialsPerhaps the most obvious place to start is the PyTorch website itself. Along with the usual resources such as an API reference, the website includes more digestible works such as a 60-minute video and text blitz through PyTorch via setting up an image classification model. There are guides for both the standard and the more esoteric features of the framework, and when a new major capability is added, such as quantization or pruning of models, you’ll normally get a quick tutorial on how to implement them in your own applications.On the downside, the code in the various tutorials tends to vary quite a lot, and sometimes standard steps will be missed or passed over in order to show off the feature that the tutorial is concentrating on rather than producing idiomatic PyTorch code. In fairness, the tutorial code has definitely improved over the past couple of years, but you do sometimes have to be a little careful. For this reason, I wouldn’t recommend using the PyTorch website as your primary resource for learning. Nevertheless, it’s a useful resource to have on hand — and the best place to learn how to use the latest new features.Udacity’s and edX’s PyTorch deep learning coursesI’m bundling Udacity’s Introduction to Deep Learning with PyTorch and edX’s Deep Learning with Python and PyTorch together here as they have similar structures, cover a lot of the same ground, and appear to suffer from the same issues. They both have a traditional series of lectures that build up from the foundations of deep learning, introducing you to concept after concept, then tackling more complex scenarios such as image and text classification by the end of the course. This is a completely fine way to go about teaching deep learning, but it does mean that you’ll be sinking some considerable time into the lessons before you get to do anything exciting with PyTorch, unlike, say, what happens with the Fast.ai course.Both the Udacity and edX courses do appear to suffer from being a little out of date in terms of content and PyTorch itself. You won’t learn anything about generative adversarial networks (GANs) or Transformer-based networks in either course, and the Udacity course is based on PyTorch 0.4. This isn’t necessarily a problem, but we’re currently at PyTorch 1.5, so you may find yourself running into deprecation warnings when trying to replicate code on the latest version. If you’re choosing between these two courses, I would give Udacity a slight edge over edX due to the Facebook stamp of approval.Fast.ai’s Practical Deep Learning for CodersSince its beginnings 2016, fast.ai has been the gold standard for free deep learning education. Every year, it has released a new iteration of its two-part course, iterating on the previous incarnation and pushing things forward a little every time. While the first year was based on Keras and TensorFlow, fast.ai switched to PyTorch from year two and hasn’t really looked back (though it has cast a few glances at Swift for TensorFlow).Fast.ai has a somewhat unique approach to teaching deep learning. Other courses devote many of the early lectures and material laying the foundations before you even consider building even the tiniest neural network. Fast.ai is, well, faster. By the end of the first lesson, you’ll have built a state-of-the-art image classifier. This has led to some criticism that the Fast.ai course leans too heavily on “magic” rather than teaching you the basics, but the following lectures do give you a good grounding in what is happening under the covers.And yet, I’d be a little hesitant to recommend Fast.ai as your sole resource for learning PyTorch. Because Fast.ai uses a library on top of the framework rather than pure PyTorch, you tend to learn PyTorch indirectly rather than explicitly. That’s not to say it’s a bad approach; the Part Two Lessons of the 2019 course include an astonishing set of lectures that builds a somewhat-simplified version of PyTorch from scratch, solving bugs in actual PyTorch along the way. (This set of lectures, I think, puts paid to any notion that Fast.ai is too magical, for what it’s worth.) That said, you might want to use Fast.ai in conjunction with another course in order to understand what Fast.ai’s library is doing for you versus standard PyTorch. EPFL’s Deep Learning (EE-559)Next up, how about a course from an actual university? EE-559, taught by François Fleuret at the École Polytechnique Fédérale de Lausanne, in Switzerland, is a traditional university course, with slides, exercises, and video clips. While it begins with the basics, it does ramp up beyond what’s on offer with the Udacity and edX courses by taking in GANs, adversarial samples, and closes out with Attention mechanisms and Transformer models. It also has the advantage of being current with recent PyTorch releases, so you should be confident that you’re learning techniques and code that are not using deprecated features of the framework.Other PyTorch learning resourcesThere are a few more resources that are very useful but perhaps not core to learning PyTorch itself. First, there’s PyTorch Lightning, which some describe as PyTorch’s equivalent to Keras. While I wouldn’t go that far, as PyTorch Lightning is not a complete high-level API for PyTorch, it is a great way of producing organized PyTorch code. Further, it provides implementations of standard boilerplate (for details like training, testing, validation, and taking care of distributed GPU/CPU setups) that you would otherwise end up re-writing for most of your PyTorch work.The documentation on the project’s website includes some good tutorials to get you started. In particular, there’s a wonderful video that shows off the process of converting a normal PyTorch project to PyTorch Lightning. The video really shows off the flexibility and ease-of-use that PyTorch Lightning provides, so definitely have a look at that once you’ve mastered the basics.Second, there’s Huggingface’s Transformers library, which has become the  standard for Transformer-based models over the past 18 months. If you want to do anything approaching state-of-the-art with deep learning and text processing, Transformers is a wonderful place to start. Containing implementations for BERT, GPT-2, and a brace of other Transformer models (with more being added seemingly on a weekly basis), it is an amazing resource. Happily, it also includes a selection of Google Colab notebooks that will get you up and running with the library swiftly.And third, I can’t write his article without mentioning Yannic Kilcher’s explainer videos. These are not PyTorch specific at all, but they are a great way to keep track of current papers and research trends, with clear explanations and discussion. You probably won’t need to watch these when you start learning PyTorch, but by the time you’ve gone through some of the coursework mentioned here, you’ll be wanting to know what else is out there, and Kilcher’s videos point the way.Learning PyTorch deep learningIf you’re looking to learn PyTorch, I think your best bet is to work through both the Fast.ai course and one of the more traditional courses at the same time. (My pick for the companion course would be EE-559, since it stays current with PyTorch.) As a bonus, there’s a Fast.ai book coming out in August that will be one of the best introductory texts for deep learning.Based on the new FastAI2 library (which among other things has a multi-tiered API structure for easier integration with standard PyTorch), the Fast.ai book is likely to be essential for getting started in the field really quickly. And while I recommend buying a physical copy, you can read it all for free in notebook form on GitHub. Dive into the book, and you’ll be telling dogs from cats in no time at all!", "pub_date": "2020-07-06"},
{"title": "How to choose a data analytics platform", "overview": "A brief guide to the analytics lifecycle, the expanding array of tools and technologies, and selecting the right data platform for your needs", "image_url": "https://images.idgesg.net/images/article/2020/07/ifw_ts_analytics__by-monsitj-getty-images_2400x1600-100851119-large.jpg", "url": "https://www.infoworld.com/article/3564537/how-to-choose-a-data-analytics-platform.html", "body": "Whether you have responsibilities in software development, devops, systems, clouds, test automation, site reliability, leading scrum teams, infosec, or other information technology areas, you’ll have increasing opportunities and requirements to work with data, analytics, and machine learning.AnalyticsHow to choose a data analytics platform (InfoWorld)6 best practices for business data visualization (Computerworld)Healthcare analytics: 4 success stories (CIO)SD-WAN and analytics: A marriage made for the new normal (Network World)How to protect algorithms as intellectual property (CSO)Your exposure to analytics may come through IT data, such as developing metrics and insights from agile, devops, or website metrics. There’s no better way to learn the basic skills and tools around data, analytics, and machine learning than to apply them to data that you know and that you can mine for insights to drive actions.Things get a little bit more complex once you branch out of the world of IT data and provide services to data scientist teams, citizen data scientists, and other business analysts performing data visualizations, analytics, and machine learning.How data analysis, AI, and IoT will shape the post-pandemic ‘new normal’First, data has to be loaded and cleansed. Then, depending on the volume, variety, and velocity of the data, you’re likely to encounter multiple back-end databases and cloud data technologies. Lastly, over the last several years, what used to be a choice between business intelligence and data visualization tools has ballooned into a complex matrix of full-lifecycle analytics and machine learning platforms.The importance of analytics and machine learning increases IT’s responsibilities in several areas. For example:IT often provides services around all the data integrations, back-end databases, and analytics platforms.Devops teams often deploy and scale the data infrastructure to enable experimenting on machine learning models and then support production data processing.Network operations teams establish secure connections between SaaS analytics tools, multiclouds, and data centers.IT service management teams respond to data and analytics service requests and incidents.Infosec oversees data security governance and implementations.Developers integrate analytics and machine learning models into applications.Given the explosion of analytics, cloud data platforms, and machine learning capabilities, here is a primer to better understand the analytics lifecycle, from data integration and cleaning, to dataops and modelops, to the databases, data platforms, and analytics offerings themselves.Analytics begins with data integration and data cleaningBefore analysts, citizen data scientists, or data science teams can perform analytics, the required data sources must be accessible to them in their data visualization and analytics platforms.To start, there may be business requirements to integrate data from multiple enterprise systems, extract data from SaaS applications, or stream data from IoT sensors and other real-time data sources.These are all the steps to collect, load, and integrate data for analytics and machine learning. Depending on the complexity of the data and data quality issues, there are opportunities to get involved in dataops, data cataloging, master data management, and other data governance initiatives.We all know the phrase, “garbage in, garbage out.” Analysts must be concerned about the quality of their data, and data scientists must be concerned about biases in their machine learning models. Also, the timeliness of integrating new data is critical for businesses looking to become more real-time data-driven. For these reasons, the pipelines that load and process data are critically important in analytics and machine learning.Databases and data platforms for all types of data management challengesLoading and processing data is a necessary first step, but then things get more complicated when selecting optimal databases. Today’s choices include enterprise data warehouses, data lakes, big data processing platforms, and specialized NoSQL, graph, key-value, document, and columnar databases. To support large-scale data warehousing and analytics, there are platforms like Snowflake, Redshift, BigQuery, Vertica, and Greenplum. Lastly, there are the big data platforms, including Spark and Hadoop.Large enterprises are likely to have multiple data repositories and to use cloud data platforms like Cloudera Data Platform or MapR Data Platform, or data orchestration platforms like InfoWorks DataFoundy, to make all of those repositories accessible for analytics.The major public clouds, including AWS, GCP, and Azure, all have data management platforms and services to sift through. For example, Azure Synapse Analytics is Microsoft’s SQL data warehouse in the cloud, while Azure Cosmos DB provides interfaces to many NoSQL data stores, including Cassandra (columnar data), MongoDB (key-value and document data), and Gremlin (graph data).Data lakes are popular loading docks to centralize unstructured data for quick analysis, and one can pick from Azure Data Lake, Amazon S3, or Google Cloud Storage to serve that purpose. For processing big data, the AWS, GCP, and Azure clouds all have Spark and Hadoop offerings as well.Analytics platforms target machine learning and collaborationWith data loaded, cleansed, and stored, data scientists and analysts can begin performing analytics and machine learning. Organizations have many options depending on the types of analytics, the skills of the analytics team performing the work, and the structure of the underlying data.Analytics can be performed in self-service data visualization tools such as Tableau and Microsoft Power BI. Both of these tools target citizen data scientists and expose visualizations, calculations, and basic analytics. These tools support basic data integration and data restructuring, but more complex data wrangling often happens before the analytics steps. Tableau Data Prep and Azure Data Factory are the companion tools to help integrate and transform data.Analytics teams that want to automate more than just data integration and prep can look to platforms like Alteryx Analytics Process Automation. This end-to-end, collaborative platform connects developers, analysts, citizen data scientists, and data scientists with workflow automation and self-service data processing, analytics, and machine learning processing capabilities.Alan Jacobson, chief analytics and data officer at Alteryx, explains, “The emergence of analytic process automation (APA) as a category underscores a new expectation for every worker in an organization to be a data worker. IT developers are no exception, and the extensibility of the Alteryx APA Platform is especially useful for these knowledge workers.”There are several tools and platforms targeting data scientists that aim to make them more productive with technologies like Python and R while simplifying many of the operational and infrastructure steps. For example, Databricks is a data science operational platform that enables deploying algorithms to Apache Spark and TensorFlow, while self-managing the computing clusters on the AWS or Azure cloud. Now some platforms like SAS Viya combine data preparation, analytics, forecasting, machine learning, text analytics, and machine learning model management into a single modelops platform. SAS is operationalizing analytics and targets data scientists, business analysts, developers, and executives with an end-to-end collaborative platform.David Duling, director of decision management research and development at SAS, says, “We see modelops as the practice of creating a repeatable, auditable pipeline of operations for deploying all analytics, including AI and ML models, into operational systems. As part of modelops, we can use modern devops practices for code management, testing, and monitoring. This helps improve the frequency and reliability of model deployment, which in turn enhances the agility of business processes built on these models.​”Dataiku is another platform that strives to bring data prep, analytics, and machine learning to growing data science teams and their collaborators. Dataiku has a visual programming model to enable collaboration and code notebooks for more advanced SQL and Python developers.Other analytics and machine learning platforms from leading enterprise software vendors aim to bring analytics capabilities to data center and cloud data sources. For example, Oracle Analytics Cloud and SAP Analytics Cloud both aim to centralize intelligence and automate insights to enable end-to-end decisions.Choosing a data analytics platformSelecting data integration, warehousing, and analytics tools used to be more straightforward before the rise of big data, machine learning, and data governance. Today, there’s a blending of terminology, platform capabilities, operational requirements, governance needs, and targeted user personas that make selecting platforms more complex, especially since many vendors support multiple usage paradigms. Businesses differ in analytics requirements and needs but should seek new platforms from the vantage point of what is already in place. For example:Companies that have had success with citizen data science programs and that already have data visualization tools in place may want to extend this program with analytics process automation or data prep technologies.Enterprises that want a toolchain that enables data scientists working in different parts of the business may consider end-to-end analytics platforms with modelops capabilities.Organizations with multiple, disparate back-end data platforms may benefit from cloud data platforms to catalog and centrally manage them.Companies standardizing all or most data capabilities on a single public cloud vendor ought to investigate the data integration, data management, and data analytics platforms offered.With analytics and machine learning becoming an important core competency, technologists should consider deepening their understanding of the available platforms and their capabilities. The power and value of analytics platforms will only increase, as will their influence throughout the enterprise. ", "pub_date": "2020-07-13"},
{"title": "Deeplearning4j: Deep learning and ETL for the JVM", "overview": "Aimed at integrating models with Java applications, Deeplearning4j offers a stack of components for building JVM-based applications that incorporate AI", "image_url": "https://images.idgesg.net/images/article/2020/07/machine-learning-and-mlops-hpe-ezmeral-softwaretg-100851051-large.jpg", "url": "https://www.infoworld.com/article/3567055/deeplearning4j-deep-learning-and-etl-for-the-jvm.html", "body": "Eclipse Deeplearning4j is an open source, distributed, deep learning library for the JVM. Deeplearning4j is written in Java and is compatible with any JVM language, such as Scala, Clojure, or Kotlin. The underlying computations are written in C, C++, and Cuda. Keras will serve as the Python API. Integrated with Hadoop and Apache Spark, Deeplearning4j brings AI to business environments for use on distributed GPUs and CPUs.Deeplearning4j is actually a stack of projects intended to support all the needs of a JVM-based deep learning application. Beyond Deeplearning4j itself (the high-level API), it includes ND4J (general-purpose linear algebra,), SameDiff (graph-based automatic differentiation), DataVec (ETL), Arbiter (hyperparameter search), and the C++ LibND4J (underpins all of the above). LibND4J in turns calls on standard libraries for CPU and GPU support, such as OpenBLAS, OneDNN (MKL-DNN), cuDNN, and cuBLAS.Also on InfoWorld: JDK 15: The new features in Java 15The goal of Eclipse Deeplearning4j is to provide a core set of components for building  that incorporate AI. AI products within an enterprise often have a wider scope than just machine learning. The overall goal of the distribution is to provide smart defaults for building deep learning applications.Deeplearning4j competes, at some level, with every other deep learning framework. The most comparable project in scope is TensorFlow, which is the leading end-to-end deep learning framework for production. TensorFlow currently has interfaces for  Python, C++, and Java (experimental), and a separate implementation for JavaScript. TensorFlow uses two ways of training: graph-based and immediate mode (eager execution). Deeplearning4j currently only supports graph-based execution.PyTorch, probably the leading deep learning framework for research, only supports immediate mode; it has interfaces for Python, C++, and Java. H2O Sparkling Water integrates the H2O open source, distributed in-memory machine learning platform with Spark. H2O has interfaces for Java and Scala, Python, R, and H2O Flow notebooks.Commercial support for Deeplearning4j can be purchased from Konduit, which also supports many of the developers working on the project.How Deeplearning4j worksDeeplearning4j treats the tasks of loading data and training algorithms as separate processes. You load and transform the data using the DataVec library, and train models using tensors and the ND4J library.You ingest data through a  interface, and walk through the data using a . You can choose a  class to use as a preprocessor for your . Use the  for image data, the  if you have a uniform range along all dimensions of your input data, and  for most other cases. If necessary, you can implement a custom  class. objects are containers for the features and labels of your data, and keep the values in several instances of : one for the features of your examples, one for the labels, and two additional ones for masking, if you are using time series data. In the case of the features, the  is a tensor of the size . Typically you’ll divide the data into mini-batches for training; the number of examples in an  is small enough to fit in memory but large enough to get a good gradient.If you look at the Deeplearning4j code for defining models, such as the Java example below, you’ll see that it’s a very high-level API, similar to Keras. In fact, the planned Python interface to Deeplearning4j will use Keras; right now, if you have a Keras model, you can import it into Deeplearning4j.The  class is the simplest network configuration API available in Eclipse Deeplearning4j; for DAG structures, use the  instead. Note that the optimization algorithm (SGD in this example) is specified separately from the updater (Nesterov in this example). This very simple neural network has one dense layer with a  activation function and one output layer with  loss and a  activation function, and is solved by back propagation. More complex networks may also have , , , and others of the two dozen supported layer types and sixteen layer space types.The simplest way to train the model is to call the  method on the model configuration with your  as an argument. You can also reset the iterator and call the  method for as many epochs as you need, or use an .To test model performance, use an  class to see how well the trained model fits your test data, which should not be the same as the training data.Deeplearning4j provides a listener facility help you monitor your network’s performance visually, which will be called after each mini-batch is processed. One of most often used listeners is .Installing and testing Deeplearning4jAt the moment, the easiest way to try out Deeplearning4j is by using the official quick start. It requires a relatively recent version of Java, an installation of Maven, a working Git, and a copy of IntelliJ IDEA (preferred) or Eclipse. There are also a few user-contributed quick starts. Start by cloning the eclipse/deeplearning4j-examples repo to your own machine with Git or GitHub Desktop. Then install the projects with Maven from the dl4j-examples folder.Once the installation is complete, open the dl4j-examples/ directory with IntelliJ IDEA and try running some of the examples.The README under dl4j-examples lists all the examples and briefly describes them. By the way, you can use the IntelliJ IDEA preferences to install a new version of the JDK and apply it to the project.The well-known Iris dataset has just 150 samples and is generally easy to model, although a few of the irises are often misclassified. The model used here is a three-layer dense neural network.Running the Iris classifier shown in the previous figure yields a fairly good fit: accuracy, precision, recall, and F1 score are all ~98%. Note in the confusion matrix that only one of the test cases was misclassified.The Linear Classifier demo runs in a few seconds and generates probability plots for the training and test datasets. The data was generated specifically to be linearly separable into two classes.A multi-layer perceptron (MLP) classification model for the MNIST hand-written digit dataset yields accuracy, precision, recall, and F1 score all ~97% after about 14K iterations. That isn’t as good or as fast as the results of convolutional neural networks (such as LeNet) on this dataset.", "pub_date": "2020-07-20"},
{"title": "Is the carbon footprint of AI too big?", "overview": "Artificial intelligence obviously requires electricity and computers to work its practical magic. But don’t ignore the downstream environmental benefits", "image_url": "https://images.idgesg.net/images/article/2018/01/tree_forest_perspective-100745996-large.jpg", "url": "https://www.infoworld.com/article/3568680/is-the-carbon-footprint-of-ai-too-big.html", "body": "It’s no surprise that AI has a carbon footprint, which refers to the amount of greenhouse gases (carbon dioxide and methane, primarily) that producing and consuming AI releases into the atmosphere. In fact, training AI models requires so much computing power, some researchers have argued that the environmental costs outweigh the benefits. However, I believe they’ve not only underestimated the benefits of AI, but also overlooked the many ways that model training is becoming more efficient. Greenhouse gases are what economists refer to as an “externality” — a cost borne inadvertently by society at large, such as through the adverse impact of global warming, but inflicted on us all by private participants who have little incentive to refrain from the offending activity. Typically, public utilities emit these gases when they burn fossil fuels in order to generate electricity that powers the data centers, server farms, and other computing platforms upon which AI runs.How data analysis, AI, and IoT will shape the post-pandemic ‘new normal’Consider the downstream carbon offsets realized by AI appsDuring the past few years, AI has been unfairly stigmatized as a major contributor to global warming, owing to what some observers regard as its inordinate consumption of energy in the process of model training.Unfortunately, many AI industry observers contribute to this stigma by using an imbalanced formula for calculating AI’s overall carbon footprint. For example, MIT Technology Review published an article a year ago in which University of Massachusetts researchers reported that the energy needed to train a single machine learning model could emit carbon dioxide at nearly five times the lifetime emissions of the average American car.This manner of calculating AI’s carbon footprint does the technology a huge disservice. At the risk of sounding pretentious, this discussion suggests Oscar Wilde’s remark about a cynic being someone who “knows the price of everything and the value of nothing.” I’m not taking issue with the UMass researchers’ finding on the carbon cost of AI training, or with the need to calculate and reduce that cost for this and other human activities. I am curious why the researchers didn’t also discuss the value that AI provides downstream, often indirectly, in reducing human-generated greenhouse gases from the environment.If an AI model delivers a steady stream of genuinely actionable inferences over an application’s life, it should generate beneficial, real-world outcomes. In other words, many AI apps ensure that people and systems take optimal actions in myriad application scenarios. Many of these AI-driven benefits may be carbon-offsetting, such as reducing the need for people to get in their cars, take business trips, occupy expensive office space, and otherwise engage in activities that consume fossil fuels.Perhaps a quick “traveling salesman” thought experiment is in order. Let’s say that a manufacturing company has a national sales force of six people, and each has a company-provided car. If the company implements a new AI-based sales force automation system that enables one of those individuals to do the work of the entire team—such as through improved lead prospecting and route optimization—that organization could conceivably dismiss the other five individuals, scrap their company cars, and close their respective branch offices.So, in one fell swoop, the five-car carbon footprint of the AI model at the heart of the sales force automation app would be entirely offset (and then some) by eliminating the greenhouse gases of exactly five cars, as well as the electricity savings from closing those offices and associated equipment.We might quibble over the feasibility of this particular example, but we must admit that it’s entirely plausible. This thought experiment highlights the fact that AI’s productivity, efficiency, and acceleration benefits often produce downstream efficiencies in energy utilization.I’m not going to argue that every AI application—or even most of them—has a substantial downstream impact on reducing carbon emissions. But I do take issue with observers, such as an AI expert quoted in this recent Wall Street Journal article, who trivialize the productivity impact from AI with statements such as: “If people could see the true cost of these systems, I think we’d have a lot of harder questions about whether that convenience [of AI-based digital assistants, for example] is worth the planetary cost.”Sentiments such as these obscure the fact that (to use digital assistants as an example) many real-world AI use cases deliver “convenience” in the form of data-driven recommendations that help people buy the right product, take the optimal route to their destination, follow the best practice in managing their finances, and so on. Many of these actionable recommendations may have an impact—large or small—on the energy that people use in their homes, offices, cars, and elsewhere.Upstream AI training may drive greater downstream carbon offsetsMany AI apps have the potential to generate downstream carbon offsets that counterbalance the emissions associated with electricity needed to train the underlying models. If AI lets us do more work with only a fraction of the office space, meetings, and travel, the technology will be contributing mightily to the battle against global warming.Consequently, achieving carbon-neutrality in AI apps may very well depend on intensively training the underlying models to be more effective at their assigned tasks. Equivalent to a capital investment, a well-trained AI model may be amortized over time through deployment into future applications.Bear in mind that even as AI developers seek to improve their models’ accuracy, training is not necessarily the resource hog we’ve been led to believe. Consider the following trends that are reducing the carbon footprint of this and other AI pipeline workloads:AI server farms powered by renewable energy sources instead of fossil fuelsMore energy-efficient chipsets, servers, and cloud providers in AI platformsLess time and data needed to train AI modelsGreater adoption of pretrained AI models in real-world appsComparisons of the energy efficiency of different models within the AI devops pipelineDevelopment of AI on “once-for-all” neural networks that can be trained to run with maximum efficiency on many different kinds of processorsAs these trends converge during the next several years, we’re likely to see dramatic drops in the carbon footprint associated with AI training. As that trend intensifies, AI pipelines will become the most environmentally sustainable platforms in the IT universe.", "pub_date": "2020-08-06"},
{"title": "How to choose a cloud machine learning platform", "overview": "12 capabilities every cloud machine learning platform should provide to support the complete machine learning lifecycle", "image_url": "https://images.idgesg.net/images/article/2020/08/ifw_ts_ai_ml_by-just_super-getty-images_2400x1600-100853890-large.jpg", "url": "https://www.infoworld.com/article/3568889/how-to-choose-a-cloud-machine-learning-platform.html", "body": "In order to create effective machine learning and deep learning models, you need copious amounts of data, a way to clean the data and perform feature engineering on it, and a way to train models on your data in a reasonable amount of time. Then you need a way to deploy your models, monitor them for drift over time, and retrain them as needed.You can do all of that on-premises if you have invested in compute resources and accelerators such as GPUs, but you may find that if your resources are adequate, they are also idle much of the time. On the other hand, it can sometimes be more cost-effective to run the entire pipeline in the cloud, using large amounts of compute resources and accelerators as needed, and then releasing them.AI and machine learning5 machine learning success stories: An inside look (CIO)AI at work: Your next co-worker could be an algorithm (Computerworld)How secure are your AI and machine learning projects? (CSO)How to choose a cloud machine learning platform (InfoWorld)How AI can create self-driving data centers (Network World)The major cloud providers — and a number of minor clouds too — have put significant effort into building out their machine learning platforms to support the complete machine learning lifecycle, from planning a project to maintaining a model in production. How do you determine which of these clouds will meet your needs? Here are 12 capabilities every end-to-end machine learning platform should provide. Be close to your dataIf you have the large amounts of data needed to build precise models, you don’t want to ship it halfway around the world. The issue here isn’t distance, however, it’s time: Data transmission speed is ultimately limited by the speed of light, even on a perfect network with infinite bandwidth. Long distances mean latency. The ideal case for very large data sets is to build the model where the data already resides, so that no mass data transmission is needed. Several databases support that to a limited extent.The next best case is for the data to be on the same high-speed network as the model-building software, which typically means within the same data center. Even moving the data from one data center to another within a cloud availability zone can introduce a significant delay if you have terabytes (TB) or more. You can mitigate this by doing incremental updates.The worst case would be if you have to move big data long distances over paths with constrained bandwidth and high latency. The trans-Pacific cables going to Australia are particularly egregious in this respect.Support an ETL or ELT pipelineETL (export, transform, and load) and ELT (export, load, and transform) are two data pipeline configurations that are common in the database world. Machine learning and deep learning amplify the need for these, especially the transform portion. ELT gives you more flexibility when your transformations need to change, as the load phase is usually the most time-consuming for big data.In general, data in the wild is noisy. That needs to be filtered. Additionally, data in the wild has varying ranges: One variable might have a maximum in the millions, while another might have a range of -0.1 to -0.001. For machine learning, variables must be transformed to standardized ranges to keep the ones with large ranges from dominating the model. Exactly which standardized range depends on the algorithm used for the model.Support an online environment for model buildingThe conventional wisdom used to be that you should import your data to your desktop for model building. The sheer quantity of data needed to build good machine learning and deep learning models changes the picture: You can download a small sample of data to your desktop for exploratory data analysis and model building, but for production models you need to have access to the full data.Web-based development environments such as Jupyter Notebooks, JupyterLab, and Apache Zeppelin are well suited for model building. If your data is in the same cloud as the notebook environment, you can bring the analysis to the data, minimizing the time-consuming movement of data.Support scale-up and scale-out trainingThe compute and memory requirements of notebooks are generally minimal, except for training models. It helps a lot if a notebook can spawn training jobs that run on multiple large virtual machines or containers. It also helps a lot if the training can access accelerators such as GPUs, TPUs, and FPGAs; these can turn days of training into hours.Support AutoML and automatic feature engineeringNot everyone is good at picking machine learning models, selecting features (the variables that are used by the model), and engineering new features from the raw observations. Even if you’re good at those tasks, they are time-consuming and can be automated to a large extent.AutoML systems often try many models to see which result in the best objective function values, for example the minimum squared error for regression problems. The best AutoML systems can also perform feature engineering, and use their resources effectively to pursue the best possible models with the best possible sets of features.Support the best machine learning and deep learning frameworksMost data scientists have favorite frameworks and programming languages for machine learning and deep learning. For those who prefer Python, Scikit-learn is often a favorite for machine learning, while TensorFlow, PyTorch, Keras, and MXNet are often top picks for deep learning. In Scala, Spark MLlib tends to be preferred for machine learning. In R, there are many native machine learning packages, and a good interface to Python. In Java, H2O.ai rates highly, as do Java-ML and Deep Java Library.The cloud machine learning and deep learning platforms tend to have their own collection of algorithms, and they often support external frameworks in at least one language or as containers with specific entry points. In some cases you can integrate your own algorithms and statistical methods with the platform’s AutoML facilities, which is quite convenient.Some cloud platforms also offer their own tuned versions of major deep learning frameworks. For example, AWS has an optimized version of TensorFlow that it claims can achieve nearly-linear scalability for deep neural network training.Offer pre-trained models and support transfer learningNot everyone wants to spend the time and compute resources to train their own models — nor should they, when pre-trained models are available. For example, the ImageNet dataset is huge, and training a state-of-the-art deep neural network against it can take weeks, so it makes sense to use a pre-trained model for it when you can.On the other hand, pre-trained models may not always identify the objects you care about. Transfer learning can help you customize the last few layers of the neural network for your specific data set without the time and expense of training the full network.Offer tuned AI servicesThe major cloud platforms offer robust, tuned AI services for many applications, not just image identification. Example include language translation, speech to text, text to speech, forecasting, and recommendations.These services have already been trained and tested on more data than is usually available to businesses. They are also already deployed on service endpoints with enough computational resources, including accelerators, to ensure good response times under worldwide load.Manage your experimentsThe only way to find the best model for your data set is to try everything, whether manually or using AutoML. That leaves another problem: Managing your experiments.A good cloud machine learning platform will have a way that you can see and compare the objective function values of each experiment for both the training sets and the test data, as well as the size of the model and the confusion matrix. Being able to graph all of that is a definite plus.Support model deployment for predictionOnce you have a way of picking the best experiment given your criteria, you also need an easy way to deploy the model. If you deploy multiple models for the same purpose, you’ll also need a way to apportion traffic among them for a/b testing.Monitor prediction performanceUnfortunately, the world tends to change, and data changes with it. That means you can’t deploy a model and forget it. Instead, you need to monitor the data submitted for predictions over time. When the data starts changing significantly from the baseline of your original training data set, you’ll need to retrain your model.Also on InfoWorld: Applying devops in data science and machine learningControl costsFinally, you need ways to control the costs incurred by your models. Deploying models for production inference often accounts for 90% of the cost of deep learning, while the training accounts for only 10% of the cost.The best way to control prediction costs depends on your load and the complexity of your model. If you have a high load, you might be able to use an accelerator to avoid adding more virtual machine instances. If you have a variable load, you might be able to dynamically change your size or number of instances or containers as the load goes up or down. And if you have a low or occasional load, you might be able to use a very small instance with a partial accelerator to handle the predictions.", "pub_date": "2020-08-10"},
{"title": "What will cloud security look like in 3 years?", "overview": "Cloud security has been better than on-premises security for several years now. Increased automation and interoperability will cement its position as a best practice.", "image_url": "https://images.idgesg.net/images/article/2020/10/cso_security_hack_control-center_breach_gettyimages-808157576_by-gorodenkoff_2400x1600px-100861757-large.jpg", "url": "https://www.infoworld.com/article/3591236/what-will-cloud-security-look-like-in-3-years.html", "body": "Gartner states through 2020, public IaaS workloads will suffer at least 60 percent fewer security incidents than workloads in traditional data centers. When I pointed this out several years ago, many scoffed at the claim.Both the hyperscalers and third-party security providers are spending about 70 to 80 percent of their R&D budgets on supporting public clouds. It should be no surprise that the quality and functionality of most cloud security technologies will be superior to traditional on-premises systems.Also on InfoWorld: When hybrid multicloud has technical advantagesWhat do we have coming down the line in terms of cloud security? Here is what I think the landscape will look like in three years, maybe sooner. Some security systems automate existing processes today, but in five years this will be taken to the next level. We’ll have uber-dynamic interactions with potential threats, backed up by a machine learning system, using intercloud and intracloud orchestration of many different resources to find and stop attacks.This moves cloud security from a passive state to an active one. We’re no longer waiting to get attacked; we can detect when an attack is imminent and automatically challenge the attacker with automated defenses before the first penetration attempt. In some cases, we’ll have the ability to launch automated counterattacks. As we move to a multicloud world, we’re finding that using native security systems for each public cloud is way too laborious and causes complexity and confusion that can lead to breaches.  As I’ve stated before, multicloud is really not about cloud. It’s about the technology that exists between the clouds. Technology that has access to native interfaces, but logically runs above all public clouds. This means that you can orchestrate services to put up a unified defense as well as share knowledgebases as to how to best defend against specific kinds of attacks.  You will also need visibility into all major applications, databases, and storage systems within all public clouds; for instance, being able to see a CPU saturation that should be checked as a possible attack.     You may think of a Terminator-like scenario where the machines turn on us, but the reality is that humans are the weakest link in the security chain. Gartner states that through 2025, 99 percent of cloud security failures will be the customer’s fault. In my world, it’s more like 99.999 percent.  No matter if it’s misconfigurations that leave doors open or plain mistakes because of lack of training, the more we factor humans out of the cloud security equation, the more secure we’ll be.    This goes back to the “automate everything” approach that most security systems will use to provide cloud security within three years. If you’re worried about your job, don’t be. Somebody has to set up these automations and continuously improve them over time.   The bottom line is that security will improve, and the cloud will become the safest place to be. As long as the R&D dollars pour into cloud-based security, this is a foregone conclusion.  ", "pub_date": "2020-11-06"},
{"title": "Technology disenfranchisement is now a thing", "overview": "With the cloud taking over R&D dollars and IT budgets, traditional systems are being left behind.   ", "image_url": "https://images.idgesg.net/images/article/2020/09/old_computer_system_covered_in_cobwebs_outdated_obsolete_retro_vintage_in_need_of_an_update_by_pixel_piggettyimages-168267659_2400x1600-100858540-large.jpg", "url": "https://www.infoworld.com/article/3596317/technology-disenfranchisement-is-now-a-thing.html", "body": "We’ve heard of technical debt: the act of leveraging solutions to speed up deployment with an understanding that the solution is not optimal but will have to be fixed later. However, there may be a new category of technical costs that we’ve yet to understand but will very soon.I’m speaking of technical disenfranchisement. Simply put, this is the concept of traditional technology (legacy technology) not having the power to hold onto R&D dollars as everything moves towards more modern platforms (public cloud computing).Also on InfoWorld: AWS vs. Azure vs. Google Cloud: Which free tier is best?Back in 2018 this was such an obvious trend that it generated a blog post around the forced march to cloud computing. It introduced the idea that according to the R&D spending data, the public cloud had become the favored platform. Most of the funding had begun to funnel into cloud-native and third-party technologies that support public clouds. This included security, monitoring and management, governance, and application development—really the core things that keep systems running and outages rare.Trouble is coming for a great deal of enterprises running legacy technology. If all of the technology spending is going towards cloud-based systems, what’s left over for traditional platforms? Those supporting more traditional systems will find themselves disenfranchised relative to the power they wielded just a few years ago.  There will be some significant disadvantages for companies that continue to use disenfranchised technology, including:Lack of updates and fixes to support ongoing systems maintenance. This can be as innocuous as missing a few performance enhancements, to as serious as missing security patches that could prevent a major breach.Little power to influence the product roadmap. If 80 percent of the R&D dollars are spent elsewhere, you won’t get much of a say as to what the product should be, short term or long term.Reduced ROI. Despite the fact that support and enhancement have diminished or will diminish, the license or subscription fees still remain. You’re not likely to receive any discounts for decreased value delivered and increased risk. What can be done? Not much, other than understand how the market is shifting and plan accordingly. Those most at risk are enterprises that support applications and datasets that are contraindicated for public clouds. They may have to stay on-premises or within an MSP and must deal with technical disenfranchisement.This is nothing new, really. We’ve all owned platforms that were discontinued or perhaps purchased and then neglected. Change forces enterprise IT to look for new digs—hopefully ones that won’t lose market share and interest anytime soon.", "pub_date": "2020-11-10"},
{"title": "Microsoft adds a new Linux: CBL-Mariner", "overview": "Azure’s container infrastructure Linux host gets a public outing on GitHub.", "image_url": "https://images.techhive.com/images/article/2016/12/nww_linux_predictions_slide-1-100696919-large.jpg", "url": "https://www.infoworld.com/article/3596347/microsoft-adds-a-new-linux-cbl-mariner.html", "body": "Think of Microsoft and Linux, and you’re likely to think about its work building an optimized Linux kernel for the Windows Subsystem for Linux (WSL). Pushed out through Windows update, Microsoft supports all the WSL2 Linux distributions, including Ubuntu and SUSE.But WSL2’s kernel isn’t Microsoft’s only Linux offering. We’ve looked at some of the others here in the past, including the secure Linux for Azure Sphere. Others include the SONiC networking distribution designed for use with Open Compute Project hardware and used by many public clouds and major online services, and the hosts for Azure ONE (Open Network Emulator) used to validate new networking implementations for Azure.Also on InfoWorld: What is Jamstack? The static website revolution upending web developmentMicrosoft’s Linux Systems GroupWith an ever-growing number of Microsoft Linux kernels and distributions, there’s now an official Linux Systems Group that handles much of the company’s Linux work. This includes an Azure-tuned kernel available as patches for many common Linux distributions, optimizing them for use with Microsoft’s Hyper-V hypervisor, and a set of tools to help deliver policy-based enforcement of system integrity, making distributions more secure and helping manage updates and patches across large estates of Linux servers and virtual machines.The team recently released a new Linux distribution: CBL-Mariner. Although the release is public, much of its use isn’t, as it is part of the Azure infrastructure, used for its edge network services and as part of its cloud infrastructure. The result is a low-overhead, tightly focused distribution that’s less about what’s in it, and much more about what runs on it.Introducing CBL-Mariner: Microsoft’s Linux container hostInvesting in a lightweight Linux such as CBL-Mariner makes a lot of sense, considering Microsoft’s investments in container-based technologies. Cloud economics require hosts to use as few resources as possible, allowing services such as Azure to get a high utilization. At the same time, Kubernetes containers need as little overhead as possible, allowing as many nodes per pod as possible, and allowing new nodes to be launched as quickly as feasible.The same is true of edge hardware, especially the next generation of edge nodes intended for use with 5G networks. Here, like the public cloud, workloads are what’s most important, shifting them and data closer to users. Microsoft uses its growing estate of edge hardware as part of the Azure Content Delivery Network outside its main Azure data centers, caching content from Azure Web apps and from hosted video and file servers, with the aim of reducing latency where possible. The Azure CDN is a key component of its Jamstack-based Azure Static Websites service, hosting pages and JavaScript once published from GitHub.In the past Red Hat’s CoreOS used to be the preferred host of Linux containers, but its recent deprecation means that it’s no longer supported. Anyone using it has had to find an alternative. Microsoft offers the Flatcar Linux CoreOS-fork for Azure users as part of a partnership with developers Kinvolk, but having its own distribution for its own services ensures that it can update and manage its host and container instances on its own schedule. Development in public is available for anyone who wants to make and use their own builds or who wants to contribute new features and optimizations, for example adding support for new networking features.Running CBL-Mariner and containersOut the box, CBL-Mariner only has the basic packages needed to support and run containers, taking a similar approach to CoreOS. At heart, Linux containers are isolated user space. Keeping shared resources to a minimum reduces the security exposure of the host OS by making sure that application containers can’t take dependencies on it. If you’re using CBL-Mariner in your own containers, ensure that you’ve tested any public Docker images before deploying, as they may not contain the appropriate packages. You may need to have your own base images in place as part of your application dockerfiles.CBL-Mariner uses familiar Linux tools to add packages and manage security updates, offering updates either as RPM packages or as complete images that can be deployed as needed. Using RPM allows you to add your own packages to a base CBL-Mariner image to support additional features and services as needed.Getting started with CBL-Mariner can be as simple as firing up an Azure service. But if you want hands-on experience or want to contribute to the project, all the source code is currently on GitHub, along with instructions for building your own installations. Prerequisites for a build on Ubuntu 18.04 include the Go language, the QEMU (Quick EMUlator) utilities, as well as rpm.Build your own installation using the GitHub repositoryYou have several different options for building from the source. Start by checking out the source from GitHub, making a local clone of the project repository. Various branches are available, but for a first build you should choose the current stable branch. From here you can build the Go tools for the project before downloading the sources.For quick builds you have two options, both of which use prebuilt packages and assemble a distribution from them. The first, for bare-metal installs, creates an ISO file ready for install. The second, for using CBL-Mariner as a container host, builds a ready-to-use VHDX file with a virtual machine for use with Hyper-V. An alternative option builds a container image that can be used as a source for your Mariner-based dockerfiles, giving you everything you need to build and run compatible containers with your applications.If you prefer to build from source, the option is available, although builds will be considerably slower than using precompiled packages. However, this will allow you to target alternative CPUs, for example building a version that works with the new generation of ARM-based edge hardware similar to that being used for AWS’s Graviton instances. You can bootstrap the entire build toolchain to ensure that you have control over the whole build process. The full build process can even be used to build supported packages, with the core files listed in a JSON configuration file.Once built, you can start to configure CBL-Mariner’s features. Out the box, these include an iptables-based firewall, support for signed updates, and a hardened kernel. Optional features can be set up at the same time, with tools to improve process isolation and encrypt local storage, important features for a container host in a multitenant environment where you need to protect local data.The result is an effective replacement for CoreOS, and one I’d like to see made available to Azure users as well as to Microsoft’s own teams. CBL-Mariner may not have the maturity of other container-focused Linuxes, but it’s certainly got enough support behind it to make it a credible tool for use in hybrid cloud and edge network architectures, where you’re running code on your own edge servers and in Microsoft’s cloud. If Microsoft doesn’t make it an option, at least you can build it yourself.", "pub_date": "2020-11-10"},
{"title": "The ‘distributed cloud’ isn't emerging — it’s already here", "overview": "Assigning data storage and processing to specific cloud regions is a good move, but still benefits from centralized control and risk awareness.   ", "image_url": "https://images.idgesg.net/images/article/2018/08/1_network_internet_connected_grid_earth_power_satellite-view-100769192-large.jpg", "url": "https://www.infoworld.com/article/3596562/the-distributed-cloud-isnt-emergingits-already-here.html", "body": "According to Gartner, “Distributed cloud is the distribution of public cloud services to different physical locations, while the operation, governance and evolution of the services remain the responsibility of the public cloud provider.” Analysts go on to explain that the distributed cloud provides a flexible agile environment for applications and data that require low-latency, data cost reduction, and data residency.This idea is not new; I’ve used it to remove latency and/or comply with data sovereignty laws from time to time. At its essence, the advantage is for end-users to have cloud computing resources closer to the physical location where the business activities happen, thus reducing latency.Also on InfoWorld: When hybrid multicloud has technical advantagesOf course, analyst groups love to assign buzzwords to emerging patterns, and I’m no different (although more people listen to a large analyst firm than to me). However, lately we seem to be naming things that are…well…already a thing. Distributed cloud falls in that category.First, public clouds have been distributed for some time, allowing developers and architects to assign specific processing and data storage to a single geographical region or regions. Second, leveraging regions to tune processing and data storage for security, performance, and compliance has been a best practice for years.Let’s look at the merits of the distributed cloud and see if this should be an option for your cloud-based workloads. Moving processing and data out to different regions is actually pretty easy. We’ve all assigned regions when provisioning storage or compute resources. However, you do have to think about coordinating these resources, including synchronizing data and distributing processing. Fortunately, we’ve done this for single distributed public clouds and multicloud also. The solutions are pretty well understood, and the technology works effectively. Multicloud is the way that most will experience distributed clouds. Not only will you be distributing processing and data across a single cloud provider, it could be as many as five providers, with processing and data existing on multicloud deployments. This will increase the number of regions you’re deployed on, multiplied by the same number of cloud brands that you’re using. I don’t mean to pop the distributed cloud balloon. It’s a sound architectural option, but you need to understand when to use it and why.", "pub_date": "2020-11-13"},
{"title": "Why AWS leads in the cloud", "overview": "The offshoot of Amazon’s online bookstore has led the public cloud market for a decade. How did it get there? Will its dominance continue? ", "image_url": "https://images.techhive.com/images/article/2017/02/winner-medal-100711135-large.jpg", "url": "https://www.infoworld.com/article/3596813/why-aws-leads-in-the-cloud.html", "body": "The rumors of Amazon Web Services’ fall from the pinnacle were premature. In the push to democratize cloud computing services, AWS had the jump on everyone from the beginning, ever since it was spun out of the mega retailer Amazon in 2002 and launched the flagship S3 storage and EC2 compute products in 2006. It still does. AWS quickly grew into a company that fundamentally transformed the IT industry and carved out a market-leading position, and has maintained that lead — most recently pegged by Synergy Research at almost double the market share of its nearest rival Microsoft Azure, with 33 percent of the market to Microsoft’s 18 percent.Also on InfoWorld: 14 ways AWS beats Microsoft Azure and Google CloudMarket tracker data from IDC for the second half of 2019 also puts AWS in a clear lead, with 13.2 percent of the public cloud services market, narrowly ahead of Microsoft with 11.7 percent.As with any business, Amazon’s cloud success comes down to a confluence of factors: good timing, solid technology, and a parent company with deep enough pockets to make aggressive capital investments early on.There are other, unique factors that have led to the success of AWS, however, including a relentless customer focus, a ruthless competitive streak, and continued commitment to “dogfooding,” or eating your own dog food — a perhaps unfortunate turn of phrase that has proliferated through the tech industry since the late eighties.Dogfooding refers to a company making a bet on its own technology — in Amazon’s case by making it publicly available as a product or service. This is what Amazon did with S3 and EC2 in 2006, and it’s what Amazon has been doing with almost all of its AWS product launches since.We asked the experts how AWS has been able to dominate the public cloud market to date, and, with worldwide adoption of cloud services due to continue climbing, according to the 2020 IDG Cloud Computing Survey, whether AWS can stay on top of the pile for years to come.First-mover advantageThere is no escaping the fact that Amazon’s jump on the competition has put them in the ascendancy from day one, giving them a six-year head start over its nearest competitor, Microsoft Azure.These years didn’t just help position AWS as the dominant cloud computing service provider in people’s minds, it also furnished the company with years of feedback to crunch through and better serve its customer base of software developers, engineers, and architects.“They invented the market space, there wasn’t the concept of public cloud like this before,” Dave Bartoletti, vice president and principal analyst at Forrester said. “We have been renting computing services for 30 or 40 years. Really what AWS did was establish in a corporate environment for a developer or IT person to go to an external service and start a server with a credit card and do computing somewhere else.”As Bartoletti notes, AWS wasn’t just first to market, it also had the deep pockets of its parent company, allowing it to blow anyone else out of the water. “They outspent their rivals,” he bluntly assessed.That being said, not all first-movers lead their market as definitively as AWS is — just ask the founders of Netscape.“Early movers don’t always have an advantage,” Deepak Mohan, research director for cloud infrastructure services at IDC, said, noting that AWS was especially rigorous in creating and bringing products to market. “Being a high-quality company and delivering a high-quality product and being responsive to customer needs all play equally important parts.” Also on InfoWorld: 13 ways Microsoft Azure beats AWSA special relationshipMohan points to Amazon’s superior ability to “eat its own dog food” as a key driver towards its success, as the cloud division had to address significant technology challenges faced by the huge ramping up of scale Amazon was seeing in the aftermath of the dotcom bubble bursting.“You have to consider the relationship between AWS and Amazon the e-commerce company,” said Ed Anderson, distinguished VP analyst at Gartner — which has AWS as its clear leader in its latest Magic Quadrant for Cloud Infrastructure and Platform Services.Just as customers of Google Cloud today want to “run like Google,” early AWS customers wanted to leverage the technology that had enabled Amazon to grow into an e-commerce giant so quickly.“A hallmark of AWS has been how technical and capable it has been,” Anderson notes. “And being really oriented around that ‘builder’ audience of developers, implementers, and architects,” he adds. “As a consequence, the sales team is very technical and capable in having those conversations, which means the experience customers have is really smooth.”Customer obsessionIt is that attention to customer needs that has long been a hallmark of the AWS value proposition, even if they don’t always get it right.As Amazon founder and CEO Jeff Bezos wrote in a 2016 letter to shareholders: “Customers are always beautifully, wonderfully dissatisfied, even when they report being happy and business is great. Even when they don’t yet know it, customers want something better, and your desire to delight customers will drive you to invent on their behalf.”It is this attention to what customers want — and don’t yet know what they want, to paraphrase Steve Jobs, by way of Henry Ford — which has been codified in Amazon’s leadership principles.“Leaders start with the customer and work backwards. They work vigorously to earn and keep customer trust. Although leaders pay attention to competitors, they obsess over customers,” Amazon’s leadership principles state.“That is a value I see exhibited over and over at AWS,” Anderson at Gartner observes. “This attention to customer requirements and the needs of builders and developers and architects, that has prioritized the features they built and is tightly aligned.”“They are incredibly customer focused and everything they build is driven by the customer,” Bartoletti at Forrester adds.” To maintain that as their large pool of customers continues growing gives them the advantage of knowing what their customers want.”Take the 2019 release of the hybrid cloud product AWS Outposts as an example. Instead of squaring neatly with Amazon’s public cloud-centric view of the world, Outposts met the customer’s needs in a different sphere — their on-prem data centers.Also on InfoWorld: 13 ways Google Cloud beats AWSEverything services-firstA key move made by Bezos in the early days of commercial cloud computing was formalizing the way AWS would build and expose products to its customers.Referencing an early-2000s internal email mandate from Bezos, former Amazon and Google engineer Steve Yegee paraphrased in his Google Platforms Rant, from 2011, that: “All teams will henceforth expose their data and functionality through service interfaces. Teams must communicate with each other through these interfaces.” Lastly, “Anyone who doesn’t do this will be fired,” Yegge added.With this mandate, Bezos spurred the creation of an enormous service-oriented architecture, with business logic and data accessible  through application programming interfaces (APIs). “From the time Bezos issued his edict through the time I left [in 2005], Amazon had transformed culturally into a company that thinks about everything in a services-first fashion. It is now fundamental to how they approach all designs, including internal designs for stuff that might never see the light of day externally,” Yegge wrote.The enormous service-oriented architecture had effectively transformed an infrastructure for selling books into an extensible, programmable computing platform. The online bookstore had become a cloud. The everything store for enterprise buildersAll of this has led to an unrivalled breadth and maturity of services available to AWS customers.And while Amazon had the jump on the competition, it hasn’t rested on its laurels, regularly pioneering new services in the public cloud, such as the cloud-based data warehouse Redshift, the high-performance relational database service Aurora, and the event-based serverless computing platform Lambda, after developing the latter service for its AI-driven virtual assistant Alexa.“Yes, Google Cloud and Microsoft have ‘closed the gap,’ but AWS is still more capable on breadth of offerings and the maturity of those individual services,” Anderson at Gartner says. “I would say when it comes to market perception, most customers feel Azure and AWS are effectively on par and Google slightly behind. In terms of pure capability, though, AWS is a more mature architecture and set of capabilities, and the breadth is wider.”At the AWS re:Invent conference in December of 2019, AWS said it had 175 services, with a wealth of options and flavors across compute, storage, database, analytics, networking, mobile, developer tools, management tools, IoT, security, and enterprise applications.“Without doubt the market leader, AWS often wins on developer functionality, due to the breadth of its services as a result of its first-mover advantage,” Nick McQuire, vice president of enterprise research at CCS Insight says. “AWS has also done a good job at translating its scale into economic benefits for customers, although there are times where cloud can be cost prohibitive.”This broad set of capabilities can also be seen as a negative for some, with the service catalog representing a dizzying maze of services and options, but this level of choice has also proved a great resource for engineers.Bartoletti at Forrester, who has called AWS the cloud “everything store” for enterprise builders, points to a key difference in approach. “AWS can have three to four different database services, and they don’t care which one you use, as long as you use it at Amazon,” he notes. “Traditionally vendors would have had to pick one and run with it. That makes AWS tough to compete with.”Also on InfoWorld: 6 ways Alibaba Cloud challenges AWS, Azure, and GCPThe next phase for cloud computingThe age of AWS dominance shows no sign of slowing, but the competition is fierce.“Microsoft has been able to close the gap by being open source focused and commercializing that in their cloud as fast as AWS,” Bartoletti says. “Google is working hard not to over rotate on the bleeding edge and focus on helping enterprises migrate workloads to the cloud.”Breadth and maturity of services, underpinned by strong engineering chops and relentless customer focus, look to keep AWS ahead of the curve for some time. Now, the company’s ability to simplify the adoption of new technology for enterprise customers through managed services will be the litmus test for the next wave of cloud computing adoption. It will also determine how AWS will fare against the ongoing fierce competition from Microsoft Azure and Google Cloud.“I think it is far from given that AWS will always dominate the cloud market,” Mohan at IDC says. At the same time, he acknowledges that the competitors have a lot of catching up to do.“Google is still quite a ways behind, and Microsoft, while a force, has certain advantages in the enterprise market,” Mohan says. “It is conceivable that companies will get closer, but I don’t expect any substantial changes in the next few years... There is a leap in capacity and scale that is yet to be built. All of this gives [AWS] a clearly dominant position for now.”As Warren Buffet said, “Never bet against America.” And when it comes to the public cloud market, we’ve learned it would be just as foolish to bet against Amazon.", "pub_date": "2020-11-16"},
{"title": "Social engineering hacks weaken cybersecurity during the pandemic", "overview": "Disinformation, malware, and an array of cyberattacks are rising as fast as case counts", "image_url": "https://images.idgesg.net/images/article/2020/04/laptop_user_wearing_a_face_mask_works_in_a_darkened_space_covid-19_coronavirus_pandemic_by_engin_akyurt_cc0_via_unsplash_2400x1600-100839367-large.jpg", "url": "https://www.infoworld.com/article/3565197/social-engineering-hacks-weaken-cybersecurity-during-the-pandemic.html", "body": "Cybersecurity inevitably suffers when scares infect the populace. The COVID-19 outbreak appears to be the most acute global crisis since the Second World War.Every aspect of the COVID-19 crisis has been exploited by opportunistic hackers, terrorists, and other criminals. In addition to capitalizing on rampant fear, uncertainty, and doubt, attackers are targeting a fresh new honeypot of federal aid, in the form of payouts from unemployment checks, stimulus checks, and the Paycheck Protection Program.DevSecOps: How to bring security into agile development and CI/CDSocial engineering cyberhacks prey on pandemic anxietiesPervasive social engineering attacks are hindering the world’s coordinated response to the COVID-19 emergency. As noted in this recent press report, cyberattacks have spiked during the first half of 2020. The FBI noted that as of May 28, it had received nearly the same number of complaints for this calendar year as for all of 2019.Preying on social engineering factors, cyberattackers exploit the following facets of society’s collective response to the pandemic:: A swelling number of malicious COVID-19 websites and emails claim to offer useful information on the coronavirus and how to protect oneself. It’s no surprise that thousands of COVID-19 scam and malware sites are being created daily. Many spread false narratives about the COVID-19 outbreak’s progression and impact while stirring anxiety, selling bogus treatments and cures, price gouging for face masks and other needed supplies, and otherwise taking advantage of nervous people’s gullibility.: DDoS attacks have bombarded websites people depend on for their quarantined existence. In addition, hackers are targeting DDoS attacks at the enterprise VPN ports and protocols used for remote access, thereby crippling employees’ ability to get their work done from the coronavirus-free comfort of home. Hackers may initiate thousands of SSL connections to an SSL VPN and then leave them hanging, exhausting memory and thereby preventing legitimate users from using the service.: Phishing attacks have increased. They are frequently cloaked in emails that include pandemic maps or other content related to the coronavirus. In addition, social media is being used as a broadcast platform for predatory and deceptive content, while the companies that run those communities attempt to nip it in the bud. Social engineering tactics in phishing and spam campaigns trick people into disclosing passwords and other sensitive personal and financial information.: People working from home for the first time are acutely exposed to cybersecurity intrusions. Many remote workers may fail to use prudent cybersecurity practices. These lapses often include not securing their passwords effectively, opting not to use multifactor authentication, or neglecting the need for a virtual private network. Corporate IT staff may themselves be working from home, lacking the resources needed to monitor and secure a huge remote workforce’s access to corporate IT assets effectively. In addition, there has been a spurt of voice phishing attacks where callers pretend to be from workplace technical support and thereby convince employees to disclose passwords or to enter authentication information into malicious websites.: More COVID-19-related ransomware attacks via email exploit people and organizations’ increasingly desperate straits due to job losses and the general recession. Some attacks involve hacking enterprise routers to direct users to bogus COVID-19 websites that trick people into downloading malware onto their computers. An uptick in text message phishing perpetrates such scams or dupes targets into loading malicious content onto mobile devices.: Cyberattacks on public-sector healthcare coordinating bodies have ramped up. The U.S. Department of Health and Human Services was recently the target of a cyberattack apparently designed to undermine the country’s response to the coronavirus pandemic. In addition, a state-sponsored hacking group attempted, albeit unsuccessfully, to breach IT systems at the World Health Organization. The FBI has detected cybersecurity attacks against the healthcare industry since the start of the outbreak, such as email fraud campaigns designed to solicit donations for nonexistent healthcare-related organizations and bogus contact-tracing apps that download malware onto a user’s device.Social distancing deepens cybersecurity vulnerabilitiesSocial distancing has become the critical response for flattening the curve of COVID-19. As in-person encounters become less frequent, we’ll have to rely on each person to ensure that they don’t fall victim to these tactics in their myriad virtual and online interactions. That will place more of a burden on the IT infrastructure—and personnel—to guide everybody in the new normal of vigilance against these risks.Exacerbating it all is the fact that many IT professionals have been thrown off balance by their own need to work from home while supporting a vastly expanded home-based workforce. The increasing demand for social distancing, lockdowns, and shutdowns has made it difficult for many IT vendors, including big cloud service providers, to keep the lights on in their facilities. As users find it harder to receive 24x7 support for cybersecurity issues that pop up during the COVID-19 emergency, the attacks on their computers, data, and other online assets will grow.Robotics, postperimeter, and AI are key cyberdefenses against social engineering tacticsIf there’s any hope to reduce society’s exposure to pandemic-stoked social engineering hacks, it comes in the form of AI-driven robotics. To the extent that we can automate more of the tasks in our lives, we’ll reduce the need for human decisions and our vulnerability to cyberscams. Fortunately, the COVID-19 crisis has brought robotic systems to the front lines in every conceivable scenario: in industry, commerce, and the consumer worlds, including (especially) in the back-end data centers that are the beating hearts of the modern economy.Postperimeter security will be another key defense against social engineering hacks in the postpandemic economy. It ensures that users access cloud apps only from managed devices and secure apps. Enterprise IT can block users from falling prey to social engineering tactics, such as requests to connect their mobile devices to unsupported or risky cloud services. In this way, postperimeter security gives people who work from home access to many resources beyond the enterprise perimeter while also giving corporate IT fine-grained control over what, when, and how they do this.Artificial intelligence (AI) will play a pivotal role in postpandemic defenses against social engineering hacks. Automated systems can’t have hard-and-fast rules for detecting the zillion potential cybersecurity attack vectors. But they can use AI’s embedded machine learning models for high-powered pattern recognition, detecting suspicious behavior, and activating effective countermeasures in real time. For example, AI-based defenses can proactively isolate or quarantine threatening components or traffic after determining that a website is navigating to malicious domains or opening malicious files, or after sensing that installed software is engaging in microbehaviors that are characteristic of ransomware attacks.However, AI-based defenses are no panacea, especially when monitoring social engineering attacks that have complex signatures and evolve rapidly. AI-based defenses detect and block abnormal behavioral patterns involving endpoints, or in the network, or in how users interact with devices, applications, and systems. If the AI-learned attack vector is too broad, it’s at risk of blocking an excessive number of legitimate user behaviors as cybersecurity attacks. If the pattern is too narrow, the cybersecurity program risks permitting a wide range of actual attacks to proceed unchecked.Moving forwardThese and other cyberdefenses will crystallize into a new normal for enterprises in the postpandemic era. It’s likely that many people will continue to work from home or, at the very least, switch back and forth between home and traditional offices in their normal routines. As the global community stays on high alert for signs of new pandemics—or recurrence of the present one—safeguards will need to ensure that these anxieties don’t expose enterprise IT assets to social engineering tactics perpetrated by hackers, terrorists, and other criminals.", "pub_date": "2020-07-09"},
{"title": "The ‘intelligent edge’ isn’t always the smartest choice ", "overview": "Just because you can, doesn’t mean you should. Complexity, latency, and network outages may give you pause.", "image_url": "https://images.idgesg.net/images/article/2020/05/edge-computing_iot_edge_bby-mf3d-getty-100842491-large.jpg", "url": "https://www.infoworld.com/article/3597388/the-intelligent-edge-isnt-always-the-smartest-choice.html", "body": "The notion of the intelligent edge has been around for a few years. It refers to placing processing out on edge devices to avoid sending data all the way back to the centralized server, typically existing on public clouds.While not always needed, the intelligent edge is able to leverage machine learning technology at the edge, moving knowledge building away from centralized processing and storage. Applications vary, from factory robotics to automobiles to on-premises edge systems residing in traditional data centers. It’s good in any situation where it makes sense to do the processing as close to the data source as you can get.Also on InfoWorld: Amazon, Google, and Microsoft take their clouds to the edgeWe’ve wrestled with this type of architectural problem for many years. With any distributed system, including cloud computing, you have to consider the trade-off of process and storage placement on different physical or virtual devices. The intelligent edge is no different. Keep in mind that your edge devices to centralized systems are always many-to-one. Managing a centralized systems is fairly simplistic considering that it’s in one virtual location. When you have to manage hundreds or thousands of intelligent edge devices, including configuration management, security, and governance, it becomes an operational nightmare. I’m finding companies that pushed processing and data storage out to the edge often pull them back to the centralized servers just due to management complexity. We depend on networks to keep us connected with edge computers, in many cases mobile and thus connected via cellular networks. You’ll probably have to deal with disconnected situations more often than you like, and you must figure out a way to ensure that these outages and performance issues don’t kill your overall system, both edge and centralized.If you don’t, you’ll find that data does not sync and processing is not managed properly. You may get to a point where the systems become unreliable and untrusted. Try explaining to a commercial pilot that the in-flight engine diagnostics on the intelligent edge failed due to a network problem. The resulting flameout won’t go over well on the flight deck.Of course, not all edge limitations are that profound. Typically you’re making architectural mistakes that won’t be discovered until the system begins to scale. By that time, too much has been committed to the intelligent edge architecture, and fixes require a systemic change. Try telling your boss that. It’s won’t go over well there, either. Make sure you consider the trade-offs.", "pub_date": "2020-11-17"},
{"title": "Public clouds and big tech target low-code capabilities", "overview": "Big players are joining the low-code game, offering platforms to help developers and amateurs create apps for everything imaginable.", "image_url": "https://images.idgesg.net/images/article/2019/06/legos_building-blocks_easy_simple_low-code_no-code-apps_by-iker-urteaga-unsplash-100799751-large.jpg", "url": "https://www.infoworld.com/article/3598071/public-clouds-and-big-tech-target-low-code-capabilities.html", "body": "I’ve been using low-code and no-code platforms for almost two decades to build internal workflow applications and rapidly develop customer-facing experiences. I always had development teams working on Java, .NET, or PHP applications built on top of SQL and NoSQL datastores, but the business demand for applications far exceeded what we could develop. Low-code and no-code platforms provided an alternative option when the business requirements matched the platform’s capabilities.I recently shared seven low-code platforms developers should know and what IT leaders can learn from low-code platform CTOs. Many of these platforms have been around longer than a decade, and some support tens of thousands of business applications. Over time these platforms have improved capabilities, developer experiences, hosting options, enterprise security, devops tools, application integrations, and other competencies that enable rapid development and easy maintainenance of functionally rich applications.Also on InfoWorld: 7 low-code platforms developers should knowSo when the public clouds and big tech companies got more interested in low-code platforms, I was skeptical. First, the public clouds target development and engineering teams that want to code applications, automate CI/CD (continuous integration/continuous deployment) pipelines, and instantiate infrastructure as code. Developing products for citizen developers and others who want to develop with low-code requires different experiences, tools, and functionality.Second, the stand-alone low-code platforms have evolved through multiple computing paradigms; some go back to client-server days. I had my doubts that newcomers would offer matching capabilities and the strategies and motivations to re-engineer to remain relevant.I found a mix of different developer experiences and advanced capabilities from the public cloud and big tech companies. In some cases, the low-code platforms are woefully behind stand-alone platforms. In other cases, they are demonstrating how low-code can enable machine learning, chatbots, voice interfaces, spatial search, and more.Power Apps leads with apps, integration, and machine learningMicrosoft Power Apps and Power Automate (formally Microsoft Flow) went general availability in October 2016, and new versions are released weekly. In that time, Power Apps has evolved into a very rich low-code application development environment to build forms and workflows that connect to multiple data sources. Power Automate now has more than 400 connectors that developers can use to move data in and out of applications and a separate robotic process automation skill.What sets Power Apps apart is AI Builder, a set of low-code capabilities enabling developers to connect data to natural language processing, image processing, and machine learning. You can train models to categorize and identify entities in text, extract information from forms, identify objects in images, or run predictive models. Some examples include training models to pull invoice data from PDF forms or process photos to update product inventory automatically.The newest offering, Power Virtual Agents, a low-code chatbot platform, went general availability at the end of 2019. The combination of building a low-code app with an embedded chatbot and integrating workflow with other SaaS solutions can be very powerful. Some example tutorials include chatbots that help users submit project ideas, answer questions during the COVID crisis, or perform simple functions such as getting today’s weather.Apple’s, Google’s, and Oracle’s ongoing investment in low-code capabilitiesApple and Google have low-code platforms under independently managed subsidiaries. Apple spun off FileMaker back in 1986 and in 2019 they rebranded the company back to its original name, Claris. The low-code platform is now Claris FileMaker, and they added Claris Connect, a platform for integrations, by acquiring Stamplay in 2019.FileMaker is its 19 major version, and Claris recently launched new add-ons, including calendars, Kanban boards, and photo galleries. FileMaker can now integrate with machine learning models and have Siri answer questions in FileMaker applications. The platform has recently been used by a hospital to track and report patient progress and care and by schools to overcome COVID-19 back-to-school challenges.Meanwhile, Google bought its way into the low-code scene by acquiring AppSheet in early 2020. I spoke with Amit Zavery, VP/GM and head of platform at Google Cloud, about AppSheet’s growth and emerging capabilities. “Over the last nine months, we also started to see a lot of end-user applications built on top of AppSheet—initially started with the idea that you can build pretty much a departmental app, but people want to automate approval processes, embed video conferencing, inspect documents, and connect to APIs.”AppSheet has many of the options you find in other low-code platforms, including integrations, forms, and views, but its advanced capabilities demonstrate Google’s plans. For example, the integration of G-Suite and AppSheet can be used to develop intelligent workflows, while developers can enable connection to their APIs using Apigee as a data source.Some interesting AppSheet applications include a med student’s app that organizes related medical conditions and concepts, a housing association’s app that helps manage back-end operations, and an industrial services company’s app that improve fieldwork efficiency. There are also sample applications to help reduce workplace risk during COVID-19 and others developed for specific industries, including nonprofits and manufacturing.Oracle is a longtime proponent of low-code development. I reviewed Oracle Application Express (APEX), which has a history dating back to an acquisition made in 2006. APEX has more than 50,000 customers, and developers are generating more than 3,000 applications daily. APEX enables no-code data-driven applications. As Michael Hichwa, SVP Software Development at Oracle states, “APEX uses the power of SQL in a business context.”One standout feature uses the Oracle database to manage location data exposed through an APEX-developed application. An example is this application to optimize grocery delivery routes during COVID. The APEX.world community site has more than 30 applications related to COVID-19.Amazon, Salesforce, SAP, IBM, and Alibaba increase low-code capabilitiesThe low-code story doesn’t end there, as other public clouds and tech giants add low-code features and platforms.Amazon launched Honeycode in June 2020, a ”very lightweight tool” designed for citizen developers to build Web and mobile applications.Salesforce has a long history of providing developer tools, and its latest additions include Salesforce Lightning and the incorporation of MuleSoft API integration capabilities.SAP partnered with low-code platform provider Mendix to extend S/4HANA functionality and integrate with machine learning, artificial intelligence, IoT (Internet of Things), and other technologies.IBM has several low-code offerings on the IBM Cloud, including a partnership with Mendix and the June 2020 release of IBM Cloud Pak for Automation with low-code integration of machine learning for operational decisions.Alibaba launched Yida Plus in 2019, a low-code development platform used in retail, hospitality, manufacturing, health care, energy, and education.Software and application development are strategic for organizations investing in customer experiences and employee productivity and wanting to become data driven. The tasks of automating, integrating, and developing experiences is getting easier as more public cloud and technology companies enable low-code and citizen development capabilities.", "pub_date": "2020-11-23"},
{"title": "A short guide to AWS Savings Plans", "overview": "The potential for cost cutting is real, but you need to understand the nuances of AWS Savings Plans to reap the biggest savings. ", "image_url": "https://images.techhive.com/images/article/2015/09/piggy-bank-savings-save-100613877-large.jpg", "url": "https://www.infoworld.com/article/3587346/a-short-guide-to-aws-savings-plans.html", "body": "Cloud buying is getting renewed attention as enterprises seek added cost savings after migrating systems to the cloud in response to COVID-19. It’s time for corporate cloud buyers to learn more about the Amazon Web Services (AWS) Savings Plans, launched in November 2019, which provide cost savings of up to 72% for cloud buyers.A recent survey of more than 50 IT executives on Pulse.qa, a social research platform, showed that only 16% of IT executives believe that AWS Savings Plans are saving them money on their cloud spending. While the potential for cost cutting is real, you need to understand the nuances of this offering to ensure you’re reaping the biggest savings.Also on InfoWorld: How to make the most of the AWS free tierHow AWS Savings Plans workAWS Savings Plans involve a one-year to three-year commitment to a consistent amount of usage measured in dollars per hour. For example, if you commit to $100 of compute usage per hour, you will get the Savings Plans prices on that usage up to $100, and AWS will charge you on-demand rates for any usage beyond the commitment. AWS offers two Savings Plans:The Compute Savings Plan is the most flexible of the two plans. Compute pricing on this plan are up to 66% off on-demand rates.The EC2 Instance Savings Plan provides savings up to 72% off on-demand rates. As a customer, you must commit to a specific instance family in your chosen AWS region. This plan applies automatically to usage regardless of size, operating system, and tenancy within the specified family in the region. You can’t increase a Savings Plan budget in the middle of a period of performance. Instead, you’ll need to provision a new and separate Savings Plan.If you’re running a new AWS account, you may encounter some limitations to your Savings Plans. If this happens to you, it’s best to work with your system integrator or AWS account representative to resolve any restrictions you might be facing.One advantage of AWS Savings Plan is being able to apply savings to on-demand instances, instead of only reserved instances. Compute Savings Plans can apply to instances across different AWS regions. You can also apply Savings Plans to Fargate (instance-based PaaS for containers) and Lambda.AWS Savings Plans don’t apply to Relational Database Service (RDS), Redshift, or ElastiCache (in-memory database and cache). Savings Plans also don’t cover any services you provision from the AWS Marketplace.Visualizing the spend is vital to both you as the end customer and your systems integrator. Your AWS utilization and coverage reports offer the ability to understand AWS Savings Plans cost and usage better. These reports show an aggregate Savings Plan utilization trending over time. You can also use these reports to view each of your Savings Plans with their respective utilization, costs, and savings in a tabular format.Navigating AWS Savings PlansAs a cloud buyer who wants to take full advantage of AWS Savings Plans, you need to “shift left” with cloud spending management. While your organization may not have a cloud economist on staff (or even contract), you can use your cloud management platform to track your cloud spending over the past three to six months to get a historical perspective. That historical data should feed your decision-making process to determine if Savings Plans save you money. The data should serve you well in any service provider negotiations.You can get started with AWS Savings Plans from your AWS dashboard by using the AWS Cost Explorer or by using the API/CLI. If you’re buying cloud services through a systems integrator, then it’s best to inquire about their use of Savings Plans.Understanding your cloud workloads is key to making good use of AWS Savings Plans, according to Venkat Ramasamy, COO for FileCloud, an enterprise file sharing, sync, backup, and remote access solution provider. Ramasamy stresses knowing if you’re running a variable or static workload. Unless you’re building and operating SaaS applications, chances are you’re running static loads, so AWS Savings Plans would make sense for you and could save you a lot of money.Scott Dix, senior solutions delivery manager for Cloudtamer.io, a cloud management platform provider, suggests using a cloud roadmap to guide your decision-making process about AWS Savings Plans. With a cloud roadmap in place, you can model your savings from reserved and on-demand instance spending from the early phases of your cloud projects—not after your first cloud billing surprise.AWS Savings Plans and your system integratorThe AWS Savings Plan, when used by a system integrator, has the potential to create more profit margin when a customer orders on-demand EC2 Instances. The aforementioned Pulse.qa survey of IT executives showed that 64% negotiated hard with their system integrators for the best discounts on their cloud spending. Negotiating is essential because Savings Plans can deliver around 25% savings overall for the SI.System integrators run into risks when they don’t put themselves in their customers’ shoes, according to Amir Shariff, VP of product and marketing at Opsani, a cloud optimization solutions provider. He said that SIs need to be an agent for their customer when managing AWS Savings Plans. “Use the golden rule and guide them through bringing their cloud costs down through shifting their cloud resources,” Shariff advises.Shariff emphasizes the need for the SI to know about the customer’s business so they can best advise them on how to lower their cloud spending. You should expect as much from your system integrator. Also on InfoWorld: Which multicloud architecture will win out?AWS Savings Plans and youDix from Cloudtamer.io recommends erring on the side of caution when committing to reserved instances and AWS Savings Plans. “What ends up happening is folks jump in and think the savings are incredible, so let’s go to the maximum and pay for the long term,” he explains. Proceed slowly, keeping a close eye on your usage and savings. Beyond the commitment that Dix mentioned, treat AWS Savings Plans as a cloud economics exercise. Take a systematic approach. Do the historical analysis and make the process adjustments and reporting changes necessary to give you and your stakeholders the full confidence that AWS Savings Plans are working to deliver significant savings on your AWS bill. ", "pub_date": "2020-11-05"},
{"title": "What is quantum computing? Solutions to impossible problems", "overview": "Quantum computing has great promise to solve problems that are too hard for classical computers to solve in reasonable amounts of time, but they are not yet practical", "image_url": "https://images.idgesg.net/images/article/2020/02/microsoft-quantum-computer-source-ms-quantum-100832328-large.jpg", "url": "https://www.infoworld.com/article/3574488/what-is-quantum-computing-solutions-to-impossible-problems.html", "body": "There’s no lack of hype in the computer industry, although even I have to admit that sometimes the technology does catch up to the promises. Machine learning is a good example. Machine learning has been hyped since the 1950s, and has finally become generally useful in the last decade.Quantum computing was proposed in the 1980s, but still isn’t practical, although that hasn’t dampened the hype. There are experimental quantum computers at a small number of research labs, and a few commercial quantum computers and quantum simulators produced by IBM and others, but even the commercial quantum computers still have low numbers of qubits (which I’ll explain in the next section), high decay rates, and significant amounts of noise.Also on InfoWorld: A hands-on look at Amazon Braket quantum computingQuantum computing explainedThe clearest explanation of quantum computing that I’ve found is in this video by Dr. Talia Gershon of IBM. In the video, Gershon explains quantum computing to a child, a teenager, a college student, and a graduate student, and then discusses quantum computing myths and challenges with Professor Steve Girvin from Yale University.To the child, she makes the analogy between bits and pennies. Classical bits are binary, like pennies lying on the table, showing either heads or tails. Quantum bits () are like pennies spinning on the table, which could eventually collapse into states that are either heads or tails.To the teenager, she uses the same analogy, but adds the word  to describe the states of a spinning penny. Superposition of states is a quantum property, commonly seen in elementary particles and in the electron clouds of atoms. In popular science, the usual analogy is the thought experiment of Schrödinger’s Cat, which exists in its box in a superposed quantum state of both alive and dead, until the box is open and it is observed to be one or the other.Gershon goes on to discuss quantum  with the teenager. This means that the states of two or more entangled quantum objects are linked, even if they are separated.By the way, Einstein hated this idea, which he dismissed as “spooky action at a distance,” but the phenomenon is real and observable experimentally, and has recently even been photographed. Even better, light entangled with quantum information has been sent over a 50-kilometer optical fiber.Finally, Gershon shows the teenager IBM’s quantum computer prototype with its dilution refrigerator, and discusses possible applications of quantum computers, such as modeling chemical bonds.With the college student, Gershon goes into more detail about the quantum computer, the quantum chip, and the dilution refrigerator that takes the temperature of the chip down to 10 mK (milliKelvin). Gershon also explains quantum entanglement in more detail, along with quantum superposition and interference. Constructive quantum interference is used in quantum computers to amplify signals leading to the right answer, and destructive quantum interference is used to cancel signals leading to the wrong answer. IBM makes qubits out of superconducting materials.With the grad student, Gershon discusses the possibility of using quantum computers to speed up key parts of the training of deep learning models. She also explains how IBM uses calibrated microwave pulses to manipulate and measure the quantum state (the qubits) of the computing chip.The principal algorithms for quantum computing (discussed below), which were developed before even one qubit had been demonstrated, assumed the availability of millions of perfect, fault-tolerant, error-corrected qubits. We currently have computers with 50 qubits, and they are not perfect. New algorithms under development are intended to work with the limited numbers of noisy qubits we have now.Steve Girvin, a theoretical physicist from Yale, tells Gershon about his work on fault-tolerant quantum computers, which don’t yet exist. The two of them discuss the frustration of quantum decoherence — “You can only keep your information quantum for so long” — and the essential sensitivity of quantum computers to noise from the simple act of being observed. They took a stab at the myths that in five years quantum computers will solve climate change, cancer, and <laughter>. Girvin: “We are currently at the vacuum tube or transistor stage of quantum computing, and we are struggling to invent quantum integrated circuits.”Quantum algorithmsAs Gershon mentioned in her video, the older quantum algorithms assume millions of perfect, fault-tolerant, error-corrected qubits, which are not yet available. Nevertheless, it’s worth discussing two of them to understand their promise and what countermeasures can be used to protect against their use in cryptographic attacks.Grover’s algorithm, devised by Lov Grover in 1996, finds the inverse of a function in O(√N) steps; it can also be used to search an unordered list. It provides a quadratic speedup over classical methods, which need O(N) steps.Other applications of Grover’s algorithm include estimating the mean and median of a set of numbers, solving the collision problem, and reverse-engineering cryptographic hash functions. Because of the cryptographic application, researchers sometimes suggest that symmetric key lengths be doubled to protect against future quantum attacks.Shor’s algorithm, devised by Peter Shor in 1994, finds the prime factors of an integer. It runs in polynomial time in log(N), making it exponentially faster than the classical general number field sieve. This exponential speedup promises to break public-key cryptography schemes, such as RSA, if there were quantum computers with “enough” qubits (the exact number would depend on the size of the integer being factored) in the absence of quantum noise and other quantum-decoherence phenomena.If quantum computers ever become large and reliable enough to run Shor’s algorithm successfully against the sort of large integers used in RSA encryption, then we would need new “post-quantum” cryptosystems that don’t depend on the difficulty of prime factorization.Quantum computing simulation at AtosAtos makes a quantum simulator, the Quantum Learning Machine, which acts as though it has 30 to 40 qubits. The hardware/software package includes a quantum assembly programming language and a Python-based high-level hybrid language. The device is in use at a few national labs and technical universities.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesQuantum annealing at D-WaveD-Wave makes quantum annealing systems such as the DW-2000Q, which are a little different and less useful than general-purpose quantum computers. The annealing process does optimization in a way that is similar to the stochastic gradient descent (SGD) algorithm popular for training deep learning neural networks, except that it allows for many simultaneous starting points and quantum tunneling through local hills. D-Wave computers cannot run quantum programs such as Shor’s algorithm.D-Wave claims that the DW-2000Q system has up to 2,048 qubits and 6,016 couplers. To reach this scale, it uses 128,000 Josephson junctions on a superconducting quantum processing chip, cooled to less than 15 mK by a helium dilution refrigerator. The D-Wave package includes a suite of open-source Python tools hosted on GitHub. The DW-2000Q is in use at a few national labs, defense contractors, and global enterprises.Quantum computing at Google AIGoogle AI is doing research on superconducting qubits with chip-based scalable architecture targeting two-qubit gate error < 0.5%, on quantum algorithms for modeling systems of interacting electrons with applications in chemistry and materials science, on hybrid quantum-classical solvers for approximate optimization, on a framework to implement a quantum neural network on near-term processors, and on quantum supremacy.In 2018 Google announced the creation of a 72-qubit superconducting chip called Bristlecone. Each qubit can connect with four nearest neighbors in the 2D array. According to Hartmut Neven, the director of Google’s Quantum Artificial Intelligence lab, quantum-computing power is increasing on a double-exponential curve, based on the number of conventional CPUs that the lab needs to replicate results from their quantum computers.In late-2019, Google announced that it had achieved quantum supremacy, the condition where quantum computers can solve problems that are intractable on classical computers, using a new 54-qubit processor named Sycamore. The Google AI Quantum team published the results of this quantum supremacy experiment in the  article, “Quantum Supremacy Using a Programmable Superconducting Processor.” Quantum computing at IBMIn the video that I discussed earlier, Dr. Gershon mentions that “There are three quantum computers sitting in this lab that  can use.” She is referring to IBM Q systems, which are built around transmon qubits, essentially niobium Josephson junctions configured to behave like artificial atoms, controlled by microwave pulses that fire microwave resonators on the quantum chip, which in turn address and couple to the qubits on the processor.IBM offers three ways to access its quantum computers and quantum simulators. For “anyone” there is the Qiskit SDK, and a hosted cloud version called the IBM Q Experience (see screenshot below), which also provides a graphical interface for designing and testing circuits. At the next level, as part of IBM Q Network, organizations (universities and large companies) are provided with access to IBM Q’s most advanced quantum computing systems and development tools.Qiskit supports Python 3.5 or later and runs on Ubuntu, macOS, and Windows. To submit a Qiskit program to one of IBM’s quantum computers or quantum simulators, you need IBM Q Experience credentials. Qiskit includes an algorithm and application library, Aqua, which provides algorithms such as Grover’s Search and applications for chemistry, AI, optimization, and finance.IBM unveiled a new generation of IBM Q system with 53 qubits in late-2019, as part of an expanded fleet of quantum computers in the new IBM Quantum Computation Center in New York State. These computers are available in the cloud to IBM’s over 150,000 registered users and nearly 80 commercial clients, academic institutions and research laboratories.IBM Q Experience home screen. Note that only the smaller IBM Q systems (5 to 14 qubits, plus a simulator) are currently available for general use. The larger ones are reserved for commercial clients.Quantum computing at IntelResearch at Intel Labs has led directly to the development of Tangle Lake, a superconducting quantum processor that incorporates 49 qubits in a package that is manufactured at Intel’s 300-millimeter fabrication facility in Hillsboro, Oregon. This device represents the third-generation of quantum processors produced by Intel, scaling upward from 17 qubits in its predecessor. Intel has sent Tangle Lake processors to QuTech in the Netherlands for testing and work on system-level design.Intel is also doing research on spin qubits, which function on the basis of the spin of a single electron in silicon, controlled by microwave pulses. Compared to superconducting qubits, spin qubits far more closely resemble existing semiconductor components operating in silicon, potentially taking advantage of existing fabrication techniques. Spin qubits are expected to remain coherent far longer than superconducting qubits, and to take much less space.Quantum computing at MicrosoftMicrosoft has been researching quantum computers for over 20 years. In the public announcement of Microsoft’s quantum computing effort in October 2017, Dr. Krysta Svore discussed several breakthroughs, including the use of topological qubits, the Q# programming language, and the Quantum Development Kit (QDK). Eventually, Microsoft quantum computers will be available as co-processors in the Azure cloud.The topological qubits take the form of superconducting nanowires. In this scheme, parts of the electron can be separated, creating an increased level of protection for the information stored in the physical qubit. This is a form of topological protection known as a Majorana quasi-particle. The Majorana quasi-particle, a weird fermion that acts as its own anti-particle, was predicted in 1937 and was detected for the first time in the Microsoft Quantum lab in the Netherlands in 2012. The topological qubit provides a better foundation than Josephson junctions since it has lower error rates, reducing the ratio of physical qubits to logical, error-corrected qubits. With this reduced ratio, more logical qubits are able to fit inside the dilution refrigerator, creating the ability to scale.Microsoft has variously estimated that one topological Majorana qubit is worth between 10 and 1,000 Josephson junction qubits in terms of error-corrected logical qubits. As an aside, Ettore Majorana, the Italian theoretical physicist who predicted the quasi-particle based on a wave equation, disappeared in unknown circumstances during a boat trip from Palermo to Naples on March 25, 1938.", "pub_date": "2020-09-09"},
{"title": "Building a hybrid SQL Server infrastructure", "overview": "Pairing your on-prem SQL Server with a cloud-based instance for high availability has its challenges, but they can be overcome. Here’s how.", "image_url": "https://images.techhive.com/images/article/2016/08/puzzle-pieces-problem-solved-teamwork-collaboration-100678164-large.jpg", "url": "https://www.infoworld.com/article/3584425/building-a-hybrid-sql-server-infrastructure.html", "body": "The geographic distribution of cloud data centers makes it easy to configure SQL Server instances for high availability (HA) and/or disaster recovery (DR), but what good is that to you if you’re already invested in a well-tuned SQL Server system that’s firing away on all cylinders on-premises? The idea of ensuring higher levels of availability or easily adding a disaster recovery site may sound very attractive, but the thought of moving  into the cloud to achieve those ends may give rise to cold sweats and nightmares.But here’s the thing: The cloud is not an all-or-nothing proposition. You  mix on-prem and cloud infrastructures to create a  infrastructure. Some parts of your solution will be on-prem, while other parts will be in the cloud. Ensuring that your hybrid infrastructure can deliver the availability support that you want, though, requires an awareness of both the differences between your on-prem and cloud configuration options as well as an understanding of how to bridge those differences.Also on InfoWorld: Amazon, Google, and Microsoft take their clouds to the edgeFeet on the ground, head in the cloudsLet’s assume you want to continue to use your on-prem SQL Server configuration as your primary SQL system. It ain’t broke, as they say, so why move it? But if you want to protect your organization from operational downtime or infrastructure loss that might occur in the event of a catastrophe—a hurricane, for example, or a major earthquake that compromises your on-prem infrastructure—you can put another instance of your SQL Server configuration in the cloud, where it is unlikely to be affected by the local disaster. If a catastrophe takes your on-prem infrastructure offline, the infrastructure in the cloud can take over.One way to look at the cloud, in this scenario, is as if it were a remote data center in which you have a standby server, ready to take over in a moment’s notice. There are two fundamental ways to do this. You could configure a multi-node Always On Availability Group (AG), a feature of SQL Server itself, or you could use Windows Server Failover Clustering (WSFC) to configure a multi-node SQL Server failover cluster instance (FCI).While the former approach is specific to SQL Server and the latter is generic to Windows Server, both approaches enable you to deploy SQL Server on cluster nodes running in geographically separate locations and both orchestrate failover from a primary node (on premises, in this scenario) to a secondary node (in the cloud) in the event of a catastrophe.Hybrid cluster configuration challengesThe challenges you’ll encounter if you view your hybrid architecture this way are twofold. First, you need to ensure that the cloud-based instance of SQL Server can access the data that your on-prem instance has been using, and how you do that depends on the approach you take.If you’re using an AG, the AG can be configured to replicate your user-defined SQL databases from the primary (on-prem) instance of SQL Server to your secondary (cloud-based) instance of SQL Server. However, if you’re using the  AG feature of SQL Server Standard Edition you are limited to replicating from one node to a second (and you are limited to one database per AG). If your on-prem architecture already has two nodes configured as a Basic AG, your cloud-based instance of SQL Server would represent a  node, which Basic AGs do not support. To replicate your on-prem data to that cloud-based instance you’ll have to upgrade your on-prem and cloud-based instances of SQL Server to the Enterprise Edition (2012 or later). That’s a costly upgrade, but that’s the only way you can use AG to support replication of user-defined databases among more than two SQL Server instances.If you’re using WSFC to create a hybrid cluster, you’ll need to think about storage in a different way. Traditionally, Windows Server failover clusters share data on a storage area network (SAN), which all the individual cluster nodes can access. No matter which node in the cluster is running the active instance of SQL Server, it can interact with the databases residing on the SAN. However, there’s no option to share a SAN in the cloud. You  configure a hybrid cluster that binds on-prem and cloud-based infrastructure, but the cloud-based VMs cannot interact with a shared on-prem SAN. Nor can a SAN be put in the cloud to be shared among cloud-based compute nodes. Every failover cluster node in the cloud must be configured with attached storage of its own, and you’ll need to deploy a mechanism to replicate the data from your on-prem storage to the storage attached to each VM running in the cloud.The solution to this challenge is to build a SANless failover cluster using SIOS DataKeeper. SIOS DataKeeper performs block-level replication of all the data on your on-prem storage to the local storage attached to your cloud-based VM. If disaster strikes your on-prem infrastructure and the WSFC fails SQL Server over to the cloud-based cluster node, that cloud-based node can access its own copy of your SQL Server databases and can fill in for your on-prem infrastructure for as long as you need it to.One other advantage afforded by the SANless failover cluster approach is that there is no limit on the number of databases you can replicate. Where you would need to upgrade to SQL Server Enterprise Edition to replicate your user databases to a third node in the cloud, the SANless clustering approach works with both the SQL Server Standard and Enterprise editions. While SQL Server Standard Edition is limited to two nodes in the cluster, DataKeeper allows you to replicate to a third node in the cloud with a manual recovery process. With Enterprise Edition the third node in the cloud can simply be part of the same cluster.Inter-node communications in a hybrid environmentIn addition to addressing the challenge of  to synchronize SQL Server data between your on-prem and cloud-based infrastructures, you need to address the challenge of synchronizing your data securely and expeditiously. Since one of the purposes of a DR infrastructure is to shield you from loss arising from a local disaster, you should plan to locate your DR infrastructure in a cloud data center that’s unlikely to be affected by any localized disaster that would affect your on-prem infrastructure. Such distances are going to require you to use asynchronous replication settings whether you are using an AG approach or a SANless clustering approach, as the distances between the data centers are going to be too great to support synchronous replication.Still, to ensure that your on-prem and cloud-based instances are as close to synchronized as possible, you want to connect them using the fastest network channel you can afford to acquire. AWS, Azure, and Google all offer direct cloud connections as premium options (AWS Direct Connect, Azure Express Route, and GCP Cloud Interconnect). These options provide you with secure, high-speed ways to bypass the public Internet when communicating with your cloud-based infrastructure, but they add further cost to your configuration. If you opt not to use one of these direct connections, you’ll need to configure a VPN channel to secure your connection to the remote infrastructure. Hence your ability to synchronize your data will be gated by the speed of your Internet gateway and the overhead of the VPN you’re using.Note, though, that no matter the speed of your network connection, if you’re using asynchronous replication your on-prem and cloud-based instances may rarely be in perfect harmony. On a busy day, your cloud-based SQL databases may always be slightly behind your active on-prem database. That is not a shortcoming of a hybrid architecture; it’s just the nature of asynchronous replication between any distant points.As a consequence, if there’s a catastrophic incident that prompts an unanticipated failover to your cloud infrastructure, your cloud-based instance of SQL Server may come online without some of the transactions and database updates that had already been committed in the former primary instance. If at some point you can recover the data from the storage associated with the on-prem instance of SQL Server, you may be able to recover and restore those transactions. If not, they may be lost.Hybrid configuration considerations beyond SQL ServerThe failover management components built into AGs and WSFC can ensure a SQL Server failover that is as speedy and as seamless as possible in any disaster scenario. They can orchestrate interactions and updates with ancillary infrastructure components such as load balancers and DNS servers, enabling you to have your cloud instance of SQL Server fully operational, and with minimal data loss, in moments.However, when you’re running SQL Server on-premises and configuring a hybrid architecture for DR, there are additional operational elements you should take into consideration. How you configure elements such as application or terminal servers that may ultimately interact with SQL Server is beyond the scope of this article, but we’ll leave you with a checklist of things to consider when envisioning a hybrid architecture designed for DR.Different disasters have different scopes. An isolated fire in the data center could take your on-prem SQL Server infrastructure offline but leave your application servers and user workstations untouched. An electrical grid issue could take your entire data center offline, making both SQL Server and your application server infrastructure inaccessible, while user workstations in another location remain unaffected. A hurricane could devastate the region, bringing down your SQL Server, applications server, and the workstation infrastructure. These are just examples, for each environment is unique, but each of these scenarios calls for a distinct DR response.: SQL Server alone goes offline. Data center, application servers, and workstation infrastructure are unaffected.DR response:Failover SQL Server to the cloud infrastructure as outlined.Redirect client connections from applications servers to cloud-based SQL infrastructure.: The entire data center goes offline, rendering SQL Server and your application servers inaccessible. User workstations are unaffected.DR response:Failover SQL Server to the cloud infrastructure as outlined.Recover applications servers in the cloud infrastructure using tools such as Azure Site Recovery, AWS CloudEndure, or a third-party product such as Veeam.Redirect client connections to the cloud infrastructure.: The entire data center  the user workstation infrastructure go offline.DR response:Failover SQL Server to the cloud infrastructure as outlined.Recover application servers in the cloud infrastructure using tools such as Azure Site Recovery, AWS CloudEndure, or a third-party product such as Veeam.Redirect client connections to the cloud infrastructure, as in DR scenario 2.Enable users to work from home using remote desktop services running on cloud-based infrastructure.In the end, if your on-prem infrastructure is working for you, run with it. You can take advantage of specific benefits afforded by the cloud—like its ability to provide a DR safety net far more easily than you could build one yourself—without having to move everything into the cloud. Just build that safety net properly and be sure you test it regularly to be sure it can catch you if your on-prem infrastructure takes an unanticipated fall.SIOS Technology", "pub_date": "2020-10-28"},
{"title": "Quantum AI is still years from enterprise prime time", "overview": "Quantum computing’s greatest potential for widespread adoption during this decade is in  artificial intelligence ", "image_url": "https://images.idgesg.net/images/article/2020/02/ms-quantum-computer-microsoft-100832183-large.jpg", "url": "https://www.infoworld.com/article/3546010/quantum-ai-is-still-years-from-enterprise-prime-time.html", "body": "Quantum computing’s potential to revolutionize AI depends on growth of a developer ecosystem in which suitable tools, skills, and platforms are in abundance. To be considered ready for enterprise production deployment, the quantum AI industry would have to, at the very least, reach the following key milestones:Find a compelling application for which quantum computing has a clear advantage over classical approaches to building and training AI.Converge on a widely adopted open source framework for building, training, and deploying quantum AI.Build a substantial, skilled developer ecosystem of quantum AI applications.Also on InfoWorld: The 6 best programming languages for AI developmentThese milestones are all still at least a few years in the future. What follows is an analysis of the quantum AI industry’s maturity at the present time.Lack of a compelling AI application for which quantum computing has a clear advantageQuantum AI executes ML (machine learning), DL (deep learning), and other data-driven AI algorithms reasonably well.As an approach, quantum AI has moved well beyond the proof-of-concept stage. However, that’s not the same as being able to claim that quantum approaches are superior to classical approaches for executing the matrix operations upon which AI’s inferencing and training workloads depend.Where AI is concerned, the key criterion is whether quantum platforms can accelerate ML and DL workloads faster than computers built entirely on classical von Neumann architectures. So far there is no specific AI application that a quantum computer can perform better than any classical alternative. For us to declare quantum AI a mature enterprise technology, there would need to be at least a few AI applications for which it offers a clear advantage—speed, accuracy, efficiency—over classical approaches to processing these workloads.Nevertheless, pioneers of quantum AI have aligned its functional processing algorithms with the mathematical properties of quantum computing architectures. Currently, the chief algorithmic approaches for quantum AI include: This associates quantum-state amplitudes with the inputs and outputs of computations performed by ML and DL algorithms. Amplitude encoding allows for statistical algorithms that support exponentially compact representation of complex multidimensional variables. It supports matrix inversions in which the training of statistical ML models reduces to solving linear systems of equations, such as those in least-squares linear regressions, least-squares version of support vector machines, and Gaussian processes. It often requires the developer to initialize a quantum system in a state whose amplitudes reflect the features of the entire data set.: This uses an algorithm that finds with high probability the unique input to a black box function that produces a particular output value. Amplitude amplification is suitable for those ML algorithms that can be translated into an unstructured search task, such as k-medians and k-nearest neighbors. It can be accelerated through random walk algorithms where randomness comes from stochastic transitions between states, such as in that inherent to quantum superposition of states and the collapse of wave functions due to state measurements.: This determines the local minima and maxima of a machine-learning function over a given set of candidate functions. It starts from a superposition of all possible, equally weighted states of a quantum ML system. It then applies a linear, partial differential equation to guide the time evolution of the quantum-mechanical system. It eventually yields an instantaneous operator, known as the Hamiltonian, that corresponds to the sum of the kinetic energies plus the potential energies associated with the quantum system’s ground state.Leveraging these techniques, some current AI implementations use quantum platforms as coprocessors on select calculation workloads, such as autoencoders, GANs (generative adversarial networks), and reinforcement learning agents.As quantum AI matures, we should expect that these and other algorithmic approaches will show a clear advantage when applied to AI grand challenges that involve complex probabilistic calculations operating over highly multidimensional problem domains and multimodal data sets. Examples of heretofore intractable AI challenges that may yield to quantum-enhanced approaches include neuromorphic cognitive models, reasoning under uncertainty, representation of complex systems, collaborative problem solving, adaptive machine learning, and training parallelization.But even as quantum libraries, platforms, and tools prove themselves out for these specific challenges, they will still rely on classical AI algorithms and functions within end-to-end machine learning pipelines.Lack of a widely adopted open source modeling and training frameworkFor quantum AI to mature into a robust enterprise technology, there will need to be a dominant framework for developing, training, and deploying these applications. Google’s TensorFlow Quantum is an odds-on favorite in that regard. Announced this past March, TensorFlow Quantum is a new software-only stack that extends the widely adopted TensorFlow open source AI library and modeling framework.TensorFlow Quantum brings support for a wide range of quantum computing platforms into one of the dominant modeling frameworks used by today’s AI professionals. Developed by Google’s X R&D unit, it enables data scientists to use Python code to develop quantum ML and DL models through standard Keras functions. It also provides a library of quantum circuit simulators and quantum computing primitives that are compatible with existing TensorFlow APIs.Developers can use TensorFlow Quantum for supervised learning on such AI use cases as quantum classification, quantum control, and quantum approximate optimization. They can execute advanced quantum learning tasks such as meta-learning, Hamiltonian learning, and sampling thermal states. They can use the framework to train hybrid quantum/classical models to handle both the discriminative and generative workloads at the heart of the GANs used in deep fakes, 3D printing, and other advanced AI applications.Recognizing that quantum computing is not yet mature enough to process the full range of AI workloads with sufficient accuracy, Google designed the framework to support the many AI use cases with one foot in traditional computing architectures. TensorFlow Quantum enables developers to rapidly prototype ML and DL models that hybridize the execution of quantum and classic processors in parallel on learning tasks. Using the tool, developers can build both classical and quantum datasets, with the classical data natively processed by TensorFlow and the quantum extensions processing quantum data, which consists of both quantum circuits and quantum operators.Google designed TensorFlow Quantum to support advanced research into alternative quantum computing architectures and algorithms for processing ML models. This makes the new offering suitable for computer scientists who are experimenting with different quantum and hybrid processing architectures optimized for ML workloads.To this end, TensorFlow Quantum incorporates Cirq, an open source Python library for programming quantum computers. It supports programmatic creation, editing, and invoking of the quantum gates that constitute the Noisy Intermediate Scale Quantum (NISQ) circuits characteristic of today’s quantum systems. Cirq enables developer-specified quantum computations to be executed in simulations or on real hardware. It does this by converting quantum computations to tensors for use inside TensorFlow computational graphs. As an integral component of TensorFlow Quantum, Cirq enables quantum circuit simulation and batched circuit execution, as well as estimation of automated expectation and quantum gradients. It also enables developers to build efficient compilers, schedulers, and other algorithms for NISQ machines.In addition to providing a full AI software stack into which quantum processing can now be hybridized, Google is looking to expand the range of more traditional chip architectures on which TensorFlow Quantum can simulate quantum ML. Google also announced plans to expand the range of custom quantum-simulation hardware platforms supported by the tool to include graphics processing units from various vendors as well as its own Tensor Processing Unit AI-accelerator hardware platforms.Google’s latest announcement lands in a fast-moving but still immature quantum computing marketplace. By extending the most popular open source AI development framework, Google will almost certainly catalyze use of TensorFlow Quantum in a wide range of AI-related initiatives.However, TensorFlow Quantum comes into a market that already has several open source quantum-AI development and training tools. Unlike Google’s offering, these rival quantum AI tools come as parts of larger packages of development environments, cloud services, and consulting for standing up full working applications. Here are three full-stack quantum AI offerings:Azure Quantum, announced in November 2019, is a quantum-computing cloud service. Currently in private preview and due for general availability later this year, Azure Quantum comes with a Microsoft open-sourced Quantum Development Kit for the Microsoft-developed quantum-oriented Q# language as well as Python, C#, and other languages. The kit includes libraries for development of quantum apps in ML, cryptography, optimization, and other domains.Amazon Braket, announced in December 2019 and still in preview, is a fully managed AWS service. It provides a single development environment to build quantum algorithms, including ML, and test them on simulated hybrid quantum/classical computers. It enables developers to run ML and other quantum programs on a range of different hardware architectures. Developers craft quantum algorithms using the Amazon Braket developer toolkit and use familiar tools such as Jupyter notebooks.IBM Quantum Experience is a free, publicly available, cloud-based environment for team exploration of quantum applications. It provides developers with access to advanced quantum computers for learning, developing, training, and running AI and other quantum programs. It includes IBM Qiskit, an open source developer tool with a library of cross-domain quantum algorithms for experimenting with AI, simulation, optimization, and finance applications for quantum computers.TensorFlow Quantum’s adoption depends on the extent to which these and other quantum AI full-stack vendors incorporate it into their solution portfolios. That seems likely, given the extent to which all these cloud vendors already support TensorFlow in their respective AI stacks.TensorFlow Quantum won’t necessarily have the quantum AI SDK field all to itself going forward. Other open source AI frameworks—most notably, the Facebook-developed PyTorch—are contending with TensorFlow for the hearts and minds of working data scientists. One expects that rival framework to be extended with quantum AI libraries and tools during the coming 12 to 18 months.We can catch a glimpse of the emerging multitool quantum AI industry by considering a pioneering vendor in this regard. Xanadu’s PennyLane is an open-source development and training framework for AI, executing over hybrid quantum/classical platforms.Launched in November 2018, PennyLane is a cross-platform Python library for quantum ML, automatic differentiation, and optimization of hybrid quantum-classical computing platforms. PennyLane enables rapid prototyping and optimization of quantum circuits using existing AI tools, including TensorFlow, PyTorch, and NumPy. It is device-independent, enabling the same quantum circuit model to be run on different software and hardware back ends, including Strawberry Fields, IBM Q, Google Cirq, Rigetti Forest SDK, Microsoft QDK, and ProjectQ.Lack of a substantial and skilled developer ecosystemAs killer apps and open source frameworks mature, they are sure to catalyze a robust ecosystem of skilled quantum-AI developers who are doing innovative work driving this technology into everyday applications.Increasingly, we’re seeing the growth of a developer ecosystem for quantum AI. Each of the major quantum AI cloud vendors (Google, Microsoft, Amazon Web Services, and IBM) is investing heavily in enlarging the developer community. Vendor initiatives in this regard include the following:", "pub_date": "2020-05-28"},
{"title": "Figuring out programming for the cloud", "overview": "A new model of declarative programming languages has emerged for building infrastructure as code, promising more simplicity and safety than imperative languages. ", "image_url": "https://images.techhive.com/images/article/2015/12/to_reach_out_to_the_sky_by_myrzik137-100633619-large.jpg", "url": "https://www.infoworld.com/article/3597054/figuring-out-programming-for-the-cloud.html", "body": "In the last dozen years or so, we’ve witnessed a dramatic shift from general purpose databases (Oracle, SQL Server, etc.) to purpose-built databases (360 of them and counting). Now programming languages seem to be heading in the same direction. As developers move to API-driven, highly elastic infrastructure (where resources may live for days instead of years), they’re building infrastructure as code (IaC). But  to build IaC remains an open question. For a variety of reasons, the obvious place to start was with imperative languages like C or JavaScript, telling a program  to achieve a desired end state.Also on InfoWorld: Tim O’Reilly says the golden age of the programmer is overBut a new model of declarative languages — like HCL from HashiCorp or Polar from Oso — has emerged, where the developer tells the program  the desired end state is, not overly worrying about how it gets to that state. Of the two approaches, declarative programming might well be the better option, resulting in leaner, safer code. Let’s look at why this is.The why of declarative programmingWhen I asked Sam Scott, co-founder and CTO of Oso, whether we really needed another programming language, his answer was “yes.” The longer version: “With imperative languages there is often a mismatch between the language and its purpose; that is, these languages were designed for people to build apps and scripts from the ground up, as opposed to defining configurations, policies, etc.”Introducing new, declarative languages like Oso’s Polar may actually save us from language proliferation, Scott went on: “It’s not just about introducing another language for solving a specific set of problems. Rather, it’s creating something to avoid people from having to invent their own language time and time again to do some form of embedded logic.”Take, for example, the JSON code written for AppSync authorization:To get to balance between data and logic, the developer has written inline, comment-style permissions in Apache Velocity Template Language. This is just one example of the gymnastics developers go through to try to express authorization (“authZ”) logic using a combination of static configuration and templating.But, really, this isn’t about whether we should use imperative programming, VMware’s Jared Rosoff stressed in an interview. Instead, “It’s a question of ‘We don’t currently use a programming language to express authorization rules, but maybe we should…’.” After all, we use data, not a programming language, to express authZ rules. This is fine, Rosoff notes, when your authZ rules are pretty simple. You basically have a lookup table that you consult when a request comes in to decide if it’s authorized or not. Easy, peasy.Imperative may not fit IaCBut the AppSync example above shows how the data-oriented, imperative approach gets complicated quickly. “What started as a relatively simple JSON document evolved into something with branching and conditionals and variables,” Rosoff continued. In taking this approach, the developer is trying to recreate language-like semantics within the syntax of static data, resulting in a “crappy programming language that has terrible syntax, no libraries, no debuggers, etc.” It’s not ideal.It’s also not concise, says Scott. “It’s hard to encode your authorization logic in traditional languages,” he notes. “Doing it in a special-purpose, declarative language like Polar is more expressive and more concise.”As Scott further explained, there are a lot of problem domains where if a developer were to try to encode a process in Java or Python it would require thousands of lines of code, which is a problem in and of itself, but becomes more problematic in that it obscures the actual business logic. By contrast, a declarative language can accomplish the same job but in tens of lines of code instead.So instead of forklifting configuration syntax onto imperative languages, says Rosoff, we should be using little programs to handle authZ and related functions. There are many programming problems today where we use data to configure a system but it might make more sense to use a program, according to Rosoff. To name a few:AuthorizationDeployment templates (e.g. Terraform, Ansible, etc.). “Each of these tries to create language-like constructs within the confines of a data language with terrible results.” Pulumi offers an interesting approach to this.Workflows. Temporal is an interesting example of taking what would have previously been done in a diagrammatic workflow language and bringing it into a more general purpose programming language format.Just enough freedom, with simplicity and safetyThe trick, says Rosoff, is to give the programmer enough of a language to express the authorization rule, but not so much freedom that they can break the entire application if they have a bug. How does one determine which language to use? Rosoff offers three decision criteria:Does the language allow me to express the complete breadth of programs I need to write? (In the case of authorization, does it let me express all of my authZ rules?)Is the language concise? (Is it fewer lines of code and easier to read and understand than the YAML equivalent?)Is the language safe? (Does it stop the programmer from introducing defects, even intentionally?)We still have a ways to go to make declarative languages the easy and obvious answer to infrastructure-as-code programming. One reason developers turn to imperative languages is that they have huge ecosystems built up around them with documentation, tooling, and more. Thus it’s easier to start with imperative languages, even if they’re not ideal for expressing authorization configurations in IaC.We also still have work to do to make the declarative languages themselves approachable for newbies. This is one reason Polar, for example, tries to borrow imperative syntax.Given the benefits of a declarative approach, however, it may simply be a matter of time until they become standard for IaC.", "pub_date": "2020-11-16"},
{"title": "Data science during COVID-19: Some reassembly required", "overview": "Most likely, the assumptions behind your data science model or the patterns in your data did not survive the coronavirus pandemic. Here’s how to address the challenges of model drift", "image_url": "https://images.idgesg.net/images/article/2019/11/broken-light-bulb_innovation-fail_fragile_binary_by-patpitchaya-getty_thinkstock-2400x1600-100817834-large.jpg", "url": "https://www.infoworld.com/article/3561290/data-science-during-covid-19-some-reassembly-required.html", "body": "The enormous impact of the COVID-19 pandemic is obvious. What many still haven’t realized, however, is that the impact on ongoing data science production setups has been dramatic, too. Many of the models used for segmentation or forecasting started to fail when traffic and shopping patterns changed, supply chains were interrupted, and borders were locked down.In short, when people’s behavior changes fundamentally, data science models based on prior behavior patterns will struggle to keep up. Sometimes, data science systems adapt reasonably quickly when the new data starts to represent the new reality. In other cases, the new reality is so fundamentally different that the new data is not sufficient to train a new system. Or worse, the base assumptions built into the system just don’t hold anymore, so the entire process from model creation to production deployment must be revisited.Also on InfoWorld: The best free data science courses during quarantineThis post describes different scenarios and a few examples of what happens when old data becomes completely outdated, base assumptions are no longer valid, or patterns in the overall system change. I then highlight some of the challenges data science teams face when updating their production system and conclude with a set of recommendations for a robust and future-proof data science setup.Data science impact scenario: Data and process changeThe most dramatic scenario is a complete change of the underlying system — one that not only requires an update of the data science process but also a revision of the assumptions that went into its design in the first place. This requires a full new data science creation and productionization cycle: understanding and incorporating business knowledge, exploring data sources (possibly to replace data that doesn’t exist anymore), and selecting and fine-tuning suitable models. Examples include traffic predictions (especially near suddenly closed borders), shopping behavior under more or less stringent lockdowns, and healthcare-related supply chains.A subset of the above is the case where the availability of the data has changed. An illustrative example here is weather predictions, where quite a bit of data is collected by commercial passenger aircraft that are equipped with additional sensors. With the grounding of those aircraft, the volume of available data has been drastically reduced. Because base assumptions about weather systems remain the same (ignoring for a moment that changes in pollution and energy consumption may affect the weather as well) “only” a retraining of the existing models may be sufficient. However, if the missing data represents a significant portion of the information that went into model construction, the data science team would be wise to rerun the model selection and optimization process as well.Data science impact scenario: Data changes, process remains the sameIn many other cases, the base assumptions remain the same. For example, recommendation engines will still work very much the same, but some of the dependencies extracted from the data will change. This is not necessarily very different from, say, a new bestseller entering the charts, but the speed and magnitude of change may be far bigger — as we saw with the sudden spike in demand for health-related supplies. If the data science process has been designed flexibly enough, its built-in change detection mechanism should quickly identify the shift and trigger a retraining of the underlying rules. Of course, that presupposes that change detection was in fact built-in and that the retrained system achieves sufficient quality levels.Also on InfoWorld: Applying devops in data science and machine learningData science impact scenario: Data and process continue to workThis brief list is not complete without stressing that many data science systems will continue to work just as they always have. Predictive maintenance is a good example. As long as the usage patterns stay the same, engines will continue to fail in exactly the same ways as before. The important question for the data science team is: Are you sure? Is your performance monitoring setup thorough enough that you can be sure you are not losing quality? Do you even know when the performance of your data science system changes?As noted in the first two impact scenarios above, change to your data science system could happen abruptly (when borders are closed from one day to the next, for example) or only gradually over time. Some of the bigger economic impacts will become apparent in customer behavior only over time. For example, in the case of a SaaS business, customers may not cancel their subscriptions overnight but over coming months.Model drift detection is keyOne most often encounters two types of production data science setups. There are the older systems that were built, deployed, and have been running for years without any further refinements, and then there are the newer systems that may have been the result of a consulting project, possibly even a modern automated machine learning (AutoML) type of project. In both cases, if you are fortunate, automatic handling of partial model change has been incorporated into the system, so at least some model retraining is handled automatically. However, none of the currently available AutoML tools allow for performance monitoring and automatic retraining, and usually the older, “one shot” projects don’t worry about that either. As a result, you may not even be aware that your data science process has failed.If you are lucky to have a setup where the data science team has made numerous improvements over the years, chances are higher that automatic model drift detection and retraining have been built-in. However, even then (and especially in the case where a complete model change is required) it is far more likely that the system cannot easily be recreated. Unless all of the steps of your data science process are well documented, and the experts who wrote the code are still with the company, it will be difficult to revisit the assumptions and update the process. The only solution may be to start an entirely new project.Reinvention vs. reassemblyObviously, if your data science process was set up by an external consulting team, you don’t have much of a choice other than to bring them back in. If your data science process is the result of an automated machine learning service, you may be able to re-engage that service, but especially in the case of the change in business dynamics, you should expect to be involved quite a bit—similar to the first time you embarked on this project. Also on InfoWorld: How dataops improves data, analytics, and machine learningOne side note here: Be skeptical when someone pushes for supercool new methods. In many cases, a new approach is not needed. Rather, one should focus on carefully revisiting the assumptions and data used for the previous data science process. Only in very few cases is this really a “data 0” problem where one tries to learn a new model from very few data points. Even then, one should also explore the option of building on top of the previous models and keeping them involved in some weighted way. Very often, new behavior can be well represented as a mix of previous models with a sprinkle of new data.But if your data science development is done in-house, now is the time an integrative and uniform environment that is 100% backward compatible comes in very handy. In such a platform, the assumptions are modeled and documented in one place, allowing well-informed changes and adjustments to be made much more easily. It’s even better if you can validate, test, and deploy the changes into production from that same environment without the need for manual interaction.TwitterLinkedInKNIME blog—newtechforum@infoworld.com", "pub_date": "2020-06-10"},
{"title": "Working with the Azure Kinect Developer Kit", "overview": "Building applications on top of the Kinect depth sensor", "image_url": "https://images.idgesg.net/images/article/2020/06/azure-kinect-02-lg-100848930-large.jpg", "url": "https://www.infoworld.com/article/3562738/working-with-the-azure-kinect-developer-kit.html", "body": "Microsoft announced its Azure Kinect camera modules alongside HoloLens 2 early in 2019. Both devices use the same mixed-reality camera module, using a time-of-flight depth sensor to map objects around the camera. But where HoloLens is a wearable mixed-reality device, the Azure Kinect modules are intended to provide Azure-hosted machine learning applications with connected sensors that can be mounted anywhere in a workspace.Azure Kinect is a direct descendent of the second-generation Kinect modules that shipped with the Xbox One, but instead of providing real-world inputs for gaming, it’s targeted at enterprise users and applications. Intended to work with Azure’s Cognitive Services, the first Azure Kinect developer kit started shipping at the end of 2019 in the United States, adding several other countries in early 2020.Also on InfoWorld: What is CUDA? Parallel programming for GPUsOpening the boxThe $399 Azure Kinect Developer Kit is a small white unit with two camera lenses, one for a wide-angle RGB camera and one for the Kinect depth sensor, and an array of microphones. It has an orientation sensor, allowing you to use the camera to build complex 3-D images of environments, ready for use in mixed reality. You can chain multiple devices together for quick 3-D scans or to provide coverage of an entire room, using the orientation sensor to help understand device position.Along with the camera unit, you get a power supply, an Allen key to remove the chaining ports cover, and a USB cable to connect to a development PC. I’d recommend getting a desktop tripod or another type of mount, as the bundled plastic stand is rather small and doesn’t work with most desks or monitors. There’s no software in the box, only a link to online documentation where you can download the device SDK.Before you get started, you should update the device firmware. This ships with the SDK and includes a command line installation tool. When you run the updater it first checks the current firmware state before installing camera and device firmware and then rebooting. Once the camera has rebooted, use the same tool to check that the update has installed successfully. If there’s a problem with an install you can use the camera’s hardware reset (hidden under the tripod mount) to restore the original factory image.Sensing the worldWith the SDK installed you get access to the device sensors from your code. There are three SDKs: one for low-level access to all the camera’s sensors, another to use the familiar Kinect body-tracking features, and one to link the camera’s microphone array to Azure’s speech services. A prebuilt Kinect Viewer app shows the available camera views and streams data from the device’s sensors. You get access to the wide-angle RGB camera, a depth camera view, and the image from the depth sensor’s infrared camera. SDKs are available for both Windows and for Linux, specifically Canonical’s Ubuntu 18.04 LTS release, and can be downloaded directly from Microsoft or from GitHub.It’s a good idea to spend some time playing with the Kinect Viewer. It lets you see how the different depth camera modes operate, helping you choose either a narrow or wide field of view. You can see data from the position sensors, both the accelerometer and gyroscope, and from the microphone array. With the Azure Kinect Developer Kit connected to a development PC and working, you can start to write code for it. A command line recorder app can be used to capture data for playback in the viewer, storing depth information in an MKV (Matroska Video) format file.Building your first depth-sensing applicationMicrosoft provides sample code for building a simple C application to work with the Azure Kinect Development Kit. There’s only one library needed, and this provides the objects and methods needed to work with the camera. Any application first needs to check how many cameras are connected to the host PC before you configure your device data streams. Devices are identified by their serial number, so you can use this to address a specific camera when working with several connected to the same PC or chained together.The Azure Kinect Developer Kit only delivers streaming data, so applications need to configure the data rate in frames per second, along with image color formats and resolutions. Once you’ve created a configuration object you can open a connection using your configuration object, ready to stream data. When you’re finished reading a data stream, stop and close the device.Images are captured in a capture object, with a depth image, an IR image, and a color image for each individual image, taken from the device’s stream. Once you have a capture, you can extract the individual images ready for use in your application. Image objects can be delivered to the Azure machine vision APIs, ready for object recognition or anomaly detection. One example Microsoft has used in its demonstrations is an application that uses captured video to detect when a worker on a factory floor gets too close to operating machinery; another detects someone smoking near a gas pump.Images are captured from the device in a correlated way. Each captured image has a depth image, an IR image, a color image, or a combination of images.A similar process gives you data from the position and motion sensors. As motion data is captured at a higher rate than image data, you must implement some form of synchronization in your code to avoid losing any data. Audio data is captured using standard Windows APIs, including those used by Azure’s speech services.Although the Azure Kinect hardware captures a lot of data, the SDK functions help transform it into a usable form; for example, adding depth data to an RGB image to produce RGB-D images that are transformed to the viewpoint of the RGB camera (and vice versa). As the two sensors are off-set, this requires warping an image mesh to merge the two cameras’ viewpoints, using your PC’s GPU. Another transform generates a point cloud, allowing you to get depth data for each pixel in your capture. One useful option in the SDK is the ability to capture video and data streams in a Matroska-format file. This approach allows bandwidth-limited devices to batch data and deliver it to, say, Azure Stack Edge devices with Cognitive Services containers for batch processing.Body tracking a digital skeletonThe original Kinect hardware introduced body tracking, with a skeletal model that could be used to quickly evaluate posture and gestures. That same approach continues in the Azure Kinect Body Tracking SDK, which uses Nvidia’s CUDA GPU parallel processing technology to work with 3-D image data from your device’s depth sensor. A bundled sample app shows some of the features of the SDK, including the ability to track more than one person at a time. The Azure Kinect Body Tracking Viewer shows a 3-D point cloud and tracked bodies.The Body Tracking SDK builds on the Azure Kinect SDK, using it to configure and connect to a device. Captured image data is processed by the tracker, storing data in a body frame data structure. This contains a collection of skeletal structures for identified bodies, a 2-D index map to help visualize your data, along with the underlying 2-D and 3-D images that were used to construct the tracking data. Each frame can be used to construct animations or to feed information to machine learning tools that can help process tracked positions in relation to a room map or to ideal positions.Azure’s Cognitive Services are a powerful tool for processing data, and the addition of Azure Kinect makes it possible to use them in a wide range of industrial and enterprise scenarios. With a focus on workplace 3-D image recognition, Microsoft is attempting to show how image recognition can be used to reduce risk and improve safety. There’s even the option of using an array of devices as a quick volumetric capture system, which can help build both mixed-reality environments and provide source data for CAD and other design tools. The result is a flexible device that, with a little code, becomes a very powerful sensing device.", "pub_date": "2020-06-16"},
{"title": "3 ways to apply agile to data science and dataops", "overview": "Take an agile approach to dashboards, machine learning models, cleansing data sources, and data governance", "image_url": "https://images.idgesg.net/images/article/2018/01/digital-transformation_binary_change_agile_growth-100746092-large.jpg", "url": "https://www.infoworld.com/article/3562346/3-ways-to-apply-agile-to-data-science-and-dataops.html", "body": "Just about every organization is trying to become more data-driven, hoping to leverage data visualizations, analytics, and machine learning for competitive advantages. Providing actionable insights through analytics requires a strong dataops program for integrating data and a proactive data governance program to address data quality, privacy, policies, and security.Delivering dataops, analytics, and governance is a significant scope that requires aligning stakeholders on priorities, implementing multiple technologies, and gathering people with diverse backgrounds and skills. Agile methodologies can form the working process to help multidisciplinary teams prioritize, plan, and successfully deliver incremental business value.Also on InfoWorld: How to address data and architecture standards in agile developmentAgile methodologies can also help data and analytics teams capture and process feedback from customers, stakeholders, and end-users. Feedback should drive data visualization improvements, machine learning model recalibrations, data quality increases, and data governance compliance.  Defining an agile process for data science and dataopsApplying agile methodologies to the analytics and machine learning lifecycle is a significant opportunity, but it requires redefining some terms and concepts. For example:Instead of an agile product owner, an agile data science team may be led by an analytics owner who is responsible for driving business outcomes from the insights delivered.Data science teams sometimes complete new user stories with improvements to dashboards and other tools, but more broadly, they deliver actionable insights, improved data quality, dataops automation, enhanced data governance, and other deliverables. The analytics owner and team should capture the underlying requirements for all these deliverables in the backlog.Agile data science teams should be multidisciplinary and may include dataops engineers, data modelers, database developers, data governance specialists, data scientists, citizen data scientists, data stewards, statisticians, and machine learning experts. The team makeup depends on the scope of work and the complexity of data and analytics required.An agile data science team is likely to have several types of work. Here are three primary ones that should fill backlogs and sprint commitments.1. Developing and upgrading analytics, dashboards, and data visualizationsData science teams should conceive dashboards to help end-users answer questions. For example, a sales dashboard may answer the question, “What sales territories have seen the most sales activity by rep during the last 90 days?” A dashboard for agile software development teams may answer, “Over the last three releases, how productive has the team been delivering features, addressing technical debt, and resolving production defects?”Agile user stories should address three questions: Who are the end-users? What problem do they want addressed? Why is the problem important? Questions are the basis for writing agile user stories that deliver analytics, dashboards, or data visualizations. Questions address who intends to use the dashboard and what answers they need.It then helps when stakeholders and end-users provide a hypothesis to an answer and how they intend to make the results actionable. How insights become actionable and their business impacts help answer the third question (why is the problem important) that agile user stories should address.The first version of a Tableau or Power BI dashboard should be a “minimal viable dashboard” that’s good enough to share with end-users to get feedback. Users should let the data science team know how well the dashboard addresses their questions and how to improve. The analytics product owner should put these enhancements on the backlog and consider prioritizing them in future sprints.2. Developing and upgrading machine learning modelsThe process of developing analytical and machine learning models includes segmenting and tagging data, feature extraction, and running data sets through multiple algorithms and configurations. Agile data science teams might record agile user stories for prepping data for use in model development and then creating separate stories for each experiment. The transparency helps teams review the results from experiments, decide on the next priorities, and discuss whether approaches are converging on beneficial results.There are likely separate user stories to move models from the lab into production environments. These stories are devops for data science and machine learning, and likely include scripting infrastructure, automating model deployments, and monitoring the production processes.Once models are in production, the data science team has responsibilities to maintain them. As new data comes in, models may drift off target and require recalibration or re-engineering with updated data sets. Advanced machine learning teams from companies like Twitter and Facebook implement continuous training and recalibrate models with new training set data.3. Discovering, integrating, and cleansing data sourcesAgile data science teams should always seek out new data sources to integrate and enrich their strategic data warehouses and data lakes. One important example is data siloed in SaaS tools used by marketing departments for reaching prospects or communicating with customers. Other data sources might provide additional perspectives around supply chains, customer demographics, or environmental contexts that impact purchasing decisions.Analyst owners should fill agile backlogs with story cards to research new data sources, validate sample data sets, and integrate prioritized ones into the primary data repositories. When agile teams integrate new data sources, the teams should consider automating the data integration, implementing data validation and quality rules, and linking data with master data sources.Julien Sauvage, vice president of product marketing at Talend, proposes the following guidelines for building trust in data sources. “Today, companies need to gain more confidence in the data used in their reports and dashboards. It’s achievable with a built-in trust score based on data quality, data popularity, compliance, and user-defined ratings. A trust score enables the data practitioner to see the effects of data cleaning tasks in real time, which enables fixing data quality issues iteratively.”The data science team should also capture and prioritize data debt. Historically, data sources lacked owners, stewards, and data governance implementations. Without the proper controls, many data entry forms and tools did not have sufficient data validation, and integrated data sources did not have cleansing rules or exception handling. Many organizations have a mountain of dirty data sitting in data warehouses and lakes used in analytics and data visualizations.Just like there isn’t a quick fix to address technical debt, agile data science groups should prioritize and address data debt iteratively. As the analytics owner adds user stories for delivering analytics, the team should review and ask what underlying data debt must be itemized on the backlog and prioritized.Implementing data governance with agile methodologiesThe examples I shared all help data science teams improve data quality and deliver tools for leveraging analytics in decision making, products, and services.In a proactive data governance program, issues around data policy, privacy, and security get prioritized and addressed in parallel to the work to deliver and improve data visualizations, analytics, machine learning, and dataops. Sometimes data governance work falls under the scope of data science teams, but more often, a separate group or function is responsible for data governance.Organizations have growing competitive needs around analytics and data governance regulations, compliance, and evolving best practices. Applying agile methodologies provides organizations with a well-established structure, process, and tools to prioritize, plan, and deliver data-driven impacts.", "pub_date": "2020-06-18"},
{"title": "How to manage cloud migration remotely", "overview": "These days, everyone on the cloud migration team is dispersed all over the city or country. This presents some challenges, but I see more opportunities. ", "image_url": "https://images.techhive.com/images/article/2016/07/cloud_migration-100672646-large.jpg", "url": "https://www.infoworld.com/article/3586299/how-to-manage-cloud-migration-remotely.html", "body": "It’s Monday morning standup, and 22 faces spread across your screen on yet another Zoom call. The focus is on the day’s tasks, as well as looking at problems that need solving.  Unique to this experience is that just seven months ago, all of these faces were present in the same conference room, before COVID-19 sent them home to work remotely. Also on InfoWorld: PaaS, CaaS, or FaaS? How to chooseOne of the more common question I get lately is how to manage a remote migration team and do so without allowing schedules to slip? It’s really not as hard as it sounds; indeed many aspects are easier. Let me offer some advice to those who are tasked with managing remote teams and those who are on remote teams.   No matter if it’s Slack, Teams, Yammer, or something else, there needs to be a mechanism for instantaneous communications and real-time collaboration on tasks. Text messages won’t do here. Communication tools should be integrated with migration tools, capable of launching migrated application testing from the collaboration tool and returning results directly to the team.  Of course, you can go overboard with tools and use too many. Complexity leads to confusion, which leads to mistakes. You’ll need to find a balance.  One of the benefits of everyone sitting in the same building is that everyone is protected by the same security mechanisms: firewalls, local encryption, and even security management tools.  Working remotely means the same security is spread across networks and firewalls out of your direct control. Managing a remote migration team requires a focus on security management and putting technology in place that can manage heterogenous networks(including a variety of home networks) as well as—or better than—when everyone was within the enterprise.  Gone are the days when leaders walked around looking over everyone’s shoulders, sometimes to watch the screen change suddenly.  Those who try to continue such control over remote workers won’t find any positive outcomes from this leadership style. Indeed, turnover will likely increase, and more so when employees realize that they don’t have to carry the dreaded cardboard box out of the building, since they are not allowed in the building.  Those leading migrations, remote or not, need to get over the whole control thing. Focus on expected productivity. Agree on what’s expected and how employees will be measured. Productivity will likely increase, considering that staffers don’t feel watched, and also are no longer spending hours commuting each week.These aren’t exactly secrets, but I’m seeing these mistakes kill migration efforts. With a bit of forethought, you’ll find you can actually move faster.  ", "pub_date": "2020-10-20"},
{"title": "Redis 6: A high-speed database, cache, and message broker", "overview": "Redis is a powerful blend of speed, resilience, scalability, and flexibility, and Redis Enterprise takes it even further", "image_url": "https://images.techhive.com/images/article/2016/10/speed-164063_12801-100689806-large.jpg", "url": "https://www.infoworld.com/article/3563354/redis-6-a-high-speed-database-cache-and-message-broker.html", "body": "Like many, you might think of Redis as only a cache. That point of view is out of date.Essentially, Redis is a NoSQL in-memory data structure store that can persist on disk. It can function as a database, a cache, and a message broker. Redis has built-in replication, Lua scripting, LRU eviction, transactions, and different levels of on-disk persistence. It provides high availability via Redis Sentinel and automatic partitioning with Redis Cluster.The core Redis data model is key-value, but many different kinds of values are supported: Strings, Lists, Sets, Sorted Sets, Hashes, Streams, HyperLogLogs, and Bitmaps. Redis also supports geospatial indexes with radius queries and streams.To open source Redis, Redis Enterprise adds features for additional speed, reliability, and flexibility, as well as a cloud database as a service. Redis Enterprise scales linearly to hundreds of millions of operations per second, has active-active global distribution with local latency, offers Redis on Flash to support large datasets at the infrastructure cost of a disk-based database, and provides 99.999% uptime based on built-in durability and single-digit-seconds failover.Further, Redis Enterprise extends the core Redis functionality to support any data modeling method with modules such as RediSearch, RedisGraph, RedisJSON, RedisTimeSeries, and RedisAI, and allows operations to be executed across and between modules and core. All this is provided while keeping database latency under one millisecond.Core Redis features and use casesWhat does it mean that Redis can now function as a database, cache, and message broker? And what are the use cases those roles support? is the classic function of Redis. Essentially, Redis sits in front of a disk-based database and saves queries and results; the application checks the Redis cache for stored results first, and queries the disk-based database for results not currently in the cache. Given the sub-millisecond response rate of Redis, this is usually a big win for application performance. Expiration timers and LRU (least recently used) eviction from the Redis cache help to keep the cache current and to use memory effectively.The  is an important part of modern web applications. It’s a convenient place to keep information about the user and her interactions with the application. In a web farm architecture, hosting the session store directly on the web server requires making the user “stick” to the same back-end server for future requests, which can limit the load balancer. Using a disk-based database for the session store removes the need to bind a session to a single web server, but introduces an additional source of latency. Using Redis (or any other fast in-memory database) as the session store often results in a low-latency, high-throughput web application architecture.Redis can function as a  using three different mechanisms, and one of the important use cases for Redis as a message broker is to act as glue between microservices. Redis has a low-overhead publish/subscribe notification mechanism that facilitates fire-and-forget messages, but can’t work when the destination service is not listening. For a more persistent, Kafka-like message queue, Redis uses streams, which are time-stamp ordered key-value pairs in a single key. Redis also supports doubly-linked lists of elements stored at a single key, which are useful as a first-in/first-out (FIFO) queue. Microservices can, and often do, use Redis as a cache as well as using it as a message broker, although the cache should run in a separate instance of Redis from the message queue.Basic replication allows Redis to scale without using the cluster technology of the Redis Enterprise version. Redis replication uses a leader-follower model (also called master-slave), which is asynchronous by default. Clients can force synchronous replication using a WAIT command, but even that doesn’t make Redis consistent across replicas.Redis has server-side Lua scripting, allowing programmers to extend the database without writing C modules or client-side code. Basic Redis transactions allow a client to declare a sequence of commands as a non-interruptible unit, using the MULTI and EXEC commands to define and run the sequence. This is  the same as relational transactions with rollbacks.Redis has different levels of on-disk persistence that the user can select. RDB (Redis database file) persistence takes point-in-time snapshots of the database at specified intervals. AOF (append-only file) persistence logs every write operation received by the server. You can use both RDB and AOF persistence for maximum data safety.Redis Sentinel, itself a distributed system, provides high availability for Redis. It does monitoring of the master and replica instances, notification if there is something wrong, and automatic failover if the master stops working. It also serves as a configuration provider for clients.Redis Cluster provides a way to run a Redis installation where data is automatically sharded across multiple Redis nodes. Redis Cluster also provides some degree of availability during partitions, although the cluster will stop operating if the majority of masters become unavailable.As I mentioned earlier, Redis is a key-value store that supports Strings, Lists, Sets, Sorted Sets, Hashes, Streams, HyperLogLogs, and Bitmaps as values. One of the simplest and most common use cases is using integer values as counters. In support of this, INCR (increment), DECR (decrement), and other single operations are atomic, and therefore safe in a multi-client environment. In Redis, when keys are manipulated they will automatically be created if they don’t already exist.The other kinds of value structures also have their own examples in the Try Redis tutorial. The tutorial was undergoing maintenance when I tried it myself; I expect that to be fixed soon, as Redis Labs has become involved in what was originally a community effort.There are a number of add-on modules for Redis including (in descending order of popularity) a neural network module, full-text search, SQL, a JSON data type, and a graph database. The licenses for modules are set by the authors. Some of the modules that work with Redis are primarily modules for Redis Enterprise.Redis Enterprise enhancementsUsing a shared-nothing cluster architecture, Redis Enterprise delivers infinite linear scaling without imposing any non-linear overheads in a scaled-out architecture. You can deploy multiple Redis instances on a single cluster node, to take full advantage of multi-core computer architecture. Redis Enterprise has demonstrated scaling to hundreds of millions of operations per second with five nines (99.999%) uptime. Redis Enterprise does automatic re-sharding and rebalancing while maintaining low latency and high throughput for transactional loads.Redis Enterprise offers active-active deployment for globally distributed databases, enabling simultaneous read and write operations on the same dataset across multiple geo-locations. To make that more efficient, Redis Enterprise can use conflict-free replicated data types (CRDTs) to maintain consistency and availability of data. Riak and Azure Cosmos DB are two other NoSQL databases that support CRDTs.While there is extensive academic literature on CRDTs, I admit I don’t completely understand how or why they work. The short summary of  they do is that CRDTs can resolve inconsistencies without intervention, using a mathematically derived set of rules. CRDTs are valuable for high-volume data that require a shared state, and can use geographically dispersed servers to reduce latency for users.One of the major differences between Redis and Redis Enterprise is that Redis Enterprise decouples the data path from cluster management. This improves the operation of both components. The data path is based on multiple zero-latency, multi-threaded proxies that reside on each of the cluster nodes to mask the underlying complexity of the system. The cluster manager is a governing function that provides capabilities such as resharding, rebalancing, auto-failover, rack-awareness, database provisioning, resource management, data persistence configuration, and backup and recovery. Because the cluster manager is entirely decoupled from the data path components, changes to its software components do not affect the data path components.Redis on Flash is a Redis Enterprise feature that can drastically reduce the cost of hardware for Redis. Instead of having to pay through the nose for terabytes of RAM or restrict the size of your Redis datasets, you can use Redis on Flash to place frequently accessed hot data in memory and colder values in Flash or persistent memory, such as Intel Optane DC.Redis Enterprise modules include RedisGraph, RedisJSON, RedisTimeSeries, RedisBloom, RediSearch, and RedisGears. All Redis Enterprise modules also work with open source Redis.What’s new in Redis 6?Redis 6 is a big release, both for the open source version and the Redis Enterprise commercial version. The performance news is the use of threaded I/O, which gives Redis 6 a 2x improvement in speed over Redis 5 (which was no slouch). That carries over into Redis Enterprise, which has additional speed improvements for clusters as described above.The addition of access control lists (ACLs) gives Redis 6 the concept of users, and allows developers to write more secure code. Redis Enterprise 6 builds on ACLs to offer role-based access control (RBAC), which is more convenient for the programmers and DBAs.Redis 6.0 open sourceAccess control lists (ACLs)Improved evictionsThreaded I/ORESP3 protocolRedis Enterprise 6.0Role-based access control (RBAC)Extending active-activeHyperLogLogStreamsRedis Enterprise 6.0 adds support for the Streams data type in active-active databases. That allows both high availability and low latency while concurrently reading and writing to and from a real-time stream in multiple data centers in multiple geographic locations.RedisGears is a dynamic framework that enables developers to write and execute functions that implement data flows in Redis. It lets users write Python scripts to run inside Redis, and enables a number of use cases including write-behind (Redis acts as a front-end to a disk-based database), real-time data processing, streaming and event processing, operations that cross data structures and models, and AI-driven transactions.RedisAI is a model serving engine that runs inside Redis. It can perform inference with PyTorch, TensorFlow, and ONNX models. RedisAI can run on CPUs and GPUs, and enables use cases such as fraud detection, anomaly detection, and personalization.Creating a new role in Redis Enterprise Cloud. RBAC is considerably easier to manage that ACLs.Installing RedisYou can install Redis by downloading and compiling a source tarball or by pulling a Docker image from the Docker Hub. Redis can be compiled and used on Linux, MacOS, OpenBSD, NetBSD, and FreeBSD. The source code repository is on GitHub. On Windows, you can run Redis either in a Docker container or under Windows Subsystem for Linux (WSL), which requires Windows 10.You can install Redis Enterprise on Linux or in Docker containers. The Linux downloads come in the form of binary packages (DEB or RPM depending on the flavor of Linux) and Bash shell scripts for cluster installation. The scripts check for the required four cores and 15 GB of RAM for installation.Redis Enterprise installs on Docker and on Linux systems. This page shows the Redis Enterprise 5.0 installers, as the Redis Enterprise 6.0 installers are still in the release candidate phase, although I was given the Linux installers for Redis Enterprise 6.0 RC privately when I asked.Redis Enterprise CloudThe fastest way to install Redis Enterprise is not to install it at all, but rather to run it in the Redis Enterprise Cloud. When I tried this myself for review purposes, I initially received a Redis 5 instance; I had to ask for an upgrade to Redis 6.You can run a small instance in the Redis Enterprise Cloud for free, or a full-featured instance for a monthly price that depends on the amount of RAM provisioned. While this description says that Redis modules are not available for low-throughput Essentials environments, Redis Labs is currently rolling out Module support at the Essential level starting with the AWS Mumbai zone.At the Essentials level, instances range from 30 MB of RAM (free) to 5 GB of RAM ($338 per month) and over. Redis Enterprise Cloud instances are available on AWS, GCP, and Azure. You can choose your own region or regions.Creating a database once you have a Redis Enterprise Cloud subscription is a simple matter of naming the database, choosing a Redis or Memcached protocol, choosing your replication and persistence options, choosing your data eviction policy, and setting your usage alerts.", "pub_date": "2020-06-22"},
{"title": "How PagerDuty helps customer service and IT teams improve responses", "overview": "PagerDuty uses machine learning to anticipate issues and dramatically accelerate responses, so problems are addressed before they impact customers", "image_url": "https://images.idgesg.net/images/article/2018/07/crystal_ball_reflection_architectural_dome_future_forecast_fortune_telling_predictions_greg_tockner_cc0_via_unsplash_1200x800-100764604-large.jpg", "url": "https://www.infoworld.com/article/3544929/how-pagerduty-helps-customer-service-and-it-teams-improve-responses.html", "body": "Predicting the outcome of the NCAA men’s Division I basketball tournament — an event where upsets are celebrated wildly and the outcome is notoriously difficult to foresee — is nearly as competitive as the tournament itself. For years, Warren Buffet held a contest offering a billion dollars for a perfect bracket, and nobody even came close. Speaking of unpredictability, just as fans were getting ready to make their picks for this year’s tournament, all major public sporting events were canceled. Who could have predicted ?Even though we can’t see the future, a deep understanding of variables does enable people to make better predictions and gain an edge over the competition. Picking winners by their school mascot may work every once in a while, but an in-depth study of the best teams, coaches, and athletes is a much more effective strategy.Likewise, customer service, devops, and IT issues are inherently unpredictable. It’s impossible for companies to know in advance when operational problems will arise, product defects will surface, or communications will go askew. Solutions driven by AI and machine learning can help teams improve their odds. These products can dramatically accelerate responses to issues, so problems are prevented or resolved before most customers encounter them. Companies can get thousands of alerts per minute when a problem arises within their digital app or service — a broken cart for an ecommerce website, for example — which is neither useful nor actionable for human interpreters to tackle. The overwhelming amount of noise simply leads to lost signals and many more contacts between customers and service teams before underlying problems can be addressed.Predictive solutions for customer services are built on understanding the drivers behind the signals. Quickly identifying patterns helps companies stay ahead of the curve. Machine learning tools free up a lot of cycles for response teams by cutting through the noise, rather than distracting them over and over again with alerts and information that may not be useful.When teams use machine learning in this way, they can boil down the signals to uncover the actual incidents that are driving the unmanageable number of alerts. Instead of scrambling to put out many small fires, they can see the big picture of where the problems actually lie and be more intelligent and informed in tackling a smaller group of larger issues.How predictive capabilities can improve service responsesPredictive processes must be conducted in real-time if they are going to help companies get ahead of the issues for the majority of customers. Developing problems that threaten to impact customers do not allow time to be paused for reflection or deliberation.The higher-level need for predictive customer and IT services is in training algorithms to recognize which alerts belong to which incidents. At PagerDuty, our main goal is to help companies identify issues before they cause problems within digital systems, and predict what may go wrong in the future so companies can get ahead of it. We use machine learning to group alerts together so teams can see the full scale of the issues and know precisely how to solve them.For example, multiple teams may each be working on individual complaints without understanding that they are all elements of a single issue. Insights from PagerDuty’s platform solve that problem and get everyone on the same page. Meanwhile, as responders are assigned specific issues to fix, the platform triages messages to each individual so they are not overwhelmed with issues outside of the one they are addressing.This is important because most systems don’t operate in isolation where a point failure in one place is the same as a point failure somewhere else. When problems arise, companies use PagerDuty to help find the origin point for cascading issues to try to prevent catastrophic failure. When teams can be more predictive and preventive, they gain a higher-level view of the problems and learn where their efforts will have the most impact.A structure that helps teams identify and solve problems quickly also gives much greater visibility to every level of the organization. Managers and directors can have better insights about how to deploy teams. Leaders who may have to explain problems or downtime to customers are likewise armed with information and a clear path forward.How PagerDuty uses machine learning Giving companies greater predictive and preventive power for customer service and IT starts with grouping issues in a way that helps identify underlying causes of digital issues. That grouping begins with the supposition that if two messages have similar text, then those messages are fundamentally similar. Although this is reasonable in theory, knowing whether those messages are truly similar is a fuzzy concept.At PagerDuty, the most impactful solution has been to apply a parser that takes messages and transforms them into less refined language. This process boils down the words in order to surface specific elements within the message.The system locates unique identifiers like dates, times, customer IDs, or websites with IDs inside that would only be issued in the context of customer messages and reports. These identifiers are generally unimportant to the parser in terms of content. The program simply identifies that they are present within the body of the message.After this overall blurring, the words and identifiers within each message can then be grouped together. This is where PagerDuty’s platform examines the incoming signals and determines the full extent to which messages share groups of words.This step is accomplished by vectorizing, which is the process of turning each of these series of words into a representative sequence of numbers. But it is still an imperfect system. Every sentence produces a vector representation, of course, but each vector could conceivably come from several different sentences. Usually, there is enough information to determine when sentences have the same information. But PagerDuty’s software engineers still have to account for the fact that there are many ways that a vector could have been put together.Once the system can identify a group of messages that have the same vectors, they are bundled together. These groups essentially have the same content. Their identifiers indicate they are full of all the same terms.Turning machine data into predictions and preventionFor example, a company usually learns that something is going wrong when it is suddenly flooded with reports and messages. Most of these will be machine-generated, some with custom templates and some even written by a person. Without some sort of grouping, teams have no way of seeing a higher-level view of the circumstances. They could build a grouping tool, but that would require a serious investment of time and effort, all while more incident reports pile up.Likewise, because so many of the messages have different content, simply grouping messages only when they are identical doesn't do much to diminish the volume of issues. Using AI to discern similarities lets the group accumulate relevant information over time. Instead of thousands of individual problems, each represented by a report or message, grouping alerts this way surfaces just a few core issues that are the source of other problems.At that point, the system has enabled the response teams to become both predictive and preventive. It becomes much easier to find the biggest issues and fix the underlying causes that will prevent future problems. Prioritizing a little engineering work on core issues causes the incident load to drop dramatically, all from basic AI-driven grouping.In theory, this should be a very reliable process. Once messages are parsed, identified, and vectorized, it should be easy for the system to group them together as similar. They are all textually related and the vectors let the platform measure the strength of the relatedness.In practice, of course, it’s not always so simple. The flexibility of language means that the system quite often gets it wrong. This is why PagerDuty builds ample and powerful feedback systems into our products.Improving results with human feedbackWhen end users provide feedback to the system, they are giving us new data points to help perfect the process. This is usually an acknowledgment that, yes, A and B look like they should be related. However, the human context of the message reveals that they don’t have much to do with each other.PagerDuty’s feedback system gives greater weight to messages that have been positively correlated because they share terms, but then human feedback reveals they are not similar. This evaluation and modification could be accomplished in software through a very large reinforcement learning system, but for the user it is a simple evaluation whether terms and messages should or should not be together.  Customers, of course, do not need to see the nuts and bolts of how this works. The customer service and IT teams should have simple tools to provide feedback that will delineate which terms don’t match.On a higher level, PagerDuty’s feedback systems give users broad options for merging and separating groups of terms within the alerts. This is simply grabbing items and moving them in or out of a group; in essence indicating that certain items belong with each other, but another does not match.Another less sophisticated but equally powerful product may only require literal thumbs-up and thumbs-down buttons. The user essentially approves of a match or indicates a flaw in the process.Anything can and will happen to frustrate and disappoint customers, as anyone who has worked in customer service will tell you. Improving your odds in those unpredictable circumstances requires learning, understanding and solving problems as quickly as they emerge. The foremost integrated event intelligence and incident response solutions in the space combine machine and human telemetry by looking at both digital signals and human response behavior.PagerDuty—newtechforum@infoworld.com", "pub_date": "2020-06-24"},
{"title": "AI is now a C-suite imperative", "overview": "Survey from Appen finds that C-level involvement in AI initiatives has jumped significantly ", "image_url": "https://images.idgesg.net/images/article/2018/02/priority_high_priorities_importance_important_urgent_thinkstock_680720450-100749329-large.jpg", "url": "https://www.infoworld.com/article/3563885/ai-is-now-a-c-suite-imperative.html", "body": "Executive involvement in enterprise artificial intelligence (AI) initiatives is growing rapidly and more emphasis is being placed on high-quality training data. Both C-suite ownership of AI and budgets over $500K nearly doubled in 2020 due to the COVID-19 pandemic serving as a catalyst for accelerated AI initiatives. A key lesson learned from the pandemic is that businesses need to be ready for anything that requires a high level of business agility. It’s Darwinism at its finest as businesses that can adapt to market trends faster than their competition can become market leaders and maintain that position. Those that can’t do this will fade into obscurity with many going away.How data analysis, AI, and IoT will shape the post-pandemic ‘new normal’But how do business leaders even know what decisions to make? There is a massive amount of data to be analyzed and people can’t process the information fast enough to find those key insights that drive business change. Machines can work at infinitely higher speed and the pressure on the C-suite has never been higher. Now the execs are turning to AI to help them make the best decisions in as short a time as possible.These findings come from Appen Limited’s annual State of AI and Machine Learning Report, which surveyed 374 business and technology decision-makers between April and May of this year. The report measured the state of AI for enterprises with more than 1,000 full-time employees and commercial organizations with fewer than 1,000 full-time employees.C-suite involvement in AI takes a massive jumpThe report uncovered an important change. The C-suite is now more engaged in AI initiatives than ever before, with a whopping 71 percent of organizations reporting executive involvement. In comparison, only 39 percent of executives owned AI initiatives in 2019. CTOs made up 42 percent of the 71 percent of C-suite AI ownership, which partially explains why AI budgets are increasing in 2020. COVID-19 may be temporary but don’t expect AI to be. I believe it will be the biggest driver of business change since the rise of the Internet.Executives see AI as invaluable to their business success. This is true for companies of all sizes across different industries. For 27 percent of survey respondents, enterprise AI budgets have exceeded $1M, while 10 percent said their AI budget is more than $5M. These numbers are expected to continue rising steeply as businesses adopt AI on a global scale.C-level interest brings a focus on risk and ethicsWith increased C-suite visibility, businesses are focusing more on risk management, governance, and ethics as key aspects of rolling out AI initiatives globally or to their full user base. As companies start using AI to supplement human capabilities, responsible use of AI must be part of the process to ensure fairness, privacy, transparency, and security and avoid inappropriate uses of data.  Although businesses believe responsible AI is important to their success, only 25 percent view unbiased AI as mission-critical. Half of the respondents either don’t see it as a major issue or are just starting to think about it. The findings indicate businesses must take a more proactive approach toward responsible AI. Simply having accurate data or algorithms isn’t enough. AI governance with clear ethical standards is also necessary.Data management gets in the way of quality AIAnother big challenge for businesses is data management. Three out of four companies surveyed by Appen said they update their AI models at least quarterly. For 40 percent of the respondents, lack of data or data management are the leading roadblocks to effectively utilizing AI in the enterprise.Most respondents (93 percent) expressed the need for high-quality training data, as businesses continue to deal with more data types and complex data compared to previous years. The report revealed many businesses are still behind on AI adoption, especially when it comes to training data. For this reason, company leaders are going beyond in-house resources and turning to third-party providers to help carry out AI deployments.Data management and quality comes up in almost every AI discussion I have with business leaders. Companies have massive amounts of data — more than ever before and it continues to grow exponentially but much of the data resides in silos and is in a myriad of different formats. In data sciences, there’s an axiom that states “good data leads to good insights.” The reverse holds true too, as bad data will lead to bad insights and partial data will lead to partial insights. Getting a handle on data and data quality needs to be as important as the AI initiatives themselves.With AI, cloud is the wayIn 2020, four times as many business and technology decision-makers reported using cloud machine learning providers, including Microsoft Azure (49 percent), Google Cloud (36 percent), IBM Watson (31 percent), AWS (25 percent), and Salesforce Einstein (17 percent). Each of these providers saw double-digit adoption of cloud machine learning tools in 2020 versus 2019, attributing the surge to businesses looking for solutions that can scale as their AI initiatives grow in complexity.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesDespite dealing with a global pandemic, the majority (70 percent) of businesses surveyed during those months did not expect COVID-19 to negatively impact AI strategies. In fact, nearly half of businesses have fast-tracked their AI strategies, with 20 percent reporting significant acceleration. Only 9 percent of businesses anticipated substantial delays.Increased C-suite investment in enterprise AI is an indication that businesses are choosing to spend money on key initiatives even in a time of crisis. Clearly, these companies view AI-driven agility as the key to both short term survival and long-term leadership. With the right tools and strategies in place, businesses can access clean, high-quality, ethical data to successfully implement AI across the enterprise.", "pub_date": "2020-06-25"},
{"title": "Major R language update brings big changes", "overview": "R 4.0.0 brings numerous and significant changes to syntax, strings, reference counting, grid units, and more", "image_url": "https://images.idgesg.net/images/article/2019/02/high_voltage_letter_r_programming_language_by_vrender_gettyimages_plus_circles_halftone_by_hakkiarslan_gettyimages_2400x1600-100789395-large.jpg", "url": "https://www.infoworld.com/article/3540989/major-r-language-update-brings-big-changes.html", "body": "Version 4.0.0 of the R language for statistical computing has been released, with changes to the syntax of the language as well as features pertaining to error-checking and long vectors.The upgrade was published on April 24. Source code for R 4.0.0 is accessible at cran.r-project.org. A GNU project, R has gathered steam with the rise of data science and machine learning, currently ranking 10th in the Tiobe Index of language popularity and seventh in the PyPL Popularity of Programming Language index.Changes and features introduced in R 4.0.0 include:A new syntax is offered for specifying _raw_ character constants similar to the one used in C++, where  can be used to define a literal string. This makes it easier to write strings containing backslashes or both single and double quotes.The language now uses a  default, and thus by default no longer converts strings to factors in calls to  and . Many packages relied on the previous behavior and will need updating.The S3 generic function  now is in package base rather than package graphics; it is reasonable to have methods that do not use the graphics package. The generic currently is re-exported from the graphics namespace to allow packages importing it from there to keep working, but this could change in the future. Packages that define S4 graphics for  should be re-installed and package code using such generics from other packages must ensure they are imported rather than relying on being looked for on the search path.S3 methods for class array now are dispatched for matrix objects.Reference counting now is used instead of the NAMED mechanism for determining when objects can be safely mutated into base C code. This reduces the need to copy in some cases and should allow future optimizations. It also is expected to help make internal code easier to maintain. and  in package tools now can check for specifierror or warning classes via the new optional second argument ., the utility for the data frame method , now works without parsing and explicit evaluation.Long vectors now are supported as the  argument of a  loop. now converts character columns to factors and factors to integers. now explicitly lists all exports in the NAMESPACE file.The internal implementation of grid units has changed. The only visible effects at the user level should be a slightly different print format for some units, faster performance for unit operations, and two new functions,  and .Printing  now uses a new  method.Packages must be re-installed under the new version of R.This version of R is built against the PCRE2 library for Perl-like regular expressions if available.The beginnings of support for C++ 20.Time needed to start a homogeneous PSOCK cluster on localhost with many nodes has been significantly reduced.There also are a number of deprecations. For example, make macro F77_VISIBILITY has been removed and replaced with F_VISIBILITY; deprecated support for specifiying C++ 98 for package installation has been removed; and many defunct functions have been removed from the base and methods packages. ", "pub_date": "2020-04-28"},
{"title": "How AIOps improves application monitoring", "overview": "Devops and site reliability engineers are vital to keep applications functioning. AIOps boosts effectiveness another notch", "image_url": "https://images.idgesg.net/images/article/2018/01/control_room_security_network_monitor_support-100745955-large.jpg", "url": "https://www.infoworld.com/article/3541308/how-aiops-improves-application-monitoring.html", "body": "IT operation teams use many tools to monitor, diagnose, and resolve system and application performance issues. In a recent survey of 1,300 IT professionals on the future of monitoring and AIOps, 42 percent report using more than 10 monitoring tools; 19 percent use more than 25 tools.That’s a lot of technology just to keep the lights on and provide the data required to monitor, alert, research, and resolve application incidents.Also on InfoWorld: Applying devops in data science and machine learningMonitoring tools are not one size fits all, especially for organizations running mission-critical applications in multicloud environments. As organizations invest in mobile apps, microservices, dataops, and data science programs, new monitoring tools are being added to provide domain-specific monitoring capabilities.AIOps platforms aim to simplify this landscape of monitoring tools. AIOps helps organizations that require high application service levels better manage the complexity of their monitoring tools and IT operational workflows. As the name suggests, AIOps brings machine learning and automation capabilities to the IT operations domain. These technologies aim to resolve incidents faster, identify operational trends that impact performance, and simplify the procedures required to resolve issues.AIOps is an emerging platform. In the survey, 42 percent of respondents either had never heard of AIOps or had thought that applying machine learning to operations was “not a thing.” Only 4 percent are using an AIOps tool in production today. Although AIOps is an emerging platform, there’s a solid business case for many organizations to consider it.AIOps is driven by business need and operational complexityMore businesses today rely on applications to serve customers and run operations. That drives higher requirements and expectations on the reliability, performance, and security of the applications.It also fuels demand for application development teams to build new applications and enhance them more frequently. The job responsibility of maintaining application service levels has also broadened during the past decade.Once upon a time, organizations staffed the NOC (network operations center) as the front line of defense. If you ever walked into a NOC, you would likely see dozens of computer monitors with warning lights and trend visuals to help the staff pinpoint issues—ideally before an end-user experienced one and opened tickets.Business and IT leaders began changing this model by introducing devops practices and site reliability engineers. Devops changes the IT department’s culture by establishing a collective responsibility to enable frequent deployments and better support customer and employee needs. Tools and practices such as CI/CD (continuous integration and continuous delivery) and IaC (infrastructure as code) are part of what enables more frequent deployments.  But devops practices also require a shared operational responsibility ensuring that applications are reliable, perform well, and are secure. That means more people in the IT organization need access to all the different monitoring tools.Many IT organizations also hire SREs (site reliability engineers) to connect development and operations. SREs take a software engineering approach to system administration topics. In another survey that targeted SREs, they indicate that incident response is a massive part of their job: 49 percent claim to respond to at least one incident every week.Maturing devops practices and hiring site reliability engineers is how a growing number of IT organizations are facing increasing operational challenges. But just expecting them to make sense of the dozens of monitoring tools being used is a recipe for poor performance.AIOps platform capabilities and technical architectureHow can AIOps improve the status quo? AIOps platforms typically have the following architecture components and capabilities:A central data platform for aggregating raw logs and data from different monitoring tools.Out-of-the-box integrations with the most common log formats, monitoring tools, IT service management tools, agile development tools, and other collaboration platforms.Machine learning capabilities to help identify patterns in the aggregated data.Consoles, dashboards, and analytics to help IT operations see and manage multiple systems from a central interface.Automation capabilities that enable IT to communicate status, route issues, and autorespond to common problems.What differentiates AIOps from other IT operational platforms is the ability to aggregate data easily, leverage machine learning to find problems, and use automation as a tool to resolve them. AIOps doesn’t replace the existing monitoring tools. It integrates with them so that more people in the IT department have improved visibility to problems without the complexity of learning and using multiple monitoring tools.Similarly, AIOps platforms typically don’t replace existing IT service management, workflow, agile, and other communication tools. Instead, they are a central platform to interface with them while alerting and resolving an incident.Monitoring mission-critical applications without AIOpsImagine your e-commerce application experiences slow performance when users try to complete a purchase. The first indicator that starts to send out alerts is the shopping cart abandonment rate.The e-commerce leader quickly opens a ticket about the issue in Cherwell’s mobile interface, but the IT team has already been alerted to the problem. As more users try to make purchases, the underlying Web servers hang and database connections stay open. Alerts from DataDog report these issues, and Splunk reports Java exceptions in the e-commerce application’s log files.Now imagine the NOC responding to this issue. Where should they start, given the number of alerts going off at the same time? The SREs called in to assist must also investigate the different alerts from different tools. Meanwhile, the e-commerce leader is upset because no one responded to her ticket!AIOps helps IT address issues faster and with less stressHere’s how AIOps platforms can potentially address this issue faster and more effectively.First, AIOps sees that multiple alerts are going off, including application alerts. It automatically alerts the SREs, and when one responds, it automatically updates Cherwell that the incident has been answered by an SRE. No one had to manually update any system to send out these communications.Second, the alerts from Cherwell, the e-commerce platform, Splunk, and DataDog are all aggregated and time sequenced. The SRE immediately knows which alert came before the others triggered. That’s incredibly useful because the SRE can quickly see that the Web server hanging and the pooling database connections all started after the Java application exceptions.The AIOps platform’s machine learning capabilities are fairly sophisticated, so in addition to reporting on alerts, it also highlights other outlier operating conditions. In this case, the e-commerce application has many slow outbound connections to a single IP address. There are no alerts or exceptions on this issue, but its timing precedes any of the other alerts.It doesn’t take the SRE much longer to figure out that this is a connection to a third-party service that validates the city, state, and ZIP code of the buyer. This service is clearly having performance issues that are rippling through the entire application.With a root cause identified, the SRE adds a high-severity defect to the e-commerce development team’s Jira backlog, alerting them to the problem. A high-severity issue flags the agile development team to disrupt their sprint and address it. It’s a quick fix to circumvent the impacting service, and it’s easy to test and deploy the change through their Jenkins CI/CD pipeline.The AIOps platform tracks this defect, the deployment, and the drop in all the alerts and keeps the e-commerce leader updated on the progress. Even though the SRE is monitoring the situation, the AIOps platform closes the issue automatically when all the monitors return to normal.Implementing this scenario isn’t trivial, but neither is it science fiction with AIOps platforms.", "pub_date": "2020-04-30"},
{"title": "When not to use AIops for cloudops", "overview": "The excitement around AIops means that it’s sometimes being deployed for the wrong reasons.  Here are a few situations where AIops is contraindicated", "image_url": "https://images.idgesg.net/images/article/2019/06/square-ped-round-hole-100799456-large.jpg", "url": "https://www.infoworld.com/article/3563908/when-not-to-use-aiops-for-cloudops.html", "body": "Artificial intelligence for IT operation platforms, better known as AIops, is an evolving and expanded use of technologies that for the past several years were categorized as IT operations analytics. The growth of AIops has been clear to anyone watching the market, but if you need some statistics, Gartner reports that by 2022, 40 percent of large enterprises will use AIops tools to support or replace monitoring and service desk tasks, up from 5 percent today.That’s a pretty big jump. However, it’s also an indication that many enterprises may pick AIops tools for the wrong purposes—mistakes that will likely cost millions. Here is what I’m seeing.Also on InfoWorld: AI, machine learning, and deep learning: Everything you need to know Those who haven’t planned a proper cloud solution for the business, and even an on-premises solutions paired with public clouds, are attempting to fix systemic issues. A poor plan will lead to performance problems and outages with AIops. Just like “you can’t fix stupid,” poorly planned architectures need to be corrected before you apply AIops tools and use them properly. AIops tools work on the assumption that the solution's configuration is sound before they can process alarms and resolutions properly. If not done in that order, you’ll just be teaching your AIops system how to correlate gigabytes of data coming from cloud and non-cloud systems, attempting fixes that are unlikely to be successful because they kick off other alerts and triggers. Those working cloudops now are, in essence, inventing a new discipline. Enterprises have seen the growth in cloudops specialists who are commanding some pretty good salaries. This has driven up costs, reducing the value they thought they would get by using public clouds.     I’ve seen enterprises invest in AIops tools based on a business case pointing to fewer ops team members needed, and the ability to automate costs out of the ops equation. Although there is a potential to reduce cost and staff much further down the road, AIops requires a great deal of ops expertise. You typically see AIops drive an expansion of the cloudops team, and costs initially rise for at least a few years. You have to invest in efficiency; you can’t remove dollars and expect good outcomes.  Those who do cloud security already understand that the ops automation processes is not a good way to defend your cloud-based applications and data. Indeed, the conflation of AIops with cloud security actually makes things less secure, considering that you’ll be dealing with security systems that are more complex.  AIops is one of those tools that needs to be implemented with great care in order to be effective. The market is moving at such speed that mistakes are going to happen; however, with a bit of common sense, you’ll find that AIops will eventually provide the value you’re looking for.       ", "pub_date": "2020-06-23"},
{"title": "AWS unveils open source model server for PyTorch", "overview": "Intended to ease production deployments of PyTorch models, TorchServe supports multi-model serving and model versioning for A/B testing", "image_url": "https://images.idgesg.net/images/article/2019/07/automation_iot_machine-learning_process_ai_artificial-intelligence-by-zapp2photo-getty-100802298-large.jpg", "url": "https://www.infoworld.com/article/3540415/aws-unveils-open-source-model-server-for-pytorch.html", "body": "Amazon Web Services (AWS) has unveiled an open source tool, called TorchServe, for serving PyTorch machine learning models. TorchServe is maintained by AWS in partnership with Facebook, which developed PyTorch, and is available as part of the PyTorch project on GitHub.Released on April 21, TorchServe is designed to make it easy to deploy PyTorch models at scale in production environments. Goals include lightweight serving with low latency, and high-performance inference.Also on InfoWorld: 5 reasons to choose PyTorch for deep learningThe key features of TorchServe include:Default handlers for common applications such as object detection and text classification, sparing users from having to write custom code to deploy models.Multi-model serving.Model versioning for A/B testing.Metrics for monitoring.RESTful endpoints for application integration.Any deployment environment can be supported by TorchServe, including Kubernetes, Amazon SageMaker, Amazon EKS, and Amazon EC2. TorchServe requires Java 11 on Ubuntu Linux or MacOS. Detailed installation instructions can be found on GitHub. ", "pub_date": "2020-04-24"},
{"title": "How to move data science into production", "overview": "With new Integrated Deployment extensions, data scientists can capture entire KNIME workflows for automatic deployment to production or reuse", "image_url": "https://images.idgesg.net/images/article/2018/01/robot_intelligence_production_automat_conveyor-100746070-large.jpg", "url": "https://www.infoworld.com/article/3541230/how-to-move-data-science-into-production.html", "body": "Deploying data science into production is still a big challenge. Not only does the deployed data science need to be updated frequently but available data sources and types change rapidly, as do the methods available for their analysis. This continuous growth of possibilities makes it very limiting to rely on carefully designed and agreed-upon standards or work solely within the framework of proprietary tools.KNIME has always focused on delivering an open platform, integrating the latest data science developments by either adding our own extensions or providing wrappers around new data sources and tools. This allows data scientists to access and combine all available data repositories and apply their preferred tools, unlimited by a specific software supplier’s preferences. When using KNIME workflows for production, access to the same data sources and algorithms has always been available, of course. Just like many other tools, however, transitioning from data science creation to data science production involved some intermediate steps.Also on InfoWorld: Artificial intelligence today: What’s hype and what’s realIn this post, we are describing a recent addition to the KNIME workflow engine that allows the parts needed for production to be captured directly within the data science creation workflow, making deployment fully automatic while still allowing every module to be used that is available during data science creation.Why is deploying data science in production so hard?At first glance, putting data science in production seems trivial: Just run it on the production server or chosen device! But on closer examination, it becomes clear that what was built during data science creation is not what is being put into production.I like to compare this to the chef of a Michelin star restaurant who designs recipes in his experimental kitchen. The path to the perfect recipe involves experimenting with new ingredients and optimizing parameters: quantities, cooking times, etc. Only when satisfied, are the final results — the list of ingredients, quantities, procedure to prepare the dish — put into writing as a recipe. This recipe is what is moved “into production,” i.e., made available to the millions of cooks at home that bought the book.This is very similar to coming up with a solution to a data science problem. During data science creation, different data sources are investigated; that data is blended, aggregated, and transformed; then various models (or even combinations of models) with many possible parameter settings are tried out and optimized. What we put into production is not all of that experimentation and parameter/model optimization — but the combination of chosen data transformations together with the final best (set of) learned models.This still sounds easy, but this is where the gap is usually biggest. Most tools allow only a subset of possible models to be exported; many even ignore the preprocessing completely. All too often what is exported is not even ready to use but is only a model representation or a library that needs to be consumed or wrapped into yet another tool before it can be put into production. As a result, the data scientists or model operations team needs to add the selected data blending and transformations manually, bundle this with the model library, and wrap all of that into another application so it can be put into production as a ready-to-consume service or application. Lots of details get lost in translation.For our Michelin chef above, this manual translation is not a huge issue. She only creates or updates recipes every other year and can spend a day translating the results of her experimentation into a recipe that works in a typical kitchen at home. For our data science team, this is a much bigger problem: They want to be able to update models, deploy new tools, and use new data sources whenever needed, which could easily be on a daily or even hourly basis. Adding manual steps in between not only slows this process to a crawl but also adds many additional sources of error.The diagram below shows how data science creation and productionization intertwine. This is inspired by the classic CRISP-DM cycle but puts stronger emphasis on the continuous nature of data science deployment and the requirement for constant monitoring, automatic updating, and feedback from the business side for continuous improvements and optimizations. It also distinguishes more clearly between the two different activities: creating data science and putting the resulting data science process into production.Often, when people talk about “end-to-end data science,” they really only refer to the cycle on the left: an integrated approach covering everything from data ingestion, transforming, and modeling to writing out some sort of a model (with the caveats described above). Actually consuming the model already requires other environments, and when it comes to continued monitoring and updating of the model, the tool landscape becomes even more fragmented. Maintenance and optimization are, in many cases, very infrequent and heavily manual tasks as well. On a side note: We avoid the term “model ops” purposely here because the data science production process (the part that’s moved into “operations”) consists of much more than just a model.Removing the gap between data science creation and data science productionIntegrated deployment removes the gap between data science creation and data science production by enabling the data scientist to model both creation as well as production within the same environment by capturing the parts of the process that are needed for deployment. As a result, whenever changes are made in data science creation, these changes are automatically reflected in the deployed extract as well. This is conceptually simple but surprisingly difficult in reality.If the data science environment is a programming or scripting language, then you have to be painfully detailed about creating suitable subroutines for every aspect of the overall process that could be useful for deployment — also making sure that the required parameters are properly passed between the two code bases. In effect, you have to write two programs at the same time, ensuring that all dependencies between the two are always observed. It is easy to miss a little piece of data transformation or a parameter that is needed to properly apply the model.Using a visual data science environment can make this more intuitive. The new Integrated Deployment node extensions from KNIME allow those pieces of the workflow that will also be needed in deployment to be framed or captured. The reason this is so simple is that those pieces are naturally a part of the creation workflow. This is because first, the exact same transformation pieces are needed during model training, and second, evaluation of the models is needed during fine tuning. The following image shows a very simple example of what this looks like in practice:The purple boxes capture the parts of the data science creation process that are also needed for deployment. Instead of having to copy them or having to go through an explicit “export model” step, now we simply add Capture-Start/Capture-End nodes to frame the relevant pieces and use the Workflow-Combiner to put the pieces together. The resulting, automatically created workflow is shown below:The Workflow-Writer nodes come in different shapes that are useful for all possible ways of deployment. They do just what their name implies: write out the workflow for someone else to use as a starting point. But more powerful is the ability to use Workflow-Deploy nodes that automatically upload the resulting workflow as a REST service or as an analytical application to KNIME Server or deploy it as a container — all possible by using the appropriate Workflow-Deploy node.Many data science solutions promise end-to-end data science, complete model-ops, and other flavors of “complete deployment.” Below is a checklist that covers typical limitations.Can you mix and match technologies (R, Python, Spark, TensorFlow, cloud, on-prem), or are you limited to a particular technology/environment only?Can you use the same set of tools during creation as well as the deployment setup, or does one of the two only cover a subset of the other?Can you deploy automatically into a service (e.g., REST), an application, or a scheduled job, or is the deployment only a library/model that needs to be embedded elsewhere?Is the deployment fully automatic, or are (manual) intermediate steps required?Can you roll back automatically to previous versions of both the data science creation process and the models in production?Can you run both creation as well as production processes years later with guaranteed backward compatibility of all results?Can a revised data science process be deployed in less than one minute?The purpose of this article is not to describe the technical aspects in great detail. Still, it is important to point out that this capture and deploy mechanism works for all nodes in KNIME — nodes that provide access to native data transformation and modeling techniques as well as nodes that wrap other libraries such as TensorFlow, R, Python, Weka, Spark, and all of the other third-party extensions provided by KNIME, the community, or the partner network.With the new Integrated Deployment extensions, KNIME workflows turn into a complete data science creation and productionization environment. Data scientists building workflows to experiment with built-in or wrapped techniques can capture the workflow for direct deployment within that same workflow. For the first time, this enables instantaneous deployment of the complete data science process directly from the environment used to create that process.TwitterLinkedInKNIME blog—newtechforum@infoworld.com", "pub_date": "2020-04-30"},
{"title": "GPipe and PipeDream: Scaling AI training in every direction", "overview": "New frameworks Google GPipe and Microsoft PipeDream join Uber Horovod in distributed training for deep learning", "image_url": "https://images.idgesg.net/images/article/2019/09/light-bulb-100811494-large.jpg", "url": "https://www.infoworld.com/article/3539741/gpipe-and-pipedream-scaling-ai-training-in-every-direction.html", "body": "Data science is hard work, not a magical incantation. Whether an AI model performs as advertised depends on how well it’s been trained, and there’s no “one size fits all” approach for training AI models.The necessary evil of distributed AI trainingScaling is one of the trickiest considerations when training AI models. Training can be especially challenging when a model grows too resource hungry to be processed in its entirety on any single computing platform. A model may have grown so large it exceeds the memory limit of a single processing platform, or an accelerator has required developing special algorithms or infrastructure. Training data sets may grow so huge that training takes an inordinately long time and becomes prohibitively expensive.Also on InfoWorld: Artificial intelligence predictions for 2020Scaling can be a piece of cake if we don’t require the model to be particularly good at its assigned task. But as we ramp up the level of inferencing accuracy required, the training process can stretch on longer and chew up ever more resources. Addressing this issue isn’t simply a matter of throwing more powerful hardware at the problem. As with many application workloads, one can’t rely on faster processors alone to sustain linear scaling as AI model complexity grows.Distributed training may be necessary. If the components of a model can be partitioned and distributed to optimized nodes for processing in parallel, the time needed to train a model can be reduced significantly. However, parallelization can itself be a fraught exercise, considering how fragile a construct a statistical model can be.The model may fail spectacularly if some seemingly minor change in the graph—its layers, nodes, connections, weights, hyperparameters, etc.—disrupts the model’s ability to make accurate inferences. Even if we leave the underlying graph intact and attempt to partition the model’s layers into distributed components, we will then need to recombine their results into a cohesive whole.If we’re not careful, that may result in a recombined model that is skewed in some way in the performance of its designated task.New industry frameworks for distributed AI trainingThroughout the data science profession, we continue to see innovation in AI model training, with much of it focusing on how to do it efficiently in multiclouds and other distributed environments.In that regard, Google and Microsoft recently released new frameworks for training deep learning models: Google’s GPipe and Microsoft’s PipeDream. The frameworks follow similar scaling principles.Though different in several respects, GPipe and PipeDream share a common vision for distributed AI model training. This vision involves the need to:Free AI developers from having to determine how to split specific models given a hardware deployment.Train models of any size, type, and structure on data sets of any scale and format, and in a manner that is agnostic to the intended inferencing task.Partition models so that parallelized training doesn’t distort the domain knowledge (such as how to recognize a face) that it was designed to represent.Parallelize training in a fashion that is agnostic to the topology of distributed target environments.Parallelize both the models and the data in a complex distributed training pipeline.Boost GPU (graphics processing units) compute speeds for various training workloads.Enable efficient use of hardware resources in the training process. Reduce communication costs at scale when training on cloud infrastructure.Also on InfoWorld: AI, machine learning, and deep learning: Everything you need to knowScaling training when models and networks become extremely complexWhat distinguishes these two frameworks is the extent to which they support optimized performance of training workflows for models with sequential layers (which is always more difficult to parallelize) and in more complex target environments, such as multicloud, mesh, and cloud-to-edge scenarios.Google’s GPipe is well suited for fast parallel training of deep neural networks that incorporate multiple sequential layers. It automatically does the following:Partitions models and moves the partitioned models to different accelerators, such as GPUs or TPUs (Tensor processing units), which have special hardware that has been optimized for different training workloads.Splits a mini-batch of training examples into smaller micro-batches that can be processed by the accelerators in parallel.Enables internode distributed learning by using synchronous stochastic gradient descent and pipeline parallelism over a distributed machine learning library.Microsoft’s PipeDream also exploits model and data parallelism, but it’s more geared to boosting performance of complex AI training workflows in distributed environments. One of the AI training projects in Microsoft Research’s Project Fiddle initiative, PipeDream accomplishes this automatically because it can:Separate internode computation and communication in a way that leads to easier parallelism of data and models in distributed AI training.Partition AI models into stages that consist of a consecutive set of layers.Map each stage to a separate GPU that performs the forward and backward pass neural network functions for all layers in that stage.Determine how to partition the models based on a profiling run that is performed on a single GPU.Balance computational loads among different model partitions and nodes, even when a training environment’s distributed topology is highly complex.Minimize communications among the distributed worker nodes that handle the various partitions, given that each worker has to communicate only to a single other worker and to communicate only subsets of the overall model’s gradients and output activations.Further details on both frameworks are in their respective research papers: GPipe and PipeDream.The need for consensus and scalabilityTraining is a critical feature of AI’s success, and more AI professionals are distributing these workflows across multiclouds, meshes, and distributed edges.Going forward, Google and Microsoft should align their respective frameworks into an industry consensus approach for distributed AI training. They might want to consider engaging Uber in this regard. The ride sharing company already has a greater claim to the first-to-market distinction in distributed training frameworks. It open sourced its Horovod project three years ago. The project, which is hosted by the Linux Foundation’s AI Foundation, has been integrated with leading AI modeling environments such as TensorFlow, PyTorch, Keras, and Apache MXNet.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesScalability should be a core consideration of any and all such frameworks. Right now, Horovod has some useful features in that regard but lacks the keen scaling focus that Google and Microsoft have built into their respective projects. In terms of scalability, Horovod can run on single or multiple GPUs, and even on multiple distributed hosts without code changes. It is capable of batching small operations, automating distributed tuning, and interleaving communication and computation pipelines.Scalability concerns will vary depending on what training scenario you consider. Regardless of which framework becomes dominant—GPipe, PipeDream, Horovod, or something else—it would be good to see industry development of reference workflows for distributed deployment of the following specialized training scenarios:Semisupervised learning that uses small amounts of labeled data (perhaps crowdsourced from human users in mobile apps) to accelerate pattern identification in large, unlabeled data sets, such as those ingested through IoT devices’ cameras, microphones, and environmental sensors.Reinforcement learning that involves building AI modules, such as those deployed in industrial robots, that can learn autonomously with little or no “ground truth” training data, though possibly with human guidance.Collaborative learning that has distributed AI modules, perhaps deployed in swarming drones, that collectively explore, exchange and exploit optimal hyperparameters, thereby enabling all modules to converge dynamically on the optimal trade-off of learning speed versus accuracy.Evolutionary learning that trains a group of AI-driven entities (perhaps mobile and IoT endpoints) through a procedure that learns from an aggregate of self-interested decisions they make, based both on entity-level knowledge and on varying degrees of cross-entity model-parameter sharing.Transfer learning that reuses any relevant training data, feature representations, neural-node architectures, hyperparameters and other properties of existing models, such as those executed on peer nodes.On-device training that enables apps to ingest freshly sensed local data and rapidly update the specific AI models persisted in those devices.Robot navigation learning that works with raw sensory inputs, exploits regularities in environment layouts, and requires only a little bit of training data.This list doesn’t even begin to hint at the diversity of distributed AI training workflows that will be prevalent in the future. To the extent that we have standard reference frameworks in place in 2020, data scientists will have a strong foundation for carrying the AI revolution forward in every direction.", "pub_date": "2020-04-23"},
{"title": "MongoDB Atlas unveils multicloud cluster support", "overview": "MongoDB Atlas database-as-a-service now allows distributed MongoDB databases to span the Amazon, Google, and Microsoft clouds", "image_url": "https://images.idgesg.net/images/article/2018/08/atlas_on_the_top_of_schloss_linderhof_by_gustavo_trapp_public_domain_1200x800-100769693-large.jpg", "url": "https://www.infoworld.com/article/3586628/mongodb-atlas-unveils-multicloud-cluster-support.html", "body": "NoSQL database vendor MongoDB will enable users to deploy a distributed MongoDB database across AWS, Google Cloud, and Microsoft Azure via a multicloud clusters capability being added to the company’s MongoDB Atlas cloud database service.Through the multicloud clusters functionality, MongoDB Atlas users are spared the operational complexity of managing data replication and migration across clouds. The multicloud support lets users take advantage of unique capabilities and the reach of the different cloud providers while also providing for uptime assurances.Also on InfoWorld: The best open source software of 2020Starting today, MongoDB Atlas, which is available in 79 AWS, Google Cloud, and Microsoft Azure regions worldwide, now supports automatic fail-over to another cloud serving the same geographic area to preserve low-latency access and data resiliency requirements. Previously, MongoDB Atlas users had to select a single cloud provider per deployment.MongoDB cites several thematic areas of the multicloud support:Taking advantage of cloud-specific functionality, such as having a user whose primary cloud is AWS but wants to leverage Google Cloud machine learning capabilities.Data mobility, with users able to move data from one cloud to another with no downtime.High availability, with users able to spread redundancy across three providers and withstand global outages of any one of them.Enabling companies to expand their cloud access beyond their established platform. Some companies may be required to use or not use a specific cloud, based on customer requirements.MongoDB provides a document-oriented NoSQL database that features an object-native JSON-style data model, as opposed to storing data across tables as relational databases such as Oracle do, MongoDB said. Applications deployed on MongoDB Atlas range from major consumer financial apps to large-scale games. ", "pub_date": "2020-10-20"},
{"title": "The best free data science courses during lockdown", "overview": "6 excellent online courses and one book to learn statistics, machine learning, and deep learning while you’re locked in the house", "image_url": "https://images.techhive.com/images/article/2015/12/thinkstockphotos-466788590-100632491-large.jpg", "url": "https://www.infoworld.com/article/3540434/the-best-free-data-science-courses-during-lockdown.html", "body": "If you are locked down because of the COVID-19 pandemic, you just might have some extra time on your hands. Binging Netflix is all well and good, but perhaps you are getting tired of that and you would like to learn something new.One of the most lucrative fields to open up in the last couple of years is data science. The resources I list below will help those technical enough to understand math at the level of statistics and differential calculus to incorporate machine learning into their skill sets. They might even help you start a new career as a data scientist. Also on InfoWorld: Artificial intelligence predictions for 2020If you already can program in Python or R, that skill will give you a leg up on applied data science. On the other hand, the programming isn’t the hard part for most people — it’s the numerical methods.Coursera offers many of the following courses. You can audit them for free, but if you want credit you need to pay for them.I recommend starting with the book  so that you can learn the math and the concepts before you start writing code.I should also note that there are several good courses at Udemy, although they are not free. They usually cost about $200 each for lifetime access, but I’ve seen many of them discounted to less than $20 in recent days.Jeff Prosise of Wintellectnow tells me that he’s planning to make a few more of his courses free, so stay tuned.The Elements of Statistical Learning, Second EditionBy Trevor Hastie, Robert Tibshirani, and Jerome Friedman, Springerhttps://web.stanford.edu/~hastie/Papers/ESLII.pdfThis free 764-page ebook is one of the most widely recommended books for beginners in data science. It explains the fundamentals of machine learning and how everything works behind the scenes, but contains no code. Should you prefer a version of the book with applications in R, you can buy or rent it through Amazon.Applied Data Science with Python SpecializationBy Christopher Brooks, Kevyn Collins-Thompson, V. G. Vinod Vydiswaran, and Daniel Romero, University of Michigan/Courserahttps://www.coursera.org/specializations/data-science-pythonThe five courses (89 hours) in this University of Michigan specialization introduce you to data science through the Python programming language. This specialization is intended for learners who have a basic Python or programming background, and who want to apply statistical, machine learning, information visualization, text analysis, and social network analysis techniques through popular Python toolkits such as Pandas, Matplotlib, Scikit-learn, NLTK, and NetworkX to gain insight into their data.Data Science: Foundations using R SpecializationBy Jeff Leek, Brian Caffo, and Roger Peng, Johns Hopkins/Courserahttps://www.coursera.org/specializations/data-science-foundations-rThis 68-hour specialization (five courses) covers foundational data science tools and techniques, including getting, cleaning, and exploring data, programming in R, and conducting reproducible research.Deep LearningBy Andrew Ng, Kian Katanforoosh, and Younes Bensouda Mourri, Stanford/deeplearning.ai/Courserahttps://www.coursera.org/specializations/deep-learningIn 77 hours (five courses) this series teaches the foundations of deep learning, how to build neural networks, and how to lead successful machine learning projects. You will learn about Convolutional networks (CNNs), Recurrent neural networks (RNNs), Long Short Term Memory networks (LSTM), Adam, Dropout, BatchNorm, Xavier/He initialization, and more. You will work on case studies from healthcare, autonomous driving, sign language reading, music generation, and natural language processing. In addition to the theory, you will learn how it is applied in industry using Python and TensorFlow, which they also teach.Fundamentals of Machine LearningBy Jeff Prosise, Wintellectnowhttps://www.wintellectnow.com/Videos/Watch?videoId=fundamentals-of-machine-learningIn this free two-hour introductory video course, Prosise takes you through regression, classification, Support Vector Machines, Principal Component Analysis, and more, using Scikit-learn, the popular Python library for machine learning. Machine LearningBy Andrew Ng, Stanford/Courserahttps://www.coursera.org/learn/machine-learningThis 56-hour video course provides a broad introduction to machine learning, data mining, and statistical pattern recognition. Topics include supervised learning (parametric/non-parametric algorithms, support vector machines, kernels, neural networks), unsupervised learning (clustering, dimensionality reduction, recommender systems, deep learning), and best practices in machine learning and AI (bias/variance theory and innovation process). You’ll also learn how to apply learning algorithms to building smart robots, web search, anti-spam, computer vision, medical informatics, audio, database mining, and other areas.Machine LearningBy Carlos Guestrin and Emily Fox , University of Washington/Courserahttps://www.coursera.org/specializations/machine-learningThis 143-hour (four course) specialization from leading researchers at the University of Washington introduces you to the exciting, high-demand field of Machine Learning. Through a series of practical case studies, you will gain applied experience in major areas of Machine Learning including Prediction, Classification, Clustering, and Information Retrieval. You will learn to analyze large and complex datasets, create systems that adapt and improve over time, and build intelligent applications that can make predictions from data.", "pub_date": "2020-04-27"},
{"title": "3 cloud architecture secrets your cloud provider won’t tell you", "overview": "You may think you know everything about the proper configuration of a cloud computing solution, but cloud providers are keeping a few things to themselves.", "image_url": "https://images.idgesg.net/images/article/2020/07/magnifying_lens_examines_top_secret_confidential_trade_secrets_by_dny59_gettyimages-185124853_abstract_binary_by_traffic_analyzer_gettyimages-1176400518_2400x1600-100851059-large.jpg", "url": "https://www.infoworld.com/article/3584411/3-cloud-architecture-secrets-your-cloud-provider-wont-tell-you.html", "body": "Do you have an optimized architecture? This means that your solution maximizes efficiency and minimizes costs. You’ve selected the right cloud resources to configure the best storage systems, databases, and compute platforms—at least that’s what you think.What I’m seeing out there, over and over again, is the selection of the wrong cloud resources for the wrong reasons. Cloud providers are pushing something that maximizes their revenue rather than being right for you. Also on InfoWorld: When hybrid multicloud has technical advantagesSo, here are three cloud architecture secrets that you’ll never hear from your cloud provider:You’ve probably heard that it’s better to go with a native database, cloudops system, or security system that’s part of a single public cloud offering. Now that we’ve moved to a mostly multicloud world, that’s just not the case.   It’s much better to pick general-purpose and heterogeneous solutions that span public clouds instead of a native solution that’s only good on a single public cloud. You’ll never see this in the architecture guide offered by your cloud provider. Non-native resources should be considered each and every time.   Cloud solutions that depend on a lot of data ingress and egress are almost never a good idea. No brainer, considering that you’ll see data leaving and entering a public cloud provider on your monthly cloud bill, and it is not cheap. However, this is often overlooked when considering a core architecture.   This is typically an issue for IT organizations that want to keep some data on-premises, usually due to outdated concerns about compliance and security. The providers won’t advise you otherwise, considering that they make bank on the exit and entrance charges. Keep your data in the cloud if you’re looking for the best performance and security and the lowest costs.I often see security systems bound to a single application’s workload. The application leverages its own encryption system, identity management systems, role-based security, etc. Typically, these are also native to a single cloud provider where the application is hosted. The issue here is that a cloud provider wants the workload in the cloud ASAP and will often advise for the speed of movement instead of a sound security architecture. This can’t scale, considering that you’ll be creating one-off security solutions for all applications, and it will create so much security complexity that you’ll have security issues just from the complexity.Security should be systemic to all things in the core architecture. Applications should use very similar security patterns—and the same security systems, if at all possible. Again, these are typically non-native, and your cloud provider won’t benefit as much.By the way, I’m not picking on cloud providers. They are only acting in their best interests. However, the savvier you are, the more you know when to accept and reject their advice.  ", "pub_date": "2020-10-02"},
{"title": "The best open source software of 2020", "overview": null, "image_url": null, "url": "https://www.infoworld.com/article/3575858/the-best-open-source-software-of-2020.html", "body": "", "pub_date": "2020-10-05"},
{"title": "Spell machine learning platform goes on-prem", "overview": "An end-to-end machine learning platform designed for ease of use, Spell now offers incarnations for both public cloud and data center deployment", "image_url": "https://images.idgesg.net/images/article/2019/11/ai_artificial_intelligence_ml_machine_learning_vector_by_kohb_gettyimages_1146634284-100817775-large.jpg", "url": "https://www.infoworld.com/article/3541757/spell-machine-learning-platform-goes-on-prem.html", "body": "Spell, an end-to-end platform for machine learning and deep learning—covering data prep, training, deployment, and management—has announced Spell for Private Machines, a new version of its system that can be deployed on your own hardware as well as on cloud resources.Spell was founded by  Piantino, former director of engineering at Facebook and founder of Facebook’s AI Research group. Spell allows teams to create reproducible machine learning systems that incorporate familiar tools such as Jupyter notebooks and that leverage cloud-hosted GPU compute instances.Also on InfoWorld: The best free data science courses during quarantineSpell emphasizes ease of use. For example, hyperparameter optimization for an experiment is a high-level, one-command function. Nor must users do much to configure the infrastructure; Spell detects what hardware is available and orchestrates to suit. Spell also organizes experiment assets, so both experiments and their data can be versioned and check-pointed as part of the development process.Spell originally ran only in the cloud; there’s been no “behind-the-firewall” deployment until now. Spell For Private Machines allows developers to run the platform on their own hardware. Both on-prem and cloud resources can be mixed and matched as needed. For instance, a prototype version of a project could be created on local hardware, then scaled out to an AWS instance for production deployment.Much of Spell’s workflow is already designed to feel as if it runs locally, and to complement existing workflows. Python tools for Spell work can be set up with , for example. And because the Spell runtime uses containers, multiple versions of an experiment with different hyperparameter turnings can be run side by side. ", "pub_date": "2020-05-06"},
{"title": "6 Python libraries for parallel processing", "overview": "Want to distribute that heavy Python workload across multiple CPUs or a compute cluster? These frameworks can make it happen", "image_url": "https://images.idgesg.net/images/article/2019/11/women-spinning-plates_asynchronous-programming_synchrony_multi-tasking_by-graemenicholson-getty-images-2400x1600-100818018-large.jpg", "url": "https://www.infoworld.com/article/3542595/6-python-libraries-for-parallel-processing.html", "body": "Python is long on convenience and programmer-friendliness, but it isn’t the fastest programming language around. Some of its speed limitations are due to its default implementation, cPython, being single-threaded. That is, cPython doesn’t use more than one hardware thread at a time.And while you can use the  module built into Python to speed things up,  only gives you , not . It’s good for running multiple tasks that aren’t CPU-dependent, but does nothing to speed up multiple tasks that each require a full CPU. Also on InfoWorld: The best free data science courses during quarantinePython does include a native way to run a Python workload across multiple CPUs. The  module spins up multiple copies of the Python interpreter, each on a separate core, and provides primitives for splitting tasks across cores. But sometimes even  isn’t enough.Sometimes the job calls for distributing work not only across , but also across . That’s where these six Python libraries and frameworks come in. All six of the Python toolkits below allow you to take an existing Python application and spread the work across multiple cores, multiple machines, or both.RayDeveloped by a team of researchers at the University of California, Berkeley, Ray underpins a number of distributed machine learning libraries. But Ray isn’t limited to machine learning tasks alone, even if that was its original use case. Any Python tasks can be broken up and distributed across systems with Ray.Ray’s syntax is minimal, so you don’t need to rework existing apps extensively to parallelize them. The  decorator distributes that function across any available nodes in a Ray cluster, with optionally specified parameters for how many CPUs or GPUs to use. The results of each distributed function are returned as Python objects, so they’re easy to manage and store, and the amount of copying across or within nodes is kept to a minimum. This last feature comes in handy when dealing with NumPy arrays, for instance.Ray even includes its own built-in cluster manager, which can automatically spin up nodes as needed on local hardware or popular cloud computing platforms.DaskFrom the outside, Dask looks a lot like Ray. It, too, is a library for distributed parallel computing in Python, with its own task scheduling system, awareness of Python data frameworks like NumPy, and the ability to scale from one machine to many.Dask works in two basic ways. The first is by way of parallelized data structures — essentially, Dask’s own versions of NumPy arrays, lists, or Pandas DataFrames. Swap in the Dask versions of those constructions for their defaults, and Dask will automatically spread their execution across your cluster. This typically involves little more than changing the name of an import, but may sometimes require rewriting to work completely.The second way is through Dask’s low-level parallelization mechanisms, including function decorators, that parcel out jobs across nodes and return results synchronously (“immediate” mode) or asynchronously (“lazy”). Both modes can be mixed as needed, too.One key difference between Dask and Ray is the scheduling mechanism. Dask uses a centralized scheduler that handles all tasks for a cluster. Ray is decentralized, meaning each machine runs its own scheduler, so any issues with a scheduled task are handled at the level of the individual machine, not the whole cluster.Also on InfoWorld: 8 great Python libraries for natural language processingDask also offers an advanced and still experimental feature called “actors.” An actor is an object that points to a job on another Dask node. This way, a job that requires a lot of local state can run in-place and be called remotely by other nodes, so the state for the job doesn’t have to be replicated. Ray lacks anything like Dask’s actor model to support more sophisticated job distribution.DispyDispy lets you distribute whole Python programs or just individual functions across a cluster of machines for parallel execution. It uses platform-native mechanisms for network communication to keep things fast and efficient, so Linux, MacOS, and Windows machines work equally well.Dispy syntax somewhat resembles  in that you explicitly create a cluster (where  would have you create a process pool), submit work to the cluster, then retrieve the results. A little more work may be required to modify jobs to work with Dispy, but you also gain precise control over how those jobs are dispatched and returned. For instance, you can return provisional or partially completed results, transfer files as part of the job distribution process, and use SSL encryption when transferring data.Pandaral·lelPandaral·lel, as the name implies, is a way to parallelize Pandas jobs across multiple nodes. The downside is that Pandaral·lel works  with Pandas. But if Pandas is what you’re using, and all you need is a way to accelerate Pandas jobs across multiple cores on a single computer, Pandaral·lel is laser-focused on the task.Note that while Pandaral·lel does run on Windows, it will run only from Python sessions launched in the Windows Subsystem for Linux. MacOS and Linux users can run Pandaral·lel as-is. IpyparallelIpyparallel is another tightly focused multiprocessing and task-distribution system, specifically for parallelizing the execution of Jupyter notebook code across a cluster. Projects and teams already working in Jupyter can start using Ipyparallel immediately.Ipyparallel supports many approaches to parallelizing code. On the simple end, there’s , which applies any function to a sequence and splits the work evenly across available nodes. For more complex work, you can decorate specific functions to always run remotely or in parallel.Jupyter notebooks support “magic commands” for actions only possible in a notebook environment. Ipyparallel adds a few magic commands of its own. For example, you can prefix any Python statement with  to automatically parallelize it.JoblibJoblib has two major goals: run jobs in parallel and don’t recompute results if nothing has changed. These efficiencies make Joblib well-suited for scientific computing, where reproducible results are sacrosanct. Joblib’s documentation provides plenty of examples for how to use all its features.Joblib syntax for parallelizing work is simple enough—it amounts to a decorator that can be used to split jobs across processors, or to cache results. Parallel jobs can use threads or processes.Also on InfoWorld: 8 signs you’re doing Python rightJoblib includes a transparent disk cache for Python objects created by compute jobs. This cache not only helps Joblib avoid repeating work, as noted above, but can also be used to suspend and resume long-running jobs, or pick up where a job left off after a crash. The cache is also intelligently optimized for large objects like NumPy arrays. Regions of data can be shared in-memory between processes on the same system by using .One thing Joblib does not offer is a way to distribute jobs across multiple separate computers. In theory it’s possible to use Joblib’s pipeline to do this, but it’s probably easier to use another framework that supports it natively. What is Python? Powerful, intuitive programmingWhat is PyPy? Faster Python without painWhat is Cython? Python at the speed of CCython tutorial: How to speed up PythonHow to install Python the smart wayThe best new features in Python 3.8Better Python project management with PoetryVirtualenv and venv: Python virtual environments explainedPython virtualenv and venv do’s and don’tsPython threading and subprocesses explainedHow to use the Python debuggerHow to use timeit to profile Python codeHow to use cProfile to profile Python codeGet started with async in PythonHow to use asyncio in PythonHow to convert Python to JavaScript (and back again)Python 2 EOL: How to survive the end of Python 212 Pythons for every programming need24 Python libraries for every Python developer7 sweet Python IDEs you might have missed3 major Python shortcomings—and their solutions13 Python web frameworks compared4 Python test frameworks to crush your bugs6 great new Python features you don’t want to miss5 Python distributions for mastering machine learning8 great Python libraries for natural language processing", "pub_date": "2020-05-13"},
{"title": "Hybrid cloud is where the action is", "overview": "Enterprises should be more concerned with getting their hybrid cloud strategy right than worrying about multicloud.", "image_url": "https://images.techhive.com/images/article/2016/11/rock_panorama-100696133-large.jpg", "url": "https://www.infoworld.com/article/3583974/hybrid-cloud-is-where-the-action-is.html", "body": "Multicloud is definitely a thing. However, it’s not exactly clear what that “thing” is. According to new survey data from database vendor MariaDB, 71% of survey respondents report running databases on at least two different cloud providers today. Yet when asked what would keep them from going all in on a cloud database, a vendor’s “lack of a multicloud offering” ranked dead last. In other words, everyone is doing multicloud, but no one knows why.Which perhaps supports Gartner analyst Lydia Leong’s contention that, “Most organizations multicloud, rather than  to be multicloud in a deliberate and structured way.” Pragmatism, not dogmatism, rules.The database is getting cloudyIf you’re bullish on cloud, MariaDB’s survey data will confirm your bias. According to the survey, roughly 89% of those surveyed expect to see at least 50% of their databases running in the cloud by 2025. Nor is MariaDB’s data an outlier here: Gartner goes one step further, projecting that 75% of all databases will be deployed or migrated to a cloud platform by 2022.As much as I hope this is true (I work for a cloud vendor, after all), I will admit that it seems overly ambitious. Let’s do some math. First, Gartner pegs global IT spending at $3.9 trillion in 2020. Cloud spending, by contrast, will hit $266 billion. I’m no math genius but I think that means just 7% of IT spending is going toward cloud this year (and, of course, a fraction of that will be spent on databases).It’s  that tens of billions of dollars in database cloud spend will shift from on-premises to cloud workloads within two years, but IT tends to move slowly, particularly with databases. This isn’t to suggest cloud isn’t a big deal. It is. But it’s going to take time.And yet...There are plenty of reasons for organizations to want to move their database workloads to the cloud. According to the MariaDB survey, here are the primary benefits companies already derive from cloud databases:Ease of use (24.0% of respondents)Time savings (23.6%)Modernization (21.3%)Cost savings (21.1%)[Frees up resources to] Focus on other business aspects (9.3%)Pretty compelling, right? Spurred by these benefits, respondents were asked what would keep them from going all in on a cloud database:Security (73.3%)Price (46.2%)Compatibility (44.9%)Scalability (35.2%)Migration (33.1%)Lack of multi-cloud offering (21.1%)Other (1.3%)It’s interesting that “multicloud” ranks last here, despite (as noted above) the fact that 71% report running databases in at least two clouds. Multicloud doesn’t seem to be a motivation to buy. Rather, it’s just a fact of enterprise existence.Enterprise is as enterprise doesMany years ago, Billy Marshall coined the phrase, “The CIO is the last to know.” He was referring to how open source software makes its way into enterprises through developer laptops, but the same phenomenon plays out today with cloud services, even at smaller companies. At one startup we first guessed how many SaaS applications we had running; I don’t remember the number but it was small, like 10. When we actually tallied up the true number it was closer to 80.As noted above, “ease of use” is the primary benefit companies derive from the cloud. This convenience factor opens the door to more cloud finding its way into the enterprise, not less. Leong calls out this longstanding enterprise principle with her typical style:Multicloud is inevitable in almost all organizations. Cloud IaaS+PaaS spans such a wide swathe of IT functions that it’s impractical and unrealistic to assume that the organization will be single-vendor over the long term. Just like the enterprise tends to have at least three of everything (if not 10 of everything), the enterprise is similarly not going to resist the temptation of being multicloud, even if it’s complex and challenging to manage, and significantly increases management costs. It is a rare organization that both has diverse business needs and can exercise the discipline to use a single provider.To Leong’s point, multicloud may be a fact of life but it doesn’t tend to be one that companies choose, for the reasons she cites.But what  an intentional, critically important factor in cloud (and cloud databases) today is hybrid. Asked how important hybrid cloud was to corporate objectives, respondents were very clear:Not at all important (2.5%)Somewhat important (19.4%)Very important (39.2%)Extremely important (38.9%)That is, 97.5% of respondents believe hybrid cloud is at least somewhat important, with a whopping 78.1% saying it’s very important or extremely important.The reason for this should be clear from the IT spending data I laid out earlier. However much enterprises may want to get to the cloud as quickly as possible, they still have workloads that will stay on-premises for a time, and sometimes forever. As such, enterprises rightly are much more concerned with getting their hybrid cloud strategy right than worrying about their multicloud non-strategy. It may not win enterprise IT any buzzword bingo contests, but hybrid is where the action is.", "pub_date": "2020-10-05"},
{"title": "Julia vs. Python: Which is best for data science?", "overview": "Python has turned into a data science and machine learning mainstay, while Julia was built from the ground up to do the job", "image_url": "https://images.techhive.com/images/article/2014/10/boxing_fight_conflict_clash_hand_fist_punch_silhouette_strength_thinkstock-100476658-large.jpg", "url": "https://www.infoworld.com/article/3241107/julia-vs-python-which-is-best-for-data-science.html", "body": "Among the many use cases Python covers, data analytics has become perhaps the biggest and most significant. The Python ecosystem is loaded with libraries, tools, and applications that make the work of scientific computing and data analysis fast and convenient.But for the developers behind the Julia language — aimed specifically at “scientific computing, machine learning, data mining, large-scale linear algebra, distributed and parallel computing”—Python isn’t fast or convenient . Julia aims to give scientists and data analysts not only fast and convenient development, but also blazing execution speed. What is the Julia language?Created in 2009 by a four-person team and unveiled to the public in 2012, Julia is meant to address the shortcomings in Python and other languages and applications used for scientific computing and data processing. “We are greedy,” they wrote. They wanted more: We want a language that’s open source, with a liberal license. We want the speed of C with the dynamism of Ruby. We want a language that’s homoiconic, with true macros like Lisp, but with obvious, familiar mathematical notation like Matlab. We want something as usable for general programming as Python, as easy for statistics as R, as natural for string processing as Perl, as powerful for linear algebra as Matlab, as good at gluing programs together as the shell. Something that is dirt simple to learn, yet keeps the most serious hackers happy. We want it interactive and we want it compiled.(Did we mention it should be as fast as C?)Here are some of the ways Julia implements those aspirations: For faster runtime performance, Julia is just-in-time (JIT) compiled using the LLVM compiler framework. At its best, Julia can approach or match the speed of C. Julia includes a REPL (read-eval-print loop), or interactive command line, similar to what Python offers. Quick one-off scripts and commands can be punched right in. Julia’s syntax is similar to Python’s—terse, but also expressive and powerful. You can specify types for variables, like “unsigned 32-bit integer.” But you can also create hierarchies of types to allow general cases for handling variables of specific types—for instance, to write a function that accepts integers without specifying the length or signing of the integer. You can even do without typing entirely if it isn’t needed in a particular context.Julia can interface directly with external libraries written in C and Fortran. It’s also possible to interface with Python code by way of the PyCall library, and even share data between Python and Julia. Julia programs can generate other Julia programs, and even modify their own code, in a way that is reminiscent of languages like Lisp. Julia 1.1 introduced a debugging suite, which executes code in a local REPL and allows you to step through the results, inspect variables, and add breakpoints in code. You can even perform fine-grained tasks like stepping through a function generated by code.Perfect for IT, Python simplifies many kinds of work, from system automation to working in cutting-edge fields like machine learning.Julia vs. Python: Julia language advantagesJulia was designed from the start for scientific and numerical computation. Thus it’s no surprise that Julia has many features advantageous for such use cases: Julia’s JIT compilation and type declarations mean it can routinely beat “pure,” unoptimized Python by orders of magnitude. Python can be  faster by way of external libraries, third-party JIT compilers (PyPy), and optimizations with tools like Cython, but Julia is designed to be faster right out of the gate. A major target audience for Julia is users of scientific computing languages and environments like Matlab, R, Mathematica, and Octave. Julia’s syntax for math operations looks more like the way math formulas are written outside of the computing world, making it easier for non-programmers to pick up on. Like Python, Julia doesn’t burden the user with the details of allocating and freeing memory, and it provides some measure of manual control over garbage collection. The idea is that if you switch to Julia, you don’t lose one of Python’s common conveniences. Math and scientific computing thrive when you can make use of the full resources available on a given machine, especially multiple cores. Both Python and Julia can run operations in parallel. However, Python’s methods for parallelizing operations often require data to be serialized and deserialized between threads or nodes, while Julia’s parallelization is more refined. Further, Julia’s parallelization syntax is less top-heavy than Python’s, lowering the threshold to its use. Flux is a machine learning library for Julia that has many existing model patterns for common use cases. Since it's written entirely in Julia, it can be modified as needed by the user, and it uses Julia's native just-in-time compilation to optimize projects from inside out. Julia vs. Python: Python advantagesAlthough Julia is purpose-built for data science, whereas Python has more or less evolved into the role, Python offers some compelling advantages to the data scientist. Some of the reasons “general purpose” Python may be the better choice for data science work:In most languages, Python and C included, the first element of an array is accessed with a zero—e.g.,  in Python for the first character in a string. Julia uses 1 for the first element in an array. This isn’t an arbitrary decision; many other math and science applications, like Mathematica, use 1-indexing, and Julia is intended to appeal to that audience. It’s possible to support zero-indexing in Julia with an experimental feature, but 1-indexing by default may stand in the way of adoption by a more general-use audience with ingrained programming habits. Python programs may be slower than Julia programs, but the Python runtime itself is more lightweight, and it generally takes less time for Python programs to start and deliver first results. Also, while JIT compilation speeds up execution time for Julia programs, it comes at the cost of slower startup. Much work has been done to make Julia start faster, but Python still has the edge here. The Julia language is young. Julia has been under development only since 2009, and has undergone a fair amount of feature churn along the way. By contrast, Python has been around for almost 30 years. The breadth and usefulness of Python’s culture of third-party packages remains one of the language’s biggest attractions. Again, Julia’s relative newness means the culture of software around it is still small. Some of that is offset by the ability to use existing C and Python libraries, but Julia needs libraries of its own to thrive. Libraries like Flux and Knet make Julia useful for machine learning and deep learning, but the vast majority of that work is still done with TensorFlow or PyTorch. A language is nothing without a large, devoted, and active community around it. The community around Julia is enthusiastic and growing, but it is still only a fraction of the size of the Python community. Python’s huge community is a huge advantage.  Aside from gaining improvements to the Python interpreter (including improvements to multi-core and parallel processing), Python has become easier to speed up. The mypyc project translates type-annotated Python into native C, far less clunkily than Cython. It typically yields four-fold performance improvements, and often much more for pure mathematical operations.", "pub_date": "2020-05-27"},
{"title": "Survey finds cloud complexity increases challenges", "overview": "Convoluted cloud solutions are a big reason cloud implementations don’t work as hoped.", "image_url": "https://images.idgesg.net/images/article/2019/02/confusion_complexity_navigation_lost_cloud_computing_personal_data_mapping_by_gremlin_gettyimages-1086462500_2400x1600-100788502-large.jpg", "url": "https://www.infoworld.com/article/3584360/survey-finds-cloud-complexity-increases-challenges.html", "body": "Bridging the Cloud Transformation Gap is a report that evaluates the findings of Aptum’s Global Cloud Impact Study. Aptum’s annual study seeks the opinions of 400 senior IT decision makers. Keep in mind that these sponsored reports can be self-serving, either for lead generation or to promote a publisher’s dogma. This report is behind a registration wall. Also on InfoWorld: Which multicloud architecture will win out?Despite possible biases, some good data points here address the intent to migrate to clouds versus the reality of doing so. I found the information realistic, considering that similar reports only cover the positive aspects of cloud migration. One of the more interesting bits of data points to the issue of complexity:The survey reveals this with 62 percent of respondents citing complexity and abundance of choice as a hindrance when planning a cloud transformation. One of the biggest sources of complexity that crops up in more advanced cloud projects are legacy systems.If these issues sound familiar, I’ve been waving my hands about them for a while now. I even took some early arrows as cloud providers, peers, and other cloud vendors felt my insistence in sounding the complexity alarm had a negative impact on their ability to sell the cloud. Back to the survey, 62 percent of respondents is a pretty big number, even more than I thought would be aware of the problem. Of course, these are only the IT leaders who understand they have a problem. Based on my experience, the percentage of IT execs who actually do have complexity problems is higher.What we’ve tried to figure out in the past few years is where complexity originates. My implementation experiences are backed up by this report. The “abundance of choice” or the need to select the best of breed is a prime culprit. This usually results in a technological smorgasbord, where hundreds of decoupled cloud dev and migration teams make their own calls around what technology to use. Complexity naturally arises when it’s time to join and coordinate those apples and oranges. The legacy issue is even less understood. The primary culprit here is that legacy systems don’t seem to be going away. Although you can create a different ops plan for these older systems, the need for them to work and play well with cloud-based systems results in a deeper problem. The end state of both issues? Too many technologies exist within the solution sets for the enterprise to cost-effectively or efficiently operationalize. That’s where cloud typically falls on its face. IT groups are forced to put off deployments or go back and mediate the complexity. Also on InfoWorld: When hybrid multicloud has technical advantagesIf there’s a standard approach for dealing with complexity, the approach itself will take some effort. It’s not an easy problem to solve, either before or after deployment. Some operations tools (such as AIops) can sometimes provide assistance here, as well as other monitoring and management tools that work with cloud and legacy workloads. I’ve had some success with these tools; they can abstract the complexity from the ops team, either using a single-pane-of-glass approach or layers of automation. Of course, tools can take you only so far. The real challenge is to first understand that you have a problem, and then take proactive steps to solve it. Be aware of potential complexity at all stages of the planning process, and you will greatly increase your odds of success with cloud implementations.", "pub_date": "2020-10-06"},
{"title": "Static versus dynamic cloud migration ", "overview": "Many plan to use the same tools from one cloud migration project to the next. A command and control center might give different advice.  ", "image_url": "https://images.idgesg.net/images/article/2018/07/square_peg_in_a_round_hole_wooden_block_shapes_toy_by_robynmac_getty_images_1200x800-100763279-large.jpg", "url": "https://www.infoworld.com/article/3584430/static-versus-dynamic-cloud-migration.html", "body": "It’s 9:00 AM on a Wednesday. You’re in the boardroom giving a status update on the latest migration project that will remove most of the vulnerabilities found during the recent pandemic. This is the third migration project, all less than 100 workloads and 10 data sets. All have taken place in parallel, and all leverage different cloud migration teams.Company leadership notes that the metrics were very different between the projects. Project One shows nearly 80 percent efficiency in terms of code refactoring, testing, deployment, security implementation, etc. The others were closer to 30 and 40 percent. Why the differences? Also on InfoWorld: How to choose a cloud machine learning platformMost efficiency issues arise from dynamic versus static migration approaches and tools. Most people who currently do cloud migrations gravitate toward the specific processes, approaches, and migration tool suites that worked for past projects. This static approach to cloud migration forces a specific set of processes and tools onto a wide variety of migration projects and problem domains. The misuse of specific processes and tools as generic solutions often leads to failure. Core to this issue is our drive to find a specific suite of tools and technology bundles that the industry considers best-of-breed, and our desire to leverage best practices. We in IT love to follow the crowd: \"I read about these tools and this approach that worked for Joe and Jane and Bob at companies kind of like mine, so they’ll work for me, too.\" We make the erroneous assumption that we remove risk by not making our own choices, even if the choices are situational. As an expert in this field, I would love to list a standard set of migration tools that will address everyone’s needs—code scanners, configuration management, continuous integration and development, testing tools, and more. But the real answer is that your selections of tools and approaches must be based on the requirements of the applications and databases you are migrating to public clouds--or any other platform, for that matter. The project criteria and review processes for migration projects typically include, but are not limited to:“As is” platform assessmentApplication assessmentData assessmentConfiguration management planSecurity migration toolsGovernance migration toolsRefactoring and redeploymentCI/CDTesting and deploymentCloudops/IT operations…and more.The tool categories listed above will have different solutions based on the “as is” and “to be” platforms, development approaches, and databases used, as well as the identified storage, security, and governance requirements. Although a one-size-fits-all approach might work, it will rarely provide the promised efficiencies and could actually derail the entire project if the approaches and tools are too far off. My message here is that you need an extra step in the process. Select the tools and approaches based on what you want to move, where it needs to go, and the attributes it needs to have upon arrival. Almost always, this will drive the selection of a different set of tools for each migration project.The bottom-line message is that the same tools and processes can rarely be reused with optimal results from one migration project to the next.Now it’s time to hear about the exceptions. To take advantage of these exceptions, there needs to be centralized command and control of all migrations, such as a center of excellence where problem patterns and solutions during each migration project can be documented. With centralized command and control (CCC), your best practices will emerge, and they will have a much higher likelihood of working in future migration projects. This is the point in the overall enterprise migration where some tools and processes can begin to be reused. CCC needs to become part of the process when teams initially identify migration targets and begin migration plans. It should be CCC’s duty to compare and contrast planned migration projects with past projects. If CCC can recommend approaches, tools, and processes that worked for previous migrations with similar attributes, you will never need to reinvent the wheel. It’s a hard lesson to learn. The silver lining here is that even if one size rarely fits all, a few sizes can often fit most. As always, the key to success is to do your homework at the beginning instead of the end of a project, document everything, and make that documentation easily accessible to help plan future projects. ", "pub_date": "2020-10-09"},
{"title": "No, you don’t have to run like Google", "overview": "Just because Google, Amazon, or Facebook does it doesn’t mean you should. Here are four ‘best practices’ of the hyperscalers you have permission to ignore.", "image_url": "https://images.techhive.com/images/article/2016/11/07_future-100692258-large.jpg", "url": "https://www.infoworld.com/article/3585176/no-you-dont-have-to-run-like-google.html", "body": "Years ago, Google struggled with how to pitch its cloud offerings. Back in 2017 I suggested that the company should help mainstream enterprises to “run like Google,” but in a conversation with a senior Google Cloud product executive, he suggested that the company shied away from this approach. The concern? That maybe mainstream enterprises didn’t share Google’s needs, or maybe Google would simply intimidate them.For the mere mortals that run IT within such mainstream enterprises (read: almost everyone), fear not. It turns out there are many things that Google might do that make no sense for your own IT needs.Also on InfoWorld: The best open source software of 2020Just ask Colm MacCárthaigh, AWS engineer and one of the authors of the Apache HTTP Server, who asked for “examples of technical things that don’t make sense for everyone just because Amazon, Google, Microsoft, Facebook” do them. The answers—excessive uptime guarantees, site reliablity engineering, microservices, and monorepos among the highlights—are instructive.Excessive uptime guarantees“Five or five-plus nines availability guarantees,” says Pete Ehlke. “Outside of medicine and 911 call centers, I can’t think of anything shy of FAANG [Facebook, Amazon, Apple, Netflix, and Google] scale that actually needs five nines, and the ROI pretty much never works out.”I remember this one well from the variety of startups for which I worked, as well as when I was at Adobe (whose service-level commitments tend not to be five nines, but are arguably higher than necessary). Are you going to be OK if the multi-player game goes down? Yep. What about Office 365 for a few minutes, or even hours? Yes and yes.Site reliability engineeringA bit of a spin on devops (though it predates the devops movement), SRE (named in multiple replies to MacCárthaigh) came out of Google in 2003, and was designed to infuse engineering with an operational focus. A few core principles guide SRE:Embrace riskUtilize service level objectives (SLOs)Eliminate toilMonitor distributed systemsLeverage automation and embrace simplicityOr, as Ben Traynor, who developed Google’s SRE practice, describes it:SRE is fundamentally doing work that has historically been done by an operations team, but using engineers with software expertise and banking on the fact that these engineers are inherently both predisposed to, and have the ability to, substitute automation for human labor. In general, an SRE team is responsible for availability, latency, performance, efficiency, change management, monitoring, emergency response, and capacity planning.SREs spend much of their time on automation, with the ultimate goal being to automate away their job. They spend considerable time on “operations/on-call duties and developing systems and software that help increase site reliability and performance,” says Silvia Pressard.This sounds important, and even more so if you equate “site reliability” with “business availability.” But do most companies really need their developers to become operational experts? SRE might be critical at Google or Amazon, but it’s arguably a heavy lift for most enterprises, tasking developers with too much of an operational load for them to manage it successfully.Microservices architectureAs commentator “Buzzy” tells it, “Definitely microservices. The number of 20-staff-in-total companies I’ve had to talk down from that ledge….” Nor is he the only one to call out microservices as a needless complication for most enterprises. Many of the replies to MacCárthaigh’s tweet mentioned microservices.As Martin Fowler has argued, “While [microservices] is a useful architecture—many, indeed most, situations would do better with a monolith.” Wait, what? Aren’t monoliths an evil relic of the past? Of course it’s not that simple. As I’ve written,The great promise of microservices is freedom. Freedom to break up an application into distinct services that are independently deployable. Freedom to build these disparate services with different teams using their preferred programming language, tooling, database, etc. In short, freedom for development teams to get stuff done with minimal bureaucracy.But for many applications, that freedom comes at unnecessary costs, as Fowler highlights:Distribution: Distributed systems are harder to program, since remote calls are slow and are always at risk of failure.Eventual consistency: Maintaining strong consistency is extremely difficult for a distributed system, which means everyone has to manage eventual consistency.Operational complexity: You need a mature operations team to manage lots of services, which are being redeployed regularly.This last point is underlined by Sam Newman: “For a small team, a microservice architecture can be hard to justify, as there is work required just to handle the deployment and management of the microservices themselves.”It’s not to say that a microservices approach is always wrong. No, it’s simply a suggestion that we shouldn’t default to a more complicated (but scalable) approach simply because the hyperscalers use it (generally because scale is so critical).A monorepo to rule them allWhether microservices or monolithic in nature, you probably shouldn’t store your code in a “monorepo.” This was a common response to MacCárthaigh’s request. Monorepos store all of a company’s code in a single version control system (VCS), to seize on the (supposed) benefits of reducing duplication of code and increasing collaboration between teams.That’s the theory.The practice, however, is very different. “It quickly becomes unreasonable for a single developer to have the entire repository on their machine, or to search through it using tools like grep,” says Matt Klein, an engineer who has built some of the most sophisticated systems at Amazon, Twitter, and now Lyft. “Given that a developer will only access small portions of the codebase at a time, is there any real difference between checking out a portion of the tree via a VCS or checking out multiple repositories? .”Klein continues:In terms of collaboration and code sharing, at scale, developers are exposed to subsections of code through higher layer tooling. Whether the code is in a monorepo or polyrepo is irrelevant; the problem being solved is the same, and .You be youOf course, some companies may benefit from monorepos or five-nines availability or microservices or SRE. They might also benefit from rolling their own framework, building their own infrastructure, or any of the other things that commentators on MacCárthaigh deride. The point is that just because Google, Facebook, Amazon, or another hyperscaler does it, doesn’t mean  should. When in doubt, doubt. Start with the individual needs of your company and figure out the right approach to building and managing software according to who you are, not who you wish you were.", "pub_date": "2020-10-12"},
{"title": "Apache Spark 3.0 adds Nvidia GPU support for machine learning", "overview": "The next major release of the in-memory data processing framework will support GPU-accelerated functions courtesy of Nvidia RAPIDS", "image_url": "https://images.techhive.com/images/article/2014/05/sparking-jumper-cables-94261543-100265638-large.jpg", "url": "https://www.infoworld.com/article/3543319/apache-spark-30-adds-nvidia-gpu-support-for-machine-learning.html", "body": "Apache Spark, the in-memory big data processing framework, will become fully GPU accelerated in its soon-to-be-released 3.0 incarnation. Best of all, today’s Spark applications can take advantage of the GPU acceleration without modification; existing Spark APIs all work as-is.The GPU acceleration components, provided by Nvidia, are designed to complement all phases of Spark applications including ETL operations, machine learning training, and inference serving.InfoWorld review: Nvidia RAPIDS brings Python analytics to the GPUNvidia’s Spark contributions draw on the RAPIDS suite of GPU-accelerated data science libraries. Many of RAPIDS’ internal data structures, like dataframes, complement Spark’s own, but getting Spark to use RAPIDS natively has taken nearly four years of work.Spark 3.0 speedups don’t come solely from GPU acceleration. Spark 3.0 also reaps performance gains by minimizing data movement to and from GPUs. When data does need to be moved across a cluster, the Unified Communication X framework shuttles it directly from one block of GPU memory to another with minimal overhead.Also on InfoWorld: The best free data science courses during quarantineAccording to Nvidia, a preview release of Spark 3.0 running on the Databricks platform yielded a seven-fold performance improvement when using GPU acceleration, though details about the workload and its dataset were not available. No firm date has been given for general availability of Spark 3.0. You can download preview releases from the Apache Spark project website.", "pub_date": "2020-05-14"},
{"title": "How data analysis, AI, and IoT will shape the post-pandemic ‘new normal’", "overview": "Only a common public health infrastructure of AI, cloud computing, streaming, and the Internet of Things can marshal our data against pandemics", "image_url": "https://images.idgesg.net/images/article/2020/04/coronavirus_covid-19_pandemic_app_mobile_5009502_by_gerd_altmann_pixabay_cc0_2400x1600-100839463-large.jpg", "url": "https://www.infoworld.com/article/3543770/how-data-analysis-ai-and-iot-will-shape-the-post-pandemic-new-normal.html", "body": "Pandemics are shocks to communities throughout the world. Each community’s response emerges from the countless changes that individuals make in their daily lives to protect themselves while trying to maintain a semblance of normality.Grassroots responses often emerge first in such crises, but they may not be the most effective approach for slowing the contagion’s spread. From a technological standpoint, solutions invariably involve various blends of remote collaboration, contactless transactions, and replacement of manual processes with automated, robotic, and other human-free processes.Also on InfoWorld: Artificial intelligence today: What’s hype and what’s realWhen a contagion is raging, grassroots responses can be counterproductive if everybody’s operating at cross-purposes. Lack of central coordination can confuse the situation for everybody, stoking a panic-driven infodemic that social media can exacerbate, drowning out guidance from public health officials and other reliable sources. To ensure effective orchestration of community-wide responses to a contagion, there is no substitute for authoritative data analytics to drive effective responses at all levels of society.Going forward, we can expect to see more data-driven, top-down orchestration of pandemic preparedness and remediation among public, private, and nonprofit organizations. China’s experience is instructive in this regard. Though the outbreak’s inception in Wuhan was less than half a year ago, the country has responded rapidly with a top-down, nationwide approach to manage the crisis. Chinese authorities are orchestrating vast resources to save lives, control the spread of infection, and guide individuals for testing, treatment, and quarantining.In contrast, the United States and other nations seem to be responding to the emergency in a chaotic, bottom-up fashion. The key elements in China’s response are impressive. Leveraging sophisticated data analytics and other digital tools, it has responded to COVID-19 through:: Self-service screening tools have reduced nonessential hospital visits and caregiver workloads in China while mitigating the risks of cross-infection. Within the country, Tencent, Alibaba, and vertical online healthcare platforms now offer remote medical services to the public. People consult with doctors online, conduct self-assessments, and decide whether to go to a hospital for further medical checks or remain at home.: China’s digital platforms allow volunteer teams of community residents to assist in disinfection and deliver supplies aided by digital community management and communication tools. Citizens receive a health QR code that lets them submit information regarding travel to major epidemic outbreak regions. It enables compilation of data on close contacts with infected people and other relevant matters, and it enables an assessment on a three-color scale that indicates a person’s recent virus-related health history.: Digital technologies have allowed China’s healthcare professionals to apply their talents to a large number of COVID-19 cases over long distances. China’s 5G networks have allowed many Wuhan hospitals to connect with counterparts in Beijing, who provide real-time consultation based on transmission of ultra-high-definition medical images.: China has used these same technologies, along with the Internet of Things, to rapidly orchestrate an entire manufacturing, logistics, and healthcare supply chain. This has enabled the country to coordinate thousands of domestic firms to build and equip hospitals for testing and treatment of COVID-19 patients. The country has been able to rapidly scale up the production of masks, protective clothing, and disinfectants.: China has implemented a differentiated, location-specific response to limiting COVID-19 transmission. It uses big data analytics and artificial intelligence to estimate the probability that a particular neighborhood or individual was exposed to COVID-19. It matches the locations of smartphones to known locations of infected individuals or groups. It uses this information plus travel data to target government-mandated virus testing to high-risk individuals.Likewise, neighboring Taiwan has put together a comprehensive program of technology-driven tactics for controlling COVID-19’s spread in the country. Distilling from those East Asian nations’ experiences, I’m proposing a new normal for every country; it involves the following data-driven pillars of top-down response to raging pandemics:: In the new normal, most nations will institute counter-contagion nerve centers to coordinate societal responses to these threats. These operations will consolidate information from national health, immigration, customs, telecommunications, and travel databases. They will apply sophisticated AI to identify cases, generate real-tim;e alerts, and coordinate medical interventions. They will use real-time streaming apps to actively surveil and screen all ports into the country. These tools will provide the intelligence necessary to authoritatively close borders, certify which potential entrants are in good health, and quarantine or turn away people and animals that might be spreading contagion.: Painfully, the human race is learning from the COVID-19 crisis how to manage the quarantines, lockdowns, and closures that will be necessary in all future pandemics. In the future, public health authorities will use AI-driven predictive tools to rapidly plan and orchestrate these decisions in order to slow outbreaks while keeping the economy from crashing and minimizing community disruptions. Community contagion dashboards will give every person data-driven intelligence on how to adjust daily routines, based on conditions that currently impact their immediate environment. These and other sources of AI-driven intelligence will allow organizations to calculate the risk of having employees work in shared offices or from home, and to adjust their strategies day to day based on infection risk factors. When considering whether to bring employees back to the office while the pandemic is still going on, organizations base their decisions on data-driven analytics, as well as on site surveys informed by facility-embedded biosensors.: Going forward, we’re likely to see governments mandate AI-driven solutions for keeping people out of range of those who might be spreading infection. Proximity sensors will become ubiquitous in the aftermath of the current pandemic. Embedded in smartphones and wearables, they will feed personal digital assistants with real-time ambient intelligence on crowd conditions. Already, computer vision applications use AI to automate surveillance of people in public places, workplaces, stores, and elsewhere. Real-time AI tools will tell people if they’re standing too close to each other. Wait-time metering and crowd-limiting applications will become a standard feature of many public and private facilities. And we’ll see greater uptake of machine vision applications that can detect and optionally send “keep ‘em separated” alerts when someone moves too close to someone else.: The COVID-19 emergency is accelerating the Internet of Things’ sensor revolution, and there’s little doubt that much of this will be built in to public infrastructure everywhere. The new normal is likely to proliferate biosensors for detecting viral pathogens in the air, water, soil, surfaces, and human and animal tissues. More commonly these biosensors will be wearables, connected socially to detect the potential spread of infections from person to person and to monitor a disease’s progression among large groups of people, such as hospital patients and nursing home residents. AI-driven service robots will interrogate passersby in public places to see if they show signs of the virus. To detect symptoms even before people realize they’re infected, automated environment sensing will use multimodal AI to monitor the environment (pairing facial recognition with temperature scanning and listening to audio of people coughing). Infrared thermal imaging will enable active surveillance and screening for infected and carrier persons at borders, airports, and elsewhere in each country. We can expect governments to mandate embedded contact-tracing apps on every mobile phone.Putting this vision in place in the post-pandemic world will require a pervasive infrastructure of AI, cloud computing, streaming, and the Internet of Things. Managed as a common public health infrastructure, these capabilities will allow humanity to close ranks against the dread diseases that threaten us all.", "pub_date": "2020-05-14"},
{"title": "A foolproof way to understand cloud optimization ", "overview": "Once your cloud architecture works, it’s time to optimize it for efficiency and cost. An audit will reveal how much value it adds to the business. ", "image_url": "https://images.idgesg.net/images/article/2018/08/easy_simple_pixel-hand_computer-pointer-100770206-large.jpg", "url": "https://www.infoworld.com/article/3584387/a-foolproof-way-to-understand-cloud-optimization.html", "body": "We’ve discussed the notion of cloud architecture optimization here in the past. Now it’s time to understand how it’s measured. You really have no way to prove your architecture isn’t optimized unless you do an audit, which includes a review of the solution’s approach and any attached costs.In the past, the people who built and deployed cloud solutions were reluctant to have their choices questioned. These days, because we want the most value from the cloud solutions, many have changed their minds about questions and oversight—or more often, company leadership changed their minds for them. Many of the projects I take on these days focus on review and improvement audits rather than on build and migrate deployments.Also on InfoWorld: Which multicloud architecture will win out?Once everything works in cloud architectures, you can deploy and operationalize. Just because it works does not mean that it’s optimized. If you look at the differences between your architecture and one that’s optimized, you could have a “working” solution that costs you millions of dollars each week.To present this visually, see the figure below. Note that positions 1 and 37 are the least optimized. They cost more money and are the most inefficient.When we look at each side, note that cloud solutions can be under-utilized or overutilized, such as with containers and serverless computing. Those on the left of the chart might not have included enough containers, whereas those scoring on the right have used containers too much. The point of optimization is to be in position 19, where we use the right number of containers to make the most of costs and solution efficiency.Of course, you can leverage this metric for any holistic architecture or all configured technology. You could even apply it to microarchitectures, such as a few applications moving to serverless or containers, for instance. Note that including polynomial views of the data for smoothing creates a curve that’s more likely to reflect typical real-life behavior. In the real world, this data never actually runs in a straight line.What does a cloud architecture audit mean? If you’re just starting your cloud journey, it’s a way to test and track the optimization of your proposed solutions. You want to get as close to the middle of the chart as you can.Also on InfoWorld: When hybrid multicloud has technical advantagesIf you’re mid-journey, which most of us are, an audit is a way to evaluate solutions already in place as well as solutions that are currently under development. The hard part will be to allow others to evaluate your decisions. In many instances, they will second-guess the initial architecture using a forced ranking process which will put your solution at some point between 1 and 37 on this chart. Hopefully you land in the middle, but it’s unlikely.It’s not a sign of weakness to have others check your work. People who call for architecture audits on themselves should be praised. The alternative is to spend millions of unnecessary dollars or even do irreparable damage to the business. The stakes are just too high to let egos get in the way. Let’s be smart.", "pub_date": "2020-10-14"},
{"title": "Getting started with Azure SQL Edge", "overview": "Microsoft scaled down its flagship database, squeezing it into 500MB and running it on edge hardware.", "image_url": "https://images.idgesg.net/images/article/2020/08/edge-cloud-100854505-large.jpg", "url": "https://www.infoworld.com/article/3585795/getting-started-with-azure-sql-edge.html", "body": "The cloud is becoming increasingly distributed, with container technologies allowing easy deployment of what had been cloud functionality to devices at the edge of the network. What began with function-as-a-service runtimes has now graduated to supporting PaaS tools and technologies, stretching the cloud platform from public hyperscale data centers down to low-cost devices running on your network.Microsoft has been talking about “the intelligent cloud and the intelligent edge” for a long time now, with a focus on finding ways around the bandwidth crunch between data sources and data processing in cloud architectures. There’s a certain level of self-interest in this approach, with hyperscale data centers close to cheap power but a long way from metropolitan population densities and relatively cheap bandwidth. If power is cheap and data is expensive, then why not process it close to the source?Also on InfoWorld: Amazon, Google, and Microsoft take their clouds to the edgeSQL Server everywhereAt the heart of much of Microsoft’s cloud platform is its SQL Server database, now grown into a family of databases that work across public and private clouds; at desktop, data center, and hyperscale; and on both Windows and Linux. That family of databases has recently been joined by a new member: Azure SQL Edge. Designed for IoT (Internet of Things) and edge gateway scenarios on Intel and ARM hardware, it’s a small, containerized database that can operate both while connected and disconnected, providing a place to preprocess and stream data that can be developed remotely and delivered and managed from Azure.Like the rest of the SQL Server family, Azure SQL Edge is a relational database with nonrelational capabilities. You can use it for traditional table-based storage, or instead work with JSON documents, with graph content, or with time-series data. That combination ensures you can use it for many different roles, but with a focus on IoT applications. It’s built on the SQL Server engine so there’s no need to learn new skills; the same techniques and programming tools you use on larger SQL Server instances work just as well on Azure SQL Edge, with T-SQL queries and functions developed elsewhere fully supported.Azure SQL Edge deployment optionsAzure SQL Edge can be deployed in two different ways. If you’re using Azure IoT Edge to deploy and manage IoT applications, you can find it on the Azure Marketplace and deploy it directly into your instances from the Azure Portal. If you’re using it on unmanaged systems, then it’s available in a container in Docker Hub and can be downloaded and run as a stand-alone container or used as part of a Kubernetes container orchestration. This last option is particularly interesting if you’re considering working with one of the edge-focused Kubernetes implementations, such as Canonical’s MicroK8s or Rancher’s K3s.It’s important to remember that Azure SQL Edge is a variant of the Linux SQL Server release, and its container’s base operating system is Ubuntu 18.04. To get support, any host will need to run either 18.04 or 20.04 LTS releases of Ubuntu (so you can build and test applications on any Windows 10 PC running Windows Subsystem for Linux). You can use other Linux hosts or even the Windows version of Docker, but they won’t be tested and may not have all the services and applications needed to run your database.Once a container instance is up and running, you can connect to it using the built-in command line tools (if you’re not running on ARM64, which doesn’t currently support sqlcmd). From here you can create tables and test queries, much like any other SQL Server database. In practice, you’re more likely to work with it remotely, using your choice of management and development tools. You need to ensure that appropriate container ports are open as part of the initial configuration process to use external tools.Much of Azure SQL Edge is designed to support traditional database operations; there’s additional support for basic stream analytics to work with real-time data from devices. You can use this to build applications that filter data before uploading to a central processing application in Azure, or as a feed into machine-learning applications that work with models developed on Azure or development PCs and exported as ONNX (Open Neural Network Exchange).Designing database applications for the edgeYou don’t need new tools to manage or develop for Azure SQL Edge. It’s designed to work with existing SQL Server development tools, so you can continue to work as usual when designing databases and building triggers or stored procedures. If you’re developing against a live instance on lab hardware, you might want to take advantage of both the remote development and SQL Server extensions for Visual Studio Code. Being able to build, test, and debug applications in a single environment will help keep development time to a minimum.As the container is only 500MB, it’s possible to run Azure SQL Edge on smaller devices such as the 8GB Raspberry Pi 4, especially after recent firmware updates have allowed it to be booted off SSD rather than SD cards. SSD storage makes devices like the Pi faster and more reliable; relatively slow and cheap SD card storage is not designed to handle the write cycles of a modern database.Azure SQL Edge may be a variant of SQL Server, but you shouldn’t expect to be able to bring all your existing applications to the edge, at least not without some modifications. Azure SQL Edge is designed to run only a subset of the core SQL Server database services, so you won’t have access to other parts of the platform, like Analysis or Reporting Services or most of its Machine Learning tools. In practice that shouldn’t be too much of a problem, as those features work best either on interactive systems or with the horsepower necessary for building and training models with large data sets. Using ONNX for inferencing on the edge allows you to deliver pretrained models, ready to filter data or flag exceptions for further analysis.Although Azure SQL Edge can operate with a direct connection to Azure SQL instances in the cloud, it’s designed to function as a stand-alone database on disconnected, low-power systems. With that in mind, you should build applications that don’t rely on connected features such as stretch databases or memory-intensive tools like OLTP, as they’re not part of the Azure SQL Edge build.Also on InfoWorld: When hybrid multicloud has technical advantagesYou get what you pay forLicensing is relatively simple. There are two editions: one for production, one for development. The development SKU supports as many as four cores and 32GB of memory; the production version supports as many as eight cores and 64GB of memory. Those are relatively high specifications for edge hardware, but they’re what you’d expect from hub devices at the network edge. In practice, this hardware would collate and process data from multiple edge devices. Pricing starts at $10 per device per month, with reductions for one-year and three-year reserved instances that can bring it down to $60 per device per year.Even though the price for Azure SQL Edge is relatively low, you do need to compare it with open source alternatives that can interoperate with cloud databases. It has an advantage if you’re already building SQL Server applications as part of an IoT strategy, if you need to take some of their functionality to the edge in order to manage bandwidth costs, or if you need to operate in a partially connected mode, leaving edge hardware operating autonomously for much of the time.", "pub_date": "2020-10-14"},
{"title": "3 unexpected predictions for cloud computing next year", "overview": "It's clear that cloud computing will continue to grow, but here are three less-obvious aspects to keep on your radar.", "image_url": "https://images.idgesg.net/images/article/2020/01/gettyimages-1064982786-100828145-large.jpg", "url": "https://www.infoworld.com/article/3586191/3-unexpected-predictions-for-cloud-computing-next-year.html", "body": "It’s that time of the year when PR folk promote their clients as somebody who can make predictions for 2021 that all should regard. I’m often taken back by the obvious nature of the assertions, such as “cloud computing will continue to grow” or “there will be an expanded need for cloud security.” Okay, that’s not at all helpful.Also on InfoWorld: When hybrid multicloud has technical advantagesThe reality is that although we can see much of the future based on obvious past and current patterns, other portions of the cloud computing market are much tougher to forecast. Enterprises won’t see some trends until it’s too late. Here are three to at least put on your radar: Today we’ve not seen a demand for the three major cloud brands to work and play well together at deeper levels. This is largely because they operate in their own self-interest, and building bridges between public clouds is bad for business.  With more than 90 percent of enterprises using multicloud, there is a need for intercloud orchestration. The capability to bind resources together in a larger process that spans public cloud providers is vital. Invoking application and database APIs that span clouds in sequence can solve a specific business problem; for example, inventory reorder points based on a common process between two systems that exist in different clouds.       Emerging technology has attempted to fill this gap, such as cloud management platforms and cloud service brokers. However, they have fallen short. They only provide resource management between cloud brands, typically not addressing the larger intercloud resource and process binding.   This a gap that innovative startups are moving to fill. Moreover, if the public cloud providers want to truly protect their market share, they may want to address this problem as well.   Self-healing is a feature where a tool can take automated corrective action to restore systems to operation. However, you have to build these behaviors yourself, including automations, or wait as the tool learns over time. We’ve all seen the growth of AIops, and the future is that these behaviors will come prebuilt with pre-existing knowledge that can operate distributed or centralized. This means that from day one you can automate most of the issues that cloud and non-cloud systems will need to deal with, and knowledge will build and be shared over time. No matter if it’s operations model changes, skills gaps, or flattening organization structures, enterprises need to renew their focus on the people aspect of cloud computing.   Skills, organizational structures, and processes need to change around the use of cloud computing. Unfortunately, most organizations view these changes as “opening Pandora’s box,” as one client put it years ago. It becomes a can that leadership kicks down the road. Truthfully, you won’t get much out of cloud computing without the courage to push these changes through leadership. The alternative is good technology and bad technology usage, and that means failure.   I suspect more unexpected things will happen next year than expected ones. Watch this space for a discussion of those emerging issues.", "pub_date": "2020-10-16"},
{"title": "How to make the most of the AWS free tier", "overview": "10 tips for stretching Amazon’s free services to the limit and keeping your cloud bill near zero. ", "image_url": "https://images.techhive.com/images/article/2017/04/free-neon-sign-100719704-large.jpg", "url": "https://www.infoworld.com/article/3585757/how-to-make-the-most-of-the-aws-free-tier.html", "body": "Free is a powerful incentive. When I taught a course on web frameworks at the local college, we designed the assignments to ensure that all of the experiments could be done quickly with Amazon Web Services’s collection of free machines. Each student created, built out, and stood up more than a dozen different servers and they didn’t add a penny to their student debt.This is a good example of why Amazon and the other cloud services offer hundreds of different ways to try out their products. New products are born, tested, poked, and prodded for only the cost of the developer’s time. If the code makes it big and starts generating enough revenue, the developers can grow into paying customers. If it doesn’t and they don’t, at least the developers will become comfortable with the tools and probably turn to Amazon for the next project.Also on InfoWorld: Amazon, Google, and Microsoft take their clouds to the edgeThe free tier is not just for ramen-eating students. Sometimes asking the boss for a budget line, no matter how small, means triggering a series of questions and meetings that demand explanations. A number of good developers test their plans on free machines because it’s much more impressive to present a running prototype than a slide deck with some mockups.Amazon offers three different kinds of free services. Some are short-term samples, allowing you to evaluate a new service for a month or so. They’re meant to get teams to explore new products. Others are like a generous welcome wagon for new developers who sign up for an AWS account. They can start exploring without the worry of a bill because they last a full year after you create your new account.The most generous are the “always free” offerings that keep going and going. Some developers make it a point to build their products to live in the free tier as long as possible. It’s a bit of a game because development resources aren’t too expensive at first. They may be saving a few dollars. But this focus on the bottom line can produce good applications that are cleanly engineered to use a minimum of AWS’s resources. When they scale, the bills will scale a bit more slowly. Here are 10 suggestions for how to play the AWS stack and generate the smallest bills using the freest of services.Waste not want notMost of the AWS services in the free tier come with a limit, usually enforced each month. Some of these seem impossibly large like AWS Lambda’s grant of one million function calls. After you’re finished paying homage to Dr. Evil from the Austin Powers movies by echoing his pronunciation of “million,” you can start budgeting your use of these function calls to the most important jobs. Even the generous limits can be exhausted. A million can come pretty soon if you’re not careful.Go staticThe options for computation in the free tier are pretty limited and so it pays to reduce the server-side computation as much as possible. Static site generators like Jekyl or Gatsby turn the data in your dynamic website into HTML, JavaScript, and CSS files that sit out in a static web server. Perhaps you’ll move them to a CDN like Amazon’s CloudFront. Perhaps you’ll serve them directly from Amazon S3. Perhaps you’ll even park them in the corner of another server around your office. The point is to save computational resources that would be generating your web pages dynamically so you can stay within the free tier.Go serverlessAWS Lambda is the only Amazon compute option that remains free after one year. It’s also arguably the best option for a service that will scale smoothly to handle thousands, millions, or billions of requests. Choosing Lambda from the beginning sets your application up for success in the future.Go NoSQLAmazon also encourages us to use their DynamoDB by including 20GB of storage space that’s always free. DynamoDB may not offer the same clever indexing and normalization options that relational database lovers have embraced over the years, but NoSQL remains a smart and flexible architectural choice that’s especially forgiving for evolving prototypes and pivoting startups.Also on InfoWorld: 6 ways Alibaba Cloud challenges AWS, Azure, and GCPCombine AJAX callsSometimes you’re going to need to make your site interactive. The best approach is to bundle the calls to your web services into as few transactions as possible. The Amazon API Gateway free tier, for instance, includes one million API calls and one million HTTP calls. Bundling all of your data into one call makes these limits last longer than dutifully invoking the calls immediately. The simplest way to accomplish this is to cut back on storing documents or form data for the user. Yes, this can make the service a bit less robust and crash resistant, but that’s the cost of doing things for free.Empower the clientWhile cookies and their lesser known cousins like the local Web Storage API have a reputation for helping big business track people, they also offer the opportunity for users to control their privacy by storing their local data. It also makes it easier to build a free tier web application by offloading the cost of storing client data on the client’s own machine. The users’ machines store the data so you don’t have to!More privacy and less central costs. It would be a perfect solution if it weren’t for the total catastrophe that follows a lost phone, a crashed local disk, or any of a million other failures. It’s best to use this for casual data, not mission-critical information.Avoid gimmicksSome websites have added flashy interactive features like autocomplete. These may be fun and they may generate attention, but each of these features usually requires another request to the cloud and that eats into your limit. Avoiding unnecessary moving parts is the simplest way to save compute resources.Run your own databaseThe Amazon-managed relational database services like MySQL or PostgreSQL are great tools for starting up and maintaining a database to hold your app’s information, but the free tier only offers you one of them and it’s only for the first 12 months. There’s nothing stopping you from running your own database on one of the free EC2 instances that are also available for the first 12 months. Yes, you’ll need to install them and configure them yourself, but it will double your database options.Log carefullyAll of the free storage at AWS comes with limits. Good developers create good log files to debug problems and catch failures, but most log files are never used. Staying within the limits for storage is simpler if you clean out your logs frequently. Some just throw away the data and some download it to their desktop disk.Use non-cloud resourcesIt’s not exactly a fair answer to say that you can get more out of the free tier by running your own server back on your desk. Still, some judicious use of non-AWS services can really stretch the work being done on the cloud. Database backups, for instance, could move to your desktop, which might have several terabytes of empty space waiting for some of the random detritus. And you’re probably going to want to back up your projects outside of the cloud anyway. Any service or data that doesn’t need the immediate response and constant uptime of the cloud is fair game.Recognize the limitsThe free tier is an excellent way to explore AWS and it’s fun to strip away all of the extraneous features to try to generate bills for $0.00, but at the end of the day AWS is a business and the free tier is a well-designed marketing tool not a public charity. Some people openly create new accounts with new email addresses to continue to restart the 12 month clock. This may work with disposable projects but not with those that have started attracting users that will be disrupted when you switch accounts.Also on InfoWorld: 14 ways AWS beats Microsoft Azure and Google CloudWhen your creations have found an audience, it’s time to start finding a way to pay the bills. The good news is that all of the lessons you’ve learned from living in the free tier will keep your bills much lower. The API Gateway, for instance, charges just $1 for a million invocations. If you’ve been successfully running in the free tier, then your bills won’t be more than a few dollars a month.That should hold until everything goes insanely viral and your outrageous good fortune makes the AWS bill the least of your worries.", "pub_date": "2020-10-19"},
{"title": "Understanding GraphQL engine implementations", "overview": "Among the many ways of implementing a GraphQL engine, only one approach offers the same  performance, scalability, and ACID guarantees as the underlying database. ", "image_url": "https://images.idgesg.net/images/article/2020/09/interlocking_gears_and_binary_code_abstract_technology_virtual_machines_process_manufacturing_by_krulua_gettyimages-1254571610_2400x1600-100859759-large.jpg", "url": "https://www.infoworld.com/article/3575530/understanding-graphql-engine-implementations.html", "body": "When we talk about advantages of GraphQL we often hear one-liners such as “only fetch what you need,” “only requires one generic endpoint,” “data source agnostic,” or “more flexibility for developers.” But as always things are more subtle than a one-liner could ever describe.Generic and flexible are the key words here and it’s important to realize that it’s hard to keep generic APIs performant. Performance is the number one reason that someone would write a highly customized endpoint in REST (e.g. to join specific data together) and that is exactly what GraphQL tries to eliminate. In other words, it’s a tradeoff, which typically means we can’t have the cake and eat it too. However, is that true? Can’t we get both the generality of GraphQL and the performance of custom endpoints? It depends!Also on InfoWorld: The best free programming courses during quarantineLet me first explain what GraphQL is, and what it does really well. Then I’ll discuss how this awesomeness moves problems toward the back-end implementation. Finally, we’ll zoom into different solutions that boost the performance while keeping the generality, and how that compares to what we at Fauna call “native” GraphQL, a solution that offers an out-of-the-box GraphQL layer on top of a database while keeping the performance and advantages of the underlying database.GraphQL is a specificationBefore we can explain what makes a GraphQL API “native,” we need to explain GraphQL. After all, GraphQL is a multi-headed beast in the sense that it can be used for many different things. First things first: GraphQL is, in essence, a specification that defines three things: schema syntax, query syntax, and a query execution reference.The schema simply defines what your data looks like (attributes and types) and how it can be queried (query name, parameters, and return types). For example, a todo application could have Todos and a List of todos, if it only provides one way to read this data — e.g., get a list of todos via the id, then that schema would look like this:Once you have a schema that defines how your data and queries look, it’s super easy to retrieve data from your GraphQL endpoint. Do you want a list? Just call the getList item and specify what attributes of the list you want to return.Do you want to join that data with todos? No problem! Just add a small snippet of JSON to the mix.But how does this join happen? Of course we did not yet define how this query actually maps to data that comes from our data source.It’s relatively easy to understand the schema and see how you can query. It’s harder to understand what the performance implications are since there are so many different implementations out there. GraphQL is  an implementation to retrieve your data. Rather, GraphQL provides guidelines on how a request query should be broken down into multiple “resolvers” and turned into a response.Resolvers define how one element of the query (what GraphQL calls a field) can be turned into data. Then, depending on the framework or GraphQL provider, you either implement these resolvers yourself or they are provided automagically. When resolvers are provided for you, the implementation will determine whether we can talk about native GraphQL or not. In essence, the resolvers are just functions that have a certain signature. In JavaScript, such a resolver could look like this:And each of our “fields” will have a corresponding resolver. Fields are just the attributes in your schema.Each of these fields will have a resolver function (either generated by the library or implemented manually). The execution of a GraphQL query starts at a root field resolver, in this case, getList. Since getList promises to return a List, we will also need to fill in the fields of the List; therefore we need to call the resolvers for these fields and so on. In essence, it’s a process of recursive function calls. Let’s look at an example:For the above query, we would traverse three fields, each with a resolver function: returns a List with only the field todos receives the List item from getList and returns a list of Todos related to that list. receives a Todo item from the todos resolver and returns a string from the title.This is in itself a very elegant recursive way to answer the query. However, we will see that the choices made in the actual implementation will have a huge impact on performance, scalability, and the behavior of your API.Patterns for writing resolversIn order to understand the different approaches, we need to learn how to get started building a GraphQL execution engine. The syntax of that depends on the server library we choose, but each library adheres to the resolver guidelines described above.As we have explained, implementing a GraphQL engine is all about implementing functions called resolvers with a specific signature.The arguments serve different purposes for which you can find the details here. The ones that are most important for this implementation are: is the previous object. In the previous example, we mentioned that the todos resolver receives the List object that was resolved by getList. The object parameter is meant to pass on the result of the previous resolver. is the argument(s). In getList, we pass an ID, so args will be {id: \"some id\"}.We can’t do much with one resolver function, and our GraphQL server library will need to know how to map a query to the different resolvers and how we delegate work from one resolver to the next resolver. Each library has a slightly different syntax to specify this, but the general idea remains the same. For example, one syntax to specify the mapping of queries to resolvers could look as follows:If we write the query below, we can match this to resolvers by first looking into the root resolvers (the ones in Query) where we will find the getList resolver. Since that resolver returns a List, the GraphQL execution engine knows that getList returns a List and therefore it needs to go search in List for the resolver of the todos field. The way this resolves is called the resolver chain.Now that we know how GraphQL libraries want us to write resolvers, we can start thinking about how it affects performance.The above explanation might make you think that resolving a GraphQL query is quite linear, but in fact resolver chains are more like chains that keep on splitting... Oh wait, that’s just called a tree!GraphQL approach #1: The naive implementationWhen we implement resolvers naively in this rather elegant recursive system we can easily write an API that is slow. By playing human interpreter on the previous query we can see how a naive implementation results in a tree-like execution plan that sends many queries to the database.Although there are only two database calls in the above implementation, one of these calls, getTodo(id), is called for each Todo that is associated with the List. That means that we will be hammering our database with 1 (getting the list) + N (getting each todo) calls and joining all this data in the back end. This is known as the N+1 problem.Remember that, in contrast, a clean REST API would allow us to call each of these resources separately from the front end and require us to join these somewhere (e.g. in the front end) or require us to build custom endpoints. There were two things we wanted to improve upon by using GraphQL: Multiple calls from the front end, requiring that the data also has to be joined in the front end.Workarounds that require you to build a custom endpoint for performance or for each new front end or app requirement. For example, if you have a new UI for your mobile application, it might have different requirements. It might require other joins or fewer attributes.An example that depicts the number of calls in a REST back end, both when we follow REST practice by separating endpoints per entity type and when we optimize by writing custom endpoints to join.Although our naive GraphQL endpoint effectively solves these issues, it creates another problem further down the road.The introduction of GraphQL in combination with a naive implementation moves a problem that was transparent to the back end, where the problem might still be present but is hidden from the API user. In essence, we replaced the problem of multiple REST calls with the N+1 problem. It is not necessarily a problem to express generic queries in REST because similar things can be done with Odata. The difficulty is to provide an efficient implementation for such a generic endpoint.With the naive implementation, we now have an even higher number of queries between the back end and the database, and the results have to be merged in the back end. Joining many small requests requires memory and might cripple our database as we scale. This is clearly not ideal.GraphQL approach #2: Batching with DataloaderFacebook has created a great piece of software, called Dataloader, that allows you to solve a part of the problem. If you plug in Dataloader, similar queries will be essentially cached per frame or per tick. Consider the following execution, without Dataloader.Dataloader takes in all of these queries, waits for the end of the frame, then combines them into one query.Besides that, Dataloader does in-memory caching of certain queries’ results. With Dataloader, we have now improved the number of database calls significantly. However, it only helps for similar queries that fetch the same entity type, which means that we are still joining data in memory and we are still doing multiple calls. If we look at the queries that are sent to the database, the approach now looks like this: This is already great, but we can do much better. After all, joining in memory in the back end is just not scalable as data grows and our schema becomes more complex, requiring that multiple entities be fetched. For example, consider the following model from a Twitter-based example application called Fwitter.We need to retrieve not only the Fweets (which are like Twitter’s tweets), but their comments, hashtags, statistics, author, original fweets in case of a refweet, etc. We face a complex in-memory join, and again, many database calls. Also on InfoWorld: 7 best practices for remote agile teamsGraphQL approach #3: Generating the queryIdeally, if possible, we want a GraphQL query that translates into one database query. Sounds simple right? However, we’ll see that code generation can be hard or impossible depending on the underlying data storage and query language.For starters, this approach can raise many questions. Do we generate one query or multiple queries? Can we even generate one query that expresses a complex GraphQL statement? If we generate a big query with many joins, is it still efficient?Since native GraphQL is a special case of “generating the query,” let’s first (and finally) define what we at Fauna call “native” GraphQL. We’ll answer these questions in the subsequent section, “Why is native GraphQL so difficult to achieve?” GraphQL approach #4: Native GraphQLWe can take the previous approach one step further and run this translation layer as a part of our infrastructure close to the database. That means that an extra hop will be eliminated. And if our database is multi-region, our GraphQL API will not lose the latency benefits of that.If the GraphQL layer is a part of the database and allows you to do highly efficient queries straight from your application, then your response latencies will be much better. You’ll be fetching all the entities you need in one hop and only one call. At Fauna, we consider a GraphQL implementation native when the GraphQL layer lives on the infrastructure of the database and adheres to the following conditions:One GraphQL query = one database query = one transactionGraphQL queries offer the same ACID guarantees as the underlying databaseThe underlying database allows efficient execution of such queries", "pub_date": "2020-10-21"},
{"title": "Cloud adoption in a post-COVID world ", "overview": "Many thought that COVID-19 would cause the Great Cloud Slowdown; instead, the opposite happened. What will cloud adoption look like when the masks come off?", "image_url": "https://images.idgesg.net/images/article/2020/09/life_after_coronavirus_covid-19_map_stats_mobile_phone_new_normal_by_da-kuk_gettyimages-1224333085_2400x1600-100857724-large.jpg", "url": "https://www.infoworld.com/article/3586597/cloud-adoption-in-a-post-covid-world.html", "body": "I drew some big lines in the sand by predicting that the pandemic would cause a run on cloud computing. The pandemic quickly proved that companies leveraging more public cloud resources than their competitors and peers have much less exposure to risk than those that do not. Priorities soon shifted to cloud migration on the fastest path possible. The result is a pandemic run on cloud migration tools, experienced cloud pros, cloud consulting, and public clouds themselves. Most of us didn’t believe the experts’ predictions that pandemic quarantines and restrictions could linger until the end of summer, with another spike during flu season in the fall. It’s now October, the start of flu season. Cases are spiking to record levels. Pandemic restrictions are still in place in many states. I don’t need a crystal ball to predict that the current run on cloud will continue until the end of the pandemic. Also on InfoWorld: When hybrid multicloud has technical advantagesThe good news is that the pandemic will end, whether by an effective vaccine, improved treatments, better disease management, or a combination of all three. If progress continues as forecast, the COVID-19 pandemic could be contained worldwide by next summer. Even without a vaccine or the scientific advances available to today’s research teams, the pandemic of 1918 ended after 26 months. We may never return to the old normal, but, one way or another, this virus will eventually run its course.Now, what does postpandemic cloud computing look like? First, for most enterprises, a reduction in momentum appears unlikely. However, priorities will shift. Today’s focus is on the low-hanging fruit to achieve migration at speed. Eventually we’ll run out of easily migrated workloads.Once the number of applications migrated slows down, the complexity and value of those workloads will increase. That means we’ll need at least the same amount of resources to refactor more complex applications to run correctly on cloud platforms. The focus will move from lift and shift at speed to application modifications and modernizations that take advantage of the native features of the target public clouds. Second, because we’ll be fixing mistakes made during the initial pandemic run, we’ll increase the required resources. This is due to excess complexity caused by little or no coordination between cloud migration and cloud development teams, all under pressure to migrate or build faster to fix the risk exposures revealed during the early days of lockdown. This will surprise most enterprises. Budgets will need to move away from migration to solve these issues. Overall I still expect budget increases because cloud is now considered strategic to the business. Finally, there’s a silver lining to today’s new and renewed interest in cloud computing. Changes in business that were once unthinkable happened overnight, and they happened worldwide. Today it’s easier to accept that cultures and processes must change to allow cloud computing to work properly. The acceptance of cloud computing within most enterprises is a major shift. Enterprises that embrace the changes are likely to adopt new technologies that were once career risks to even mention in meetings, such as cloud, artificial intelligence, edge computing, etc. This acceptance and desire for change and disruption will allow the business to become more agile, and thus respond better to customer and market dynamics. We will also see global, national, and local disasters in the future that will continue to change the way we use technology to run our businesses and our world. The sooner we understand that change is inevitable—and it’s coming—the better off we’ll be. ", "pub_date": "2020-10-23"},
{"title": "How edge analytics will drive smarter computing", "overview": "Tapping edge computing and IoT devices for real-time analytics holds great promise, but designing analytical models for edge deployment presents a challenge. ", "image_url": "https://images.idgesg.net/images/article/2018/02/big_data_analysis_analytics_iot_internet_of_things_thinkstock_829075370-100749748-large.jpg", "url": "https://www.infoworld.com/article/3586550/how-edge-analytics-will-drive-smarter-computing.html", "body": "Many analytics and machine learning use cases connect to data stored in data warehouses or data lakes, run algorithms on complete data sets or a subset of the data, and compute results on cloud architectures. This approach works well when the data doesn’t change frequently. But what if the data does change frequently?Today, more businesses need to process data and compute analytics in real-time. IoT drives much of this paradigm shift as data streaming from sensors requires immediate processing and analytics to control downstream systems. Real-time analytics is also important in many industries including healthcare, financial services, manufacturing, and advertising, where small changes in the data can have significant financial, health, safety, and other business impacts.Also on InfoWorld: Amazon, Google, and Microsoft take their clouds to the edgeIf you’re interested in enabling real-time analytics—and in emerging technologies that leverage a mix of edge computing, AR/VR, IoT sensors at scale, and machine learning at scale—then understanding the design considerations for edge analytics is important. Edge computing use cases such as autonomous drones, smart cities, retail chain management, and augmented reality gaming networks all target deploying large scale, highly reliable edge analytics.Edge analytics, streaming analytics, and edge computingSeveral different analytics, machine learning, and edge computing paradigms are related to edge analytics:Edge analytics refers to analytics and machine learning algorithms deployed to infrastructure outside of cloud infrastructure and “on the edge” in geographically localized infrastructure.Streaming analytics refers to computing analytics in real time as data is processed. Streaming analytics can be done in the cloud or on the edge depending on the use case.Event processing is a way to process data and drive decisions in real time. This processing is a subset of streaming analytics, and developers use event-driven architectures to identify events and trigger downstream actions.Edge computing refers to deploying computation to edge devices and network infrastructure.Fog computing is a more generalized architecture that splits computation among edge, near edge, and cloud computing environments.When designing solutions requiring edge analytics, architects must consider physical and power constraints, network costs and reliability, security considerations, and processing requirements.  Reasons to deploy analytics on the edgeYou might ask why you would deploy infrastructure to the edge for analytics? There are technical, cost, and compliance considerations that factor into these decisions.Applications that impact human safety and require resiliency in the computing architecture are one use case for edge analytics. Applications that require low latency between data sources such as IoT sensors and analytics computing infrastructure are a second use case that often requires edge analytics. Examples of these use cases include: Self-driving cars, automated machines, or any transportation where control systems are automating all or parts of the navigation.Smart buildings that have real-time security controls and want to avoid having dependencies on network and cloud infrastructure to allow people to enter and exit the building safely.Smart cities that track public transportation, deploy smart meters for utility billing, and smart waste management solutions. Cost considerations are a significant factor in using edge analytics in manufacturing systems. Consider a set of cameras scanning the manufactured products for defects while on fast-moving conveyor belts. It can be more cost-effective to deploy edge computing devices in the factory to perform the image processing, rather than having high-speed networks installed to transmit video images to the cloud.Also on InfoWorld: When hybrid multicloud has technical advantagesI spoke with Achal Prabhakar, VP of engineering at Landing AI, an industrial AI company with solutions that focus on computer vision. “Manufacturing plants are quite different from mainstream analytics applications and therefore require rethinking AI including deployment,” Prabhakar told me. ”A big focus area for us is deploying complex deep learning vision models with continuous learning directly on production lines using capable but commodity edge devices.”Deploying analytics to remote areas such as construction and drilling sites also benefits from using edge analytics and computing. Instead of relying on expensive and potentially unreliable wide area networks, engineers deploy edge analytics infrastructure on-site to support the required data and analytics processing. For example, an oil and gas company deployed a streaming analytics solution with an in-memory distributed computing platform to the edge and reduced the drilling time by as much as 20 percent, from a typical 15 days to 12 days. Compliance and data governance is another reason for edge analytics. Deploying localized infrastructure can help meet GDPR compliance and other data sovereignty regulations by storing and processing restricted data in the countries where the data is collected.Designing analytics for the edgeUnfortunately, taking models and other analytics and deploying them to edge computing infrastructure isn’t always trivial. The computing requirements for processing large data sets through computationally intensive data models may require re-engineering before running and deploying them on edge computing infrastructure.For one thing, many developers and data scientists now take advantage of the higher-level analytics platforms that are available on public and private clouds. IoT and sensors often utilize embedded applications written in C/C++, which may be unfamiliar and challenging terrain for cloud-native data scientists and engineers.Another issue may be the models themselves. When data scientists work in the cloud and scale computing resources on-demand at relatively low costs, they are able to develop complex machine learning models, with many features and parameters, to fully optimize the results. But when deploying models to edge computing infrastructure, an overly complex algorithm could dramatically increase the cost of infrastructure, size of devices, and power requirements.I discussed the challenges of deploying AI models to the edge with Marshall Choy, VP of product at SambaNova Systems. “Model developers for edge AI applications are increasingly focusing more on highly-detailed models to achieve improvements in parameter reduction and compute requirements,” he noted. “The training requirements for these smaller, highly-detailed models remains daunting.”Another consideration is that deploying a highly reliable and secure edge analytics system requires designing and implementing highly fault-tolerant architectures, systems, networks, software, and models.I spoke with Dale Kim, senior director of product marketing at Hazelcast, about use cases and constraints when processing data at the edge. He commented that, while equipment optimizations, preventive maintenance, quality assurance checks, and critical alerts are all available at the edge, there are new challenges like limited hardware space, limited physical accessibility, limited bandwidth, and greater security concerns.“This means that the infrastructure you’re accustomed to in your data center won’t necessarily work,” Kim said. “So you need to explore new technologies that are designed with edge computing architectures in mind.”Also on InfoWorld: How to choose a cloud machine learning platformThe next frontier in analyticsThe more mainstream use cases for edge analytics today are data processing functions, including data filtering and aggregations. But as more companies deploy IoT sensors at scale, the need to apply analytics, machine learning, and artificial intelligence algorithms in real-time will require more deployments on the edge. The possibilities at the edge make for a very exciting future of smart computing as sensors become cheaper, applications require more real-time analytics, and developing optimized, cost-effective algorithms for the edge becomes easier. ", "pub_date": "2020-10-26"},
{"title": "Why you’re doing cloudops wrong", "overview": "Now that operational best practices for cloud computing are well known, why do mistakes keep piling up?  ", "image_url": "https://images.idgesg.net/images/article/2019/10/businesswoman_about_to_step_on_a_banana_peel_risks_mistakes_vulnerabilities_by_baona_gettyimages-635792860_binary_by__gerd_altmann_cc0_via_pixabay_2400x1600-100814146-large.jpg", "url": "https://www.infoworld.com/article/3587389/why-youre-doing-cloudops-wrong.html", "body": "Cloud operations, aka cloudops, is the long tail in the cloud computing migration and development story. It takes place after you deploy cloud-based solutions and then operate them over a long period of time. Cloudops determines the success of a migration or development effort and the success of user and customer experiences.Some things going wrong in cloudops right now need some attention. First is too many types of operational tools, such as management and monitoring. These tools drive more operational complexity, which can result in human errors that, in turn, cause operational issues. Another problem is enterprises that underestimate the resources required to drive cloudops. Because we’re moving to largely heterogeneous, multicloud deployments, we’ve tripled the number of resources under management in the past four years, yet the size of most ops staffs has remained the same. Third is a lack of cross-cloud security solutions. Generally speaking, you can’t scale native security services on each public cloud; risk and vulnerabilities begin to emerge. Security needs to be more than an afterthought.Also on InfoWorld: Which multicloud architecture will win out?More is going wrong than just the issues mentioned here, although these are the most common. Take time to understand each of these problems, one at a time. At the same time, look for common and holistic solutions. A few suggestions to do cloudops right:Strive to remove much of the complexity from operations but normalize the number of tools employed. This includes common security tools that span cloud and platform, and common management and monitoring tools, such as AIops tooling.With a bit of planning, you can cut the number of cloudops tools in half. This reduction comes with lower risk and fewer resources (people) needed to drive operations. But don’t kid yourself. This approach will change operational processing and playbooks. The goal is to find the most optimized solution that requires the least number of tools and people. At the same time, the solution should increase the effectiveness of operations and uptime. If you take this commonality approach seriously, most problems will disappear.I often observe ops teams doing something over and over again without question, even if they suspect there is a better way. This including processing and tooling. Continuous improvement of cloudops encourages teams to question all aspects of  procedures and tools. Those who promote this approach often find that change is not as readily accepted as they’d hoped. Reticence can typically be overcome if you empower those who do the daily cloudops jobs with the authority to launch proof-of-concepts for tools and procedures in search of new and better solutions. This should be at least 10 percent of the cloudops budget. I’m not saying that those who do cloudops wrong are not good at it. Like any other technical discipline, evolving and improving is just part of the game. If your team or your project experience problems, review your operational tools, resources, and security with an eye toward common tasks and tools. Then empower your staff to make continuous improvements. With the right approach, skills will improve and problems will get solved—or be avoided altogether. ", "pub_date": "2020-10-27"},
{"title": "PyTorch 1.5 adds C++ power, distributed training", "overview": "The powerful deep learning system for Python now makes it easier to integrate high performance C++ code and train models on multiple machines at once", "image_url": "https://images.techhive.com/images/article/2016/10/fire_match_ignite_flame-100689723-large.jpg", "url": "https://www.infoworld.com/article/3539275/pytorch-15-adds-c-power-distributed-training.html", "body": "PyTorch, the Python framework for quick-and-easy creation of deep learning models, is now out in version 1.5.PyTorch 1.5 brings a major update to PyTorch’s C++ front end, the C++ interface to PyTorch functionality. The C++ front end now offers complete feature parity with the Python API. Normally one would write PyTorch applications exclusively in Python, but this change makes it readily possible to prototype the application in Python, then move it to C++ without losing any features, or to freely mix C++ and Python.Also on InfoWorld: 5 reasons to choose PyTorch for deep learningPyTorch 1.5 also adds a way to bind custom C++ classes to TorchScript and Python. TorchScript lets you create models in Python and run them without Python dependencies—e.g., in C++. The new binding system allows your C++ code to be visible to TorchScript, so C++ objects and memory spaces can be created and manipulated using TorchScript or Python. This feature is still considered experimental.PyTorch 1.5 also drops support for Python 2 and requires Python 3.5 and higher. This is in line with other major Python frameworks, as Python 2 is now at end-of-life status and is no longer receiving any updates.Finally, PyTorch 1.5 brings a stable version of the distributed RPC framework and RPC API. Introduced in PyTorch 1.4 as an experimental feature, the RPC framework provides mechanisms for running PyTorch functions on remote machines and thus allows training models across multiple machines for faster training results. Also on InfoWorld: 8 great Python libraries for natural language processingWith PyTorch 1.5, the RPC framework can be used to build training applications that make use of distributed architectures if they’re available. The RPC framework is designed to minimize the amount of data copying across nodes, so work is always done as close to the data as possible.", "pub_date": "2020-04-22"},
{"title": "Looking into the post-2020 future of Big Tech", "overview": "A strong showing during the pandemic leaves Apple, Amazon, Facebook, Google, and Microsoft poised for continued growth and increased regulatory scrutiny.", "image_url": "https://images.idgesg.net/images/article/2019/09/light-bulb-100811494-large.jpg", "url": "https://www.infoworld.com/article/3586797/looking-into-the-post-2020-future-of-big-tech.html", "body": "The year isn’t over, not for more than two months. But it sure feels like 2020 should end soon, seeing as how one pandemic-stressed, politics-inflamed day bleeds into the next.Nevertheless, it might be healthy to start putting 2020 behind us. It’s been a disruptive year in high tech, as in every other industry, but it’s also been a highly public proving ground for cloud, streaming, artificial intelligence, and other pillars of 21st century civilization.Also on InfoWorld: How data analysis, AI, and IoT will shape the post-pandemic ‘new normal’Tech has been the economy’s mainstay during the pandemicAs we squint past the upcoming US presidential election into the new year, the following trends will shape the technology landscape for the rest of this decade, regardless of who occupies the White House on January 20.For starters, the pandemic will drag on longer than people would like to believe. Distancing, quarantining, and lockdowns will still hammer the global economy for at least another 6 to 12 months. However, the tech sector has already figured out how to keep operating in spite of these obstacles. Just as important, its customers now rely intimately on cloud, streaming, remote collaboration, robotics, smart sensors, and other digital technologies in order to stay alive.In 2021, enterprise technology professionals will adjust their strategies with one eye on COVID-19 trends and the other on their digital transformation initiatives. Predictive containment management will become a central element of every facilities administrator’s high-tech toolkit.Socially distanced living will persist long past the present pandemic’s end. People will continue to socially distance in most public interactions for at least the next three to five years. Through fits and starts, the global culture is getting used to personal protective equipment, ubiquitous biosensors, contactless interactions, remote collaboration, sanitization-intensive maintenance, and other changes in how we live and work. Many of these new living styles depend on AI, cloud services, mobile devices, robotics, and other technological platforms.The tech vendors who stand to gain the most from this trend are those—such as Amazon, Google, and Microsoft—that provide full, cloud-to-edge ecosystems that enable seamless “new normal” lifestyles.Furthermore, effective COVID-19 vaccines will come to market only after long delays in R&D and in clinical trials. Hopes for a quick resolution to the pandemic crisis will wither as the grinding process of developing and testing COVID-19 vaccines continues. If we consider the history of vaccines for recent pandemics such as Ebola, it may take two to three years before an effective COVID-19 vaccine arrives. However, the tech sector will benefit from the fact that vaccine development will critically depend on AI, high-performance computing, quantum technology, and other advanced tools.As the human race sifts through false hopes and counterproductive approaches for treatment and cure, tech companies’ solutions will literally become a lifeline for distinguishing what works from what doesn’t.FAANGs will cement their dominance in the global economyAgainst the backdrop of these trends, I wouldn’t bet against the FAANG (Facebook, Amazon, Apple, Netflix, Google) vendors (and also Microsoft) further cementing their tech-industry dominance throughout the 2020s.These six vendors—often grouped together as “Big Tech”—are the foremost business success stories in the present crisis. Tech stocks have been at the heart of this year’s counterintuitive stock market recovery.The mini crash of March is receding in the rearview mirror as the FAANG vendors have proven how essential cloud, streaming, remote collaboration, edge computing, and other digital technologies are to economic resilience. During the past several months, the tech giants have outperformed the rest of the stock market, with the Nasdaq-100 index up 24 percent this year.Also on InfoWorld: Coding together apart: Software development after COVID-19Even as the pandemic wanes, investors will flock to the Big Tech vendors as the safest place to put their money. Tech companies have proven to be low-risk investments, considering that many of today’s leading tech companies are highly profitable and can easily cover their respective debt holdings from available cash.Political and regulatory pressure will build on Big TechOne big wild card in Big Tech’s future may be legal and regulatory pressures to divest themselves of assets that give them what some call an unfair, monopolistic advantage in their core markets.Recently, the US House of Representatives antitrust subcommittee released its investigative findings that Google, Apple, Facebook, and Amazon “have become the kinds of monopolies we last saw in the era of oil barons and railroad tycoons.” The subcommittee specifically called out Amazon’s dominance in e-commerce, Google’s in search and advertising, Facebook’s in social networking, and Apple’s in both mobile content and apps. The companies were cited for such anticompetitive practices as acquiring potential competitors and using their platforms to limit competition, control access, and favor their own products.To address these concerns, the subcommittee recommended updating antitrust laws to reflect today’s digital economy. It also called for additional FTC oversight over Big Tech mergers and acquisitions. Most controversially, the legislators advocated breaking up the Big Tech companies or requiring them to divest themselves of key assets to encourage competition.At first glance, there doesn’t appear to be any natural way to partion these vendors to achieve the stated objective. Requiring Amazon to spin off its third-party seller marketplace wouldn’t lessen the dominance of its core “first-party sales” online retailing site. Splitting off Google’s search assets from its mobile OS, digital advertising, cloud computing, and office productivity assets wouldn’t dent its momentum in any of those other segments. Mandating that Facebook sell off its Instagram, WhatsApp, and Messenger products wouldn’t dilute its hold on its core social networking subscribers. And opening up Apple’s devices to other app stores, music channels, and streaming media channels probably wouldn’t diminish the first-to-market advantages of Apple-founded offerings on those devices.Dominant tech vendors will not back downIf legislators and regulators attempt to force Big Tech’s hand in this regard, they’re likely to set the stage for a long, drawn-out, lawyerly trench war.Even if any of these megalithic digital companies were forced to partition, M&A activity in the impacted segments might bring the pieces back together. This is not a farfetched scenario if we consider how the “Baby Bells” organically re-coalesced in the years after the 1984 AT&T divestiture or the “Baby Standards” reasserted their collective muscle in the decades after the 1911 Standard Oil antitrust breakup.Another likely outcome is that one or more of the post-breakup FAANG companies might rapidly rebuild itself to “800-pound gorilla” scale through the growth-hacking savvy that pervades this industry. All of the FAANG vendors began as startups, as did many of the assets they gained from strategic acquisitions. Nothing would be more natural than for the newly independent, split-off divisions to come roaring back with fresh capital, vision, and momentum.During this decade, Big Tech vendors will face greater regulatory scrutiny if they attempt to buy out direct competitors or substantial vendors in adjacent niches. However, any regulatory effort to hamstring the FAANGs in their core markets is likely to set off a new round of acquisitions in nontraditional (for them) industries. We expect that other Big Tech companies will follow Amazon’s lead in acquiring strong brands in brick-and-mortar sectors that have been severely impacted by the COVID-19 recession and from the tech-instigated disruptions of the past decade. In this way, defunct brands in retail, hospitality, airlines, and other sectors may resurface in new roles within predominantly virtual business models.This almost goes without saying. After all, the FAANGs are sitting on huge piles of cash and are avidly scouting for new investment opportunities, in the classic fashion of any self-respecting capitalist. As of this past May, Apple held cash, cash equivalents, and marketable securities to the tune of $192.8 billion, while Microsoft was sitting on $137.6 billion, Google parent Alphabet had $117.2 billion, Facebook had amassed $60.3 billion, and Amazon held $49.3 billion. Also, the sharp rise in all of their stock prices during the COVID-19 crisis gives them even more purchasing power for snatching up valuable competitive assets.Also on InfoWorld: Amazon, Google, and Microsoft take their clouds to the edgeDuring the coming year, it wouldn’t be surprising if one or more of the Big Tech vendors decide to voluntarily self-partition, without a specific legal or regulatory mandate to do so. We can see foreshadowings of this in Google’s restructuring of its business units into Alphabet subsidiaries four years ago. HP split into enterprise and consumer businesses in 2014 and IBM separated technical services from its core product portfolio earlier this year.If Apple, Amazon, Facebook, and other Big Tech vendors can find a shareholder rationale for partitioning their respective businesses in the coming year, they will probably waste no time putting it into effect. If doing so helps keep regulators from breathing down their necks, then, all the better.", "pub_date": "2020-10-22"},
{"title": "O’Reilly pulls the plug on in-person events", "overview": "The company believes the events business is ‘forever changed’ due to the coronavirus pandemic, so is moving its conferences online", "image_url": "https://images.idgesg.net/images/article/2018/10/white-outlet_electrical-outlet_plug_unplugged_sad-face-100777512-large.jpg", "url": "https://www.infoworld.com/article/3534429/oreilly-pulls-the-plug-on-in-person-events.html", "body": "In the wake of the COVID-19 virus pandemic, prominent technology conference producer O’Reilly has shut down its events business, permanently. From now on, O’Reilly events will be held online.The producer of events such as OSCON (O’Reilly Open Source Software Conference) and the Strata Data & AI conference, O’Reilly noted in a March 24 bulletin the impact of the virus on its in-person events division. In response, the company recently switched its Strata conference, which was to be held in San Jose last week, to an online format, drawing more than 4,600 remote attendees.Also on InfoWorld: Artificial intelligence predictions for 2020“Without understanding when this global health emergency may come to an end, we can’t plan for or execute on a business that will be forever changed as a result of this crisis,” said Laurie Baldwin, O’Reilly president. “With large technology vendors moving their events completely on-line, we believe the stage is set for a new normal moving forward when it comes to in-person events.”Baldwin noted that large technology vendors have moved events online as well. Microsoft, for one, is moving its Microsoft Build 2020 developer conference, originally planned for Seattle in May, to be all-digital.Keep up with the latest developments in software development, cloud computing, data analytics, and machine learning with the InfoWorld Daily newsletterO’Reilly employees who had been involved in the in-person events business have been let go. In addition to the events business, O’Reilly has a technology publishing business and provides interactive coding events and custom training.", "pub_date": "2020-03-25"},
{"title": "How AI helped Domino’s improve pizza delivery", "overview": "Sharing a container environment and Nvidia GPU server has enabled Domino’s data scientists to create more complex and accurate models to improve store and delivery operations", "image_url": "https://images.techhive.com/images/article/2015/04/drone-pizza-delivery-thinkstock-100579669-large.jpg", "url": "https://www.infoworld.com/article/3535230/how-ai-helped-domino-s-improve-pizza-delivery.html", "body": "When the words artificial intelligence (AI) and machine learning (ML) are used, people often think of advanced industries such as space exploration and biomedicine that rely heavily on research and development. The fact is, AI and ML should be something all industries are looking at, including retail. We are now in the customer service era and small differences in service can make a big difference in market share.This past week Nvidia held a virtual version of its annual GPU Technology Conference (GTC), which has become a showcase for real life AI/ML use cases. Historically, the show has been a highly technical one, but over the years, it has evolved into an event where companies showcase how they use advanced technologies to transform their businesses.Also on InfoWorld: Artificial intelligence predictions for 2020Domino’s is an example of a familiar retail business that presented how it’s using AI and ML. The company has come up with a successful recipe to change the way it operates. The secret ingredient is Nvidia’s technology, which the leading pizza chain is using to improve store and online operations, provide a better customer experience, and route orders more efficiently.As a result, Domino’s is seeing happier customers and more tips for its drivers. But that’s only a small piece of the multifaceted pie. So, what does it take to get pizza from a Domino’s store to someone’s house? The answer is quite complex.The data science team at Domino’s tested the company’s speed and efficiency by leveraging Nvidia’s DGX-1 server, an integrated software and hardware system for deep learning research. For those not familiar with the DGX server line, Nvidia has created a series of turnkey appliances businesses can drop in and start using immediately. The alternative is to cobble together hardware, software, and AI platforms and tune the entire system correctly. This can take weeks to do.The Domino’s team created a delivery prediction model that forecasts when an order would be ready, using attributes of the order and what is happening in a Domino’s store, such as the number of employees, managers, and customers present at that moment. The model was based on a large dataset of five million orders, which isn’t massive but large enough to create accurate models. All future orders are fed back into the system to further increase model accuracy.Domino’s previous models used GPU-enabled laptops and desktops and would take more than 16 hours to train. The long timeframe made it extremely difficult to improve on the model, said Domino’s data science and AI manager Zachary Fragoso during a presentation at virtual GTC 2020.The extra compute power of the DGX-1 enabled Domino’s data scientists to train more complex models in less time. The system reduced the training time to under an hour and increased accuracy for order forecasts from 75 percent to 95 percent. The test demonstrated how Domino’s could boost productivity by training models faster, Fragoso said.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesDomino’s uncovered another benefit in the process: resource sharing. Each individual GPU on the DGX-1 is so large—with 32 GB of RAM—Domino’s data scientists could use a fraction of the GPUs and run multiple tests simultaneously. With eight such GPUs at their fingertips, the data scientists found themselves sharing resources and knowledge, as well as collaborating across teams.In the past, sharing work across teams—including code reviews and quality assurance testing—was challenging, since data scientists worked in their own local environments. Now that data scientists are working with a common DGX-1 server, they’re easily able to share Docker containers that are fully customizable and reproducible. This gives the data scientists a large resource pool to work with and access to resources when needed, so they’re not sitting idle. The Docker solution that Domino’s integrated with DGX-1 also makes it easier to reproduce code across different environments because all the data is contained within the Docker image.Domino’s recently purchased a second DGX-1 and started adding the Kubernetes container management system to the mix. With Kubernetes managed by an optimization engine, Domino’s can dynamically allocate resources to all its data scientists and launch containers faster. According to Fragoso, even data scientists who aren’t familiar with Linux can point-and-click to launch Docker containers.On the deployment side, Domino’s created an inferencing stack, which includes a Kubernetes cluster and four Nvidia GPUs. This way, data scientists can interact with and build their models using the same Docker container framework they use on the DGX-1.Domino’s also acquired a machine learning operations platform called Datatron, which sits on top of the Kubernetes cluster with the GPUs and assists Domino’s with ML-specific functionalities. Datatron allows for model performance monitoring in real time, so data scientists can be notified if their model requires retraining.Bringing the inference stack in-house allows Domino’s to have all the benefits that the cloud providers offer for hosting ML models, while keeping all data and resources on premises. It has changed the way the data scientists deploy models, giving them much more control over the deployment process, Fragoso explained in his presentation.Keep up with the latest developments in software development, cloud computing, data analytics, and machine learning with the InfoWorld Daily newsletterFragoso concluded with advice for other companies looking to bring these technologies in-house: “Think about how your data scientists will work together and collaborate. In our case, the DGX-1 and our data scientists are interacting in a common workspace. It was something that our team didn’t really consider when we first acquired this product and has been a real value for us.”Historically, data scientists operated as an independent silo within companies. More and more, IT organization are being asked to take on the task of providing the right technology to AI and ML initiatives. Data scientists are expensive resources for most companies and having them sit around waiting for models to finish is akin to tossing good pizza out the window. The right infrastructure, such as the DGX server series, enables companies to speed up processing time to let the data scientists work more and wait less.", "pub_date": "2020-03-31"},
{"title": "Review: Amazon SageMaker plays catch-up", "overview": "With Studio, Autopilot, and other additions, Amazon SageMaker is now competitive with the machine learning environments available in other clouds", "image_url": "https://images.idgesg.net/images/article/2020/02/cw_hoyhoy_210120-100832442-large.jpg", "url": "https://www.infoworld.com/article/3534699/review-amazon-sagemaker-plays-catch-up.html", "body": "When I reviewed Amazon SageMaker in 2018, I noted that it was a highly scalable machine learning and deep learning service that supports 11 algorithms of its own, plus any others you supply. Hyperparameter optimization was still in preview, and you needed to do your own ETL and feature engineering. Since then, the scope of SageMaker has expanded, augmenting the core notebooks with IDEs (SageMaker Studio) and automated machine learning (SageMaker Autopilot) and adding a bunch of important services to the overall ecosystem, as shown in the diagram below. This ecosystem supports machine learning from preparation through model building, training, and tuning to deployment and management — in other words, end to end.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesAmazon SageMaker Studio improves on the older SageMaker notebooks, and a number of new services have enhanced the SageMaker ecosystem to support end-to-end machine learning.What’s new in SageMaker?What’s new? Given that I last looked at SageMaker just after it was released, the list is rather long, but let’s start with the most visible services.SageMaker Studio, an IDE based on JupyterLabSageMaker Autopilot, which automatically builds and trains up to 50 feature-engineered models that can be examined in SageMaker StudioSageMaker Ground Truth, which helps to build and manage training datasetsSageMaker Notebooks now offer elastic compute and single-click sharingSageMaker Experiments, which helps developers visualize and compare machine learning model iterations, training parameters, and outcomesSageMaker Debugger, which provides real-time monitoring for machine learning models to improve predictive accuracy, reduce training times, and facilitate greater explainabilitySageMaker Model Monitor, which detects concept drift to discover when the performance of a model running in production begins to deviate from the original trained modelOther notable improvements include the optional use of spot instances for notebooks to reduce the cost; a new P3dn.24xl instance type that includes eight V100 GPUs; an AWS-optimized TensorFlow framework, which achieves close to linear scalability when training multiple types of neural networks; Amazon Elastic Inference, which can dramatically decrease inference costs; AWS Inferentia, which is a high-performance machine learning inference chip; and new algorithms, both built-in to SageMaker and  available in the AWS Marketplace. In addition, SageMaker Neo compiles deep learning models to run on edge computing devices, and SageMaker RL (not shown on the diagram) provides a managed reinforcement learning service.SageMaker StudioJupyterLab is the next-generation, web-based user interface for Project Jupyter. SageMaker Studio uses JupyterLab as the basis for an IDE that’s a unified online machine learning and deep learning workstation with collaboration features, experiment management, Git integration, and automatic model generation.The screenshot below shows how to install the SageMaker examples into a SageMaker Studio instance, using a terminal tab and the Git command line. The instructions for doing this are in the README for this example, which is kind of a Catch-22. You can read them by browsing to the Getting Started example on GitHub, or by cloning the repository to your own machine and reading it there.SageMaker Studio showing the folder view at the left, and a terminal at the right. In the terminal, we have run a Git command line to populate the instance with Amazon’s SageMaker examples from GitHub.Amazon’s Getting Started example contains a notebook called xgboost_customer_churn_studio.ipynb, which was adapted from a blog post about predicting customer churn. As Jupyter notebooks go, it has lots of explanations, as you can see in the screenshots below.Also on InfoWorld: The 6 best programming languages for AI developmentAmazon SageMaker Studio Walkthrough. This example touches on four of the major features of SageMaker Studio: Experiments, the debugger, model hosting, and the model monitor. It starts with prepping the data, then training, then hosting and monitoring.In steps 4 and 5 of the XGBoost notebook we take a quick look at the training data and upload the files to Amazon S3. If the required bucket doesn’t exist, we create it.In steps 6 through 9 we set up the Experiment and define the hyperparameters to use for the XGBoost training. We start with the built-in XGBoost algorithm.In step 10 we create a trial and run the first training. Note that we’re using one ml.m4.xlarge instance for the training.In step 11 we run five more trainings, for different values of the min_child_weight hyperparameter. The chart at the bottom is a static example, but I was able to follow the instructions to create my own chart from the components of this trial.The example goes on to run an additional training with an external XGBoost algorithm modified to save debugging information to Amazon S3 and to invoke three debugging rules. This is in what’s called  mode, meaning that it’s not a built-in algorithm.When the trainings are all done, you can compare the results in the Experiments tab.Here we are comparing the results of six trials in algorithm mode in the Experiments tab. The results are ordered by validation error in this view.The example then hosts the model using its  method and tests the deployed endpoint using its  method. Finally, it creates a baselining job with the training dataset and a scheduled monitoring job that reports any constraint violations.Also on InfoWorld: 8 great Python libraries for natural language processingBy the way, XGBoost is only one of the many algorithms built into SageMaker. A full list is shown in the table below — and you can always create your own model.Amazon SageMaker’s built-in algorithms. You can always use other algorithms in “framework” mode by supplying the code.SageMaker AutopilotSuppose you don’t know how to do feature engineering and you aren’t very familiar with the different algorithms available for the various machine learning tasks. You can still use SageMaker — just let it run on autopilot. SageMaker Autopilot is capable of handling datasets up to 5 GB.In the screenshot below we are running the Direct Marketing with Amazon SageMaker Autopilot example. It starts by downloading the data, unzipping it, uploading it to an S3 bucket, and launching an Autopilot job by calling the create_auto_ml_job API. Then we track the progress of the job as it analyzes the data, does feature engineering, and does model tuning, as shown below.At right, we see a SageMaker Autopilot job running in SageMaker Studio. It completed in about four hours. At left, we see SageMaker Studio’s Git support.The example then picks the best model, uses it to create and host an endpoint, and runs a transform job to add the model predictions to a copy of the test data. Finally, it finds the two notebooks created by the Autopilot job.There is a user interface to the Autopilot results, although it’s not obvious. If you right-click on the automl experiment you can see all the trials with their objective values, as shown below.Click on the Objective column title to sort the results and bring the best one to the top; click on the buttons at the top right to open the candidate generation and data exploration notebooks.Also on InfoWorld: 5 reasons to choose PyTorch for deep learningSageMaker Ground TruthIf you’re lucky, all your data will be labeled, or otherwise annotated, and ready to be used as a training dataset. If not, you can annotate the data manually (the standard joke is that you give the task to your grad students), or you can use a semi-supervised learning process that combines human annotations with automatic annotations. SageMaker Ground Truth is such a labeling process.As you can see in the diagram below, Ground Truth can be applied to a number of different tasks. With Ground Truth, you can use workers from either Amazon Mechanical Turk, or a vendor company that you choose, or an internal, private workforce along with machine learning to enable you to create a labeled dataset.Amazon SageMaker Ground Truth combines human annotations with automatic annotations to turn raw data into training data for model building.Amazon provides seven walkthroughs that demonstrate various ways of using SageMaker Ground Truth.SageMaker NeoUntil recently, deploying trained models on edge devices — smartphones and IoT devices, for example — has been difficult. There have been specific solutions, such as TensorFlow Lite for TensorFlow models and TensorRT for Nvidia devices, but SageMaker Neo compiles and automatically optimizes TensorFlow, Apache MXNet, PyTorch, ONNX, and XGBoost models for deployment on ARM, Intel, and Nvidia processors as well as Qualcomm, Cadence, and Xilinx devices.According to AWS, Neo can double the performance of models and shrink them enough to run on edge devices with limited amounts of memory.SageMaker inference deployment optionsIn terms of compute, storage, network transfer, etc., deploying models for production inference often accounts for 90 percent of the cost of deep learning, while the training accounts for only 10 percent of the cost. AWS offers many ways to reduce the cost of inference.One of these is Elastic Inference. AWS says that Elastic Inference can speed up the throughput and decrease the latency of getting real-time inferences from your deep learning models that are deployed as Amazon SageMaker hosted models, but at a fraction of the cost of using a GPU instance for your endpoint. Elastic Inference accelerates inference by allowing you to attach fractional GPUs to any Amazon SageMaker instance.Elastic Inference is supported in Elastic Inference-enabled versions of TensorFlow, Apache MXNet, and PyTorch. To use any other deep learning framework, export your model by using ONNX, and then import your model into MXNet.Keep up with the latest developments in software development, cloud computing, data analytics, and machine learning with the InfoWorld Daily newsletterIf you need more than the 32 TFLOPS per accelerator you can get from Elastic Inference, you can use EC2 G4 instances, which have Nvidia T4 GPUs, or EC2 Inf1 instances, which have AWS Inferentia custom accelerator chips. If you need the speed of Inferentia chips, you can use the AWS Neuron SDK to compile your deep learning model into a Neuron Executable File Format (NEFF), which is in turn loaded by the Neuron runtime driver to execute inference input requests on the Inferentia chips.At this point, the Amazon SageMaker Studio preview is good enough to use for end-to-end machine learning and deep learning: data preparation, model training, model deployment, and model monitoring. While the user experience still leaves a few things to be desired, such as better discovery of functionality, Amazon SageMaker is now competitive with the machine learning environments available in other clouds.— $0.0464 to $34.272 per instance hour for compute, depending on number of CPUs and GPUs; SSD storage: $0.14 per GB-month; Data transfer: $0.016 per GB in or out.  Hosted on Amazon Web Services. ", "pub_date": "2020-04-01"},
{"title": "Is your data lake open enough? What to watch out for", "overview": "Like yesterday’s data warehouses, today’s data lakes threaten to lock us into proprietary formats and systems that restrict innovation and raise costs", "image_url": "https://images.idgesg.net/images/article/2020/04/ifw_data-lakes_outdoors_mountains_water_by-ryan-stone-via-unsplash-100837349-large.jpg", "url": "https://www.infoworld.com/article/3534516/is-your-data-lake-open-enough-what-to-watch-out-for.html", "body": "A data lake is a system or repository that stores data in its raw format along with transformed, trusted data sets, and provides both programmatic and SQL-based access to this data for diverse analytics tasks such as data exploration, interactive analytics, and machine learning. The data stored in a data lake can include structured data from relational databases (rows and columns), semi-structured data (CSV, logs, XML, JSON), unstructured data (emails, documents, PDFs), and binary data (images, audio, video).A challenge with data lakes is not getting locked into proprietary formats or systems. This lock-in restricts the ability to move data in and out for other uses or to process data using other tools, and can also tie a data lake to a single cloud environment. That’s why businesses should strive to build open data lakes, where data is stored in an open format and accessed through open, standards-based interfaces. Adherence to an open philosophy should permeate every aspect of the system, including data storage, data management, data processing, operations, data access, governance, and security. Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesAn open format is one based on an underlying open standard, developed and shared through a public, community-driven process without vendor-specific proprietary extensions. For example, an open data format is a platform-independent, machine-readable data format, such as ORC or Parquet, whose specification is published to the community, such that any organization can create tools and applications to read data in the format.A typical data lake has the following capabilities:Data ingestion and storageData processing and support for continuous data engineeringData access and consumptionData governance including discoverability, security, and complianceInfrastructure and operationsIn the following sections, we will describe openness requirements for each capability.Data ingestion and storageAn open data lake ingests data from sources such as applications, databases, data warehouses, and real-time streams. It formats and stores the data into an open data format, such as ORC and Parquet, that is platform-independent, machine-readable, optimized for fast access and analytics, and made available to consumers without restrictions that would impede the re-use of that information. An open data lake supports both pull-based and push-based ingestion of data. It supports pull-based ingestion through batch data pipelines and push-based ingestion through stream processing. For both these types of data ingestion, an open data lake supports open standards such as SQL and Apache Spark for authoring data transformations. For batch data pipelines, it supports row-level inserts and updates—UPSERT—to data sets in the lake. Upsert capability with snapshot isolation—and more generally, ACID semantics—greatly simplifies the task, as opposed to rewriting data partitions or entire data sets. The ingest capability of an open data lake ensures zero data loss and writes exactly-once or at-least-once, handles schema variability, writes in the most optimized data format into the right partitions, and provides the ability to re-ingest data when needed.Data processing and support for continuous data engineeringAn open data lake stores the raw data from various data sources in a standardized open format. However, use cases such as data exploration, interactive analytics, and machine learning require that the raw data be processed to create use-case driven trusted data sets. For data exploration and machine learning use cases, users continually refine data sets for their analysis needs. As a result, every data lake implementation should enable users to iterate between data engineering and use cases such as interactive analytics and machine learning. This can be thought of as continuous data engineering, which involves the interactive ability to author, monitor, and debug data pipelines. In an open data lake, these pipelines are authored using standard interfaces and open source tools such as SQL, Python, Apache Spark, and Apache Hive.Data access and consumptionThe most visible outcome of the data lake is the types of use cases it enables. Whether the use case is data exploration, interactive analytics, or machine learning, access to data is vital. The access to data can be through SQL or programmatic languages such as Python, R, and Scala. While SQL is the norm for interactive analysis, programmatic languages are used for more advanced applications like machine learning and deep learning. An open data lake supports data access through a standards-based implementation of  SQL with no proprietary extensions. It enables external tools to access that data through standards such as ODBC and JDBC. Also, an open data lake supports programmatic access to data via standard programming languages such as R, Python, and Scala, and standard libraries for numerical computation and machine learning, such as TensorFlow, Keras, PyTorch, Apache Spark MLlib, MXNet, and Scikit-learn.Data governance – discoverability, security, and complianceWhen data ingestion and data access are implemented well, data can be made widely available to users in a democratized fashion. When multiple teams start accessing data, data architects need to exercise oversight for governance, security, and compliance purposes. Data itself is hard to find and comprehend and not always trustworthy. Users need the ability to discover and profile data sets for integrity before they can trust them for their own use case. A data catalog enriches metadata through different mechanisms, uses it to document data sets, and supports a search interface to aid discovery.Since the first step is to discover the required data sets, it’s essential to surface metadata to end-users for exploration purposes, to see where the data resides and what it contains, and to determine if it is useful for answering a particular question. Discovery includes data profiling capabilities that support interactive previews of data sets to shine a light on formatting, standardization, labels, data shape, and so on.An open data lake should have an open metadata repository. As an example, the Apache Hive metadata repository is an open repository that prevents vendor lock-in for metadata.Increasing accessibility to the data requires data lakes to support strong access control and security features. To be open, a data lake should do this through non-proprietary security and access control APIs. As an example, deep integration with open source frameworks such as Apache Ranger and Apache Sentry can facilitate table-level, row-level, and column-level granular security. This enables administrators to grant permissions against already-defined user roles in enterprise directories such as Active Directory. By basing access control on open source frameworks, open data lakes avoid vendor lock-in that results from a proprietary security implementation.New or expanded data privacy regulations, such as GDPR and CCPA, have created new requirements around “Right to Erasure” and “Right to Be Forgotten.” These govern consumers’ rights about their data and involve stiff financial penalties for non-compliance (as much as four percent of global turnover), so they must not be overlooked. Therefore, the ability to delete specific subsets of data without disrupting a data management process is essential. An open data lake supports this ability through open formats and open metadata repositories. In this way, they enable a vendor-agnostic solution to compliance needs.Infrastructure and operationsWhether the data lake is deployed in the cloud or on-premises, each cloud provider has a specific implementation to provision, configure, monitor, and manage the data lake as well as the resources it needs. An open data lake is cloud-agnostic and portable across any cloud-native environment, including public and private clouds. This allows administrators to leverage the benefits of both public and private cloud from an economics, security, governance, and agility perspective.  Keep up with the latest developments in software development, cloud computing, data analytics, and machine learning with the InfoWorld Daily newsletterOpen for innovationThe increase in the volume, velocity, and variety of data, combined with new types of analytics and machine learning, make data lakes a necessary complement to more traditional data warehouses. Data warehouses exist largely in a world of proprietary formats, proprietary SQL extensions, and proprietary metadata repositories, and lack programmatic access to data. Data lakes don’t need to follow this proprietary path, which leads to restricted innovation and higher costs. A well designed, open data lake provides a robust, future-proof data management system that supports a wide range of data processing needs including data exploration, interactive analytics, and machine learning.Qubole", "pub_date": "2020-04-09"},
{"title": "Not all AIops tools are created equal", "overview": "These tools can solve multicloud complexity issues, but they use very different approaches", "image_url": "https://images.idgesg.net/images/article/2018/05/tools_wrench_size_iterations_iterative_development_repair_process_by_lightfieldstudios_gettyimages_1200x800_binary-100756966-large.jpg", "url": "https://www.infoworld.com/article/3537414/not-all-aiops-tools-are-created-equal.html", "body": "AIops is becoming the new norm for operational tools supporting cloud operations. This technology can be applied to all types of operational tasks, providing intelligent automation that learns as it solves operational issues.These tools must carry out preprogrammed self-corrective processes, and the AIops tools’ ability to learn during those processes creates a huge advantage. For instance, understanding that performance issues could be saturation caused by cyberattacks should kick off security processes to mount a defense. Or moving out of a performance threshold should automatically launch more resources to bring performance back to an acceptable range.Also on InfoWorld: What AI can really do for your business (and what it can’t)The number of things you can do with these tools increases each day, and it’s likely to be standard equipment for those of you deploying and operating multicloud.Most important, AIops tools can deal with thousands of data points and make correlations that most humans would not make. Moreover, as they correlate these data points the tool itself is smarter—it knows what the information actually means and how to assist the cloudops team.The trouble is that many products in this space are actually old technology made new. We’ve been using operational tools for years. Those tools were redone to support public clouds; now they have been rebranded as AIops tools with some built-in AI capabilities.The trouble with this type of evolution is that it’s happening so fast, the tools are naturally going to take different approaches. Some are very data driven, capable of analyzing historical data; other focus on real-time monitoring.Data-oriented tools look for patterns in the data—typically assisted by an AI engine—in order to find cause and effect. They get to the root cause of an issue without the cloudops staff having to cull through gobs of data.Also, AI is leveraged in different ways. Some have pretrained AI systems within the tools, which means the tools comes with a predetermined amount of knowledge. Others focus on training from scratch. Each approach has advantages.The bottom line is that AIops is still an emerging space, despite many vendors being more traditional technology players. Solution patterns differ from tool to tool, and thus you need to be more diligent to understand the basic functionality of the tools, as well as how to match your ops requirements with the proper offering.", "pub_date": "2020-04-10"},
{"title": "TensorFlow deepens its advantages in the AI modeling wars", "overview": "Despite complaints about its complexity, TensorFlow supports every AI development, training, and deployment scenario you can imagine", "image_url": "https://images.idgesg.net/images/article/2018/01/compet_strategy_win_chess_advantage-100746028-large.jpg", "url": "https://www.infoworld.com/article/3534474/tensorflow-deepens-its-advantages-in-the-ai-modeling-wars.html", "body": "TensorFlow remains the dominant AI modeling framework. Most AI (artificial intelligence) developers continue to use it as their primary open source tool or alongside PyTorch, in which they develop most of their ML (machine learning), deep learning, and NLP (natural language processing) models.In the most recent O’Reilly survey on AI adoption in the enterprise, more than half of the responding data scientists cited TensorFlow as their primary tool. This finding is making me rethink my speculation, published just last month, that TensorFlow’s dominance among working data scientists may be waning. Neverthless, PyTorch remains a strong second choice, having expanded its usage in the O’Reilly study to more than 36 percent of respondents, up from 29 percent in the previous year’s survey.Also on InfoWorld: PyTorch vs. TensorFlow: How to chooseBoosting the TensorFlow stack’s differentiation vis-à-vis PyTorchAs the decade proceeds, the differences between these frameworks will diminish as data scientists and other users value feature parity over strong functional differentiation. Nevertheless, TensorFlow remains by far the top AI modeling framework, not just in adoption and maturity, but in terms of the sheer depth and breadth of the stack in supporting every conceivable AI development, training, and deployment scenario. PyTorch, though strong for 80 percent of the core AI, deep learning, and machine learning challenges, has a long way to go before it arrives at feature parity.Last week, Google’s TensorFlow team distanced its stack further from PyTorch. It held its yearly TensorFlow Dev Summit via livestream (for reasons everybody knows). In spite of the absence of in-person buzz, plenty of important news and analysis came from this purely online event.What follows is my discussion of the announcements organized into three broad categories: enhancements to the core TensorFlow and its ecosystem, new TensorFlow devops tools, and new TensorFlow add-ons for specialized AI development challenges.Enhancements to core TensorFlow and its ecosystemIn terms of enhancements to the core open source code, the team announced TensorFlow 2.2. This latest release—which, unlike prior TensorFlow versions, is now compatible with the entire TensorFlow ecosystem—focuses on the performance and stability of the core library. It also adds eager execution at the core, support of NumPy arrays, tf.data, and TensorFlow datasets.Ensuring full support for cross-device compilation of models built in TensorFlow, the team announced TensorFlow Runtime, which will ship with the core open source code and will ensure that ML models work on different devices and platforms that implement the Multi-Level Intermediate Representation standard, which is supported by 95 percent of hardware manufacturers.Making sure that models developed in TensorFlow can run on Google’s own neural network processor, the team announced that the previous version, TensorFlow 2.1, also supports Cloud Tensor processing units.New TensorFlow devops toolsMany enterprise AI developers use TensorFlow to build, train, and deploy deep learning, ML, and NLP models within complex devops workflows. To address these requirements, the team announced new capabilities to accelerate team productivity across the TensorFlow ecosystem: The team revealed new features for TensorFlow Hub, which contains more than 1,000 ready-to-use models and code snippets.: The team announced Keras Tuner, which simplifies the task of tuning model hyperparameters and works with Keras, TensorFlow, and scikit-learn.: Google Cloud AI Pipelines makes it simpler and easier to manage pipelines using TensorFlow Extended.: The team announced dev, a new tool that lets developers and researchers host and share their model-based experiments for free. It provides information and analyses on model performance, on-device workloads, and input pipelines.: The team unveiled Neural Structured Learning, a tool for training neural networks by using structured signals in addition to feature inputs. Structured signals are often used to represent relations or similarity among samples that may be labeled or unlabeled. Leveraging these signals during neural network training harnesses both labeled and unlabeled data and can improve model accuracy, particularly when the amount of labeled data is relatively small.New TensorFlow add-ons for specialized AI development challengesTensorFlow users are working in an astonishing range of sophisticated projects that push AI’s boundaries from every direction. To support these requirements, the team announced the following new tools, extensions, and add-ons that plug into TensorFlow’s ecosystem.: Computer vision is one of the hottest niche markets for AI-driven solutions. To support these developers, the team announced TensorFlow Graphics, which provides graphics functions as a set of differentiable graphics layers to build more efficient computer-vision network architectures. It includes spatial transformers, graphics renderers, and mesh convolutions. It also includes a 3D viewer that works in TensorBoard. It enables explicit modeling of geometric priors and constraints into neural networks for the purpose of building architectures that can be trained robustly, efficiently, and in a self-supervised fashion.: Natural language processing is at the heart of many practical AI applications. The TensorFlow team announced T5 Text-to-Text Transfer Transformer, a framework that applies transfer learning to natural language processing. It also released Meena, a neural conversational model that learns to respond sensibly to a given conversational context.: More AI models are being built in TensorFlow for deployment into browsers and various client- and server-side Web application frameworks. To accelerate developer time-to-value on such projects, the team announced new domain NPM packages for browser-based execution in TensorFlow.js. These include HandPose (detects landmarks on hands using pose estimation), Face Mesh (predict 3D facial landmarks on a human face), and Mobile BERT (Bidirectional Encoder Representations from Transformers) (natural language processing on mobile devices). The team released a new command-line converter wizard to make the conversion of models to TensorFlow.js easier. It launched a new WebAssembly back end for TensorFlow.js in support of fast execution on CPUs, as well as a new Hugging Face NPM package that allows developers to perform question and answering in NodeJS.: More AI projects involve distributing computations in parallel across supercomputers, meshes, and other complex computing fabrics. The team announced Mesh TensorFlow, which provides a language for model parallelism and distributed deep learning. It specifies a broad class of distributed tensor computations for various scenarios involving n-dimensional array of processors connected by a network. These scenarios might include deployments in which model parameters are too many to fit on one device, a 3D image model or other data example is so large that its activations do not fit on one device, or in which lower-latency parallel inference is required.: Probabilistic programming tools enable easier development of AI applications that involve complex statistical reasoning. The team announced TensorFlow Probability, a library that makes it easy to program with Python models that combine statistical reasoning with deep learning and can deploy efficiently on GPUs (graphics processing units) and TPUs (tensor processing units). The library enables data scientists and other developers to encode domain knowledge for understanding data and making predictions. It includes a wide selection of probabilistic models, layers, distributions, and optimizers. RL is the core of many edge, robotics, autonomous vehicle, and other new frontiers in AI. The TensorFlow team announced TF-Agents, a library for designing, implementing, testing, and benchmarking new RL algorithms in TensorFlow.: Fairness, ethics, and bias reduction are becoming more prevalent as objectives that AI development teams must build and train their models to achieve. The team announced a tool and library that will allow developers to visualize the fairness of their models. The Fairness Indicators library enables the computation of common fairness metrics for binary and multiclass classifications, which can be visualized on TensorBoard.: Quantum computing is a proving ground for AI applications of potentially mind-boggling sophistication. The team discussed TensorFlow Quantum, a library for hybrid quantum-classical machine learning. This new software-only stack extends TensorFlow to support building and training ML models to be processed on quantum computing platforms. Developed by Google’s X R&D unit, TensorFlow Quantum enables data scientists to use Python code to develop quantum ML models through standard Keras functions. It provides a library of quantum circuit simulators and quantum computing primitives that are compatible with existing TensorFlow APIs. To see where TensorFlow Quantum fits in this emerging niche, check out my recent InfoWorld article discussing the tool, which was announced just before TensorFlow DevWorld 2020.TensorFlow remains the AI modeling framework to beatThat’s an extraordinarily deep and broad set of announcements, reflecting the deep pockets and vast brainpower that Google and its parent Alphabet are investing in the TensorFlow ecosystem. Taken together, they may not stanch grumbling among AI developers about TensorFlow, which many deem confusing, hard to use, bloated, slow, inefficient, and excessively complex. In fact, the glut of new capabilities may exacerbate those issues to the point where they cause defections to simpler AI modeling frameworks.Nevertheless, TensorFlow is still very much the framework to beat from the perspective of professional data scientists who are building mission-critical AI projects for production environments. This week’s announcements have made its advantages for heavy-hitting AI development even more pronounced.", "pub_date": "2020-03-26"},
{"title": "Will cloud architecture change after COVID?", "overview": "Cloud computing has exploded in popularity during the pandemic. The systemic changes it has caused are likely here to stay", "image_url": "https://images.idgesg.net/images/article/2020/08/covid-19_coronavirus_morphology_infection_outbreak_pandemic_viral_cells_scattered_across_keyboard_and_workspace_by_mixetto_gettyimages-1215023248_2400x1600-100854873-large.jpg", "url": "https://www.infoworld.com/article/3575854/will-cloud-architecture-change-after-covid.html", "body": "A few things have changed since the start of the COVID-19 lockdowns. The Global 2000, as well as governments, now understand that traditional data centers are more vulnerable to natural disasters (such as pandemics) than once thought.  Indeed, the use of cloud computing means that there are no physical data centers and servers to protect, and that we don’t rely on humans for physical operations as much as we did in the past.  Also on InfoWorld: How to choose a cloud machine learning platformPutting the obvious aside for now, let’s look at the future of cloud computing, specifically at the future of cloud computing architecture. I see a few changes happening now that won’t likely go away after the current crisis ends: Only a few enterprise IT shops selected data consolidation as an option, cloud or not. The reasons ranged from security risks, to the lack of willingness to go through the painful process of migrating data from complex and distributed databases, to a consolidated database in the cloud, or even on-premises.  Today, driven by the risks around COVID-19, enterprises are accelerating data consolidation efforts, typically moving to a public cloud provider or providers. This means changing schemas and even databases models, for example, from relational to object.This also means changing the applications that leverage the databases, which is where the costs and risk come in. Some stuff is just going to break. Most enterprises think the risk of not doing this outweighs the risk of data consolidation. You’ll find this will be the preferred approach post-COVID as well. I’m often taken aback by the number of enterprises that moved to public clouds yet still don’t use identity-based security and governance, such as IAM (identity and access management). With the new distributed and remote workforce, IT is finding that traditional security layers, and even role-based security, are way more risky than fined-grained security and governance configurations using identity. This will likely lead to breaches if left alone, not to mention it is more costly to operate.Of course, much like data consolidation, making the change is hard and expensive. Each device, human, API, server, etc., needs to use an identity in order to configure authorization and deauthorization of access. This means standing up directory services if they don’t exist and investing in IAM technology.   The common theme is that enterprises are stepping up and doing what’s hard, risky, and costly in order to remove much of the greater risk that has been identified by the impact of the pandemic. This is a silver lining, considering that these two things needed to be done anyway as a major best practice going forward. ", "pub_date": "2020-09-22"},
{"title": "How MariaDB achieves global scale with Xpand", "overview": "A new MariaDB storage engine provides distributed SQL and massive scalability with a shared nothing architecture, fully distributed ACID transactions, and strong consistency", "image_url": "https://images.idgesg.net/images/article/2018/02/big_data_fintech_globe_thinkstock_826139768-100749747-large.jpg", "url": "https://www.infoworld.com/article/3574077/how-mariadb-achieves-global-scale-with-xpand.html", "body": "As information and processing needs have grown, pain points such as performance and resiliency have necessitated new solutions. Databases need to maintain ACID compliance and consistency, provide high availability and high performance, and handle massive workloads without becoming a drain on resources. Sharding has offered a solution, but for many companies sharding has reached its limits, due to its complexity and resource requirements. A better solution is distributed SQL.In a distributed SQL implementation, the database is distributed across multiple physical systems, delivering transactions at a globally scalable level. MariaDB Platform X5, a major release that includes upgrades to every aspect of MariaDB Platform, provides distributed SQL and massive scalability through the addition of a new smart storage engine called Xpand. With a shared nothing architecture, fully distributed ACID transactions, and strong consistency, Xpand allows you to scale to millions of transactions per second.Also on InfoWorld: Tim O’Reilly: The golden age of the programmer is overOptimized pluggable smart enginesMariaDB Enterprise Server is architected to use pluggable storage engines (like Xpand) to optimize for particular workloads from a single platform. There is no need for specialized databases to handle specific workloads. MariaDB Xpand, our smart engine for distributed SQL, is the most recent addition to our lineup. Xpand adds massively scalable distributed transactional capabilities to the options provided by our other engines. Our other pluggable engines provide optimization for analytical (columnar), read-heavy workloads, and write-heavy workloads. You can mix and match replicated, distributed, and columnar tables to optimize every database for your specific requirements.Adding MariaDB Xpand enables enterprise customers to gain all the benefits of distributed SQL – speed, availability, and scalability – while retaining the MariaDB benefits they are accustomed to.Let’s take a high-level look at how MariaDB Xpand provides distributed SQL.Distributed SQL down to the indexesXpand provides distributed SQL by slicing, replicating, and distributing data across nodes. What does this mean? We’ll use a very simple example with one table and three nodes to demonstrate the concepts. Not shown in this example is that all slices are replicated.Figure 1. Sample table with indexesIn Figure 1 above, we have a table with two indexes. The table has some dates and we have an index on column 2, and another on columns 3 and 1. Indexes are in a sense tables themselves. They’re subsets of the table. The primary key is , the first index in the table. That’s what will be used to hash and spread the table data out around the database.Figure 2. Xpand slices and distributes data, including indexes, across nodes. (Replication is not shown for reasons of simplicity. All slices have at least two replicas.)Now we add the notion of . Slices are essentially horizontal partitions of the table. We have five rows in our table. In Figure 2, the table has been sliced and distributed. Node #1 has two rows. Node #2 has two rows, and Node #3 has one row. The goal is to have the data distributed as evenly as possible across the nodes.The  have also been sliced and distributed. This is a key difference between Xpand and other distributed solutions. Typically, distributed databases have local indexes, so every node has an index of its own data. In Xpand, indexes are distributed and stored independently of the table. This eliminates the need to send a query to all nodes (scatter/gather). In the example above, Node #1 contains rows 2 and 4 of the table, and also contains indexes for rows 32 and 35 and rows April and March. The table and the indexes are independently sliced, distributed, and replicated across the nodes.The query engine uses the distributed indexes to determine where to find the data. It looks up only the index partitions needed and then sends queries only to the locations where the needed data reside. Queries are all distributed. They’re done concurrently and in parallel. Where they go depends entirely on the data and what is needed to resolve the query.All slices are replicated at least twice. For every slice, there are replicas residing on other nodes. By default, there will be three copies of that data – the slice and two replicas. Each copy will be on a different node, and if you were running in multiple availability zones, those copies would also be sitting in different availability zones.Read and write handlingLet’s take another example. In Figure 3, we have five instances of MariaDB Enterprise Server with Xpand (nodes). There’s a table to store customer profiles. The slice with Shane’s profile is on Node #1 with copies on Node #3 and Node #5. Queries can come in on any node and will be processed differently depending on if they are reads or writes.Figure 3. Writes are processed simultaneously to all copies in a distributed transaction.Writes are made to all copies synchronously inside a distributed transaction. Any time I update my “Shane” profile because I changed my email or I changed my address, those writes go to all copies at the same time within a transaction. This is what provides strong consistency.In Figure 3, the UPDATE statement went to Node #2. There is nothing on Node #2 regarding my profile but Node #2 knows where my profile is and sends updates to Node #1, Node #3, and Node #5, then commits that transaction and returns back to the application.Reads are handled differently. In the diagram, the slice with my profile on it is on Node #1 with copies on Node #3 and Node #5. This makes Node #1 the . Every slice has a ranking replica, which could be said to be the node that “owns” the data. By default, no matter which node a read comes in on, it always goes to the ranking replica, so every SELECT that resolves to me will go to Node #1.Providing elasticityDistributed databases like Xpand are continuously changing and evolving depending on the data in the application. The rebalancer process is responsible for adapting the data distribution to current needs and maintaining the optimal distribution of slices across nodes. There are three general scenarios that call for redistribution: adding nodes, removing nodes, and preventing uneven workloads or “hot spots.”For example, say we are running with three nodes but find traffic is increasing and we need to scale – we add a fourth node to handle the traffic. Node #4 is empty when we add it as shown in Figure 4. The rebalancer automatically moves slices and replicas to make use of Node #4, as shown in Figure 5.Figure 4. Node 4 has been added to handle increased traffic. Nodes are empty when they are added to the Xpand cluster.Figure 5. The Xpand rebalancer redistributes slices and replicas to take advantage of the increased capacity.If Node #4 should fail, the rebalancer automatically goes to work again; this time recreating slices from their replicas. No data is lost. Replicas are also recreated to replace those that were residing on Node #4, so all slices again have replicas on other nodes to ensure high availability.Figure 6. If a node fails, the Xpand rebalancer recreates the slices and the replicas that resided on the failed node from the replica data on the other nodes.Balancing the workloadIn addition to scale out and high availability, the rebalancer mitigates unequal workload distribution – either hot spots or underutilization. Even when data is randomly distributed with a perfect hash algorithm, hot spots can occur. For example, it could happen just by chance that the 10 products on sale this month happen to be sitting on Node #1. The data is evenly distributed but the workload is not (Figure 7). In this type of scenario, the rebalancer will redistribute slices to balance resource utilization (Figure 8).Figure 7. Xpand has evenly distributed the data but the workload is uneven. Node 1 has a significantly higher workload than the other three nodes.Figure 8. Xpand’s rebalancer redistributes data slices to balance the workload across nodes.Scalability, speed, availability, balanceInformation and processing needs will continue to grow. That’s a given. MariaDB Xpand provides a consistent, ACID-compliant scaling solution for enterprises with requirements that can’t be met with other alternatives like replication and sharding.Distributed SQL provides scalability, and MariaDB Xpand provides the flexibility to choose how much scalability you need. Distribute one table or multiple tables or even your whole database, the choice is yours. Operationally, capacity is easily adjusted to meet changing workload demands at any given time. You never have to be over-provisioned.Xpand also transparently protects against uneven resource utilization, dynamically redistributing data to balance the workload across nodes and prevent hot spots. For developers, there’s no need to worry about scalability and performance. Xpand is elastic. Xpand also provides redundancy and high availability. With data sliced, replicated, and distributed across nodes, data is protected and redundancy is maintained in the event of hardware failure.Also on InfoWorld: Beyond NoSQL: The case for distributed SQLAnd, with MariaDB’s architecture, your distributed tables will play nicely – including cross-engine JOINs – with your other MariaDB tables. Create the database solution you need by mixing and matching replicated, distributed, or columnar tables all on a single database on MariaDB Platform.MariaDB Corporation—newtechforum@infoworld.com", "pub_date": "2020-09-23"},
{"title": "Does Snowflake mean the end of open source?", "overview": "The cloud-based enterprise data platform may mark the end of a decades-long run in the dominance of open source infrastructure ", "image_url": "https://images.techhive.com/images/article/2017/03/sign-1749093_1920-100714819-large.jpg", "url": "https://www.infoworld.com/article/3575855/does-snowflake-mean-the-end-of-open-source.html", "body": "The Snowflake IPO was a big deal, and not merely because of the company’s enormous valuation.In 2013 Cloudera co-founder Mike Olson confidently (and accurately) declared “a stunning and irreversible trend in enterprise infrastructure.” That trend? “No dominant platform-level software infrastructure has emerged in the last 10 years in closed-source, proprietary form.” Snowflake, a cloud-based enterprise data platform, may spell the end of that run. Sure, we had Splunk, but Splunk squeaked through the hypothesis police before open source had found its feet, as Lightspeed partner Gaurav Gupta told me. MySQL, Apache Hadoop, MongoDB, Apache Spark... all of them (at least initially) open source.But now... Snowflake. Is Snowflake a snowflake? Or is the era of open source infrastructure coming to a close?Also on InfoWorld: Cloud tech certifications count more than degrees nowClosing up shop?In part the answer to that question depends on just how fiercely you’re prepared to defend the underlying assumption. After all, it’s simply not the case that all “dominant platform-level software infrastructure” is open source. This isn’t really to dispute Olson’s central thesis, because it’s absolutely true that the bulk of enterprise infrastructure has trended toward open source over the past 10 to 20 years.As Gordon Haff puts it, “You can certainly construct a narrative for the infrastructure being heavily driven by open source: Most NoSQL, Hadoop, Kafka, Spark, Ceph, Jupyter, etc. But a lot in the space isn’t as well: lots of cloud services, Tableau, Splunk, etc.” And Snowflake, of course.Though you’d never guess it from the energetic proselytizing of yesteryear, developers have never been overly religious about open source. The reason for that “stunning” trend is simply that open source made it easier for developers to get their jobs done thanks to high-quality, easily accessible open source data infrastructure. There are, of course, other benefits, such as the communities that often accompany open source projects, coupled with a desire to have more granular control of one’s software stack. But ultimately open source has won because it enables developers to “get ---- done.”Which is why, for example, you’ll find developers happy to use open source software like Apache Airflow to load data into their proprietary Snowflake data platform. It’s not cognitive dissonance. It’s pragmatism.The shift to managed servicesSpeaking of such pragmatism, Tom Barber suggests that the shift to managed cloud services somewhat negates “people’s interest in open source... because with SaaS you’re not paying for licenses but for a service, which changes the thinking somewhat.” After all, he continues, “Open source meant you didn’t pay for licenses but you still had to pay someone internal or external to install it, tune it, run it…. Most people can apt/yum install MySQL but tuning it requires in-depth knowledge.”Or let’s express that another way, as Redmonk analyst James Governor does: “Cloud is a better distribution and packaging mechanism than open source ever was…. Convenience is the killer app. Managed services win.” Or, as Olson himself suggested to me,I still believe that open source software provides strategic advantage. But “elimination of friction” isn’t the differentiator it seemed a decade ago. Smart cloud folks learned that lesson; proprietary infra in the cloud is super easy to acquire and use. That’s not to say open source is irrelevant. Far from it. “Open source is not a business model but is a great way to build software, build trust, and foster community,” Governor continues.That “great way to build software” also applies to SaaS vendors like Snowflake. While services like Snowflake might not be open source, they’re actively using open source under the hood, as Gordon Haff suggests. For example, Snowflake relies on open source FoundationDB as “a key part of our architecture [because it] has allowed us to build some truly amazing and differentiating features.”A Whitesource analysis in 2019 found that 99 percent of software includes open source. Snowflake, in this respect, is no snowflake.Open source, in sum, still matters. A lot. Open source under the hoodBut for would-be buyers of services like Snowflake, open source might not be the primary attraction. As Ken Horn posits, data, not source code, needs to be the “first-class citizen” for something like Snowflake. And “once on cloud, the whole open source software thing is a bit :shrug:.”It’s not “shrug” for Snowflake and other vendors who may choose to deliver data warehousing and other such services, because open source affords them the chance to build on a rich ecosystem of open source foundational building blocks. But for the would-be buyers, they just need “to get ---- done,” and this may mean they don’t want to perform the spade work sometimes associated with open source.So is Olson’s 2013 declaration wrong? No, but perhaps we can rephrase it: No dominant platform-level software infrastructure has emerged in the last  20 years in closed-source, proprietary form .”Linux is still the standardDo developers really care about open source?Open source has a people problemThe community hat rules the company hat in open sourceWhat is Google’s Open Usage Commons — and why?What does an open source maintainer do after burnout?Do we need so many databases?How GraphQL turned web development on its headHow Redis scratched an itch — and changed databases foreverWill the solo open source developer survive the pandemic?Open source projects take all kindsThe most important part of an open source projectGatsby JS stands on the shoulders of thousandsRemember when open source was fun?Open source made the cloud in its imageZeek and Jitsi: 2 open source projects we need nowWTH? OSS knows how to WFH IRLThe secret to Kubernetes’ successOpen source companies are thriving in the cloudOpen source should learn from Linux, not MySQLMaking open source JavaScript payCustomer-driven open source is the future of softwareMagical Magento figures out how to make and takeThe real number of open source developersWhy the Rust language is on the riseShould open source licenses fight evil?Should open source software advertise?Why open source has never been stronger", "pub_date": "2020-09-23"},
{"title": "Google unveils TensorFlow tool for making mobile-ready models", "overview": "TensorFlow Lite Model Maker shrinks TensorFlow models to more efficiently serve predictions on mobile devices", "image_url": "https://images.idgesg.net/images/article/2019/11/ai_artificial_intelligence_ml_machine_learning_vector_by_kohb_gettyimages_1146634284-100817775-large.jpg", "url": "https://www.infoworld.com/article/3538913/google-unveils-tensorflow-tool-for-making-mobile-ready-models.html", "body": "Google has announced TensorFlow Lite Model Maker, a tool for converting an existing TensorFlow model to the TensorFlow Lite format used to serve predictions on lightweight hardware such as mobile devices.TensorFlow models can be quite large, and serving predictions remotely from beefy hardware capable of handling them isn’t always possible. Google created the TensorFlow Lite model format to make it more efficient to serve predictions locally, but creating a TensorFlow Lite version of a model previously required some work.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesIn a blog post, Google described how TensorFlow Lite Model Maker adapts existing TensorFlow models to the Lite format with only a few lines of code. The adaptation process uses one of a small number of task types to evaluate the model and generate a Lite version. The downside is that only a couple of task types are available for use right now — i.e., image and text classification — so models for other tasks (e.g., machine vision) aren’t yet supported.Other TensorFlow Lite tools announced in the same post include a tool to automatically generate platform-specific wrapper code to work with a given model. Because hand-coding wrappers for models can be error-prone, the tool automatically generates the wrapper from metadata in the model autogenerated by Model Maker. The tool is currently available in a pre-release beta version, and supports only Android right now, with plans to eventually integrate it into Android Studio.", "pub_date": "2020-04-17"},
{"title": "Azure Databricks previews parallelized Photon query engine", "overview": "Microsoft and Databricks say the vectorized query engine written in C++ accelerates Apache Spark workloads by up to 20x", "image_url": "https://images.idgesg.net/images/article/2020/01/spark-on-the-globe-shutterstock_7869750551-100827145-large.jpg", "url": "https://www.infoworld.com/article/3583657/azure-databricks-previews-parallelized-photon-query-engine.html", "body": "Microsoft has unveiled a preview of a C++-based vectorized query engine for the Azure Databricks cloud analytics and AI service based on Apache Spark. Azure Databricks, which is delivered in partnership with Databricks, introduced the Photon-powered Delta Engine September 22.Written in C++ and compatible with Spark APIs, Photon is a vectorized query engine that leverages modern CPU architecture and the Delta Lake open source transactional storage layer to enhance Apache Spark 3.0 performance by as much as 20x. Microsoft said that as organizations embrace data-driven decision-making, it is now imperative for them to have a platform that can quickly analyze massive amounts and types of data.Also on InfoWorld: How to choose a cloud machine learning platformPhoton offers greater parallelism of CPU processing at the data and instruction levels. Other components in Delta Engine include an improved query optimizer and a caching layer. The combination of these technologies boosts big data use cases including data engineering, machine learning, data science, and data analytics.Azure Databricks is intended to allow users to quickly set up optimized Apache Spark environments. It offers native integration with the Azure Active Directory and other Azure cloud services such as Azure Synapse Analytics and Azure Machine Learning, with customers able to build end-to-end data warehouses, machine learning, and real-time analytics solutions. Users can request access to the Photon Preview by filling out a questionnaire.", "pub_date": "2020-09-28"},
{"title": "Checking AI bias is a job for the humans", "overview": "By pre-processing or post-processing data, or even setting datasets to expire, humans can step in to correct machine learning models", "image_url": "https://images.idgesg.net/images/article/2018/02/artificial_intelligence_ai_virtual_assistant_automation_thinkstock_643956998-100749919-large.jpg", "url": "https://www.infoworld.com/article/3537968/checking-ai-bias-is-a-job-for-the-humans.html", "body": "One of the primary problems with artificial intelligence (AI) is the “artificial” part. The other is the “intelligence.” While we like to pretend that we’re setting robotic intelligences free from our human biases and other shortcomings, in reality we often transfer our failings into the AI, one dataset at a time.Hannah Davis, a data scientist, calls this out, arguing that “a dataset is a worldview,” filled with subjective meanings. But rather than leave our AI hopes moribund, she also offers some ways we might improve the data that informs our AI.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesAI has always been about peopleIt has become de rigueur to posture how very “data driven” we are, and nowhere more so than in AI, which is completely dependent on data to be of use. One of the wonders of machine learning algorithms, for example, is how fast they can sift through mountains of data to uncover patterns and respond accordingly. Such models, however, must be trained, which is why data scientists tend to congregate around established, high-quality datasets.Unfortunately, those datasets aren’t neutral, as Davis points out:[A] dataset is a worldview. It encompasses the worldview of the people who scrape and collect the data, whether they’re researchers, artists, or companies. It encompasses the worldview of the labelers, whether they labeled the data manually, unknowingly, or through a third-party service like Mechanical Turk, which comes with its own demographic biases. It encompasses the worldview of the inherent taxonomies created by the organizers, which in many cases are corporations whose motives are directly incompatible with a high quality of life.See the problem? Machine learning models are only as smart as the datasets that feed them, and those datasets are limited by the people shaping them. This could lead, as one Guardian editorial laments, to machines making our same mistakes, just more quickly: “The promise of AI is that it will imbue machines with the ability to spot patterns from data, and make decisions faster and better than humans do. What happens if they make worse decisions faster?”Complicating matters further, our own errors and biases are, in turn, shaped by machine learning models. As Manjunath Bhat has written, “People consume facts in the form of data. However, data can be mutated, transformed, and altered—all in the name of making it easy to consume. We have no option but to live within the confines of a highly contextualized view of the world.” We’re not seeing data clearly, in other words. Our biases shape the models we feed into machine learning models that, in turn, shape the data available for us to consume and interpret.Time to abandon hope, all we who enter here?Data problems are people problemsNot necessarily. As Davis goes on to suggest, one key thing we can do is to set our datasets to expire:Machine learning datasets are treated as objective. They’re treated as ground truth by both the machine learning algorithms and the creators. And datasets are hard, time-consuming, and expensive to make, so once a dataset is created, it is often in use for a long time. But there is no reason to be held to the past’s values when we as a society are moving forward; similarly, there is no reason to hold future society to our current conditions. Our datasets can and should have expiration dates.At any given point in time, the people, places, or things that are top of mind will tend to find their way into our datasets. (Davis uses the example of ImageNet, created in 2009, which returns flip phones when “cell phone” is searched.) By setting datasets to expire, we force our models to keep up with society.Also on InfoWorld: How dataops improves data, analytics, and machine learningThis calls out another option, one suggested by McKinsey research, which is to re-introduce people into AI. Whether through pre-processing of data or post-processing of data, humans can step in to correct machine learning models. The math involved in the model may be impeccable, but adding humans (yes, with biases) can help to take account of the model’s outcome and prevent biases from operating unchecked.Unless we’re careful, Davis warns, “It is easy to accidentally cause harm through something as seemingly simple as collecting and labeling data.” But with extra care, we can gain much of the benefits of AI while minimizing the potential biases and other shortcomings that the machines inherit from us humans.", "pub_date": "2020-04-21"},
{"title": "The 2020 Enterprise Architecture Awards", "overview": "The 2020 Forrester and InfoWorld EA Awards contest winners focus on business architecture, digital transformation, and governance", "image_url": "https://images.idgesg.net/images/article/2020/09/ifw_forrester-enterprise-architecture-awards-logo_2020_large-100858214-large.jpg", "url": "https://www.infoworld.com/article/3575829/the-2020-enterprise-architecture-awards.html", "body": "In the Forrester/InfoWorld Enterprise Architecture Awards competition, we look for the most dramatic stories of EA’s strategic leadership and concrete business impact. The winners of the 2020 Forrester/InfoWorld Enterprise Architecture Awards show the value of a close relationship with the business, a solid vision for enabling digital transformation, and effective governance practices — not to mention the need for a high-priority response to a global pandemic!In alphabetical order, the winners this year are the EA teams of:CSL BehringThomas Jefferson University and Jefferson HealthLexmarkSun LifeVerizon CommunicationsCongrats to all the winners — each has a compelling story of EA best practices. You can read those stories below.", "pub_date": "2020-09-21"},
{"title": "Some observations on cloud computing observability", "overview": "Observability is an approach to watching applications, data, and infrastructure in a way that provides more holistic value ", "image_url": "https://images.techhive.com/images/article/2014/04/binoculars-139709989-100264631-large.jpg", "url": "https://www.infoworld.com/article/3573801/some-observations-on-cloud-computing-observability.html", "body": "The term and the concept of observability aren't new at all. Observability first appeared in the world of engineering and control theory. Although the definition depends on whom you’re asking, some common patterns and understandings are emerging. Mostly it’s a new buzzword by the AIops providers and some thought leaders in the cloudops space (meaning me). At its essence, observability is the measure of how well internal system states can be inferred from knowledge of all external data and states. It’s a bit different than monitoring which is something you do (a verb); observability is an attribute of a system (a noun).How data analysis, AI, and IoT will shape the post-pandemic ‘new normal’If that’s a bit confusing, here's a more pragmatic definition: the ability to leverage the concept of observability and the tools that support that concept. Most monitoring and operations tools (such as AIops tools) claim some role in observability. However in some respects, to me it's the same old bourbon (monitoring) in a much fancier bottle (observability). I look at it with a bit of well-placed skepticism.On the bright side, more tools are being purpose-built for the notion of observability, including most of the emerging AIops and cloudops toolsets, as well as other monitoring platforms that deal with complex operational data, such as security, database, and network monitoring. The purpose of observability is leveraging all of these data points holistically, which most enterprises don't do these days.         Observability is important to managing and monitoring modern systems, considering we deal with both modern applications and the pace at which these applications are delivered. The widely held practice of bolting on monitoring and management after applications are deployed does not scale very well.   Part of the notion of observability is the need for up-to-date instrumentation to better understand the properties, performance, and behaviors of an application. We need tools to deal with underlying complexity and distribution. Of all the barriers to the cloud moving forward, what scares me most is that we’ll build too much complexity with no clear way to manage it at scale.    Basically, monitoring has these attributes:Passive consumption of information from systems under management to determine state and status.Dashboards provide information about the state of systems and rely on humans to respond.Tools are purpose-built for static systems and platforms that don’t change much over time.Observability takes a different approach: Holistically considers all data from systems under management to determine a proactive and automated response.Asks questions based on a continuous understanding or hypotheses, continuously improving how systems are observed, tuned, and fixed using automation.Purpose-built for dynamic systems and platforms with widely changing complexity. The tool adapts to the systems under management, not the other way around.To be honest I take these new concepts as something to consider, not to follow religiously. The history of IT is full of ideas that seemed important for a time and faded away, never to be heard from again. However, considering the importance of operations and tools than can allow us to operate under increasingly complex environments, this concept could go wide.  ", "pub_date": "2020-09-08"},
{"title": "As edge computing evolves, the cloud's role changes", "overview": "Edge computing is becoming more mainstream and more complex. Keep it under control by leveraging cloud-based technologies", "image_url": "https://images.techhive.com/images/article/2014/03/166160844-100249187-large.jpg", "url": "https://www.infoworld.com/article/3574850/as-edge-computing-evolves-the-clouds-role-changes.html", "body": "The notion of edge computing typically conjures an image of a device in a factory someplace, providing rudimentary computing and data collection to support a piece of manufacturing equipment. Perhaps it keeps the factory temperature and humidity optimized for the manufacturing process.Those who deal with edge computing these days understand that what was considered “edge” just a few years ago has morphed into something a bit more involved. Here are the emerging edge computing architecture patterns that I’m seeing:InfoWorld’s 2020 Technology of the Year Award winners: The best software development, cloud computing, data analytics, and machine learning products of the year No longer are edge devices connected directly to some centralized system, such as one residing in the cloud. They connect to other edge devices that may connect to larger edge devices that ultimately connect to a centralized system or cloud.This means that we’re leveraging very small and underpowered devices at the true edge, such as a thermostat on the wall. That device connects to a local server in the physical building, which is also considered an edge device, in a one-to-many configuration (one edge server to many thermostats). Then another edge server aggregates the data of many buildings and finally transmits to the public cloud where the edge-based data is stored, analyzed, and results returned down the hierarchy.Although this seems like we’re killing ourselves with complexity by adding layers to the edge architecture, the motivations are pragmatic. There is no need to send all of the data to the centralized storage and processing system in the cloud when it can be processed better and cheaper by edge server that are closer to the devices, especially if the collection devices (the thermostat) are not powerful enough to do any real processing.The advantages here are better performance and resiliency. The data did not have to be transmitted out of the building. The architecture is much more agile; you can repurpose each edge device without forcing changes to the centralized data storage and processing systems in the cloud.This edge architecture, simply put, allows data to flow from edge device to edge device, as well as to the back-end system in the cloud using autonomous AI-based agents charged with relocating data based on predefined rules. This is typically for storage and processing. Data from an edge device may be moved to another edge device or to a centralized server based on what needs to be done to the data.This has a clear advantage. It avoids saturating edge device that typically don’t have a lot of storage. Many edge storage devices use only one to three percent of their storage; others hover around 90 percent, which is scary.If the data does not need to be transmitted to the centralized processing systems (typically in a public cloud) and can be stored locally more efficiently, then edge architects will find autonomous edge data movement compelling, compared to ongoing upgrades to all edge devices and edge servers. The role of the cloud within these emerging architectures is to provide command and control, not just to be place for processing. The clear pattern has been to avoid sending data to the back end if it can be avoided. But there has to be a centralized “big brain” for all of this to work, and automation should exist in a central and configurable space, putting volatility into a domain.Although these architectures are rare today, I’m seeing more and more of them as enterprises attempt to move to edge computing in innovative ways adapted for their specific use and geographical distribution. Edge is likely to become even more complex, and the patterns will expand. Keeping the data away from the cloud seems counterintuitive, but we’re leveraging the cloud to be the master of all edge devices that will end up storing even more data. ", "pub_date": "2020-09-11"},
{"title": "What TikTok teaches enterprises about tech entanglement", "overview": "Our infrastructure, apps, and data must be designed in a way that makes them portable and flexible enough to be separated cleanly if needed", "image_url": "https://images.techhive.com/images/article/2016/11/double_coin_knot-rotated-100695226-large.jpeg", "url": "https://www.infoworld.com/article/3574958/what-tiktok-teaches-enterprises-about-tech-entanglement.html", "body": "The flurry of interest in acquiring TikTok shows the value of a popular social network with vast troves of consumer data. The deal also holds an important lesson about technology use that other businesses can learn from: the need to design applications in a flexible, portable way. One challenge for any company that buys TikTok will be how to carve up its technical infrastructure outside of China without destroying its value in the process. The impetus for the deal is that the US doesn’t want China “spying” on its citizens through the app. That will require untangling the data and application code on the back end. Also on InfoWorld: How cloud-native technologies defeat cloud lock-inTikTok owner ByteDance also runs a similar app called Douyin, available only in China, and the apps reportedly share technical resources including user data, server code, and the algorithms that determine which content and ads a user sees. ByteDance needs to separate those elements in the narrow timeframe it’s been given, and its ability to do that effectively will be a factor in how much a suitor is willing to pay — and perhaps whether a deal happens at all.There are lessons here for other businesses. It’s not uncommon to sell or spin off a product or company division for strategic reasons, and how those assets are architected can impact their value and the ease with which they can be sold. The days of building giant monolithic applications are long gone. Most new apps are built in the cloud using smaller microservices that can be scaled up and down as needed and updated independently. That’s good for introducing new features quickly but it can be hard to disentangle apps if they’re not designed carefully using open standards and technologies.There are three main areas to consider: the cloud infrastructure, the application code, and the data. Here’s how to design each in a way that makes them portable and flexible enough that they can be separated cleanly if needed. Keep it cloud-neutralMost businesses have a preferred cloud provider, and any app being acquired may need to be moved onto a different service. In TikTok’s case, ByteDance recently signed an $800 million deal to run the app in Google’s cloud. Certainly, Microsoft or Oracle would want to move the app to their own cloud as soon as possible to collect the revenue. One way to get locked into a cloud is using high-level managed services, like Amazon Redshift or Google Cloud Big Table. These services are alluringly simple to deploy but notorious for locking customers into proprietary formats. Even the managed versions of popular open source products like MongoDB and Elasticsearch can have subtle differences depending on the cloud provider, making it hard to port apps when you need to.Think of the public cloud as an infrastructure service, not the basis for architectural decisions. If you’re building the next TikTok, select services that aren’t specific to one public cloud and don’t limit your options by going all in on one provider.Make apps modular and portableApplications should be developed as microservices and deployed in containers based on a widely used standard like Docker. Containers package everything an app needs to run into a self-contained bundle, including the application code and any dependencies or configuration files it requires. That makes it easier to move or copy an app onto a different cloud or hardware platform.Because containerized apps are modular by nature, each application service can be dealt with individually as needed. For example, maybe the user interface code is copied over while the security services are rewritten. Or a business could sell off specific functionality and retain other parts of an app. The algorithms themselves can be updated for each country or region, depending on local market needs.Keep the data independentByteDance has a trove of data about its users around the world. Given the US’s privacy and security concerns, it will need to unbundle the data associated with its international users and ensure ByteDance has no way of accessing that information after the deal is complete. Managing data in the event of a sale starts with knowing where all your data lives, including any backups. Data should be architected intentionally and segmented by country or zone of operation. This is a good practice in any event, for compliance with regulations like GDPR or nation-specific data sovereignty laws, but may also be essential in the event of a sale.Also on InfoWorld: Kubernetes meets the real world: 3 success storiesIn addition, data shouldn’t be hard-wired to the underlying storage system. Cloud native technologies like Kubernetes and containers allow data to reside anywhere and connect to an app as needed. In the event of a sale, data can be moved to a different cloud, data center, or region without having to rewrite the application or rip out the storage hardware. In short, data should be kept highly portable and there are standards-based cloud technologies that make that possible.It’s unclear who will acquire TikTok but the deal could be a lucrative one. Facebook paid $1 billion for Instagram and the service generated $20 billion in ad revenue last year. TikTok has about 80 million users in the US and it is still growing fast. But these deals can be complex when data and applications have to be disentangled on the back end. How you architect your infrastructure can have a big impact when it comes to a sale.So let this deal serve as a lesson. Design for maximum flexibility, because you never know what the future holds.Portworxnewtechforum@infoworld.com", "pub_date": "2020-09-11"},
{"title": "Visual Studio Codespaces is moving to GitHub", "overview": "Microsoft is moving Visual Studio Codespaces into GitHub Codespaces to simplify the developer experience", "image_url": "https://images.idgesg.net/images/article/2020/07/empty_office_with_moving_box_closing_business_unit_closure_by_yvan_dube_gettyimages-473359764_2400x1600_hero-100851830-large.jpg", "url": "https://www.infoworld.com/article/3575092/visual-studio-codespaces-is-moving-to-github.html", "body": "Microsoft’s Visual Studio Codespaces, which provide cloud-hosted development environments on Microsoft Azure, will be incorporated into GitHub Codespaces, which provide hosted Visual Studio Code environments on GitHub. The current Azure-based offering will be retired in February 2021.Microsoft said the service is moving because, during a preview stage, the company found that transitioning from a repository to a codespace was the most-criticized part of the workflow. The vast majority of users preferred an integrated, native, one-click experience. GitHub being the home of 50 million developers, Microsoft decided it made sense to partner with GitHub to address the issue. Also on InfoWorld: Visual Studio Code vs. Visual Studio: How to chooseGitHub Codespaces is still in a limited public beta; developers can sign up here. When developers connect to a GitHub Codespace through a portal or the Visual Studio Code editor, they will be prompted to submit add a GitHub account to the beta. Visual Studio Codespaces users will receive an email requesting their preferred GitHub account.While GitHub Codespaces provides an optimized experience for GitHub repos, developers still can use Git repos hosted elsewhere, such as on Azure or Bitbucket, by taking a few additional configuration steps. Also, the private preview of Windows-based Codespaces support in Visual Studio 2019 will move to GitHub.Microsoft has provided an FAQ about the consolidation. The compay set the following timeline for plan:September 4, 2020: Current users begin moving to the GitHub private beta.November 20, 2020: Creation of new Visual Studio Codespaces will no longer be allowed, but existing Visual Studio Codespaces still can be used.February 17, 2021: The Visual Studio Codespaces portal will be retired.", "pub_date": "2020-09-11"},
{"title": "Kubernetes and cloud portability — it’s complicated", "overview": "Kubernetes doesn’t offer the magical application portability you might expect, but something better", "image_url": "https://images.idgesg.net/images/article/2020/04/gettyimages-1135670850-100840139-large.jpg", "url": "https://www.infoworld.com/article/3574853/kubernetes-and-cloud-portability-its-complicated.html", "body": "I’m so sorry. You were told that Kubernetes was your key to the golden age of multicloud nirvana. You believed that Kubernetes would give you portability to move applications seamlessly between clouds, whether running in your data center or on a public cloud. It’s not really your fault. Vendors have been promising all sorts of magical things with regard to portability and Kubernetes.Unfortunately, Gartner just weighed in to put the kibosh on the Kubernetes-for-portability panacea. As analyst Marco Meinardi writes, when asked whether companies should embrace “Kubernetes to make their applications portable... the answer is: no.”Also on InfoWorld: Tim O’Reilly: The golden age of the programmer is overIt’s not that Kubernetes can’t be used to make applications portable. It can. But the nature of that portability differs from how it’s commonly conceived. So how  companies think about Kubernetes-driven portability?Can’t get there from hereFirst, let me suggest that the whole idea of multicloud might be wrongheaded. Now, I could be biased. After all, I do work for a cloud provider (AWS). However, I like to think that my bias against multicloud, magical-application-portability thinking stems from it being a really bad idea. As I wrote long before I joined AWS, “Vendors are cleaning up by selling multicloud snake oil while customers keep getting stuck with lowest common denominator cloud strategies and sky-high expenses.” Of course I’m not the only one with this view. Corey Quinn of the Duckbill Group, who has made a living by snarkily trashing bad IT practices, believes multicloud is “the worst practice” for a host of reasons. As Quinn has written:[T]he idea of building workloads that can seamlessly run across any cloud provider or your own data centers with equal ease… is compelling and something I would very much enjoy.However, it’s about as practical as saying “just write bug-free code” to your developers—or actually trying to find the spherical cow your physics models dictate should exist. It’s a lot harder than it looks.”Which brings us to Gartner.There are many, many great reasons to use Kubernetes, Meinardi writes. Portability just doesn’t happen to be one of them:Inquiries show that [the] likelihood [of moving applications between cloud providers] is actually very low. Once deployed in a provider, applications tend to stay there. This is due to data lakes being hard — and expensive — to port and, therefore, end up acting as centers of gravity.Why hard? Analyst Kurt Marko provides some clues, saying that “transparent, incognizant workload movement is only possible on vanilla container platforms for the simplest of applications.” He then lists a range of hurdles that get in the way of Kubernetes-based portability, including dependence on native cloud features (managed databases, serverless functions, etc.), the difficulty of federating user identity and security policies across platforms, and more.So if multicloud isn’t necessarily the best of ideas, following Quinn, and Kubernetes isn’t going to get us there, even if it were a great idea, following Marko, what kind of Kubernetes-fueled portability  a good idea?Portability is really about peopleIt’s not that you  architect an application in such a way as to make it portable across clouds. You can, as Peter Benjamin notes, and that portability makes more sense if you think of the timeframes in terms of years rather than days or weeks, as Paul Nash posits. (Miles Ward concurs, arguing, “Our customers [want to] evaluate every few years, not every week; they just want it easy if the landscape changes.”) Rather, it’s just that this isn’t common (for the reasons identified above), nor is it as important as other aspects of portability.Perhaps the most important way to think about portability comes from Weaveworks CEO Alexis Richardson:The point is “skills portability” due to using a standard operating model and tool chain. Large organisations want developers to use standard ways of working because this reduces training costs, and removes obstacles to staff moving from project to project. It also makes it easier and cheaper to apply policy if your “platform” (or platforms) are based on the same core set of cloud native tools.This view is echoed by Knative co-founder Matt Moore (“the real win is the portability of concepts and tools”). Or, as EJ Campbell nicely summarizes, “It’s nice to have transferable skills.”Also on InfoWorld: Cloud tech certifications count more than degrees nowImportantly, those skills help both employers and employees. Kubernetes may not make it simple to move applications between environments (on-premises or cloud), but it does level-set the skills and concepts that a developer can use in each of these environments. Or, as Rancher Labs CTO and co-founder Darren Shepherd puts it, “The portability is that the same approach can be used regardless of cloud, data center, edge, laptop, stateless, stateful, AI/ML, etc.”In this way, Kubernetes is incredibly important, offering “people portability” that is good for individuals and corporations. It’s not magical application portability. It’s something much better.", "pub_date": "2020-09-14"},
{"title": "Amazon, Google, and Microsoft take their clouds to the edge", "overview": "As enterprises ramp up edge computing deployments, the big three clouds are ponying up a surprising array of edge options for a broad range of needs.", "image_url": "https://images.idgesg.net/images/article/2020/09/spot_edge_computing_3x2_2400x1600_06_ifw_a_multiple_cloud_instances_service_options_personal_clouds_by_gremlin_gettyimages-940308630-100857218-large.jpg", "url": "https://www.infoworld.com/article/3575071/amazon-google-and-microsoft-take-their-clouds-to-the-edge.html", "body": "It might surprise you to learn that the big three public clouds – AWS, Google Could Platform, and Microsoft Azure – are all starting to provide edge computing capabilities. It’s puzzling, because the phrase “edge computing” implies a mini datacenter, typically connected to IoT devices and deployed at an enterprise network’s edge rather than in the cloud.The big three clouds have only partial control over such key edge attributes as location, network, and infrastructure. Can they truly provide edge computing capabilities?Also on InfoWorld: How to choose a cloud machine learning platformThe answer is yes, although the public cloud providers are developing their edge computing services via strategic partnerships and with some early-stage limitations.Edge Computing4 essential edge computing use cases (Network World)Edge computing's epic turf war (CIO)Securing the edge: 5 best practices (CSO)Edge computing and 5G give business apps a boost (Computerworld)Amazon, Google, and Microsoft take their clouds to the edge (InfoWorld)Cloud-based edge computing offerings are a clear sign that the boundaries between public cloud, private cloud, and edge computing are blurring. The unifying goal is to provide businesses and architects with a range of choices based on the type of workload and its performance, reliability, regulatory, and safety requirements.Unfortunately, a glut of new options always means new jargon and branding, so we’ll need to do a fair bit of demystifying as we sort through the big three cloud offerings for edge computing. Before jumping in, though, let’s start with a quick primer on some key edge computing architecture considerations.Understanding edge computing requirements and architecturesFirst and foremost, engineering teams must understand the edge computing requirements. Connecting a globally dispersed network of inexpensive sensors that generate a few terabytes of data daily has different computing requirements than servicing a dozen factory floors with an array of video sensors processing petabytes of data in real-time. The architecture must address the specific data processing, analytics, and workflows needed.Then, just as important, consider the regulatory, security, and safety requirements. Medical devices deployed at hospitals or controllers for autonomous vehicles both capture and process highly personal, life-critical information. Reliability and performance demands should dictate the location, network, security, and infrastructure requirements.Understanding these requirements helps architects determine where to locate edge computing infrastructure physically, what types of infrastructure are required, the minimal connectivity requirements, and other design considerations.But the unique benefit public cloud edge computing offers is the ability to extend underlying cloud architecture and services, particularly for customers already heavily invested in one public cloud or another. Do architects and developers want to leverage AWS, Azure, or Google Cloud services deployed to the edge? That’s what the public clouds are betting on – and they are also considering 5G-enabled mobile applications that require low-latency data and machine learning processing at telco endpoints.With these questions in mind, here’s an overview of what the three major public clouds provide.Extend to Azure Edge Zones with Azure StackAzure is betting that architects and developers want to focus on the application and less on the infrastructure. Azure has three options that enable a hybrid edge, where architects can leverage 5G networks and deploy data processing, machine learning models, streaming applications, and other real-time data-intensive applications optimally.Azure Edge Zones are managed deployments of the Azure stack that can be purchased through Microsoft and are currently available in New York, Los Angeles, and Miami.Microsoft partnered with AT&T to offer Azure End Zones with Carrier in a number of locations, including Atlanta, Dallas, and Los Angeles. This option is best for 5G-enabled mobile applications that require low-latency data processing or machine learning capabilities.Lastly, businesses can also deploy a private Azure Edge Zone. Microsoft has partnered with several data center providers that enable this capability.These options provide location choices and network flexibility, while Azure Stack Edge brings Azure computing and services to the edge. Azure Stack Edge is a 1U, 2x10 Core Intel Xeon, 128GB appliance that can be configured with containers or VMs and managed as a Kubernetes cluster of appliances. This model is optimized for machine learning and IoT applications.Microsoft also offers Azure Stack HCI, a hyperconverged infrastructure for modernizing data centers, and Azure Stack Hub for deploying cloud-native applications.Like other cloud services, Microsoft sells Azure Stack Edge by subscription, with costs calculated by the utility. Microsoft manages the device and offers 99.9 percent service level availability.Extending AWS services from 5G devices to large scale analyticsAWS has a similar set of offerings to distribute AWS services to edge data centers and telco networks.AWS is starting to support edge data centers with AWS Local Zones that are currently available only in Los Angeles.AWS Wavelength is designed for low-latency applications running on 5G devices, including connected vehicles, AR/VR applications, smart factories, and real-time gaming.AWS is partnering with Verizon to deliver AWS Wavelength, which is currently available in Boston and the San Francisco Bay area.AWS offers two flavors of edge infrastructure, starting with the AWS Snow line of appliances. AWS Snowcone is the smallest appliance with two vCPUs and 4GB that is primarily used for edge data storage and transfer. Memory-intensive data processing and machine learning applications deployed to the edge likely require AWS Snowball Edge that comes in storage and compute-optimized models with up to 52 vCPUs and 208GB. For the largest scale applications, AWS Outposts are 42U racks deployed to data centers for running different EC2 instance types, containers (Amazon ECS), Kubernetes (Amazon EKS), databases (Amazon RDS), data analytics (Amazon EMR), and other AWS services.Google lags as all three clouds vie for the edgeJust as Google occupies third place in the public cloud wars, it’s also trying to catch up with its edge offerings. Google’s most recent announcements include Anthos at the Edge, collaborations with AT&T on 5G connectivity, and the Google Mobile Edge Cloud. The offering is part of Anthos, a hybrid and multi-cloud application modernization platform enabling businesses to deploy applications on GCP and in the data center.The public cloud vendors all recognize that the next wave of innovation is coming from the intersection of IoT, 5G, and machine learning analytics deployed at the edge. They’re not going to let infrastructure and data center companies like Dell or HPE dominate this new market without a fight, so their answer is to bring their cloud platforms, containers, orchestration, and services to edge data centers and telco endpoints. And they’re not doing this alone: The public clouds are partnering with telcos, infrastructure providers, and major service providers to enable their offerings.But these are early days for public cloud edge computing solutions. The fact that the big three clouds are getting serious about edge computing only serves to underscore the promise of the edge frontier. Whether enterprises choose public cloud edge solutions or opt to build their own infrastructure, network, and computing platforms, few will want to be left out of the growing wave of edge innovation.", "pub_date": "2020-09-14"},
{"title": "Rethinking 'cloud bursting'", "overview": "Nothing is the same in the world of cloud technology year-to-year. Could cloud bursting make a comeback in new packaging?", "image_url": "https://images.techhive.com/images/article/2015/09/pin_popping_balloon-100618401-large.jpg", "url": "https://www.infoworld.com/article/3575078/rethinking-cloud-bursting.html", "body": "About two years ago I wrote a piece here on the concept of cloud bursting, where I pointed out a few realities:Private clouds are no longer a thing, considering the current state of private cloud systems compared to the features and functions of the larger hyperscalers.You need to maintain workloads on both private and public clouds for hybrid cloud bursting to work; in essence, using two different platforms.It’s clear the bursting hybrid clouds concept just adds too much complexity and cost for a technology stack (the cloud) to be widely adopted by companies that want to do the most with the least.Also on InfoWorld: Tiny clouds taking on AWS, Microsoft Azure, and Google CloudIn case you missed the fervor in the tech press a few years ago, cloud bursting is the concept of leveraging public clouds only when capacity of the on-premises cloud runs out. Somehow companies thought that they could invoke a public cloud-based part of the application and have access to the same data without latency. Mostly it did not work.I stand by that posting, but now I have a few additional things to say about the concept of cloud bursting in 2020 and 2021.First, a few on-premises solutions exist today that are close analogs to public clouds, because they are sold by the public cloud providers. The larger hyperscalers, including Google, Microsoft, and AWS, have hardware and software solutions that exist in traditional data centers. Simply put, these are a scaled-down version of their public cloud solutions packaged as appliances.Cloud bursting is possible with these solutions, considering that both the on-premises platform and the public cloud platform are purpose-built to work and play well together. The objective is to eventually move the on-premises workloads to the public clouds by using these on-premises solutions as an intermediate step. Second, edge computing is a thing now. The use of IoT devices connected to public clouds has always been around, but the formal use of edge-based systems that are both devices and legit servers is part of the cloud architecture zeitgeist. This means that edge computing has processing and data storage outside of the public cloud providers that are also purpose-built to work with specific public clouds. Moreover, the public cloud providers support edge directly now. Those deploying systems that leverage edge computing infrastructure don’t have to build things from scratch to use public clouds for back-end processing.Although there are more and more architectural patterns that look like cloud bursting, the notion is really about distributing processing and storage, which is not at all new. My purpose in pointing out what’s changed is really to point out that things do indeed change. This is why I love the cloud computing business.", "pub_date": "2020-09-15"},
{"title": "How BT is shifting its engineers into the fast lane", "overview": "BT’s head of software engineering excellence lays out the telco’s route to streamlining development, modernizing applications, automating deployments", "image_url": "https://images.idgesg.net/images/article/2020/09/untitled-design-21-100857934-large.jpg", "url": "https://www.infoworld.com/article/3575414/how-bt-is-shifting-its-engineers-into-the-fast-lane.html", "body": "The broad industry shift toward the cloud and containerized workloads can often ignore the reality for many established enterprises: What happens when the unstoppable force of cloud native progress meets the immovable object of legacy technology?This is the current challenge being faced by BT, the 170-year old British telecoms business, which is looking to adopt cloud native tooling and techniques in a safe and sustainable way across a large and diverse developer population.Only then will the business be able to move fast enough to keep up with rapidly changing customer expectations and competitor efforts in the era of over-the-top media proliferation and 5G connectivity.“We have a lot of legacy and code that served us in the past,” Rajesh Premchandran, head of software engineering excellence at BT told InfoWorld. “Our developers right now are constrained by an existing stack — that’s important to acknowledge — it’s not all greenfield, there’s a constraint of architecture and design.”Engineering leaders at BT are hoping that a common, modern set of cloud services, combined with a gradual shift toward containers and Kubernetes can reduce the amount of wasted effort its developers expend on a day-to-day basis.Slowly but surely BT is looking to consolidate and modernize its existing workloads. This is where containers and Kubernetes enter the equation, but Premchandran urges caution to anyone who sees the container orchestration tool as a silver bullet.“What really happens is you’ve got to tease out what is containerizable,” he explains. “Say you have a monolith and you’ve got 50 components, all intertwined with hard-coded dependencies and some legacy stack. First you ask, what do I containerize? Where is that unit that I can tease out without impacting the others? That is the biggest challenge that most firms which have been around as long as we have face.”[Also on InfoWorld: Kubernetes meets the real world: 3 success stories]To overcome this challenge, BT has established a platform team, dedicated to helping application teams identify these containerizable elements and find the best environment in which to host them, be it in the public cloud or on a platform-as-a-service (PaaS).This has led to a common set of considerations BT engineers take when looking to modernize their workloads, namely how loosely coupled it is and whether you can effectively isolate a service and containerize it, without impacting the existing stack.In terms of infrastructure options, there is some choice, but the idea is to simplify these decisions to gain some level of consistency.Of course all three major infrastructure-as-a-service (IaaS) options — Amazon Web Services, Microsoft Azure, and Google Cloud Platform — are on the table. Where possible the platform team will push developers to use a managed Kubernetes service or, better still, the Tanzu Application Service, a PaaS from the vendor VMware, and its own Tanzu Kubernetes Grid (TKG).“You have to handhold them, otherwise they will take the biggest unit they can handle, put that into a Docker container and then lo and behold you’ve got the same monster on better infrastructure — that doesn’t solve your business problem,” Premchandran says of his developers.That being said, when push comes to shove, the preference would always be for TKG, which promises a consistent managed Kubernetes layer across on-premises and public cloud environments.“You may want to swap out your cloud provider tomorrow, you might want to move your workloads from on prem to cloud, so you need a control plane for Kubernetes that makes the underlying IaaS irrelevant,” Premchandran explains.That doesn’t mean that a certain subset of developers at BT aren’t still looking to get their hands dirty with Kubernetes.“It’s like driving an automatic car, there are aficionados who like the feel of changing gears and the clutch. With TKG available it’s more important to ask what is your specific need, not your want, and if you’ve done it the hard way, here is the easy way and it’s your choice,” Premchandran says.“Kubernetes is hard... we won’t enjoy doing that at scale, so that is where we ask developers to focus on the application logic and let the hard things be automated for you,” he adds.That doesn’t mean that self-managed Kubernetes is off the table, but really it should be reserved for use cases where developers really need that fine-grained level of control. For everything else, let the vendor take care of it.“This is a constant tussle,” he admits, “where people want Kubernetes by default, but I think you’ve got to be a bit more prescriptive in the beginning to developers and then as you see them maturing, have them make those independent choices.”Once those principles and frameworks have been established across BT’s developer community, the next task is to scale it out via documentation and word of mouth buzz, both for in-house engineers and with external partners.The advantage of having some developers getting their hands dirty with Kubernetes does mean that a new area of expertise starts to spring up across your organization. This is never a bad thing, especially when you are looking to evangelize a new technology, where a few respected voices can go a long way to getting that buy-in.“If you look at how standards are democratized, you’ve got enterprise architecture that looks at the overall architecture standards and policies, then you have solution architects, who are a level down and are specific to a product, and finally we have distinguished engineers — we call them expert engineers — who are also key influencers for other developers, who look up to them,” Premchandran explains.For partners, BT embarked on a pre-pandemic roadshow, running events and sessions to lay out the new direction and get them up to speed with the latest documentation, including a new Software Engineering Handbook and internal wiki.“You can’t have a top-down diktat saying ‘march towards the cloud and let developers figure out their own learning part’,” he says. “We’ve invested heavily in certification programs, getting online e-learning platforms that allow career paths to be developed around the cloud and agile and DevOps and all of the associated skills.”Also on InfoWorld: Tim O’Reilly: The golden age of the programmer is overNow it is about scaling that knowledge and those skills across the organization. “We’ve learned the ropes, what happens when workloads are portable, now developers need to be full-stack engineers understanding hybrid cloud workloads and have the skills to train other people to modernize their own stacks,” he says.Once all of this has been achieved, Premchandran hopes that he is overseeing a happier developer community deploying at much higher velocity than was previously possible.“Infrastructure provisioning needs to be at a much faster clip to allow DevOps to happen,” he says, but it’s not just speed that he wants.“This is an interconnected problem set. You want to go fast because your customers need things fast, but you’ve got disconnected teams,” he says. Kubernetes and other cloud native approaches promise a rote to bringing all of these teams together onto a common methodology.Over the next couple of years Premchandran will focus on getting more applications onto a modern, cloud native, microservice-based architecture; implement DevSecOps practices for “completely zero touch deployments”; and, finally, increase the number of reusable software components across the group.For example, if someone decides to build a billing component, can this be used by anyone needing to bill customers within an application? In an ideal world, yes.The goal for Premchandran is a developer community that can move as fast as it needs to, without being encumbered by legacy or infrastructure concerns.“Agile is not just about going fast, it’s about having the choice to go fast. There’s a difference. Just because your car can hit 250 miles-per-hour, that doesn’t mean you’re at that speed all the time. But I want a car that can go that fast if need be,” he says. ", "pub_date": "2020-09-17"},
{"title": "When a digital twin becomes the evil twin", "overview": "The use of digital twins is becoming more pervasive, but things are going very wrong in some instances. Here’s how to fix them now", "image_url": "https://images.idgesg.net/images/article/2018/06/digital-twins_woman-in-profile_ai_mirror_duplicate_duo_pair-100760562-large.jpg", "url": "https://www.infoworld.com/article/3574905/when-a-digital-twin-becomes-the-evil-twin.html", "body": "A digital twin is a digital replica of some physical entity, such as a person, a device, manufacturing equipment, or even planes and cars. The idea is to provide a real-time simulation of a physical asset or human to determine when problems are likely to occur and to proactively fix them before they actually arise.Although the roles of digital twins vary a great deal, the connection is established using real-time data from sensors that are able to sync the digital twin living in a virtual world with the physical twin that we can actually touch. This new synchronized simulation leverages IoT (Internet of Things), AI (artificial intelligence), machine learning, and analytics with spatial graphics to create a working simulation of the model that updates as the physical entity changes. There are all types of use cases for digital twins; the most common is a digital twin to represent machines, such as factory equipment and robotics. The twin simulates when proactive maintenance should occur, and if implemented properly should provide better machine productivity and uptime.At issue is that most digital twins exist in public clouds, for the obvious reason that they are much cheaper to run and can access all of the cloud's storage and processing, as well as special services such as AI and analytics, to support the twin. Moreover, the cloud offers purpose-built services for creating and running twins. The ease of building, provisioning, and deploying twins has led to a few issues where the digital twin becomes the evil twin and does more harm than good. Some of the examples that I’ve seen include:In manufacturing, the twin over- or understates the proactivity needed to fix issues before they become real problems. Companies are fixing things identified in the twin simulation that actually don’t need to be fixed. For example, they replace hydraulic fluid three times more often than needed in a factory robot. Or worse, the twin suggests configuration changes that result in overheating and a fire. That last one really happened.In the transportation industry, a digital twin could shut down a jet engine due to what the twin simulated to be a fire but turned out to be a faulty sensor.In the world of healthcare, a patient was indicated as having factors that would likely lead to a stroke but it was determined to be a problem with the predictive analytics model.The point I’m making is that attaching simulations to real devices, machines, and even humans has a great deal of room for error. Most of these can be tracked back to the people creating twins for the first time who don’t find their mistakes until shortly after deployment. The problem I have is that you could crash an airplane, scare the hell out of a patient, or have a factory robot go aflame.With the cloud making the use of digital twins much more affordable and speedier, I see issues like these increasing. Perhaps the problems aren't evil, but they're certainly avoidable.", "pub_date": "2020-09-18"},
{"title": "Using OPA to safeguard Kubernetes", "overview": "Open Policy Agent addresses Kubernetes authorization challenges with a full toolkit for integrating declarative policies into any number of application and infrastructure components", "image_url": "https://images.idgesg.net/images/article/2020/04/micro-segmentation_security_lock_2400x1600-100838720-large.jpg", "url": "https://www.infoworld.com/article/3573078/using-opa-to-safeguard-kubernetes.html", "body": "As more and more organizations move containerized applications into production, Kubernetes has become the de facto approach for managing those applications in private, public and hybrid cloud settings. In fact, at least 84% of organizations already use containers in production, and 78% leverage Kubernetes to deploy them, according to the Cloud Native Computing Foundation.Part of the power and allure of Kubernetes is that, unlike most modern APIs, the Kubernetes API is intent-based, meaning that people using it only need to think about what they want Kubernetes to do — specifying the “desired state” of the Kubernetes object — not  they want Kubernetes to achieve that goal. The result is an incredibly extensible, resilient, powerful, and hence popular system. The long and short of it: Kubernetes speeds app delivery.Also on InfoWorld: Cloud tech certifications count more than degrees nowHowever, changes in a cloud-native environment are constant by design, which means that runtime is extremely dynamic. Speed plus dynamism plus scale is a proven recipe for risk, and today’s modern environments do indeed introduce new security, operational, and compliance challenges. Consider this: How do you control the privilege level of a workload when it only exists for microseconds? How do you control which services can access the internet — or be accessed — when they are all built dynamically and only as needed? Where is your perimeter in a hybrid cloud environment? Because cloud-native apps are ephemeral and dynamic, the attack surface and the requirements for securing it are considerably more complex.Kubernetes authorization challengesMoreover, Kubernetes presents unique challenges regarding authorization. In the past, just that simple word, “authorization” brought up the concept of which people can perform which actions, or “who can do what.” But in containerized apps, that concept has greatly expanded to also include the concept of which software or which machines can perform which actions, aka “what can do what.” Some analysts are starting to use the term “business authorization” to refer to account-centric rules, and “infrastructure authorization” for everything else. And when a given app has a team of, say, 15 developers, but is made up of dozens of clusters, with thousands of services, and countless connections between them, it’s clear that “what can do what” rules are more important that ever — and that developers need tools for creating, managing, and scaling these rules in Kubernetes.Because the Kubernetes API is YAML-based, authorization decisions require analyzing an arbitrary chunk of YAML to make a decision. Those chunks of YAML should define the configuration for each workload. For instance, enforcing a policy, such as “ensure all images come from a trusted repository,” requires scanning the YAML to find a list of all containers, iterating on that list, extracting the particular image name, and string-parsing that image name. Another policy might be, for example, “prevent a service from running as root,” which would require scanning the YAML to find the list of containers, iterating on that list to check for any container-specific security setting, and then combining those settings with global security parameters. Unfortunately, no legacy “business authorization” access control solutions — think role-based or attribute-based access controls, IAM policies, and so on — are powerful enough to enforce policies as basic as the one above, or even things as simple as changing the labels on a pod. They simply were not designed to do so.Even in the rapidly evolving world of containers, one thing has remained constant: Security is often pushed out to the end. Today, DevOps and DevSecOps teams are striving to shift security left in development cycles, but, without the proper tools, are often left to identify and remediate challenges and compliance issues much later on. Indeed, to truly meet the time-to-market goals of a DevOps process, security and compliance policy must be implemented much earlier in the pipeline. It’s been proven that security policy works best when risk is eliminated in the early phases of development, meaning it’s less likely that security concerns will arise toward the end of the delivery pipeline.Yet, not all developers are security experts, and manual reviews of all YAML configurations is a guaranteed path to failure for already overburdened DevOps teams. But you shouldn’t have to sacrifice security for efficiency. Developers need appropriate security tooling that speeds development by implementing hard guardrails that eliminate missteps and risk — ensuring that their Kubernetes deployments are in compliance. What’s needed is a way to improve the overall process that is beneficial to developers, operations, security teams, and the business itself. The great news is there are solutions built to work with modern pipeline automation and “as-code” models that reduce both error and exhaustion.Also on InfoWorld: How to improve CI/CD with shift-left testingEnter Open Policy AgentIncreasingly, the preferred “who can do what” and “what can do what” tool for Kubernetes is Open Policy Agent (OPA). OPA is an open-source policy engine, created by Styra, that provides a domain-agnostic, standalone rules engine for business and infrastructure authorization. Developers often find OPA to be a perfect match for Kubernetes because it was designed around the premise that sometimes you need to write and enforce access control policies — and plenty of other policies — over arbitrary JSON/YAML. As a policy-as-code tool, OPA leads to increased speed and automation in Kubernetes development, while improving security and reducing risk. In fact, Kubernetes is one of the most popular use cases of OPA. If you don’t want to write, support, and maintain custom code for Kubernetes, you can use OPA as a Kubernetes admission controller and put its declarative policy language, Rego, to good use. For instance, you can take all of your Kubernetes access control policies — which are typically stored in wikis and PDFs and in people’s heads — and translate them into policy-as-code. That way, those policies can be enforced directly on the cluster, and developers running apps on Kubernetes don’t need to constantly refer to internal wiki and PDF policies while they work. This leads to fewer errors and eliminates rogue deployments earlier in the development process, all of which results in higher productivity.Another way that OPA can help address the unique challenges of Kubernetes is with context-aware policies. These are policies that condition the decisions Kubernetes makes for one resource on information about all the other Kubernetes resources that exist. For example, you might want to avoid accidentally creating an application that steals another application’s internet traffic by using the same ingress. In that case, you could create a policy to “prohibit ingresses with conflicting hostnames” to require that any new ingresses are compared to existing ingresses. More importantly, OPA ensures that Kubernetes configurations and deployments are in compliance with internal policies and external regulatory requirements — a win-win-win for developers, operations and security teams each.Securing Kubernetes across hybrid cloudOftentimes, when people say “Kubernetes,” they’re really referring to the applications that run on top of the Kubernetes container management system. That’s also a popular way to use OPA: have OPA decide whether microservice and/or end-user actions are authorized within the application itself. Because when it comes to Kubernetes environments, OPA offers a full toolkit for testing, dry-running, auditioning, and integrating declarative policies into any number of application and infrastructure components.Also on InfoWorld: 11 tools that make Kubernetes better12 tools that make Kubernetes easierIndeed, developers often expand their use of OPA to enforce policies and increase security across all of their Kubernetes clusters, particularly in hybrid cloud environments. For that, a number of users also leverage Styra DAS, which helps to validate OPA security policies in pre-runtime to see their impact, distribute them to any number of Kubernetes clusters, and then continuously monitor policies to ensure they’re having their intended effect.Regardless of where organizations are on their cloud-native and container journeys, what’s clear is that Kubernetes is now the standard for deploying containers in production. Kubernetes environments bring new, unique challenges that organizations must solve to ensure security and compliance in their cloud and hybrid-cloud environments — but solutions do exist to limit the need for ground-up thinking. For solving these challenges at speed and scale, OPA has emerged as the de facto standard for helping companies mitigate risk and accelerate app delivery through automated policy enforcement.Open Policy AgentStyra—newtechforum@infoworld.com", "pub_date": "2020-09-09"},
{"title": "Kubeflow 1.0 solves machine learning workflows with Kubernetes", "overview": "Google's machine learning toolkit for Kubernetes helps data scientists manage machine learning workflows and deploy and scale models in production ", "image_url": "https://images.idgesg.net/images/article/2018/02/difficult_easy_simplify_shortcut_solution_thinkstock_175946520-100749258-large.jpg", "url": "https://www.infoworld.com/article/3529977/kubeflow-10-solves-machine-learning-workflows-with-kubernetes.html", "body": "Kubeflow, Google’s solution for deploying machine learning stacks on Kubernetes, is now available as an official 1.0 release.Kubeflow was built to address two major issues with machine learning projects: the need for integrated, end-to-end workflows, and the need to make deploments of machine learning systems simple, manageable, and scalable. Kubeflow allows data scientists to build machine learning workflows on Kubernetes and to deploy, manage, and scale machine learning models in production without learning the intricacies of Kubernetes or its components.Also on InfoWorld: Artificial intelligence predictions for 2020Kubeflow is designed to manage every phase of a machine learning project: writing the code, building the containers, allocating the Kubernetes resources to run them, training the models, and serving predictions from those models. The Kubeflow 1.0 release provides tools, such as Jupyter notebooks for working with data experiments and a web-based dashboard UI for general oversight, to help with each phase.Google claims Kubeflow provides repeatability, isolation, scale, and resilience not just for model training and prediction serving, but also for development and research work. Jupyter notebooks running under Kubeflow can be resource-limited and process-limited, and can re-use configurations, access to secrets, and data sources.InfoWorld’s 2020 Technology of the Year Award winners: The best software development, cloud computing, data analytics, and machine learning products of the yearSeveral Kubeflow components are still under development and will be rolled out in the near future. Pipelines allow complex workflows to be created using Python. Metadata provides a way to track details about individual models, data sets, training jobs, and prediction runs. Katib gives Kubeflow users a mechanism to perform hyperparameter tuning, an automated way to improve the accuracy of predictions from models.", "pub_date": "2020-03-03"},
{"title": "3 cloud architecture problems that need solutions", "overview": "With cloud architecture becoming more art than science, some problems are stumping those who are supposed to have all the answers\r\n", "image_url": "https://images.techhive.com/images/article/2014/03/178274994-100250222-large.jpg", "url": "https://www.infoworld.com/article/3531630/3-cloud-architecture-problems-that-need-solutions.html", "body": "For the most part, cloud architecture is not that exciting. By now we know basically what works, what does not, and the process to get to the right target architecture. This means both the meta or logical architecture and added technology to get to the physical architecture.Although we know the best patterns for most of what cloud architecture requires, some problems are still being debated. No de facto solution or best practice has emerged yet. Here are my top three:Also on InfoWorld: What AI can really do for your business (and what it can’t) Edge computing has benefits, such as placing data processing closer to the source of the data. However, the question remains: How does one partition data and processes between a cloud-based server and an edge computer?Many push as much as they can to the edge, but realize that you’re moving away from a centralized system (the public cloud), to many decentralized systems (the edge devices or servers). You need to understand that you must maintain these edge systems, and they are much more difficult to monitor, govern, secure, update, and configure. Multiply that effort by hundreds of edge computing devices and you've got an operational nightmare. Many enterprises say containers are their strategy and not just an enabling technology. This almost religious belief in the power of containers has pushed many an application to the cloud in containers, but that’s really not how business should be moving there.The issue is that there are no hard and fast rules as to what can—and should—exist in a container. Legacy applications that will take a great deal of effort to refactor (rewrite) for containers are not likely candidates; however, in many instances, the cloud migration team attempts to move them first.This means that enterprises will fail to find value in containers for some of their applications that move to the cloud. It’s a million-dollar mistake that a good number of cloud architects will make. Machine learning is cheap in the clouds, as well as much easier to use than it was. This has led to many instances where enterprise IT AI-enabled an application when the use of cognitive systems as an application component was ultimately contraindicated.Much like the container trade-off outlined above, there is no definitive rule when and how to use machine learning within existing or net-new applications.Keep up with the latest developments in cloud computing with InfoWorld’s Cloud Computing Report newsletterA few things work against the use of machine learning, including the fact that you must refactor the application to take advantage of machine learning at all. However, the larger issue is if AI is even needed in the first place. Many never even ask that question.We’ll always have topics that are not easy to resolve. What’s most productive is that we’re talking about them. ", "pub_date": "2020-03-10"},
{"title": "2 egregious cloud security threats the CSA missed", "overview": "The latest Cloud Security Alliance report highlights the ‘Egregious 11’ cloud security threats. Here are a couple more to consider", "image_url": "https://images.techhive.com/images/article/2015/10/cloud-security-ts-100622309-large.jpg", "url": "https://www.infoworld.com/article/3583637/2-egregious-cloud-security-threats-the-csa-missed.html", "body": "My interesting weekend reading was this Cloud Security Alliance (CSA) report, which was vendor sponsored, highlighting 11 cloud security threats that should be on top of everyone’s mind. These threats are described as “egregious.”CSA surveyed 241 experts on security issues in the cloud industry and came up with these top 11 threats:Data breachesMisconfiguration and inadequate change controlLack of cloud security architecture and strategyInsufficient identity, credential, access, and key managementAccount hijackingInsider threatInsecure interfaces and APIsWeak control planeMetastructure and applistructure failuresLimited cloud usage visibilityAbuse and nefarious use of cloud servicesThis is a pretty good report, by the way. It’s free to download, and if you’re interested in the evolution of cloud computing security, it’s a good read.  Also on InfoWorld: Cloud tech certifications count more than degrees nowHowever, no report can be so comprehensive that it lists all threat patterns, or even derivatives to the threat patterns listed. I have a couple to add that I’m seeing over and over again.By the time attacks are identified they often do not look like attacks. Some tool watches something change over time, such as CPU and storage system saturation, and a non-security-focused ITops tool, such as an AIops tool, spots the issue. There needs to be a way for that alert to be shared with the cloud security system so it can take evasive action using automation.I’ve heard too many stories of attacks using any number of vectors that were discovered by an ITops tool and not by the security system. The reality is that security is systemic to all that is cloud, including usage and performance monitoring, governance systems, database monitoring, etc. Chances are these systems will pick up the shenanigans before the security system knows what’s going on. This is why the various systems need to be integrated and talk to each other. Most are not these days.Many in the cloud security space use the phrase “You never can be too secure.” Guess what? You can.As we get into the whole world of multifactor identification, passwords that have to change monthly, and encryption that hinders performance, we can make security a burden that costs way too much. What’s interesting is that the more complex the security systems, the less secure they seem to be. How is this the case?It comes down to human behavior. If cloud users are asked to change their passwords every month, guess what?  They just write the passwords down in digital memo systems, or I’ve seen them stuck to the screen using sticky notes. Moreover, I’ve seen people bypass encryption because it slows things down too much, even if there are compliance issues. Basically, humans will trade security for convenience or ease of doing their jobs.  The answers are not easy. Sure, you can be a jerk and come down on those violating security policies like a ton of bricks, but that will backfire as well.  The answer is to move to a more passive security plan. This means leveraging security solutions such as biometrics, where looking into a retinal scanner takes the place of frequently changed passwords. Also, encryption services can run on separate servers, thus reducing the impact on performance. Of course, we can go on for days identifying threats, either existing or emerging. The smarter approach is to look at your own cloud deployment rather than focusing on what others are calling “threats.”  ", "pub_date": "2020-09-29"},
{"title": "Review: Nvidia’s Rapids brings Python analytics to the GPU", "overview": "An end-to-end data science ecosystem, open source Rapids gives you Python dataframes, graphs, and machine learning on Nvidia GPU hardware", "image_url": "https://images.idgesg.net/images/article/2018/11/stream_river-bed_water-flow_rapids_rush-100780362-large.jpg", "url": "https://www.infoworld.com/article/3532009/review-nvidias-rapids-brings-python-analytics-to-the-gpu.html", "body": "Building machine learning models is a repetitive process. Often rote and routine, this is a game of “fastest through the cycle wins,” as the faster you can iterate, the easier it is to explore new theories and get good answers. This is one of the reasons practical enterprise use of AI today is dominated by the largest enterprises, which can throw enormous resources at the problem.Rapids is an umbrella for several open source projects, incubated by Nvidia, that puts the entire processing pipeline on the GPU, eliminating the I/O bound data transfers, while also substantially increasing the speed of each of the individual steps. It also provides a common format for the data, easing the burden of exchanging data between disparate systems. At the user level, Rapids mimics the Python API in order to ease the transition for that user base.Typical machine learning workflowRapids ecosystem architectureThe Rapids project aims to replicate, for the most part, the machine learning and data analytics APIs of Python, but for GPUs rather than CPUs. This means that Python developers already have everything they need to run on the GPU, without having to learn the low-level details of CUDA programming and parallel operations. Pythonistas can develop code on a non-GPU enabled machine, then, with a few tweaks, run it on all the GPUs available to them.Also on InfoWorld: The 6 best programming languages for AI developmentThe Nvidia CUDA toolkit provides lower level primitives for math libraries, parallel algorithms, and graph analytics. At the heart of the architecture is the GPU data frame, based on Apache Arrow, that provides a columnar, in-memory data structure that is programming language agnostic. The user interacts with the GPU dataframe via cuDF and a Pandas-like API. Dask, a Python library for parallel computing, mimics the upstream Python APIs and works with CUDA libraries for parallel computation. Think of Dask as Spark for Python.Rapids ecosystem architectureThe three main projects, cuDF, cuML and cuGraph, are developed independently, but designed to work seamlessly together. Bridges to the broader Python ecosystem are also being developed as part of the project.Rapids installationInstallation via Anaconda on a Linux machine in AWS was mostly straightforward, barring a few hiccups due to a change in dependencies in version 0.11. Installing the C/C++ libraries to use libcudf was not so easy, and I’d recommend sticking to the Python APIs and Conda installation process. Rapids includes a Jupyter notebook, also available on Google’s free Colab, that makes getting started simple. I used the Jupyter notebook version 0.10 to run the code on Google Colab, which includes an Nvidia Tesla T4 GPU.Rapids’ GPU dataframeAt the heart of any data science workflow is the dataframe. This is where feature engineering happens, and where the majority of time is spent, as data scientists wrangle dirty data. cuDF is the Rapids project for a GPU-based, Pandas-like dataframe. Underpinning cuDF is libcudf, a C++ library implementing low-level primitives for importing Apache Arrow data, performing element-wise mathematics on arrays, and executing sort, join, group by, reduction, and other operations on in-GPU memory matrices. The basic data structure of libcudf is the GPU DataFrame (GDF), which in turn is modeled on Apache Arrow’s columnar data store.cuDF technology stack The Rapids Python library presents the user with a higher level interface resembling dataframes, like those in Pandas. In many cases, Pandas code runs unchanged on cuDF. Where this is not the case, usually only minor changes are required.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesUser defined functions in cuDFOnce you’re past basic data manipulation, it is sometimes necessary to process rows and columns with user defined functions (UDFs). cuDF provides a PyData style API to write code to process more course-grained data structures like arrays, series, and moving windows. Currently only numeric and Boolean types are supported. UDFs are compiled using the Numba JIT compiler, which uses a subset of LLVM to compile numeric functions to CUDA machine code. This results in substantially faster run times on the GPU.Strings in cuDFAlthough GPUs are fantastic for rapidly processing float vectors, they haven’t typically been used for processing string data, and the reality is that most data comes to us in the form of strings. cuStrings is a GPU string manipulation library for splitting, applying regexes, concatenating, replacing tokens, etc. in arrays of strings. Like other functions of cuDF, it is implemented as a C/C++ library (libnvStrings) and wrapped by a Python layer designed to mimic Pandas. Although the string data type is not optimized for execution on GPUs, parallel execution of the code should provide a speedup over CPU-based string manipulation.Getting data in or out of cuDFDataframe I/O is handled by a dedicated library, cuIO. All of the most commonly encountered formats are supported, including Arrow, ORC, Parquet, HDF5, and CSV. If you are lucky enough to be running on DGX-2 hardware, you can use GPU Direct Storage integration to move data directly from high-speed storage to the GPU without involving the CPU. Mortal users will still appreciate the speedup the GPU gives when decompressing large data sets, and the tight integration with the Python ecosystem.GPU Direct Storage is currently in alpha, and when released will be available on most Tesla GPUs. You can create a GPU dataframe from NumPy arrays, Pandas DataFrames, and PyArrow tables with just a single line of code. Other projects can exchange data via the  for libraries that fall within the Numba ecosystem. DLPack for neural network libraries is also a supported interface.Probably the biggest drawback in using cuDF is the lack of interoperability outside of Python. I think a focus on a strong foundation of C/C++ APIs, as Arrow has done, would enable a broader ecosystem and benefit the project as a whole.Rapids’ cuMLcuML’s stated goals are to be “Python’s Scikit-learn powered by GPUs.” In theory this means you should only have to change your import statement and perhaps tune a few of the parameters to account for the differences in running on a CPU, where sometimes a brute force approach is better. The benefit of having a GPU-based Scikit-learn is hard to understate. The speedups are substantial, and data analysts can be many times more productive. The C++ API isn’t quite ready for broad consumption outside of its Python bindings, but this is expected to improve.cuML also includes APIs for helping with hyperparameter tuning via Dask, a library for scaling Python across multiple nodes. Many machine learning algorithms can be effectively made parallel, and cuML is actively developing both multi-GPU and multi-node, multi-GPU algorithms.cuML algorithmsAlso on InfoWorld: Artificial intelligence predictions for 2020Rapids’ cuGraphcuGraph is the third member of the Rapids ecosystem, and like the others, cuGraph is fully integrated with cuDF and cuML. It offers a good selection of graph algorithms, primitives, and utilities, all with GPU-accelerated performance. The selection of APIs in cuGraph is somewhat more extensive than in other parts of Rapids, with NetworkX, Pregel, GraphBLAS, and GQL (Graph Query Language) all available.cuGraph stackcuGraph is more like a toolkit in spirit than cuML. Graph technology is a fast-moving space both in academia and industry. Thus, by design, cuGraph gives developers access to the C++ layer and graph primitives, encouraging third parties to develop products using cuGraph. Several universities have contributed, and projects from Texas A&M (GraphBLAS), Georgia Tech (Hornet), and UC Davis (Gunrock) have been “productized” and included under the cuGraph umbrella. Each project provides a different set of capabilities, all GPU-accelerated, and all backed by the same cuDF dataframe.NetworkX is the Python API targeted by the Rapids team for its native interface. There are a number of algorithms available via that interface. While only page rank is multi-GPU, the team is actively working on multi-GPU versions of the others, where applicable.cuGraph algorithmsOne of the cuGraph sub-projects I found interesting is cugraphBLAS, an effort to standardize building blocks for graph algorithms in the language of linear algebra. Based on GraphBLAS (graphblas.org), a custom data structure designed for sparse dynamic graphs processing.Another cuGraph sub-project, Hornet provides a system independent format for containing graph data, analogous to the way Apache arrow provides a system independent way to process dataframes. Hornet supports most of the popular graph formats including SNAP, mtx, metis, and edges.In keeping with the spirit of being close to the Python community, Python’s native NetworkX package can be used for the study of complex networks. This includes data structures for graphs and multi-graphs, reimplemented using CUDA primitives, allowing you to reuse many of the standard graph algorithms and perform network structure and analysis measures. The majority of algorithms are single-GPU, like NetworkX. Nevertheless, running them on the GPU alone offers significant speedup, while work continues to move to multi-GPU implementations.Also on InfoWorld: What is CUDA? Parallel programming for GPUsOn the Rapids roadmap Given the tremendous speed up that GPU-based analytics provides, there are a few new projects coming into the mix in future versions.Multi-layer neural networks were one of the first workloads moved to GPUs, and a sizable body of code exists for this machine learning use case. Previously DLPack was the de-facto standard for data interchange among deep learning libraries. Nowadays the array_interface is commonly supported. Rapids supports both.Like most other projects at Rapids, cuSignal is a GPU-accelerated version of an existing Python library, in this case the SciPy Signal library. The original SciPy Signal library is based on NumPy, which is replaced with its GPU-accelerated equivalent, CuPy in cuSignal. This is a good example of the Rapids design philosophy at work. With the exception of a few custom CUDA kernels, the port to the GPU mostly involves replacing the import statement and tweaking a few function parameters. Bringing signal processing into the Rapids fold is a smart move. Signal processing is everywhere and has many immediately useful commercial applications in industry and defense.Spatial and spatiotemporal operations are great candidates for GPU acceleration, and they solve many real-world problems we face in everyday life, such as analyzing traffic patterns, soil health/quality, and flood risk. Much of the data collected by mobile devices, including drones, has a geospatial component, and spatial analysis is at the heart of the Smart City. Architected like the other components, cuSpatial is a C++ library built on CUDA primitives and the Thrust vector processing library, using cuDF for data interchange. Consumers of the C++ library can read point, polyline, and polygon data using a C++ reader. Python users are better off using existing Python packages like Shapely or Fiona to fill a NumPy array, then using the cuSpatial Python API or converting to cuDF dataframes. Visualizing data is fundamental, both within the analytics workflow and for presenting or reporting results. Yet for all the magic that GPUs can work on the data itself, getting that data out to a browser is not a trivial task. cuxfilter, inspired by the Crossfilter JavaScript library, aims to bridge that gap by providing a stack to enable third-party visualization libraries to display data in cuDF dataframes.There have been a few iterations of cuxfilter as the team sorts out the best architecture and connector patterns. The latest iteration leverages Jupyter notebooks, Bokeh server, and PyViz panels, while integration experiments include projects from Uber, Falcon, and PyDeck. This component isn’t quite yet ready for prime time, but is slated for release in Rapids 0.13. There are a lot of moving parts, and I did not get to experiment with it first hand, but if it lives up to its promise this will be a great addition to the Rapids toolkit.Also on InfoWorld: 5 reasons to choose PyTorch for deep learningScaling up and out with DaskDask is distributed task scheduler for Python, playing a similar role for Python that Apache Spark plays for Scala. Dask-cuDF is a library that provides partitioned, GPU-backed dataframes. Dask-cuDF works well when you plan to use cuML or when you are loading a data set that is larger than GPU memory or is spread across multiple files.Like a Spark RDD (Resilient Distributed Dataset), the Dask-cuDF distributed dataframe mostly behaves just like a local one, so you can experiment with your local machine and move to a distributed model when you need to scale up. Dask-cuML gives cuML multi-node capabilities, making it a good option when you do not have the budget for a DGX workstation.", "pub_date": "2020-03-11"},
{"title": "10 questions about deep learning ", "overview": "Learn why neural networks are so powerful, how and where they’re used, and how to get started — no programming necessary", "image_url": "https://images.idgesg.net/images/article/2019/11/ai_artificial_intelligence_ml_machine_learning_abstract_face_by_kentoh_gettyimages_1042827860-100817767-large.jpg", "url": "https://www.infoworld.com/article/3532058/10-questions-about-deep-learning.html", "body": "It seems everywhere you look nowadays, you will find an article that describes a winning strategy using deep learning in a data science problem, or more specifically in the field of artificial intelligence (AI). However, clear explanations of deep learning, why it’s so powerful, and the various forms deep learning takes in practice, are not so easy to come by.In order to know more about deep learning, neural networks, the major innovations, the most widely used paradigms, where deep learning works and doesn’t, and even a little of the history, we have asked and answered a few basic questions.Also on InfoWorld: Artificial intelligence today: What’s hype and what’s realDeep learning is the modern evolution of traditional neural networks. Indeed, to the classic feed-forward, fully connected, backpropagation trained, multilayer perceptrons (MLPs), “deeper” architectures have been added. Deeper means more hidden layers and a few new additional neural paradigms, as in recurrent networks and in convolutional networks.There is no difference. Deep learning networks are neural networks, just with more complex architectures than were possible to train in the 1990s. For example, long short-term memory (LSTM) units in recurrent neural networks (RNNs) were introduced in 1997 by Hochreiter and Schmidhuber but never found extensive adoption because of the long computational times and high computational resources that were required. Multilayer perceptrons with more than one hidden layer have also been around for a long time, and their benefits were clear. The main difference is that modern computational resources have made their implementation feasible. In general, faster and more powerful computational resources have allowed for the implementation and experimentation of more powerful and more promising neural architectures. It is clear that spending days in network training cannot rival the few minutes spent in training the same network with the help of GPU acceleration.The big breakthrough was in 2012 when the deep learning-based AlexNet network won the ImageNet challenge with an unprecedented margin. The top-five error rate of AlexNet was 15 percent, while the next best competitor ended up with 26 percent. This victory kicked off a surge in deep learning networks, and the best models nowadays attain error rates below the 3 percent mark.That’s particularly impressive if you consider that the human error rate is around 5 percent.In a word, flexibility. On the one hand, neural networks are universal function approximators, which is smart talk for saying that you can approximate almost anything using a neural network—if you make it complex enough. On the other hand, you can use the trained weights of a network to initialize the weights of another network that performs a similar task. This is called transfer learning, and you would be surprised how well it works, even for tasks that seem quite dissimilar at first glance.Also on InfoWorld: Artificial intelligence predictions for 2020There are four very successful and widely adopted deep learning paradigms: LSTM units in recurrent neural networks, convolutional layers in convolutional neural networks (CNNs), encoder-decoder structures, and generative adversarial networks (GANs).RNNs are a family of neural networks used for processing sequential data, like text (e.g., a sequence of words or characters) or time series data. The idea is to apply a copy of the same network at each time step and connect the different copies via some state vectors. This allows the network to remember information from the past. Popular unit network structures in RNNs are gated recurrent units (GRUs) and LSTMs.CNN layers are especially powerful for data with spatial dependencies like images. Instead of connecting every neuron to the new layer, a sliding window is used, which works like a filter.  Some convolutions may detect edges or corners, while others may detect cats, dogs or street signs inside an image.Another often used neural network structure is the encoder-decoder network. A simple example is an autoencoder where a neural network with a bottleneck layer is trained to reconstruct the input to the output. A second application of encoder-decoder networks is neural machine translation where encoder-decoder structure is used in an RNN. The LSTM-based encoder extracts a dense representation of the content in the source language, and the LSTM-based decoder generates the output sequence in the target language.And, of course, the generative adversarial networks. A generative adversarial network is composed of two deep learning networks, the generator and the discriminator. Both networks are trained in alternating steps competing to improve themselves. GANs have been successfully applied to image tensors to create anime, human figures and even van Gogh-like masterpieces.No, at least not yet. There are certain domains, like computer vision, where you can’t get around deep learning anymore, but there are other areas, such as tabular data, that have proven to be a challenge for deep learning.In the case of tabular data, which is still the main format used for storing business data, deep learning is not doing terribly poorly there. However, training a deep learning model for days on an expensive GPU server is hard to justify if you can get similar accuracy using random forests or gradient boosted trees, which you can train within a few minutes on a decent laptop.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesNot really. It is true that most deep learning paradigms are available in TensorFlow and Keras and that both of them require Python skills. However, in our open source KNIME Analytics Platform, we provide a graphical user interface (GUI) to handle exactly those Keras and deep learning libraries using TensorFlow in the back end. You can build a neural architecture as complex as you wish just by dragging and dropping the appropriate nodes one after the other.An example is shown in Figure 1 below, where we trained an LSTM-based RNN to generate free text. The model creates fake names that resemble mountain names for a new outdoor clothing line. At the top (the brown nodes), you can see where we built the neural architecture, which we then trained using the Keras Network Learner node. The trained network, opportunely modified, is then saved in a TensorFlow format.Figure 1. Constructing and training an LSTM-based RNN to generate free text. At the top, the brown nodes build the network architecture. Then, the Keras Network Learner node trains the network, which after some appropriate post-processing is saved in a TensorFlow file.You can find plenty on our community KNIME Hub. For example, recurrent neural networks with LSTM units can be found in this example for free text generation (also shown in Figure 1) or in this other example for time series prediction. Also, there are several example workflows using convolutional neural networks to process images, such as “Building a CNN from scratch” or “Train simple CNN.” A simple feed-forward, fully connected multilayer autoencoding structure was built and used as a solution to a fraud detection task. I am sure many more have been uploaded by the community as we speak.Keep up with advances in machine learning, AI, and big data analytics with InfoWorld’s Machine Learning and Analytics Report newsletterKNIME Analytics Platform is an open source application and so are its integrations, including the Keras and TensorFlow integrations. You can install them wherever you wish, i.e. in the public cloud of your choosing or on your machine. It is clear, though, that the more powerful the machine, the faster the execution. You can even apply GPU acceleration in the KNIME Keras integration. You just need a GPU-equipped machine with CUDA installed, a Conda environment with Keras for GPU installation, and the KNIME Keras integration on top of that.KNIMEPracticing Data Science: A Collection of Case StudiesTwitterLinkedInKNIME blogLinkedInLinkedInFrom Alteryx to KNIMELinkedInwww.knime.comKNIME blog", "pub_date": "2020-03-12"},
{"title": "AI companies plant the seeds for quantum machine learning", "overview": "Quantum computing promises to accelerate analytics faster than the speed of light, but it still feels slightly unreal, in spite of the first signs of broad commercialization", "image_url": "https://images.idgesg.net/images/article/2020/02/microsoft-quantum-computer-source-ms-quantum-100832328-large.jpg", "url": "https://www.infoworld.com/article/3532436/ai-companies-plant-the-seeds-for-quantum-machine-learning.html", "body": "Quantum isn’t the next big thing in advanced computing so much as a futuristic approach that could potentially be the biggest thing of all.Considering the theoretical possibility of quantum fabrics that enable seemingly magical, astronomically parallel, unbreakably encrypted, and faster-than-light subatomic computations, this could be the omega architecture in the evolution of AI (artificial intelligence).Also on InfoWorld: Artificial intelligence predictions for 2020No one doubts that the IT industry is making impressive progress in developing and commercializing quantum technologies. But this mania is also shaping up to be the hype that ends all hype. It will take time for quantum technology to prove itself a worthy successor to computing’s traditional von Neumann architecture.Though the splashy headlines boast of quantum supremacy, which refers to claims that programmable quantum devices can solve problems beyond the reach of von Neumann architectures, there has been far less focus on quantum practicality. In other words, there is still little evidence that quantum computers are being applied to real-world use cases in AI, ML (machine learning), and other advanced analytics. AI has been singled out as a quantum killer app for quite some time, but quantum so far has minimal presence in the commercial data analytics arena.We need to ask ourselves whether all the recent industry activity is setting ourselves up for the dreaded “quantum winter,” analogous to the long AI winter period of backlash against the early hype for that technology. It’s one thing to talk about the mind-blowing potential of quantum analytics that executes across parallel universes as if it were the holy grail. It’s quite another to point to a mature technology with clear killer apps that make a huge difference in our lives today.Nevertheless, there is a growing sense among researchers and even among analytics professionals that ML could become the core use case for quantum in our lives.This is not a recent revelation. An MIT professor declared in 2013 that the “first quantum application” is ML. Specifically, the professor, Seth Lloyd, proposed a “q-app” that “encodes Google-like queries with q-bits that enable quantum computers to not only perform real-time searches through even the most gigantic databases, but which also ensures their absolute privacy, since attempts to eavesdrop on the query by the search engine provider would disturb the delicate q-bit’s superposition of states.”More significantly, recent product launches and other announcements by Amazon Web Services, Microsoft, IBM, and Honeywell in the quantum computing space address AI and ML use cases to varying degrees. None of these announcements pertains to a generally available quantum product or service that’s solving practical business problems. However, most of these announcements include hooks for programmers to develop such solutions on quantum hardware platforms or cloud services.In November 2019, Microsoft announced Azure Quantum. This quantum-computing cloud service is currently in private preview and expected to become generally available later this year. It comes with a Microsoft open source Quantum Development Kit for the Microsoft-developed quantum-oriented Q# language as well as Python, C#, and other languages. The kit includes libraries for development of quantum apps in ML, cryptography, optimization, and other domains.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesA month later, AWS announced the Amazon Braket service. Still in preview, this is a fully managed AWS service that enables scientists, researchers, and developers to begin experimenting with computers from quantum hardware providers (including D-Wave, IonQ, and Rigetti) in a single place. It provides a single development environment to build quantum algorithms—including ML—and test them on simulated quantum computers. Developers can run ML and other quantum programs on a range of different hardware architectures. And it allows users to design quantum algorithms using the Amazon Braket developer toolkit and use familiar tools, such as Jupyter notebooks.Then in January, IBM announced the expansion of its Q Network, in which more than 200,000 users are running hundreds of billions of executions on IBM’s quantum systems and simulators through the IBM Cloud. Participants in the network have access to IBM’s quantum expertise and resources, open source Qiskit software and developer tools, as well as cloud-based access to the IBM Quantum Computation Center. Many of the workloads being run include ML, as well as real-time simulations of quantum computing architectures.Less than two weeks ago, Honeywell announced that its high-capacity quantum computer will be generally available within three months. It also announced that Honeywell Ventures is making investments in Cambridge Quantum Computing and Zapata Computing. Both companies have expertise in ML and other cross-vertical algorithms and software for quantum computing applications.The most important announcement, coming just a few days ago, was Google’s launch of TensorFlow Quantum. This new software-only stack extends the widely adopted TensorFlow open-source ML library and modeling framework to support building and training of ML models to be processed on quantum computing platforms.Developed by Google’s X R&D unit, TensorFlow Quantum enables data scientists to use Python code to develop quantum ML models through standard Keras functions. It provides a library of quantum circuit simulators and quantum computing primitives that are compatible with existing TensorFlow APIs.TensorFlow Quantum’s release is no big surprise, coming several months after Google declared  “quantum supremacy,” which refers to its achievement of a quantum computing feat that would have been impossible on traditional computing architecture.In addition to providing a full AI/ML software stack into which quantum processing can now be hybridized, Google intends to expand the range of more traditional chip architectures on which TensorFlow Quantum can simulate quantum ML. It has announced plans to expand the range of custom, quantum-simulation hardware platforms supported by the tool to include graphics processing units from various vendors as well as its own Tensor Processing Unit AI-accelerator hardware platforms.Recognizing that quantum computing is not yet mature enough to process the full range of ML workloads with sufficient accuracy, Google has wisely designed its new open source tool to support the many AI use cases with one foot in traditional computing architectures. TensorFlow Quantum enables developers to rapidly prototype ML models that hybridize the execution of quantum and classic processors in parallel on learning tasks. Using the tool, developers can build both classical and quantum datasets, with the classical data natively processed by TensorFlow, and the quantum extensions processing quantum data, which consists of both quantum circuits and quantum operators.Also on InfoWorld: 5 reasons to choose PyTorch for deep learningDevelopers can use TensorFlow Quantum for supervised learning on such ML use cases as quantum classification, quantum control, and quantum approximate optimization. They can also execute advanced quantum learning tasks such as meta-learning, Hamiltonian learning, and sampling thermal states.In addition, Google has designed TensorFlow Quantum to support the growing range of AI use cases, such as “deepfakes” that do video, voice, and image generation with a high degree of verisimilitude. Google ML developers can use TensorFlow Quantum to train hybrid quantum/classical models to handle both the discriminative and generative workloads at the heart of the generative adversarial networks used in such applications.Strategically, Google’s likely next move will be to combine TensorFlow Quantum with its pre-existing Quantum Computing Playground into a full-featured, managed quantum ML service. Considering the fact that Google’s top public-cloud rivals (Microsoft, AWS, and IBM) all have such services either on the market or in preview, it would be shocking if the Mountain View, Calif.-based company doesn’t try to one-up them with a quantum ML service of its own.Building “write once run anywhere” quantum ML will be tricky for quite some time, even in TensorFlow Quantum. This is because quantum researchers are experimenting with a wide range of alternative architectures even while they try to build practical applications.Here are some of the principal ways the leading quantum ML vendors are supporting these more fundamental quantum R&D requirements:Google designed TensorFlow Quantum to support advanced research in alternative quantum computing architectures and algorithms for processing ML models. This makes the new offering an invaluable research tool for computer scientists who are experimenting with different quantum and hybrid processing architectures optimized for ML workloads. To this end, TensorFlow Quantum incorporates Cirq, an open source Python library for programming quantum computes. It supports programmatic creation, editing, and invoking of the quantum gates that constitute the Noisy Intermediate-Scale Quantum (NISQ) circuits characteristic of today’s quantum systems. It also enables developer-specified quantum computations to be executed in simulations or on real hardware.Microsoft’s Azure Quantum includes libraries for simulation of alternative quantum circuit processing scenarios and prediction of likely program performance in these environments.Amazon Braket allows users to explore, evaluate, and experiment with quantum computing hardware, design quantum algorithms, and simulate performance of their programs’ execution either on low-level quantum circuits or fully managed hybrid algorithms.IBM Q Network supports real-time simulations of alternative quantum computing architectures.Quantum computing has been in wait-and-see mode for so long that we tend to overlook the fact that it’s being rapidly put to practical uses.Keep up with advances in machine learning, AI, and big data analytics with InfoWorld’s Machine Learning and Analytics Report newsletterEven as quantum computing platform vendors experiment with new materials, methodologies, and architectures, researchers around the world have been demonstrating that quantum processing of ML models is in fact feasible. We can expect that AI/ML researchers at Google and elsewhere will probably use TensorFlow Quantum to do some fairly amazing things that were never feasible on traditional AI-accelerator hardware platforms.It’s clear from all these recent industry announcements that we’ll not only see commercialized quantum ML in our lifetime but that it has already begun to emerge and will gain steady adoption in this decade.", "pub_date": "2020-03-13"},
{"title": "What is Apache Spark? The big data platform that crushed Hadoop", "overview": "Fast, flexible, and developer-friendly, Apache Spark is the leading platform for large-scale SQL, batch processing, stream processing, and machine learning", "image_url": "https://images.techhive.com/images/article/2014/11/texture_stock_bokeh_014_by_redwolf518stock-100532830-large.jpg", "url": "https://www.infoworld.com/article/3236869/what-is-apache-spark-the-big-data-platform-that-crushed-hadoop.html", "body": "Apache Spark definedApache Spark is a data processing framework that can quickly perform processing tasks on very large data sets, and can also distribute data processing tasks across multiple computers, either on its own or in tandem with other distributed computing tools. These two qualities are key to the worlds of big data and machine learning, which require the marshalling of massive computing power to crunch through large data stores. Spark also takes some of the programming burdens of these tasks off the shoulders of developers with an easy-to-use API that abstracts away much of the grunt work of distributed computing and big data processing.From its humble beginnings in the AMPLab at U.C. Berkeley in 2009, Apache Spark has become one of the key big data distributed processing frameworks in the world. Spark can be deployed in a variety of ways, provides native bindings for the Java, Scala, Python, and R programming languages, and supports SQL, streaming data, machine learning, and graph processing. You’ll find it used by banks, telecommunications companies, games companies, governments, and all of the major tech giants such as Apple, Facebook, IBM, and Microsoft.InfoWorld’s 2020 Technology of the Year Award winners: The best software development, cloud computing, data analytics, and machine learning products of the yearApache Spark architectureAt a fundamental level, an Apache Spark application consists of two main components: a  which converts the user's code into multiple tasks that can be distributed across worker nodes, and  which run on those nodes and execute the tasks assigned to them. Some form of cluster manager is necessary to mediate between the two.Out of the box, Spark can run in a standalone cluster mode that simply requires the Apache Spark framework and a JVM on each machine in your cluster. However, it’s more likely you’ll want to take advantage of a more robust resource or cluster management system to take care of allocating workers on demand for you. In the enterprise, this will normally mean running on Hadoop YARN (this is how the Cloudera and Hortonworks distributions run Spark jobs), but Apache Spark can also run on Apache Mesos, Kubernetes, and Docker Swarm.If you seek a managed solution, then Apache Spark can be found as part of Amazon EMR, Google Cloud Dataproc, and Microsoft Azure HDInsight. Databricks, the company that employs the founders of Apache Spark, also offers the Databricks Unified Analytics Platform, which is a comprehensive managed service that offers Apache Spark clusters, streaming support, integrated web-based notebook development, and optimized cloud I/O performance over a standard Apache Spark distribution.Apache Spark builds the user’s data processing commands into a , or DAG. The DAG is Apache Spark’s scheduling layer; it determines what tasks are executed on what nodes and in what sequence.  Spark vs. Hadoop: Why use Apache Spark?It’s worth pointing out that Apache Spark vs. Apache Hadoop is a bit of a misnomer. You’ll find Spark included in most Hadoop distributions these days. But due to two big advantages, Spark has become the framework of choice when processing big data, overtaking the old MapReduce paradigm that brought Hadoop to prominence.The first advantage is speed. Spark’s in-memory data engine means that it can perform tasks up to one hundred times faster than MapReduce in certain situations, particularly when compared with multi-stage jobs that require the writing of state back out to disk between stages. In essence, MapReduce creates a two-stage execution graph consisting of data mapping and reducing, whereas Apache Spark’s DAG has multiple stages that can be distributed more efficiently. Even Apache Spark jobs where the data cannot be completely contained within memory tend to be around 10 times faster than their MapReduce counterpart.The second advantage is the developer-friendly Spark API. As important as Spark’s speedup is, one could argue that the friendliness of the Spark API is even more important.Spark CoreIn comparison to MapReduce and other Apache Hadoop components, the Apache Spark API is very friendly to developers, hiding much of the complexity of a distributed processing engine behind simple method calls. The canonical example of this is how almost 50 lines of MapReduce code to count words in a document can be reduced to just a few lines of Apache Spark (here shown in Scala):By providing bindings to popular languages for data analysis like Python and R, as well as the more enterprise-friendly Java and Scala, Apache Spark allows everybody from application developers to data scientists to harness its scalability and speed in an accessible manner.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesSpark RDDAt the heart of Apache Spark is the concept of the Resilient Distributed Dataset (RDD), a programming abstraction that represents an immutable collection of objects that can be split across a computing cluster. Operations on the RDDs can also be split across the cluster and executed in a parallel batch process, leading to fast and scalable parallel processing.RDDs can be created from simple text files, SQL databases, NoSQL stores (such as Cassandra and MongoDB), Amazon S3 buckets, and much more besides. Much of the Spark Core API is built on this RDD concept, enabling traditional map and reduce functionality, but also providing built-in support for joining data sets, filtering, sampling, and aggregation.Spark runs in a distributed fashion by combining a  core process that splits a Spark application into tasks and distributes them among many  processes that do the work. These executors can be scaled up and down as required for the application’s needs.Spark SQLOriginally known as Shark, Spark SQL has become more and more important to the Apache Spark project. It is likely the interface most commonly used by today’s developers when creating applications. Spark SQL is focused on the processing of structured data, using a dataframe approach borrowed from R and Python (in Pandas). But as the name suggests, Spark SQL also provides a SQL2003-compliant interface for querying data, bringing the power of Apache Spark to analysts as well as developers.Alongside standard SQL support, Spark SQL provides a standard interface for reading from and writing to other datastores including JSON, HDFS, Apache Hive, JDBC, Apache ORC, and Apache Parquet, all of which are supported out of the box. Other popular stores—Apache Cassandra, MongoDB, Apache HBase, and many others—can be used by pulling in separate connectors from the Spark Packages ecosystem.Selecting some columns from a dataframe is as simple as this line:Using the SQL interface, we register the dataframe as a temporary table, after which we can issue SQL queries against it:Behind the scenes, Apache Spark uses a query optimizer called Catalyst that examines data and queries in order to produce an efficient query plan for data locality and computation that will perform the required calculations across the cluster. In the Apache Spark 2.x era, the Spark SQL interface of dataframes and datasets (essentially a typed dataframe that can be checked at compile time for correctness and take advantage of further memory and compute optimizations at run time) is the recommended approach for development. The RDD interface is still available, but recommended only if your needs cannot be addressed within the Spark SQL paradigm.Spark 2.4 introduced a set of built-in higher-order functions for manipulating arrays and other higher-order data types directly.Spark MLlibApache Spark also bundles libraries for applying machine learning and graph analysis techniques to data at scale. Spark MLlib includes a framework for creating machine learning pipelines, allowing for easy implementation of feature extraction, selections, and transformations on any structured dataset. MLlib comes with distributed implementations of clustering and classification algorithms such as k-means clustering and random forests that can be swapped in and out of custom pipelines with ease. Models can be trained by data scientists in Apache Spark using R or Python, saved using MLlib, and then imported into a Java-based or Scala-based pipeline for production use.Note that while Spark MLlib covers basic machine learning including classification, regression, clustering, and filtering, it does not include facilities for modeling and training deep neural networks (for details see InfoWorld’s Spark MLlib review). However, Deep Learning Pipelines are in the works.Also on InfoWorld: Artificial intelligence predictions for 2020Spark GraphXSpark GraphX comes with a selection of distributed algorithms for processing graph structures including an implementation of Google’s PageRank. These algorithms use Spark Core’s RDD approach to modeling data; the GraphFrames package allows you to do graph operations on dataframes, including taking advantage of the Catalyst optimizer for graph queries.Spark StreamingSpark Streaming was an early addition to Apache Spark that helped it gain traction in environments that required real-time or near real-time processing. Previously, batch and stream processing in the world of Apache Hadoop were separate things. You would write MapReduce code for your batch processing needs and use something like Apache Storm for your real-time streaming requirements. This obviously leads to disparate codebases that need to be kept in sync for the application domain despite being based on completely different frameworks, requiring different resources, and involving different operational concerns for running them.Spark Streaming extended the Apache Spark concept of batch processing into streaming by breaking the stream down into a continuous series of microbatches, which could then be manipulated using the Apache Spark API. In this way, code in batch and streaming operations can share (mostly) the same code, running on the same framework, thus reducing both developer and operator overhead. Everybody wins.A criticism of the Spark Streaming approach is that microbatching, in scenarios where a low-latency response to incoming data is required, may not be able to match the performance of other streaming-capable frameworks like Apache Storm, Apache Flink, and Apache Apex, all of which use a pure streaming method rather than microbatches.Structured StreamingStructured Streaming (added in Spark 2.x) is to Spark Streaming what Spark SQL was to the Spark Core APIs: A higher-level API and easier abstraction for writing applications. In the case of Structure Streaming, the higher-level API essentially allows developers to create infinite streaming dataframes and datasets. It also solves some very real pain points that users have struggled with in the earlier framework, especially concerning dealing with event-time aggregations and late delivery of messages. All queries on structured streams go through the Catalyst query optimizer, and can even be run in an interactive manner, allowing users to perform SQL queries against live streaming data.Structured Streaming originally relied on Spark Streaming’s microbatching scheme of handling streaming data. But in Spark 2.3, the Apache Spark team added a low-latency Continuous Processing Mode to Structured Streaming, allowing it to handle responses with latencies as low as 1ms, which is very impressive. As of Spark 2.4, Continuous Processing is still considered experimental. While Structured Streaming is built on top of the Spark SQL engine, Continuous Streaming supports only a restricted set of queries.Structured Streaming is the future of streaming applications with the platform, so if you’re building a new streaming application, you should use Structured Streaming. The legacy Spark Streaming APIs will continue to be supported, but the project recommends porting over to Structured Streaming, as the new method makes writing and maintaining streaming code a lot more bearable.Deep Learning PipelinesApache Spark supports deep learning via Deep Learning Pipelines. Using the existing pipeline structure of MLlib, you can call into lower-level deep learning libraries and construct classifiers in just a few lines of code, as well as apply custom TensorFlow graphs or Keras models to incoming data. These graphs and models can even be registered as custom Spark SQL UDFs (user-defined functions) so that the deep learning models can be applied to data as part of SQL statements.Apache Spark tutorialsReady to dive in and learn Apache Spark? We highly recommend Evan Heitman’s A Neanderthal’s Guide to Apache Spark in Python, which not only lays out the basics of how Apache Spark works in relatively simple terms, but also guides you through the process of writing a simple Python application that makes use of the framework. The article is written from a data scientist’s perspective, which makes sense as data science is a world in which big data and machine learning are increasingly critical.Keep up with advances in machine learning, AI, and big data analytics with InfoWorld’s Machine Learning and Analytics Report newsletterIf you’re looking for some Apache Spark examples to give you a sense of what the platform can do and how it does it, check out Spark By {Examples}. There is plenty of sample code here for a number of the basic tasks that make up the building blocks of Spark programming, so you can see the components that make up the larger tasks that Apache Spark is made for.Need to go deeper? DZone has what it modestly refers to as The Complete Apache Spark Collection, which consists of a slew of helpful tutorials on many Apache Spark topics. Happy learning!", "pub_date": "2020-03-16"},
{"title": "Explaining machine learning models to the business", "overview": "How to create summaries of machine learning system decisions that business decision makers can understand", "image_url": "https://images.techhive.com/images/article/2016/03/thinkstockphotos-523692007-100648034-large.jpg", "url": "https://www.infoworld.com/article/3533369/explaining-machine-learning-models-to-the-business.html", "body": "Explainable machine learning is a sub-discipline of artificial intelligence (AI) and machine learning that attempts to summarize how machine learning systems make decisions. Summarizing how machine learning systems make decisions can be helpful for a lot of reasons, like finding data-driven insights, uncovering problems in machine learning systems, facilitating regulatory compliance, and enabling users to appeal — or operators to override — inevitable wrong decisions.Also on InfoWorld: Artificial intelligence predictions for 2020Of course all that sounds great, but explainable machine learning is not yet a perfect science. The reality is there are two major issues with explainable machine learning to keep in mind:Some “black-box” machine learning systems are probably just too complex to be accurately summarized.Even for machine learning systems that are designed to be interpretable, sometimes the way summary information is presented is still too complicated for business people. (Figure 1 provides an example of machine learning explanations for data scientists.)Figure 1: Explanations created by H2O Driverless AI. These explanations are probably better suited for data scientists than for business users.For issue 1, I’m going to assume that you want to use one of the many kinds of “glass-box” accurate and interpretable machine learning models available today, like monotonic gradient boosting machines in the open source frameworks h2o-3, LightGBM, and XGBoost. This article focuses on issue 2 and helping you communicate explainable machine learning results clearly to business decision-makers.This article is divided into two main parts. The first part addresses explainable machine learning summaries for a machine learning system and an entire dataset (i.e. “global” explanations). The second part of the article discusses summaries for machine learning system decisions about specific people in a dataset (i.e. “local” explanations). Also, I’ll be using a straightforward example problem about predicting credit card payments to present concrete examples.General summariesTwo good ways, among many other options, to summarize a machine learning system for a group of customers, represented by an entire dataset, are variable importance charts and surrogate decision trees. Now, because I want business people to care about and understand my results, I’m going to call those two things a “main drivers chart” and a “decision flowchart,” respectively.The main drivers chart provides a visual summary and ranking of which factors are most important to a machine learning system’s decisions, in general. It’s a high-level summary and decent place to start communicating about how a machine learning system works.In the example problem, I’m trying to predict missed credit card payments in September, given payment statuses, payment amounts, and bill amounts from the previous six months. What Figure 2 tells me is that, for the machine learning system I’ve constructed, the previous month’s repayment status is by far the most important factor for most customers in my dataset. I can also see that July and June repayment statuses are the next most important factors.Figure 2: Main drivers of model decisions about missing credit card payments in September for an entire dataset of credit card customers.How did I make this chart? It’s just a slightly modified version of a traditional variable importance chart. To make sure the displayed information is as accurate as possible, I chose an interpretable model and matched it with a reliable variable importance calculation.When I know my results are mathematically solid, then I think about the presentation. In this case, first I removed all numbers from this chart. While numeric variable importance values might be meaningful to data scientists, most business people don’t have time to care about numbers that aren’t related to their business. I also replaced raw variable names with directly meaningful data labels, because no business person actually wants to think about my database schema.Once I’ve summarized my system in an understandable chart, I can go to my business partners and ask really important questions like: Is my system putting too much emphasis on August repayment status? Or, does it make sense to weigh April payment amount more than August payment amount? In my experience, accounting for those kinds of domain knowledge insights in my machine learning systems leads to the best technical and business outcomes.The decision flowchart shows how predictive factors work together to drive decisions in my machine learning system and Figure 3 boils my entire machine learning system down to one flowchart!Figure 3: A flowchart showing roughly how a complex model makes decisions about missing credit card payments in September for an entire dataset of credit card customers.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesHow did I summarize an entire machine learning system into a flowchart? I used an old data mining trick known as a surrogate model. Surrogate models are simple models of complex models. In this case my simple model is a decision tree, or a data-derived flowchart, and my complex model is the input factors and decisions of my machine learning system. So, the decision flow chart is simple machine learning on more complex machine learning.Unfortunately, this trick is not guaranteed to work every time. Sometimes machine learning systems are just too sophisticated to be accurately represented by a simple model. So a key consideration for data scientists when creating a chart like Figure 3 is: How accurate and stable is my decision tree surrogate model? On the business side, if a data scientist shows you a chart like Figure 3, you should challenge them to prove that it is an accurate and stable representation of the machine learning system.If you want to make a decision flow chart, remember to try to limit the complexity of the underlying machine learning system, keep your flow chart to a depth of say three to five decisions (Figure 3 uses a depth of three), and use human-readable data formats and not your favorite label encoder.Specific summariesIf you work in financial services, you probably get that sometimes each individual machine learning system decision for each customer has to be explained or summarized. For the rest of the data science world, explaining individual machine learning system decisions might not be a regulatory requirement, but I would argue it’s a best practice. And regulations are likely on the way. Why not get ready?No one wants to be told “computer says no,” especially when the computer is wrong. So, consumer-level explanations are important for data scientists, who might want to override or debug bad machine learning behavior, and for consumers, who deserve to be able to appeal wrong decisions that affect them negatively.I’m going to focus on two types of explanations that summarize machine learning systems decisions for specific people, Shapley values (like in Figure 2) and counterfactual explanations. Since data science jargon isn’t helpful in this context, I’m going to call these two approaches main decision drivers (again) and “counter-examples.” Also, keep in mind there are many other options for creating consumer-specific explanations.   Shapley values can be used to summarize a machine learning system for an entire dataset (Figure 2) or at the individual decision level (Figure 4). When you use the right underlying machine learning and Shapley algorithm, these individual summaries can be highly accurate.Where I think most data scientists go wrong with Shapley values is in explaining them to business partners. My advice is don’t ever use equations, and probably don’t use charts or tables either. Just write out the elegant Shapley value interpretation in plain English (or whatever language you prefer). To see this approach in action, check out Figure 4. It displays the three most important drivers for a decision about a customer that my machine learning system has decided is at an above average risk for missing their September payment.Counter-examples explain what a customer could do differently to receive a different outcome from a machine learning system. You can sometimes use software libraries to create counter-examples or you can use trial and error, changing the inputs to a machine learning system, and observing the change in the system outputs, to create your own counter-examples. It turns out that for the high-risk customer portrayed in Figure 5, if they had made their recent payments on time, instead of being late, my machine learning system would put them at a much lower risk for missing their upcoming September payment.Once you can see the logic and data points behind how a machine learning system makes a given decision, it becomes much easier for data scientists to catch and fix bad data or wrong decisions. It also becomes much easier for customers interacting with machine learning systems to catch and appeal the same kinds of wrong data or decisions.These kinds of explanations are also potentially useful for compliance with regulations like the Equal Credit Opportunity Act (ECOA) and the Fair Credit Reporting Act (FCRA) in the United States, and the General Data Protection Regulation (GDPR) in the European Union. Responsible machine learningThe myriad risks of depending on a system you don’t understand are major barriers to the adoption of AI and machine learning in the business world. When you can break down those barriers, that’s a big step forward. Hopefully you’ll find the techniques I present here useful for just that, but do be careful. Aside from the accuracy and communication concerns I’ve already brought up, there are some security and privacy concerns with explainable ML too. Keep up with advances in machine learning, AI, and big data analytics with InfoWorld’s Machine Learning and Analytics Report newsletterAlso, explainability is just one part of mitigating machine learning risks. A machine learning system could be totally transparent and still discriminate against certain groups of people, or could be both transparent and wildly unstable when deployed to make decisions using real-world data. For these reasons and more, it’s always a good idea to consider privacy, security, and discrimination risks, and not just GPUs and Python code, when white-boarding a machine learning system.All of this leads me into the responsible practice of machine learning, but that’s food for thought for my next article. Suffice to say, communicating machine learning system outcomes is incumbent on all data scientists in today’s data-driven world, and explaining AI and machine learning to a business decision maker is becoming more of a possibility with the right approach and right technology. —1. Other great options for interpretable models include elastic net regression, explainable neural networks (XNN), GA2M, and certifiably optimal rule lists (CORELS).2. I recommend monotonic gradient boosting machines plus TreeSHAP to generate accurate summaries.3. Decision tree surrogates go back to at least 1996, but alternative methods have been put forward in recent years as well.4. Governments of at least Canada, Germany, the Netherlands, Singapore, the United Kingdom, and the United States have proposed or enacted ML-specific regulatory guidance.  5. Like cleverhans or foolbox.6. Risks of model extraction and inversion attacks and membership inference attacks are all generally exacerbated by presenting explanations along with predictions to consumers of ML-based decisions.7. For a more thorough technical discussion of responsible machine learning see: “A Responsible Machine Learning Workflow.”", "pub_date": "2020-03-19"},
{"title": "Amazon Braket: Get started with quantum computing", "overview": "Amazon’s quantum computing service is currently good for learning about quantum computing and developing NISQ-regime quantum algorithms, but stay tuned", "image_url": "https://images.idgesg.net/images/article/2019/12/bloch_sphere_representation_of_qubit_fundamental_building_block_of_quantum_computers_by_glosser-ca_cc_by-sa_3-0_futuristic_technology_controls_quantum_by_thedigitalartist_aka_pete_linforth_cc0_via_pixabay-100820884-large.jpg", "url": "https://www.infoworld.com/article/3573372/amazon-braket-get-started-with-quantum-computing.html", "body": "While IBM, Microsoft, and Google have made major commitments and investments in quantum computing, Amazon has, until recently, been fairly quiet about the field. That changed with the introduction of Amazon Braket.Amazon still isn’t trying to build its own quantum computers, but with Braket it is making other companies’ quantum computers available to cloud users via AWS. Braket currently supports three quantum computing services, from D-Wave, IonQ, and Rigetti.Also on InfoWorld: A hands-on look at the Microsoft Quantum Development Kit and IBM Q and Qiskit quantum computing SDKsD-Wave makes superconducting quantum annealers, which are usually programmed using D-Wave Ocean software, although there is also an annealing module in the Braket SDK. IonQ makes trapped ion quantum processors, and Rigetti makes superconducting quantum processors. In Braket, you can program both IonQ and Rigetti processors using the Braket Python SDK circuits module. The same code also runs on local and hosted quantum simulators.The name Braket is kind of an in-joke for physicists. Bra-ket notation is the Dirac formulation of quantum mechanics, which is an easier way of expressing Schrödinger’s equation than partial differential equations. In Dirac notation, a bra  is a row vector, and a ket  is a column vector. Writing a bra next to a ket implies matrix multiplication.Amazon Braket and the Braket Python SDK compete with IBM Q and Qiskit, Azure Quantum and Microsoft Q#, and Google Cirq. IBM already has its own quantum computers and simulators available to the public online. Microsoft’s simulator is generally available, but its quantum offerings are currently in limited preview for early adopters, including access to quantum computers from Honeywell, IonQ, and Quantum Circuits, and optimization solutions from 1QBit. Microsoft hasn’t announced when its own topological superconducting quantum computers will become available, nor has Google announced when it will make its quantum computers or Sycamore chips available to the public.Amazon Braket overviewAmazon Braket is a fully managed service that helps you get started with quantum computing. It has three modules, Build, Test, and Run. The Build module centers around managed Jupyter notebooks pre-configured with sample algorithms, resources, and developer tools, including the Amazon Braket SDK. The Test module provides access to managed, high-performance, quantum circuit simulators. The Run module provides secure, on-demand access to different types of quantum computers (QPUs): gate-based quantum computers from IonQ and Rigetti, and a quantum annealer from D-Wave.Tasks may not run immediately on the QPU. The QPUs only execute tasks during execution windows.Amazon Braket SDK APIThe Braket Python SDK defines all of the operations you need to build, test, and run quantum circuits and annealers. It is organized into five packages: braket.annealing, braket.aws, braket.circuits, braket.devices, and braket.tasks.The braket.annealing package allows you to define two kinds of binary quadratic models (BQMs): Ising (a mathematical model of ferromagnetism in statistical mechanics, using magnetic dipole moments of atomic “spins”) and QUBO (Quadratic Unconstrained Binary Optimization) problems, to solve on a quantum annealer, such as a D-Wave unit. The braket.circuits package lets you define quantum circuits based on a set of gates, to solve on gate-based quantum computers, such as ones from IonQ and Rigetti.The other three packages control the running of your problem. The braket.aws package allows you to select quantum devices, load problems into tasks, and connect tasks to AWS sessions. The braket.devices package lets you run tasks on quantum devices and simulators. The braket.tasks package lets you manage, track, cancel, and get results from quantum tasks.Amazon Braket circuits and gatesCircuits in a quantum computer such as the ones from IonQ or Rigetti (or IBM or Honeywell, for that matter) are built from a standard set of gates (see figure below), although not every QPU may have an implementation of every kind of gate. In the Braket SDK you define a circuit using the  method from the braket.circuits package, qualified by the gates in the circuit and their parameters.For example, this Braket code (from Amazon’s Deep_dive_into_the_anatomy_of_quantum_circuits example) defines a circuit that initializes four qubits to a Hadamard (equal probability of 1 and 0) state, then entangles qubit 2 with qubit 0 and qubit 3 with qubit 1 using Controlled Not operations.The Braket SDK seems to have a nearly full set of quantum logic gates, as shown in this enumeration of the  class. I don’t see a Deutsch gate listed, but as far as I know it hasn’t yet been implemented on a real QPU.Rxtreme(CC BY-SA 4.0)D-Wave OceanOcean is the native Python-based software stack for D-Wave quantum annealers. For use via Braket, you can combine the Ocean software with the Amazon Braket Ocean plug-in, which translates between Ocean and Braket formats.Also on InfoWorld: Cloud tech certifications count more than degrees nowQuantum annealers function quite differently than gate-based QPUs. Essentially, you formulate your problem as a binary quadratic model (BQM) that has a global minimum at the solution you want to find. Then you use the annealer to sample the function many times (since the annealer isn’t perfect) to find the minimum. You can create the BQM for a given problem mathematically or generate the BQM using Ocean software. The code that follows, from Amazon’s D-Wave_Anatomy example, uses the Braket Ocean plug-in to solve a BQM on a D-Wave device.D-Wave SystemsTo use a quantum annealer, you need to formulate your problem as a quadratic objective function, or BQM (binary quadratic model). Then you sample the solution space to find the global minimum of the BQM.Enabling Amazon Braket and using notebooksBefore you can use Braket, you need to enable it in your AWS account.The Anatomy of Quantum Circuits example, one of the simpler tutorials, demonstrates running circuits on a simulator and controlling tasks.Then you need to create a notebook instance. Notebooks use Amazon SageMaker (read my review).Creating a Braket notebook instance is a matter of naming the notebook, picking an instance type, and either creating or reusing an IAM role with the correct permissions. Using an encryption key and VPC are both optional.When you open a notebook, you can enter new code or use one of Amazon’s examples.The Braket examples supplied by Amazon are divided into four categories: simple circuits, advanced circuits, hybrid, and annealing.You need to check the status of the QPU devices, as they aren’t always available.The Devices screen shows you the current status of each quantum processing unit. An hour after this screenshot, the D-Wave annealer was online and available, but the Rigetti QPU had a two-hour queue.While you can run them yourself, the Braket example notebooks have been saved with results from a previous run.The Anatomy of Quantum Circuits example, one of the simpler Braket tutorials, demonstrates running circuits on a simulator and controlling tasks.There are examples for both gate-based QPUs, as above, and quantum annealers, as below.The D-Wave anatomy example demonstrates how to perform quantum annealing, both with a simulator and with a D-Wave device. Block 20 in the example uses the D-Wave device, takes 1,000 samples, and performs an aggregate operation to simplify the output.Learn today, useful tomorrowAmazon Braket is a reasonable way to get your feet wet with quantum computers and simulators. Since we’re still in the NISQ (Noisy Intermediate Scale Quantum) phase of quantum computing, you can’t really expect useful results from Braket. We’ll need more qubits, less noise, and longer coherence times, all of which are being actively researched.Braket’s current QPU offerings are modest. The 2048-qubit D-Wave annealer is mostly useful for optimization problems; it’s about half the size of D-Wave’s latest-generation annealer. The 11-qubit IonQ QPU, which has relatively long coherence times, is  too small to implement the algorithms for quantum computers that should exhibit useful quantum supremacy, such as Grover’s algorithm for finding the inverse of a function and Shor’s algorithm for finding the prime factors of an integer. The 30-qubit Rigetti Aspen-8 is also too small.Braket is not free, although it is relatively cheap to use. By comparison, IBM Q is completely free, although the publicly available IBM QPUs are very small: they range from a 1 qubit QPU in Armonk to a 15-qubit QPU in Melbourne. IBM also offers a paid premium QPU service.Also on InfoWorld: Review: Amazon SageMaker plays catch-upIBM also rates its QPUs by their quantum volume (QV), a measure that combines the number of qubits with their error rate and coherence time. There are five-qubit IBM QPUs ranging from QV8 to QV64: higher is better. Honeywell has also announced achieving QV64.What Braket is currently good for is learning about quantum computing and developing NISQ-regime quantum algorithms. Stay tuned, though. As QPUs improve and are plugged into AWS, Braket will become more and more useful.— Managed notebooks: $0.04 to $34.27 per instance-hour; quantum simulator: $4.50 per hour; quantum computers: $0.30 per task plus $0.00019 to $0.01 per shot (repetition of a circuit).  AWS; installing the Braket SDK locally requires Python 3.7.2 or greater, and Git.", "pub_date": "2020-09-07"},
{"title": "Do developers really care about open source?", "overview": "If all a developer really wants is an API that’s open and a cost model that works, then maybe cloud is the best answer", "image_url": "https://images.idgesg.net/images/article/2019/05/cso_hands_forming_hearts_love_like_peace_empathy_collaboration_by_lucafabbian_gettyimages-494415321_background_by_marigold_88_gettyimages-494511111_3x2_2400x1600-100796441-large.jpg", "url": "https://www.infoworld.com/article/3572324/do-developers-really-care-about-open-source.html", "body": "FaunaDB founder Evan Weaver has a crazy thought. Even as open source projects like Linux and Kubernetes continue to thrive, he suggests that maybe, just maybe, “As long as you give developers an API that’s open and a cost model that works for them then they don’t care [about open source]. They just don’t want to operate anything.” Cloud, in other words, not code.It’s a bold thought, and not an unreasonable one. When I floated this idea with a range of industry heavyweights, however, they pushed back for a number of different reasons. Among them? Well, according to Guarav Gupta, an investor with Lightspeed (and former Elastic and Splunk product executive), “There is a deep amount of developer love and appreciation and almost like an addiction” for open source, something developers don’t feel for an API.Is there a way to have the convenience of APIs without losing developers’ sense of belonging for open source communities? The answer seems to be yes, but it’s a bit complicated to get there.Don’t forget the dataFor Aurélien Georget, co-founder and chief product officer at Strapi, which offers an open source headless CMS, one of the enduring draws of open source isn’t really about code, though sometimes that’s simply a necessity. For example, in Strapi’s case many customers want to heavily customize their CMS. In this instance, a cloud service doesn’t meet their needs. They need the code.Or even if they don’t want to tinker with the code, data drives them to it: “Our users aren’t interested in ownership of their code but of their data. For data privacy reasons, [or] sometimes from a legal point of view (e.g. banks, insurances, public administration, etc.),” they need to run their code — and keep their data — within their own data center. This isn’t to suggest that Weaver is wrong to insist on an API-centric approach, however: “Every solution should be API-oriented,” Georget concurs, as “It lets the developers be creative, imagine new use cases, and innovate.”Even so, Georget acknowledges, “Being independent and having 100 percent ownership of our data has a price.” It would perhaps be more convenient to use a cloud service but this simply isn’t always possible for certain classes of application or customer, he says.And then there’s the possibility, argues Patrick McFadin, chief evangelist at DataStax, which contributes code and operational expertise to the Apache Cassandra database community, that APIs can become a one-way door: “APIs that used to be open are getting locked down over time or put behind a paywall. The war for data is only going to get worse and the trend will be heavily proprietary.” Code, by contrast, can be given away because it’s “not the business” of most enterprises, including software vendors.Ben Bromhead, CTO at Instaclustr, might have the ideal compromise. Instaclustr runs open source software like Apache Kafka as a managed service. So long as a company builds with cloud services that closely adhere to open source standards, they never really lose their independence of code or data:Open source data-layer technologies guarantee companies full control over their own data and processes. By choosing 100 percent open source technologies, companies own their own code and maintain freedom from vendor or technical lock-in. But, more important than the code itself, true open source technologies ensure that companies’ critical information supply lines cannot be disrupted by the whims of entities that provide proprietary solutions, and might interfere with their ability to fully leverage their own data no matter what.In other words, perhaps it needn’t be a cloud  open source decision, but rather a cloud  open source choice.It’s about trustWhich brings us back to Gupta’s contention that there’s something different, and perhaps better, about open source. Even as Gupta acknowledges that “Ultimately people want to consume things as a [cloud] service,” he suggests that code is critical for forging real affinity — even affection — for technology. “There is a deep amount of developer love and appreciation and almost like an addiction to [an open source] product that you develop by being able to feel it and understand it and be part of a community.”Can you achieve this “cosmic closeness” with an API? Not really, Gupta argues. Communities, he says, are “a lot easier to build if you’re open source,” rather than a “black box cloud service with an API. People like Twilio, [but] do they love it?” He doesn’t answer his rhetorical question, but it seems to demand an unequivocal, “No!” Because, as Gupta goes on to suggest, “As a developer, you want to be part of a movement.”At the heart of such an open source movement is trust: open source, open roadmaps, open communication, open decision-making. This is where cloud vendors like Instaclustr arguably can successfully give customers what they want: the ease of cloud and the trust and control of open source. “Good open source companies build incredible amounts of trust,” says Gupta. Wise cloud companies do the same by not forfeiting the trust and affection born within open source communities.Open source has a people problemThe community hat rules the company hat in open sourceWhat is Google’s Open Usage Commons — and why?What does an open source maintainer do after burnout?Do we need so many databases?How GraphQL turned web development on its headHow Redis scratched an itch — and changed databases foreverWill the solo open source developer survive the pandemic?Open source projects take all kindsThe most important part of an open source projectGatsby JS stands on the shoulders of thousandsRemember when open source was fun?Open source made the cloud in its imageZeek and Jitsi: 2 open source projects we need nowWTH? OSS knows how to WFH IRLThe secret to Kubernetes’ successOpen source companies are thriving in the cloudOpen source should learn from Linux, not MySQLMaking open source JavaScript payCustomer-driven open source is the future of softwareMagical Magento figures out how to make and takeThe real number of open source developersWhy the Rust language is on the riseShould open source licenses fight evil?Should open source software advertise?Why open source has never been stronger", "pub_date": "2020-08-31"},
{"title": "Google CAMP helps you modernize your cloud apps", "overview": "The Google Cloud App Modernization Program offers best practices and tools for developing and delivering better apps faster in the Google Cloud", "image_url": "https://images.techhive.com/images/article/2015/08/digital_nomad_laptop_beach_camp_sunset-100599935-large.jpg", "url": "https://www.infoworld.com/article/3572925/google-camp-helps-you-modernize-your-cloud-apps.html", "body": "Google has launched a cloud application modernization effort, which the company said leverages its experience in driving application delivery at speed and scale.Unveiled August 25, the Google Cloud App Modernization Program (CAMP) Is intended to help large enterprises modernize application development and delivery and improve speed.Also on InfoWorld: Cloud tech certifications count more than degrees nowFacets of Google CAMP include:Solutions, recommendations, and best practices for application modernization. The application lifecycle is covered from writing code to running and securing applications. Practices include driving alignment between developers and operators, lean product development, and technical practices such as implementing loosely coupled architectures and continuous testing.Tailored modernization advice gained through a data-driven assessment. Regardless of whether a developer is building a Kubernetes, serverless, or mainframe application, the assessment shows where to start a modernization effort, identifies priorities and how to maximize ROI. Bottlenecks are found, as well.An extensible platform for writing code and running, securing, and operating applications. Existing Google Cloud Platform services are extended to help run legacy and new applications.Anthos, Google’s hybrid and multi-cloud modernization platform. Google on August 25 announced hybrid AI capabilities for Anthos, including general availability of Speech-to-Text On Prem and introduced Anthos attached clusters, for managing Kubernetes clusters.Google CAMP is based on the company’s experience in high-speed application delivery, with the company deploying 12 million builds and 650 million tests daily, along with processing 2.5 exabytes of logs monthly and parsing more than 14 quadrillion monitoring metrics. The program also reflects six years of devops research and assessment into practices to drive high performance, Google said. Google Cloud can be accessed at cloud.google.com.", "pub_date": "2020-08-31"},
{"title": "Dealing with sovereign data in the cloud", "overview": "International regulations make dealing with cloud-based data a nightmare. These fundamental concepts can help keep you from running afoul of data sovereignty laws    ", "image_url": "https://images.techhive.com/images/article/2015/09/lawsuit-judge-law-court-decision-sued-money-100614067-large.jpg", "url": "https://www.infoworld.com/article/3573568/dealing-with-sovereign-data-in-the-cloud.html", "body": "It’s Friday, and you’re about to close your laptop and go to happy hour, when you get an urgent e-mail stating that due to data being illegally transported out of the country, the company has been fined $250,000.  What happened?Also on InfoWorld: Cloud tech certifications count more than degrees nowAs bad luck would have it, your cloud provider backs up the database containing sovereign data to a storage system outside of the country. This is done for a good reason: Relocating data for business continuity and disaster recovery purposes to another region reduces the risk of data loss if the primary region is down. Of course, this is not the fault of the cloud provider. This is a common configuration error that occurs primarily because the cloudops team does not understand issues with laws and regulations around data. The database administrator may not have been aware it was happening. Lack of training led to this problem and the quarter million slap in the face.   Data sovereignty is more of a legal issue than a technical one. The idea is that data is subject to the laws of the nation where it’s collected and exists. Laws vary from country to country, but the most common governance you’ll see is not allowing some types of data to leave the country at any time. Other regulations enforce encryption and how the data is handled and by whom. These were pretty easy rules to follow when we had dedicated data centers in each country, but the use of public clouds that have regions and points-of-presence all over the world complicates things. Misconfigurations, lack of understanding, and just general screw-ups lead to fines, impacts to reputations, and, in some cases, disallowing the use of cloud computing altogether.   Some best practices are emerging to deal with data sovereignty in the cloud. Data governance systems are worth their weight in gold. When dealing with regulations that are bound to data, these systems will keep you out of trouble since they won’t allow humans to violate data policies that are set to reflect the law of the land where the data resides. Training is another critical point. Most of the data sovereignty issues can be traced to human error. Everyone handing the data should be knowledgeable on the regulations. Many countries mandate this.Take advantage of security systems that are purpose-built to deal with data sovereignty issues. Identity-based security systems can deal with special security needs based on the identity of data, encrypt the data per regulations, and also ensure that it’s not transmitted out of the country or stolen in other ways.  There’s no real magic bullet here. As countries get more particular about how their data is managed and the pervasive use of international public clouds continues, more issues are bound to arise. Enterprises are well advised to be proactive here, or else things can go sideways quickly.  ", "pub_date": "2020-09-01"},
{"title": "Kaggle calls data scientists to action on COVID-19", "overview": "In newest challenge, Kaggle asks AI researchers to apply machine learning tools and techniques to answering questions about COVID-19", "image_url": "https://images.techhive.com/images/article/2014/09/alert_warning_exclamation_point_thinkstock_177843422-100412486-large.jpg", "url": "https://www.infoworld.com/article/3533269/kaggle-calls-data-scientists-to-action-on-covid-19.html", "body": "Kaggle, an online community for data scientists and a platform for data science competitions, has unveiled a new and timely bounty-paying challenge: the COVID-19 Open Research Dataset Challenge, or CORD-19.CORD-19 asks AI and machine learning researchers to develop text and data mining tools to analyze a dataset comprising tens of thousands of articles on virology and infectious disease. The goal is to help provide answers for 10 tasks, or lines of inquiry about the disease.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesThe prize for each of the tasks in the CORD-19 challenge is $1,000, delivered as cash or as a charitable donation to reseach and relief efforts.The world isn’t lacking for research about COVID-19. Kaggle’s dataset contains “over 29,000 scholarly articles, including over 13,000 with full text, about COVID-19, SARS-CoV-2, and releated coronaviruses,” according to the challenge introduction. But there’s little time to paw manually through those haystacks of research for the needles we need, so Kaggle is encouraging the use of machine learning techniques like natural language processing to get relevant data into the right hands more quickly.The CORD-19 tasks revolve around common questions about COVID-19. Each of the high-level tasks (e.g., What do we know about COVID-19 risk factors?) includes a number of subtasks (e.g., What populations are more susceptible? What roles do smoking or pre-existing pulmonary diseases play?).Other COVID-19-related datasets are also available on Kaggle. These include a full RNA sequencing of the virus and details about previous infectious disease outbreaks like Ebola and SARS.Keep up with advances in machine learning, AI, and big data analytics with InfoWorld’s Machine Learning and Analytics Report newsletterPrevious Kaggle challenges related to medical science featured projects with longer and less urgent time frames, such as devising better ways to screen for cervical cancer. Because the COVID-19 outbreak requires answers immediately, the Kaggle community is facing its first major test in real time.", "pub_date": "2020-03-18"},
{"title": "Higher ed needs a course in cloud computing", "overview": "To survive in a post-pandemic world, most higher education institutions need cloud computing to enable agile learning models", "image_url": "https://images.idgesg.net/images/article/2018/05/skills_programmer_data-scientist_graduate_school_college_certification_mortar-board_valedictorian_cole-keister-via-unsplash-100759473-large.jpg", "url": "https://www.infoworld.com/article/3573751/higher-ed-needs-a-course-in-cloud-computing.html", "body": "Four-year colleges may face up to a 20 percent reduction in fall enrollment, according to SimpsonScarborough, a higher education research and marketing company. Its findings are based on surveys of more than 2,000 college-bound high school seniors and current college students. According to the Chronicle of Higher Education, nine percent of institutions plan to operate only online during the fall semester. The remainder will take a hybrid approach, with some classes offered online, while other classes will operate both online and in-person. Also on InfoWorld: Cloud tech certifications count more than degrees nowSome of the realities that colleges and universities now face include:A large percentage of students who won’t start or continue their higher education until the pandemic is largely behind us, as reflected in the study cited above.The movement to remote learning means that colleges will have much more direct competition. The higher education systems will be viewed as a commodity, with students able to select the college with the best results, not the one nearest to home.Students will demand reduced tuition because they will not use the college facilities, dormitories, or even interact directly with professors.The most appealing higher education choices will be those that deliver a better digital experience for the student, not those that have more traditional attributes.How does this relate to cloud computing technology as a tool to save many of these colleges and universities? Clearly, it’s the fastest path to a digitally enabled future. However, the more important role of cloud is its ability to leverage existing educational delivery systems that are purpose-built to quickly enable an institution to move into remote learning.For example, look at the movement to cloud-based student information systems. These systems register students for classes, document grades, deal with transcripts, and record student tests and other assessment scores. They can also build student schedules, track attendance, and manage many other student-related data needs. If this seems like something the brick and mortar institutions already have in place, that’s a sound assumption. However, they don’t always have the capabilities to deal with remote learning systems, including integration with content, usage, and tracking student progress. Without this integration, it’s unlikely that colleges will be successful in their move to digital learning. This is not about existing universities keeping up with the new world during and after the pandemic. This is more about the ability to take advantage of a technology (specifically, cloud computing) as a force multiplier to provide a better education per dollar spent. Here are a couple of emerging opportunities in this arena:Machine learning-based systems can monitor and enhance the learning process for each student. They can provide deep analysis of learning activities, change learning paths on the fly to better accommodate the needs of the student, and dynamically monitor student well-being.Actual changes to courses, authors, and instructors has always dragged far behind feedback. The larger issues are how to deal with the more systemic changes that need to occur at the same time. Institutions need to consider culture, the way the institution sells itself, the differing student personas, and—what might be a new factor for some institutions or instructors—students who never set foot on the campus because they are 100 percent remote. The end goal is to use technology to meet students’ learning expectations. The most innovative institutions will go well beyond expectations to improve the educational experience as compared to traditional educational operations just last year. This paradigm shift has been on higher education’s horizon for decades. Remote learning has always been around. Thirty years ago, personal computers made it easier.  Five to ten years ago, cloud made it more feasible. The pandemic simply pushed this shift to the top of the to-do lists within most higher education institutions.", "pub_date": "2020-09-04"},
{"title": "A reference architecture for multicloud", "overview": "Complexity is the order of the day for multicloud. Here's my vision of how all the pieces fit together", "image_url": "https://images.idgesg.net/images/article/2020/08/businessman_stands_in_front_of_maze_strewn_with_clouds_uncertainty_uncertain_future_strategy_challenges_by_gremlin_gettyimages-1171422635_cso_2400x1600-100854062-large.jpg", "url": "https://www.infoworld.com/article/3570619/a-reference-architecture-for-multicloud.html", "body": "Today’s definition of multicloud depends on who you ask. This is not a new trend. Whenever a new technology gets some hype, suddenly there is semantic confusion. My definition is a bit more holistic than most: Multicloud is a collection of public clouds, private clouds, and legacy systems (yes, I went there). Two or more public or private clouds must be present to be true multicloud. Legacy systems are optional but also good to include.InfoWorld’s 2020 Technology of the Year Award winners: The best software development, cloud computing, data analytics, and machine learning products of the yearA charted representation of my definition looks like this: Something I’ve learned over the years is that frameworks of any kind have to be generic and applicable to different problem domains. My representation includes most of what you’ll find in common multicloud deployments I see today. If this seems complex, that’s the point. If the goal is to get a healthy multicloud deployment up and running, you’ll need most of these services. That list includes everything from cost governance, storage management, and cloud service brokers, as well as orchestration and process management—and those skills require qualified talent.You’ll need a few more things, depending on your industry, such as compliance management. Figure on at least 20 percent to 30 percent more services to ultimately meet your business needs.Note that my multicloud definition includes two core meta-layers:  deals with everything that’s stored inside and outside of the public clouds. Cloud-native databases exist here, as do legacy databases that still remain on-premises. The idea is to manage these systems using common layers, such as management and monitoring, security, and abstraction.  means that we deal with behavior/services and the data bound to those services from the lower layers of the architecture. It’s pretty much the same general idea as data-focused multicloud, in that we develop and manage services using common layers of technology that span from the clouds back to the enterprise data center.Of course, there is much more to both layers. Remember that the objective is to remove humans from having to deal with the cloud and noncloud complexity using automation and other approaches. This is the core objective of multicloud complexity management, and it seems to be growing in popularity as a rising number of enterprises get bogged down by manual ops processes and traditional tools.Also note that this diagram depicts a multicloud that has very little to do with clouds, as I covered a few weeks ago. Indeed, the clouds become the source of the services and data, but the tougher problem is how you build systems in and between the public clouds, private clouds, and traditional data centers that can manage how both data and services are leveraged by the applications, depicted at the top of the diagram.This is a new paradigm for staff who have only dealt with cloud-native services for the past several years. Now we must configure architectures that span technologies and create a more valuable layer to help build and deploy business solutions. This requires a step to define exactly what that architecture looks like at each level. ", "pub_date": "2020-08-14"},
{"title": "The distributed cloud era has arrived ", "overview": "The rise of distributed clouds and app-to-app networking dictates the use of cloud-native network and security infrastructure ", "image_url": "https://images.idgesg.net/images/article/2019/07/cso_nw_cloud_computing_distributed_decentralized_network_connections_iot_internet_of_things_thinkstock_853701240_3x2_1500x1000-100801371-large.jpg", "url": "https://www.infoworld.com/article/3573776/the-distributed-cloud-era-has-arrived.html", "body": "The term “cloud” has been evolving ever since the term was first used in the early 1990s. One could argue that Cloud 1.0 was really nothing more than a euphemism for hosted services. Hosted services gave companies the ability to run critical apps off their premises in a highly secure, predictable environment. This value proposition continued with the future rise of services like AWS and Microsoft Azure, where businesses would “lift and shift” legacy apps and drop them into a cloud. Cloud 2.0 gave rise to web-optimized apps. In Cloud 2.0 the apps were truly built for the cloud and spawned companies that made the cloud their primary compute platform. However, this cloud strategy revolved around a single cloud provider and traditional monolithic app architectures. Even companies that used multiple clouds built app A on one cloud, app B on another, etc. In this case, multicloud was actually multiple clouds being used as discrete, independent infrastructure entities.Also on InfoWorld: Cloud tech certifications count more than degrees nowWe have now entered the Cloud 3.0 era, which can be thought of as multicloud on steroids. The rise of microservices and containers has allowed app developers to build apps by accessing services from multiple cloud providers. Most modern, cloud-native apps are being built this way. Edge computing is on the horizon, which will create more locations for app developers to extend access to data and app services. This is the concept of the distributed cloud, where the cloud is no longer a single location, but a set of distributed resources.Distributed cloud changes app deliveryThe evolution of the cloud – and cloud-native apps as a result – has had a profound impact on the networking and security services required to connect app components to other app components and to connect apps to users. With Cloud 1.0, IT professionals used physical appliances such as load balancers or application delivery controllers and web app firewalls. These were installed in the same data centers that hosted the app infrastructure. With Cloud 2.0, these functions were virtualized and installed as a cloud resource. The network and app architectures were largely the same, but the infrastructure shifted to cloud-resident virtual appliances.>>With distributed cloud (Cloud 3.0), app components (e.g. microservices) are modular and reside in containers across multiple clusters. This creates significant deployment and operational challenges for DevOps teams and IT professionals. The dynamic and distributed nature of containerized workloads and microservices can’t be supported by the physical or even virtual infrastructure traditionally used for monolithic apps, which rely on centralized control and visibility, as opposed to the dynamic operational model needed for highly distributed clusters and workloads. A containerized workload can be spun up and deprecated in a matter of minutes, even seconds. This means the supporting network and security infrastructure like load balancers, web app firewalls, and API gateways need to be spun up and down just as quickly. Meeting the operational challenges of distributed cloudThis week cloud-native app infrastructure provider, Volterra, announced the latest release of its VoltMesh service, which addresses many of the operational challenges associated with deploying and operating modern apps in a distributed cloud model. The company offers a wide range of what would typically be known as “application delivery services” from the cloud, made available as a single SaaS-based offering.Instead of having to deploy multiple virtual appliances per cluster, across multiple clusters, VoltMesh offers an integrated service stack that can be easily deployed across a distributed cluster with centralized management, end-to-end visibility, and policy control. By leveraging the speed and ubiquity of a SaaS-based service, DevOps can speed up the deployment of distributed cloud-native applications and simplify ongoing operations, while developers can achieve better code integration and agility as they have greater freedom of where and how to develop. VoltMesh can be deployed in clusters in every major cloud provider as well as private clouds and edge sites.  Volterra also offers its own application delivery network (ADN) to improve the performance and security of cloud-native apps – hosting them directly on Volterra’s private network and closer to end users.The cloud-native infrastructure approach Volterra is taking is for more than just speed though, as there are major security implications. As app developers shift to building cloud-native apps on microservices, new threats are emerging from embedded and unseen APIs. The number of APIs per app has exploded, creating more intra-application traffic. Security approaches like segmentation, even micro-segmentation, operate at the network layer – so they’re blind at the API layer. This means DevOps and DevSecOps teams must shift the focus of their zero-trust model from the network to the API layer, including the ability to “see” all APIs and then enforce policies on them.Also on InfoWorld: 6 ways Alibaba Cloud challenges AWS, Azure, and GCPAs part of this release, VoltMesh has unveiled its new API auto-discovery capability that can automatically find all APIs in an application through the use of its machine learning engine. It then automatically applies policies to whitelist only the APIs that are required and validated, rendering any unneeded or unsafe ones ineffective. This shrinks the attack surface significantly, increasing protection and compliance without delaying app release cycles.The concept of multicloud (or multiple clouds) plus monolithic apps is rapidly giving way to distributed cloud plus distributed apps, and this will change the way we provision networking and security infrastructure for them. Traditional application delivery infrastructure needs to evolve to be cloud-native and distributed if DevOps and NetOps teams hope to keep up with the agility of developers building modern apps today.", "pub_date": "2020-09-03"},
{"title": "9 ways to build privacy into your cloud applications", "overview": "Look to these strategies to balance privacy with functionality and protect your applications and data against attacks in the cloud", "image_url": "https://images.techhive.com/images/article/2015/10/cloud-security-ts-100622309-large.jpg", "url": "https://www.infoworld.com/article/3572600/9-ways-to-build-privacy-into-your-cloud-applications.html", "body": "Privacy is one of those nebulous ideas that everyone loves. Delivering it, though, is a job that’s full of nuance and tradeoffs. Turn the dial too far to one side and the databases are useless. Turn it too far in the other direction and everyone is upset about your plan to install camera arrays in their shower to automatically reorder soap.The good news is that there is a dial to turn. In the early days, everyone assumed that there was just a switch. One position delivered all of the wonderful magic of email, online ordering, and smartphones. The other position was the cash-only world of living off the grid in a cabin wearing an aluminum foil hat.Also on InfoWorld: 6 ways Alibaba Cloud challenges AWS, Azure, and GCPPrivacy enhancing technologies let you control how much privacy to support but limit that control to preserve functionality. They mix in encryption functions with clever algorithms to build databases that can answer some questions correctly — but only for the right people.In my book, Translucent Databases, I explored building a babysitter scheduling service that could let parents book babysitters without storing personal information in the central database. The parents and babysitters could get the correct answer from the database, but any attacker or insider with root privileges would get only scrambled noise.The field has grown dramatically over the years and there are now a number of approaches and strategies that do a good job of protecting many facets of our personal lives. They store just enough information for businesses to deliver products while avoiding some of the obvious dangers that can appear if hackers or insiders gain access.The approaches all have their limits. They will defend against the most general attacks but some start to crumble if the attackers are better equipped or the attacks are more targeted. Often the amount of protection is proportional to the amount of computation power required for the encryption calculations. Basic protections may not add noticeable extra load to the system, but providing perfect security may be out of reach for even the cloud companies.But these limits shouldn’t stop us from adding the basic protections. The perfectly secure approach may not be out there, but adding some of these simpler solutions can protect everyone against some of the worst attacks that can be enabled by the new cloud services.Here are nine strategies for balancing privacy with functionality. Use the featuresThe cloud providers understand that customers are nervous about security and they’ve slowly added features that make it easier to lock up your data. Amazon, for instance, offers more than two dozen products that help add security. The AWS Firewall Manager helps make sure the firewalls let in only the right packets. AWS Macie will scan your data looking for sensitive data that’s too open. Google Cloud and Microsoft Azure have their own collections of security tools. Understanding all of these products may take a team but it’s the best place to start securing your cloud work.Watch the secretsSecuring the passwords, encryption keys, and authentication parameters is hard enough when we’re just locking down our desktops. It’s much trickier with cloud machines, especially when they’re managed by a team. A variety of different tools are designed to help. You’ve still got to be careful with source code management, but the tools will help juggle the secrets so they can be added to the cloud machines safely. Tools like Hashicorp’s Vault,  Doppler’s Enclave,  AWS’s Key Management System, and Okta’s API management tools are just some of the options that simplify the process. All still require some care but they are better than writing down passwords in a little notebook and locking it in someone’s office.Consider dedicated hardwareIt’s hard to know how paranoid to be about sharing computer hardware with others. It’s hard to believe that an attacker may finagle a way to share the right machine and then exploit some of the different extreme approaches like rowhammer, but some data might be worth the hard work. The cloud companies offer dedicated hardware just for occasions like this. If your computing load is fairly constant, it may even make economic sense to use local servers in your own building. Some embrace the cloud company’s hybrid tools and others want to set up their own machines. In any case, taking complete control of a computer is more expensive than sharing, but it rules out many attacks.HashingOne of the simplest solutions is to use a one-way function to hide personal information. These mathematical functions are designed to be easy to compute but practically impossible to reverse. If you replace someone’s name with , someone browsing the database will only see the random encrypted noise that comes out of the one-way function.This data may be inscrutable to casual browsers, but it can still be useful. If you want to search for Bob’s records, you can compute  and use this scrambled value in your query. This approach is secure against casual browsers who may find an interesting row in a database and try to unscramble the value of . It won’t stop targeted browsing by attackers who know they are looking for Bob. More sophisticated approaches can add more layers of protection.The most common one-way functions may be the Secure Hash Algorithm or SHA, a collection of functions approved by the US National Institute of Standards and Technology. There are several different versions, and some weaknesses have been found in the earlier versions, so make sure you use a new one. Also on InfoWorld: 17 clever APIs for every developer whimPure encryptionGood encryption functions are built into many layers of the operating system and file system. Activating them is a good way to add some basic security against low-level attackers and people who might gain physical access to your device. If you’re storing data on your laptop, keeping it encrypted saves some of the worry if you lose the machine.Regular encryption functions, though, are not one-way. There’s a way to unscramble the data. Choosing regular encryption is often unavoidable because you’re planning on using the data, but it leaves another pathway for the attackers. If you can apply the right key to unscramble the data, they can find a copy of that key and deploy it too.  Make sure you read the section above about guarding secrets.Fake dataWhile some complain about “fake news” corrupting the world, fake data has the potential to protect us. Instead of opening up the real data set to partners or insiders who need to use it for projects like AI training or planning, some developers are creating fake versions of the data that have many of the same statistical properties.RTI, for instance, created a fake version of the US Census complete with more than 110 million households holding more than 300 million people. There’s no personal information of real Americans but the 300 million fake people are more or less in the same parts of the country and their personal details are pretty close to the real information. Researchers predicting the path of infectious diseases were able to study the US without access to real personal data.An AI company, Hazy, is delivering a Python-based tool that will run inside secure data centers and produce synthetic versions of your data that you can share more freely.Differential privacyThe term describes a general approach to adding just enough noise to the data to protect the private information in the data set while still leaving enough information to be useful. Adding or subtracting a few years to everyone’s age at random, for instance, will hide the exact birth years of the people but the average won’t be affected.The approach is most useful for larger statistical work that studies groups in aggregate. The individual entries may be corrupted by noise, but the overall results are still accurate.Microsoft has started sharing White Noise, an open source tool built with Rust and Python, for adding a finely tuned amount of noise to your SQL queries.Homomorphic encryptionMost encryption algorithms scramble the data so completely that no one can make any sense of the results without the proper key. Homomorphic approaches use a more sophisticated framework so that many basic arithmetic operations can be done on the encrypted data without the key. You can add or multiply without knowing the underlying information itself.The simplest schemes are practical but limited. Chapter 14 of Translucent Databases describes simple accounting tools that can, for instance, support addition but not multiplication. More complete solutions can compute more arbitrary functions, but only after much more expensive encryption.IBM is now sharing an open source toolkit for embedding homomorphic encryption in iOS and MacOS applications with the promise that versions for Linux and Android will be coming soon. The tools are preliminary, but they offer the ability to explore calculations as complicated as training a machine learning model without access to the unencrypted data.Also on InfoWorld: 5 reasons we don’t write code like we used toKeep nothingProgrammers may be packrats who keep data around in case it can be useful for debugging later. One of the simplest solutions is to design your algorithms to be as stateless and log-free as possible. Once the debugging is done, quit filling up the disk drives with lots of information. Just return the results and stop.Keeping as little information as possible has dangers. It’s harder to detect abuse or fix errors. But on the flip side, you don’t need to worry about attackers gaining access to this digital flotsam and jetsam. They can’t attack anyone’s personal data if it doesn’t exist.", "pub_date": "2020-08-31"},
{"title": "Review: AWS Bottlerocket vs. Google Container-Optimized OS", "overview": "Container-Optimized OS offers tremendous advantages for container workloads on Google Cloud Platform, but Bottlerocket is a better choice everywhere else", "image_url": "https://images.idgesg.net/images/article/2020/01/container-100828616-large.jpg", "url": "https://www.infoworld.com/article/3570158/review-aws-bottlerocket-vs-google-container-optimized-os.html", "body": "Running large numbers of containers to deploy an application requires a rethink of the role of the operating system. Google’s Container-Optimized OS and AWS’s Bottlerocket take the traditional virtualization paradigm and apply it to the operating system, with containers the virtual OS and a minimal Linux fulfilling the role of the hypervisor.Various flavors of Linux optimized for containers have been around for a few years and have evolved ever smaller footprints as the management and user-land utilities moved to the  cluster management layer or to containers. These container-optimized operating systems are ideal when you need to run applications in Kubernetes with minimal setup and do not want to worry about security or updates, or want OS support from your cloud provider.Also on InfoWorld: 13 ways Google Cloud beats AWSContainer OSs solve several issues commonly encountered when running large container clusters, such as keeping up with OS vulnerabilities and patching potentially hundreds of instances, updating packages while dealing with potentially conflicting dependencies, degraded performance from a large dependency tree, and other OS headaches. The job is challenging enough with a few racks of servers and nearly impossible without infrastructure support when managing thousands.AWS BottlerocketBottlerocket is purpose-built for hosting containers in Amazon infrastructure. It runs natively in Amazon Elastic Kubernetes Service (EKS), AWS Fargate, and Amazon Elastic Container Service (ECS).Bottlerocket is essentially a Linux 5.4 kernel with just enough added from the user-land utilities to run containerd. Written primarily in Rust, Bottlerocket is optimized for running both Docker and Open Container Initiative (OCI) images. There’s nothing that limits Bottlerocket to EKS, Fargate, ECS, or even AWS. Bottlerocket is a self-contained container OS and will be familiar to anyone using Red Hat flavors of Linux.Bottlerocket integrates with container orchestrators such as Amazon EKS to manage and orchestrate updates, and support for other orchestrators can be adding by building variants of the operating system to add the necessary orchestration agents or custom components to the build.Bottlerocket’s approach to security is to minimize the attack surface to protect against outside attackers, minimize the impact that a vulnerability would have on the system, and provide inter-container isolation. To isolate containers, Bottlerocket uses container control groups (cgroups) and kernel namespaces for isolation between containers running on the system. eBPF (enhanced Berkeley Packet Filter) is used to further isolate containers and to verify container code that requires low-level system access. The eBPF secure mode prohibits pointer arithmetic, traces I/O, and restricts the kernel functions the container has access to.The attack surface is reduced by running all services in containers. While a container might be compromised, it’s less likely the entire system will be breached, due to container isolation. Updates are automatically applied when running the Amazon-supplied edition of Bottlerocket via a Kubernetes operator that comes installed with the OS. An immutable root filesystem, which creates a hash of the root filesystem blocks and relies on a verified boot path using dm-verity, ensures that the system binaries haven’t been tampered with. The configuration is stateless and /etc/ is mounted on a RAM disk. When running on AWS, configuration is accomplished with the API and these settings are persisted across reboots, as they come from file templates within the AWS infrastructure. You can also configure network and storage using custom containers that implement the CNI and CSI specifications and deploy them along with other daemons via the Kubernetes controllers.SELinux is enabled by default, with no way to disable it. Normally that might be a problem, but in the container OS use case relaxing this requirement isn’t necessary. The goal is to prevent modification of settings or containers by other OS components or containers. This security feature is a work in progress.The Bottlerocket build system is based on Rust, which is fine considering there’s nothing to build except for support for Docker and Kubernetes. Rust just broke into the top 20 programming languages and seems to be gaining traction due to its C++ like syntax and automatic memory management. Rust is licensed under the MIT or Apache 2 license. Amazon does a good job of leveraging GitHub for their development platform, making it easy for developers to get involved. The toolchain and code workflow will be familiar to any developer, and by design end users are encouraged to create variants of the OS. This is to cater to support for multiple orchestration agents. In order to keep the OS footprint as small as possible, each Bottlerocket variant runs on a specific orchestration plane. Amazon includes variants for Kubernetes and local development builds. You could, for example, create your own update operator or your own control container by changing the URL of the container.Bottlerocket isn’t intended to be managed with a shell. Indeed, there is little of the OS that requires management, and what is required is accomplished by the HTTP API, the command-line client (eksctl), or the web console.To update you need to deploy an update container onto the instance. See the bottlerocket-update-operator (a Kubernetes operator) on GitHub. Bottlerocket accomplishes single-step updates using the “two partition pattern,” where the image has two bootable partitions on disk. Once an update has been successfully written to the inactive partition, the priority bits in the GUID partition table of each partition are swapped and the “active” and “inactive” partitions roles are reversed. Upon reboot, the system is upgraded, or, in the event of an error, rolled back to the last known-good image. There are no packages that can be installed, only containers, and updates are image based, as in NanoBSD and other embedded operating systems. The reason behind this decision was explained by Jeff Barr, AWS evangelist:Instead of a package update system, Bottlerocket uses a simple, image-based model that allows for a rapid and complete rollback if necessary. This removes opportunities for conflicts and breakage, and makes it easier for you to apply fleet-wide updates with confidence using orchestrators such as EKS.To access a Bottlerocket instance directly you run a “control” container, which is managed by a separate instance of containerd. This container runs the AWS SSM agent so you can execute remote commands or start a shell on one or more instances. The control container is enabled by default.There is also an administrative container that runs on the internal control plane of the instance (I.e. on a separate containerd instance). Once enabled, this admin container runs an SSH server that allows you to log in as ec2-user using your Amazon-registered SSH key. While this is useful for debugging, it is not really suitable for making configuration changes due to the security policies of these instances.", "pub_date": "2020-08-17"},
{"title": "6 tips for rapidly scaling applications", "overview": "How to address dramatic usage increases during COVID-19 or to start planning for growth in the post-pandemic recovery", "image_url": "https://images.techhive.com/images/article/2015/09/thinkstockphotos-106463236-100611901-large.jpg", "url": "https://www.infoworld.com/article/3571186/6-tips-for-rapidly-scaling-applications.html", "body": "While the COVID-19 pandemic continues to ravage communities and the economy, many companies in ecommerce, logistics, online learning, food delivery, online business collaboration, and other sectors are experiencing massive spikes in demand for their products and services. For many of these companies, the evolving usage patterns caused by shelter-in-place and lockdown orders have created surges in business. These surges have pushed applications to their limits or beyond, potentially resulting in outages and delays that frustrate customers.If your company is experiencing a dramatic increase in business and application load, what can you do? How can you rapidly increase the performance and scalability of your applications to ensure a great customer experience without breaking the bank? Here are six tips for rapidly scaling applications the right way.How data analysis, AI, and IoT will shape the post-pandemic ‘new normal’Tip 1: Understand the full challengeAddressing only part of the problem may not achieve the desired results. Be sure to consider all of the following. – Application performance under load (and as experienced by end users) is determined by the interplay between latency and concurrency. Latency is the time required for a specific operation, such as the time it takes for a website to respond to a user request. Concurrency is how many simultaneous requests a system can handle. When concurrency is not scalable, a significant increase in demand can cause an increase in latency because the system cannot immediately respond to all requests as they are received. This can lead to a poor customer experience as response times increase from fractions of a second to several seconds or worse, even potentially to an inability to respond to all requests. So while ensuring low latency for a single request may be essential, by itself it may not solve the challenge created by surging concurrency. You must find a way to scale the number of concurrent users while simultaneously maintaining the required response time. Further, applications must be able to scale seamlessly across hybrid environments that may span multiple cloud providers and on-premises servers. – A strategy that will take years to implement, such as rearchitecting an application from scratch, isn’t helpful for addressing immediate needs. The solution you adopt should enable you to begin scaling in weeks or months. – Few companies approach this challenge without budget restrictions, so a strategy that minimizes upfront investments and minimizes increased operational costs can be critical.Tip 2: Plan for both the short term and the long termEven as you address the challenge of increasing concurrency while maintaining latency, do not rush into a short-term fix that can lead to a costly dead end. If a complete redesign of the application isn’t planned or feasible, adopt a strategy that will enable the existing infrastructure to scale massively, as needed.Tip 3: Choose the right technologyOpen source in-memory computing solutions have proven to be the most cost-effective way to rapidly scale up system concurrency while maintaining or improving latency. Apache Ignite, for example, is a distributed in-memory computing solution which is deployed on a cluster of commodity servers. It pools the available CPUs and RAM of the cluster and distributes data and compute to the individual nodes. Deployed on-premises, in a public or private cloud, or in a hybrid environment, Ignite can be deployed as an in-memory data grid (IMDG) inserted between existing application and data layers without major modifications to either. Ignite also supports ANSI-99 SQL and ACID transactions.With an Apache Ignite in-memory data grid in place, relevant data from the database is “cached” in the RAM of the compute cluster and is available for processing without the delays caused by normal reads and writes to a disk-based data store. The Ignite IMDG uses a MapReduce approach and runs application code on the cluster nodes to execute massively parallel processing (MPP) across the cluster with minimal data movement across the network. This combination of in-memory data caching, sending compute to the cluster nodes, and MPP dramatically increases concurrency and reduces latency, providing up to a 1,000X increase in application performance compared to applications built on a disk-based database.The distributed architecture of Ignite makes it possible to increase the compute power and RAM of the cluster simply by adding new nodes. Ignite automatically detects the additional nodes and redistributes data across all nodes in the cluster, ensuring optimal use of the combined CPU and RAM. The ability to add nodes to the cluster easily also enables massive scalability to support rapid growth. Finally, the IMDG ensures data consistency by writing changes made to the data in the IMDG by the application layer back to the source data stores.Apache Ignite can also future proof your infrastructure by powering two increasingly important strategies. – A DIH architecture can power real-time business processes that require 360-degree data views. It provides a common data access layer for aggregating and processing data from a combination of data streams and on-premises and cloud-based sources, including on-premises and cloud databases, data lakes, data warehouses, and SaaS applications. Multiple customer-facing business applications can then access the aggregated data and process the data at in-memory speeds without moving the data over the network. The DIH automatically synchronizes changes to the data made by the consuming applications to the back-end data stores while reducing or eliminating the need for API calls to those data sources. – HTAP is high-speed processing of the same in-memory dataset for both transactions and analytics. This eliminates the need for a time-consuming extract, transform, and load (ETL) process to periodically copy data from an online transactional processing (OLTP) system to a separate online analytical processing (OLAP) system. Powered by an in-memory computing platform, HTAP enables running pre-defined analytics queries on the operational data without impacting overall system performance.Tip 4: Consider an open source stackTo continue creating a cost-effective, rapidly scalable infrastructure, consider these other proven open source solutions:Apache Kafka or Apache Flink for building real-time data pipelines for delivering data from streaming sources, such as stock quotes or IoT devices, into the Apache Ignite in-memory data grid.Kubernetes for automating the deployment and management of applications that have been containerized in Docker or other container solutions. Putting applications in containers and automating the management of them is key to successfully building real-time, end-to-end business processes in a distributed, hybrid, multi-cloud world.Apache Spark for processing and analyzing large amounts of distributed data. Spark takes advantage of the Ignite in-memory computing platform to more effectively train machine learning models using the huge amounts of data being ingested via a Kafka or Flink streaming pipeline.Tip 5: Build, deploy, and maintain it rightWith the desire to deploy these solutions in an accelerated timeframe – and with the consequences of delays potentially very high – it is essential to make a realistic assessment of the in-house resources available for the project. If your company lacks the expertise or the availability, don’t hesitate to consult with third-party experts. You can easily obtain support for all these open source solutions on a contract basis, making it possible to gain the required expertise without the cost and time required to expand your in-house team.Tip 6: Learn moreMany online resources are available to help you get up to speed on the potential of these technologies and determine which strategies may be right for your organization. Start by exploring the following.What is Apache Ignite?Breakout session and keynote recordings from past In-Memory Computing Summits (videos and slides)Apache Ignite communityHow-to for Apache Ignite Deployments in Kubernetes (video)What is Apache Kafka?Session recordings from the Apache Kafka Summit (videos and slides)A simple introduction to Apache FlinkApache Flink communityStep-by-Step Introduction to Basic Concept of KubernetesKubernetes communityApache Spark 101Apache Spark communityWhether your goal is to ensure an optimal customer experience in the face of surging business activity or to start planning for growth in the post-pandemic economic recovery, an open source infrastructure stack powered by in-memory computing is a cost-effective path to combining unprecedented speed with massive scalability to enable real-time business processes.GridGain Systemsnewtechforum@infoworld.com", "pub_date": "2020-08-19"},
{"title": "Gitpod open-sources cloud IDE platform", "overview": "Gitpod applies CI/CD to development environments, allowing them to be maintained as code ", "image_url": "https://images.idgesg.net/images/article/2018/06/open_sign_by_artem_bali_cc0_via_pexels_1200x800-100760553-large.jpg", "url": "https://www.infoworld.com/article/3572561/gitpod-open-sources-cloud-ide-platform.html", "body": "Development environment technology provider Gitpod has open-sourced its self-named cloud-based IDE platform for automatically spinning up ready-to-code development environments.The open-sourcing will allow the Gitpod community to participate in the technology’s development and make it easier for developers to integrate Gitpod into their workflows, the company said.Also on InfoWorld: 5 reasons we don’t write code like we used toA Kubernetes application, Gitpod allows developers to maintain development environments as code, turning manual steps into a machine-executable part of a project’s source code. The platform monitors changes in the repository, and preps development environments for every change. This preparation includes:Setting up tools.Checking out the correct Git branch.Compiling code.Downloading dependencies.Initializing whatever is needed.Developer workflows are streamlined, with teams able to build applications quicker, the company said. Coding can begin from a branch, issue, or merge or pull request, applying CI/CD concepts to development environments. Gitpod works with code-hosting platforms including GitLab, GitHub Enterprise, and Bitbucket.Benefits of Gitpod cited by the company include:Shorter lead times, with reductions in time it takes to switch contexts and maintain development environments.Elimination of “configuration drift,” with the GitOps approach embraced through versioning of configuration in the Git repository. This ensures consistent, reproducible development environments.Enabling remote collaboration, with developers able to work on code reviews, mentoring, and sharing snapshots of work. Gitpod is available under an Affero GPL license on GitHub. The technology was architected by Sven Efftinge, who co-created the Eclipse Theia IDE development platform.", "pub_date": "2020-08-25"},
{"title": "Why you should look beyond the big 3 cloud providers", "overview": "There are appealing enterprise options beyond AWS, Azure, and GCP, but buyer beware — not just any cloud service qualifies", "image_url": "https://images.techhive.com/images/article/2014/03/178803656-100249440-large.jpg", "url": "https://www.infoworld.com/article/3572388/why-you-should-look-beyond-the-big-3-cloud-providers.html", "body": "The cloud has, for the most part, become a commodity. Most enterprise cloud workloads require a small variety of hardware. After all, how many organizations are actually doing quantum computing or even AI training workloads in the cloud? The fact is, the majority of businesses simply need core cloud services: compute, block and object storage, CI/CD and testing environments, backup and failover, and low-risk options for hybrid cloud or multicloud deployments.Also on InfoWorld: 6 ways Alibaba Cloud challenges AWS, Azure, and GCPThat’s why technology analysts have recognized “alternative cloud providers” as a legitimate and growing segment of the cloud services market, clearly distinct from the Big Three hyperscale public cloud providers—Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP). 451 Research, for example, in its 2020 Trends in Managed Services and Hosting Report observes:Hyperscale public clouds make up a significant majority of public cloud market revenue. However, there is still room for alternative cloud providers to add value as enterprises increasingly adopt multicloud strategies, and smaller companies opt for services that intentionally skew away from the complexity of hyperscale cloud.For the first time in the 12-year history of cloud computing, companies have legitimate and arguably better-suited choices in providers. We’re talking about alternative cloud providers like DigitalOcean, Linode, and Vultr, who are now in many ways on par with (and, in the case of price performance, outperform) the Big Three mega clouds.An alternative cloud provider must have an extensive global network capable of equipping their customers with the following:The means to physically locate data close to users to reduce or eliminate lag times in data, media, and website page loads.The means to comply with laws in various countries regarding user privacy requirements, specifically in geographic user data storage mandates, even in recovery failovers.The ability to scale instantly and with little fuss.A significant layer of security protections.Disaster recovery and loss prevention capabilities via failover and backup redundancies.The means to “pay by the sip,” which means according to actual cloud usage. Further cost savings are delivered through the reduction of data center capex costs and IT payroll costs.Automatic updates, upgrades, patching, and other security, maintenance, and service updates so that users are always using the latest and greatest versions of hardware, firmware, middleware, and software.Analytics and dashboards that can give you control and insights over your data.Worldwide data centersOne of the myths about hyperscalers is that they offer superior networks and data center hardware. For all intents and purposes, there is no difference between the hardware on the racks of alternative cloud providers and what AWS has on its racks. The compute power is basically identical.If you’re wondering about global reach and scalability, rest assured that alternative cloud providers have got you covered. Granted, alternative cloud providers are not big enough to directly compete with AWS, Azure, or GCP, and they have no designs to. But they have extensive global networks that deliver ubiquitous availability via world-class data centers placed strategically around the world.The resulting global network is capable of blanketing users, scaling massive workloads, complying with variations in regional laws, and providing instant recoveries in flawlessly orchestrated failovers. Today, the global networks established by alternative cloud providers are already scaling to meet the needs of Fortune 50 companies, so scaling to support the workloads of small and medium-size businesses (SMBs) is not an issue.Enterprise-grade computing servicesCertainly, some people choose to pay for AWS, Azure, or GCP to have access to the hundreds of products they offer, just in case you need them. In reality, though, how many companies really need computer vision, speech understanding, super-advanced networking, and highly optimized database queries, for example? In reality, the percentage of workloads that need any “special sauce” from AWS or the other hyperscale cloud providers is relatively low.Alternative cloud providers can provide scale and quality of infrastructure needed for most enterprise workloads. The overall market has commoditized around basic services, what analysts call “core primitives” — things like servers, databases, bare metal, Kubernetes, and other core infrastructure components. Alternative cloud providers focus on delivering these core primitives rather than offering hundreds of proprietary services that, once engaged, lock you in. (Further, most of those special services are available through third parties, many of them open source, so it’s pretty easy to replicate everything that you would do on AWS, usually for much cheaper and without the lock-in.)Developers and other users find the reduction in the immense complexity encountered in using top-tier public clouds to be a significant advantage. According to 451 Research, the message of simplicity from alternative cloud providers “resonates with developers who need to deploy applications quickly, don’t need access to the full suite of advanced cloud functions, and prioritize price, performance, and access to a level of technical support.” The same holds true for SMBs, which typically have a small technical staff who would rather focus on getting work done than navigating the pitfalls and distractions of too many bells and whistles.An undeniable cost advantage The bottom line about why you should use alternative cloud providers is… your bottom line.By focusing on core services, alternative cloud providers offer developers and SMBs a legitimate alternative: typically the same or better performance at a much better price. In most cases, you can save over half of your infrastructure costs by moving from a hyperscaler to an alternative cloud provider. Why wouldn’t you want the option that gives you all the performance you need at half the cost?Also on InfoWorld: PaaS, CaaS, or FaaS? How to chooseIf you are ready to consider your alternatives, start by making sure the companies you consider meet the network qualifications of alternative cloud providers (see sidebar). Then dig a little deeper. Not all alternative cloud providers are the same. You’ll want to choose an alternative cloud provider who also offers a great track record, exceptional service, transparent and affordable pricing, and no lock-in. Most importantly, choose a cloud provider that makes your cloud computing experience simple and accessible — what we all dreamed cloud computing could be.Linode—newtechforum@infoworld.com", "pub_date": "2020-08-26"},
{"title": "Cloud is going to take time", "overview": "Cloud spend will need to grow many more years before it becomes even a meaningful fraction of total IT spending", "image_url": "https://images.idgesg.net/images/article/2020/07/hourglass_time_countdown_deadlines_by_willselarep_gettyimages-157196570_2400x1600-100851110-large.jpg", "url": "https://www.infoworld.com/article/3572407/cloud-is-going-to-take-time.html", "body": "$233.4 billion. That’s how much analyst firm IDC said the world spent on cloud (SaaS, IaaS, PaaS) in 2019. It sounds like a lot of money, right? According to a quick Internet search, $233 billion is how much China plans to spend on defense. It’s how much Bill Gates could save by “reinventing the toilet.” And it’s how much the meeting industry expects to lose due to the pandemic curtailing travel.But $233.4 billion is a rounding error compared to the $4 trillion that organizations will spend on IT in 2020, according to IDC. Just 5.8 percent. The fixation on cloud revenues only serves to distort the reality of how organizations spend their IT budgets today.Also on InfoWorld: 6 ways Alibaba Cloud challenges AWS, Azure, and GCPBut growth!Sure, if we look at where the growth in IT is, cloud is hot. (Note: IDC includes Devices, Infrastructure (server/storage/network hardware and cloud services), Software, and IT services in its IT spending number.) As IDC vice president Stephen Minton recently said,Where there is growth, most of it is in the cloud. Overall software spending is now expected to decline as businesses delay new projects and application roll-outs…. On the other hand, the amount of data that companies must store and manage is not going anywhere. Increasingly, even more of that data will be stored, managed, and increasingly also analysed in the cloud.Let’s take China as an example.On Alibaba’s recent earnings call, executive vice-chair Joe Tsai pointed out that China’s total cloud market is still relatively small compared to the western markets, just $15 to $20 billion. Even so, that’s a bigger share of the total Chinese IT market ($297 billion in 2020, according to IDC) than the global percentage.As for growth, as Tsai went on to project, “The China market is going to be a much faster-growing market in cloud than the U.S. market.” At Alibaba’s roughly $7 billion annual run rate, coupled with 59 percent annual growth in the most recent quarter, there’s a long way to go to catch up to IDC’s other prediction: 90 percent of applications in China will be cloud-native by 2025. If the Chinese market gets anywhere near that number, that represents impressive growth, indeed.And frictionBut most economies don’t function like China’s, with its five-year plans. Nor do most companies, though we’ve seen the coronavirus pandemic kickstart a torrid era of digital transformation. As CircleCI CEO Jim Rose told me a few months back, “The pandemic has compressed the time that companies are taking to get to CI/CD [and cloud]. Everything we forecasted for the next year is now happening in the next three months.”Writing about steps organizations should take to get ahead of pandemic-induced restrictions on employee movement and customer spend, InfoWorld’s David Linthicum notes a few to-do’s:Move rapidly to public cloud-based resources, including IaaS and SaaS.Eliminate data centers, either owned or leased.Reduce or eliminate facilities to a functional minimum.Change corporate culture to support remote collaboration.Some of these are easier said than done. For example, companies like GitLab that have always had a remote-first policy found it much easier to adapt to work-from-home requirements than peers used to an office culture. You can give everyone a camera and headset, but enabling people to be productive in a work-from-home environment requires cultural change, and cultural change takes time.Hurry up and waitAs such, even as we rightly expect to see a hastening toward more cloud adoption over the next few months (and years), we would be wrong to think we’ll magically go from $233 billion to $4 trillion by 2025. Don’t bet on it.Also on InfoWorld: Kubernetes meets the real world: 3 success storiesBut also don’t bet on the fanciful notion that workloads are going to “repatriate” from cloud to on-premises data centers. There are all sorts of good reasons to pick that fever dream apart (analyst Corey Quinn highlights a few of them), but here’s perhaps the biggest: organizations are slow to change. Even the “hastening” I mentioned above about digital transformation will take years. It’s simply not the case that companies will move to cloud (and then back to their data centers) on a whim. That’s not how the real world works.Don’t believe me? Let’s rewind to the beginning of this article. Yes, cloud is a big deal... but it’s still just 5.8 percent of total IT spending. Over time, I suspect we’ll see those percentages flip, with the 5.8 percent number relating to on-premises workloads, not cloud. But that’s not going to happen next year. Or the year after that. Change takes time.", "pub_date": "2020-08-26"},
{"title": "How to find cloud talent during a boom", "overview": "The pandemic has led to an explosion of cloud projects. How do you hire in an environment where 10 job reqs are chasing a single qualified candidate?", "image_url": "https://images.idgesg.net/images/article/2018/10/jobs-sign_interview_job-search_now-hiring-100774984-large.jpg", "url": "https://www.infoworld.com/article/3572323/how-to-find-cloud-talent-during-a-boom.html", "body": "We made the call from the start: COVID-19 would spike the need for cloud computing and cloud computing talent. IDC’s latest Worldwide Quarterly Cloud IT Infrastructure Tracker noted an increase in cloud spending with traditional infrastructure taking a dirt nap. The pandemic was the primary driver behind the shift in IT spending, with IDC noting that widespread remote work triggered demand for enterprise cloud-based services.Of course, that was last quarter. The outlook for the remainder of the year is explosive cloud growth with funding and acceleration of cloud projects underway right now or about to begin. What currently hinders an enterprise’s movement to the cloud is the lack of cloud talent, including architects, security specialists, developers, operations, and secops engineers, to name just a few. Also on InfoWorld: 9 career pitfalls every software developer should avoidSo, how do you find talent in a seller’s market? Here are a few creative ideas:The days of driving to an office or moving because of a job should be pretty much over. Unless the employee needs to be physically present (and I can’t think of an example when it comes to cloud computing), an employee should be able to work from anywhere with a reliable Internet connection.The pandemic helped some companies, such as online sellers, and hurt others, such as the aviation industry. Companies that have cut staff, including those with cloud skills, should be targeted by recruiters. Hire someone with IT skills but few or no cloud skills. Provide the training needed to take on a cloud-related role. I like this approach because you need continuous learners anyway in the cloud space, and hiring someone who can quickly learn and adjust is typically more valuable than limiting the hunt to a candidate who already has the right certifications. This may seem old school, but in the days when I had to hire a lot of people and had no budget for recruiters or advertising, I would host meetings for special interest groups in my office. Many of these groups already exist today; you can either start a new group or join an existing one. They are always looking for volunteers to provide meeting space. Of course, there are more traditional HR tricks to try. You’ll probably find one or two approaches that work best for your situation. Stick with what works to gather the talent you need. This is a time when the recruiting process can keep an enterprise on track or make it fail fast. ", "pub_date": "2020-08-28"},
{"title": "2021: The year of the great enterprise disconnect", "overview": "Next year enterprise cloud usage will become completely virtual and disconnected from centralized infrastructure. Here’s how to be ready", "image_url": "https://images.idgesg.net/images/article/2020/08/online_meetings_virtual_events_digital_conferences_video_conferencing_remote_teams_by_mixetto_gettyimages-1223454973_2400x1600-100854042-large.jpg", "url": "https://www.infoworld.com/article/3572272/2021-the-year-of-the-great-enterprise-disconnect.html", "body": "Due to the unprecedented shutdown of businesses during the pandemic, most white-collar workers went from hours-long commutes to working fulltime from a guest bedroom somewhere in the suburbs. In the past, most enterprises resisted or refused to consider viable telecommuting options. Suddenly, new regulations, laws, and liability policies forced the hands of most enterprises around the world. Companies had to accommodate a remote workforce in a short amount of time. Adapt or die. IT departments spent the better part of the year putting out fires caused by the sudden shift to a remote workforce. VPNs had to scale to handle the influx, networks needed upgrades, security required adjustments, and there was even work to be done with employees' ISPs to provide better and more reliable bandwidth. ISPs suddenly moved bandwidth upgrades to the top of the priority list. Also on InfoWorld: 6 ways Alibaba Cloud challenges AWS, Azure, and GCPEven the most remote homes in the country will begin to support telecommuting as early as next year, as 5G becomes a pervasive reality nationwide. If staff can live and work from anywhere, why not live where they can get the most home for the least amount of money? This trend naturally eliminates the drawbacks an enterprise once had when all of its proverbial eggs were in one geographic basket. Five or ten years ago, a major fire, flood, or weather event in the immediate vicinity would have shut down a firm’s headquarters. In some of today’s cutting-edge enterprises, staff can be so geographically scattered that most white-collar workers could continue to telecommute each morning even if the place they once reported to no longer exists. Virtual IT systems can be load-balanced across cloud platforms in deliberately staggered locations so that systems experience little or no impact on operations. To turn these opportunities into long-term reality, enterprises need to disconnect. 2021 will be the year most begin this journey. The first stage will be to stabilize their remote workforces. Then they can move to these likely next steps:This has been occurring for the past several years, but the pace will accelerate. The more difficult workloads will be placed in the queue for relocation to cloud or MSPs.Fueled by a strong economy, the growth of data centers over the past few years has led to the identification of vulnerabilities and a new understanding of the risk of owning and maintaining your own hardware and software. Why own or lease office space you won’t use? Commercial space will not be completely abandoned, but liability costs will likely force hotel-type options to share offices and conference rooms.It’s one thing to have the tools and the infrastructure to support a functional remote workforce, it’s another thing to embrace them. Culture needs to change and will take longer than one year. Major changes include how managers measure employee performance and work processes. Staff should also take the initiative to continuously improve these practices. Each company has its own demons to exorcise. The larger the enterprise, the more difficult the journey to a disconnected enterprise. Large or small, every journey begins with the first step. It’s time to lace up your hiking boots.", "pub_date": "2020-08-21"},
{"title": "Cloud migration gets harder", "overview": "Now that we’ve migrated the easy stuff, the level of difficulty is increasing—but it still needs to be done", "image_url": "https://images.idgesg.net/images/article/2020/06/man_rests_his_chin_in_his_hand_covering_mouth_eyebrows_raised_in_tense_interaction_with_coworker_uncertainty_difficulty_tough_negotiatoin_by_fizkes_gettyimages-1156269838_2400x1600-100850204-large.jpg", "url": "https://www.infoworld.com/article/3572373/cloud-migration-gets-harder.html", "body": "The chart below depicts the number of applications migrated over time in blue, and the degree of difficulty of moving those applications in orange. This is a fictional collection of applications; however, the concept that difficulty increases the more you migrate affects enterprises large and small as they move to the public cloud.  See this chart: What’s occurring is easy to explain, but the solution to the problem is not. Also on InfoWorld: Microsoft Azure cloud migration: 3 success storiesSimply put, the applications and databases that are more modern, better designed, and built to be portable are the first to relocate to the clouds. This for good reason: The cloud teams want to get some wins on the scoreboard and can do so by removing risk and reducing the degree of difficulty. This is actually my advice as enterprises begin their journey.Predictably, as the number of well-designed and modern applications that can be easily migrated decreases, migration teams are forced to face applications that are not as well designed or are built on older platforms that may not have an analog platform yet in a public cloud. Legacy applications come to mind, but also any application that needs significant refactoring to get it running correctly on a public cloud. As many of the enterprises approach the middle of the migration process, the degree of difficulty increases significantly, as shown in the chart, and it’s killing migration productivity. So, how does an enterprise that’s already freaked out about the vulnerabilities exposed in the pandemic get the remaining applications out of the enterprise data center?Here are two approaches that seem to be working: Chances are, if you’re focused on a public cloud as a primary target for your workloads, you may now be considering some valid alternatives. MSPs offer more platform analogs for the more difficult-to-replicate platforms and provide an A-to-A migration path to move to a public cloud.If you think that this is a cop-out, considering that you’re going to need to fix or modernize those applications and databases at some point, you’re right. But using an MSP will accelerate the downsizing of the existing physical data center faster. Once you’ve migrated to an MSP, you can focus on fixing or sunsetting applications on that quasi-cloud platform. From there, you can leave them or ultimately migrate them to a pubic cloud. You’ll have to do the heavy lifting up front, but it’s really the right way to do things. In essence, it’s an application migration factory optimized using automation. Too many enterprises fix each application as a one-off. That won’t scale or be consistent.  Of course, there are other approaches, including doing nothing and just fighting through it, degree of difficulty be dammed. I would encourage those enterprises to be a bit more open-minded to other solutions that may be difficult but ultimately remove problems moving forward. That’s a no brainer for me.          ", "pub_date": "2020-08-25"},
{"title": "8 great Python libraries for natural language processing", "overview": "With so many NLP resources in Python, how to choose? Discover the best Python libraries for analyzing text and how to use them", "image_url": "https://images.idgesg.net/images/article/2018/05/communication_language_translation_global_foreign_countries_country_flags_by_alexsl_gettyimages-673314454_1200x800-100757311-large.jpg", "url": "https://www.infoworld.com/article/3519413/8-great-python-libraries-for-natural-language-processing.html", "body": "Natural language processing, or NLP for short, is best described as “AI for speech and text.” The magic behind voice commands, speech and text translation, sentiment analysis, text summarization, and many other linguistic applications and analyses, natural language processing has been improved dramatically through deep learning.The Python language provides a convenient front-end to all varieties of machine learning including NLP. In fact, there is an embarrassment of NLP riches to choose from in the Python ecosystem. In this article we’ll explore each of the NLP libraries available for Python—their use cases, their strengths, their weaknesses, and their general level of popularity.Also on InfoWorld: 6 great new Python features you don’t want to missNote that some of these libraries provide higher-level versions of the same functionality exposed by others, making that functionality easier to use at the cost of some precision or performance. You’ll want to choose a library well-suited both to your level of expertise and to the nature of the project.CoreNLPThe CoreNLP library — a product of Stanford University — was built to be a production-ready natural language processing solution, capable of delivering NLP predictions and analyses at scale. CoreNLP is written in Java, but multiple Python packages and APIs are available for it, including a native Python NLP library called StanfordNLP.CoreNLP includes a broad range of language tools—grammar tagging, named entity recognition, parsing, sentiment analysis, and plenty more. It was designed to be human language agnostic, and currently supports Arabic, Chinese, French, German, and Spanish in addition to English (with Russian, Swedish, and Danish support available from third parties). CoreNLP also includes a web API server, a convenient way to serve predictions without too much additional work.The easiest place to start with CoreNLP’s Python wrappers is StanfordNLP, the reference implementation created by the Stanford NLP Group. In addition to being well-documented, StanfordNLP is also maintained regularly; many of the other Python libraries for CoreNLP have not been updated in some time.CoreNLP also supports the use of NLTK, a major Python NLP library discussed below. As of version 3.2.3, NLTK includes interfaces to CoreNLP in its parser. Just be sure to use the correct API.The obvious downside of CoreNLP is that you’ll need some familiarity with Java to get it up and running, but that’s nothing a careful reading of the documentation can’t achieve. Another hurdle could be CoreNLP’s licensing. The whole toolkit is licensed under the GPLv3, meaning any use in proprietary software that you distribute to others will require a commercial license.GensimGensim does just two things, but does them exceedingly well. Its focus is statistical semantics—analyzing documents for their structure, then scoring other documents based on their similarity.Gensim can work with very large bodies of text by streaming documents to its analysis engine and performing unsupervised learning on them incrementally. It can create multiple types of models, each suited to different scenarios: Word2Vec, Doc2Vec, FastText, and Latent Dirichlet Allocation.Gensim’s detailed documentation includes tutorials and how-to guides that explain key concepts and illustrate them with hands-on examples. Common recipes are also available on the Gensim GitHub repo.Don’t miss InfoWorld’s 2020 Technology of the Year Award winners: The best software development, cloud computing, data analytics, and machine learning products of the yearNLTKThe Natural Language Toolkit, or NLTK for short, is among the best-known and most powerful of the Python natural language processing libraries. Many corpora (data sets) and trained models are available to use with NLTK out of the box, so you can start experimenting with NLTK right away.As the documentation states, NLTK provides a wide variety of tools for working with text: “classification, tokenization, stemming, tagging, parsing, and semantic reasoning.” It can also work with some third-party tools to enhance its functionality.Keep in mind that NLTK was created by and for an academic research audience. It was not designed to serve NLP models in a production environment. The documentation is also somewhat sparse; even the how-tos are thin. Also, there is no 64-bit binary; you’ll need to install the 32-bit edition of Python to use it. Finally, NLTK is not the fastest library either, but it can be sped up with parallel processing.If you are determined to leverage what’s inside NLTK, you might start instead with TextBlob (discussed below).PatternIf all you need to do is scrape a popular website and analyze what you find, reach for Pattern. This natural language processing library is far smaller and narrower than other libraries covered here, but that also means it’s focused on doing one common job really well.Pattern comes with built-ins for scraping a number of popular web services and sources (Google, Wikipedia, Twitter, Facebook, generic RSS, etc.), all of which are available as Python modules (e.g., ). You don’t have to reinvent the wheels for getting data from those sites, with all of their individual quirks. You can then perform a variety of common NLP operations on the data, such as sentiment analysis.Pattern exposes some of its lower-level functionality, allowing you to to use NLP functions, n-gram search, vectors, and graphs directly if you like. It also has a built-in helper library for working with common databases (MySQL, SQLite, and MongoDB in the future), making it easy to work with tabular data stored from previous sessions or obtained from third parties.Also on InfoWorld: Artificial intelligence predictions for 2020PolyglotPolyglot, as the name implies, enables natural language processing applications that deal with multiple languages at once.  The NLP features in Polyglot echo what’s found in other NLP libraries: tokenization, named entity recognition, part-of-speech tagging, sentiment analysis, word embeddings, etc. For each of these operations, Polyglot provides models that work with the needed languages.Note that Polyglot’s language support differs greatly from feature to feature. For instance, the tokenization system supports almost 200 languages (largely because it uses the Unicode Text Segmentation algorithm), and sentiment analysis supports 136 languages, but part-of-speech tagging supports only 16.PyNLPIPyNLPI (pronounced “pineapple”) has only a basic roster of natural language processing functions, but it has some truly useful data-conversion and data-processsing features for NLP data formats.Most of the NLP functions in PyNLPI are for basic jobs like tokenization or n-gram extraction, along with some statistical functions useful in NLP like Levenshtein distance between strings or Markov chains. Those functions are implemented in pure Python for convenience, so they’re unlikely to have production-level performance.But PyNLPI shines for working with some of the more exotic data types and formats that have sprung up in the NLP space. PyNLPI can read and process GIZA, Moses++, SoNaR, Taggerdata, and TiMBL data formats, and devotes an entire module to working with FoLiA, the XML document format used to annotate language resources like corpora (bodies of text used for translation or other analysis). You’ll want to reach for PyNLPI whenever you’re dealing with those data types.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesSpaCySpaCy, which taps Python for convenience and Cython for speed, is billed as “industrial-strength natural language processing.” Its creators claim it compares favorably to NLTK, CoreNLP, and other competitors in terms of speed, model size, and accuracy. SpaCy’s main drawback is it’s relatively new, so it only covers English and a few other (chiefly European) languages. That said, SpaCy has already reached version 2.2 as of this writing.SpaCy includes most every feature found in those competing frameworks: speech tagging, dependency parsing, named entity recognition, tokenization, sentence segmentation, rule-based match operations, word vectors, and tons more. SpaCy also includes optimizations for GPU operations—both for accelerating computation, and for storing data on the GPU to avoid copying.Spacy’s documentation is excellent. A setup wizard generates command-line installation actions for Windows, Linux, and macOS and for different Python environments (pip, conda, etc.) as well. Language models install as Python packages, so they can be tracked as part of an application’s dependency list.TextBlobTextBlob is a friendly front-end to the Pattern and NLTK libraries, wrapping both of those libraries in high-level, easy-to-use interfaces. With TextBlob, you spend less time struggling with the intricacies of Pattern and NLTK and more time getting results.TextBlob smooths the way by leveraging native Python objects and syntax. The quickstart examples show how texts to be processed are simply treated as strings, and common NLP methods like part-of-speech tagging are available as methods on those string objects.Keep up with hot topics in software development with InfoWorld’s App Dev Report newsletterAnother advantage of TextBlob is you can “lift the hood” and alter its functionality as you grow more confident. Many default components, like the sentiment analysis system or the tokenizer, can be swapped out as needed. You can also create high-level objects that combine components—this sentiment analyzer, that classifier, etc.—and re-use them with minimal effort. This way, you can prototype something quickly with TextBlob, then refine it later.", "pub_date": "2020-02-12"},
{"title": "Can you put your trust in AIops?", "overview": "As ops tools morph into AIops, they bring greater value to more complex deployments—if users will let them", "image_url": "https://images.techhive.com/images/article/2014/10/artificial_intelligence_brain_circuit_thinkstock-100528006-large.jpg", "url": "https://www.infoworld.com/article/3526444/the-state-of-aiops.html", "body": "AIops (artificial intelligence for IT operations) is one of those cool buzzwords that is actually part of another buzzword: cloudops (cloud operations), which is a part of the mother of all buzzwords: cloud computing.The concept of AIops and the tool category of AIops are really the maturation of operational tools in general. Most of those in the traditional ops tools space, at least in the past few years, bolted an AI engine onto a tool and called it AIops.Also on InfoWorld: Artificial intelligence predictions for 2020Some purpose-built AIops tool startups out there are leveraging AI from the jump. All are worth a look as you select AIops tools; however, there are no mainstream brands. The objective was and is obvious. Since most of these tools have been data gathering tools and analytics tools from the beginning, adding AI allows them to learn from that data rather than just externalize issues with the services under management. In some cases, they can correct issues using preprogrammed routines, such as restarting a server or blocking an IP address that seems to be attacking one of your servers.Now that we’re a few years into this paradigm and its technology offerings, we’re starting to note some patterns—some good, and some not so good. Let’s explore both. As far as what’s working, AIops tools in many instances are ops tools in their fourth, fifth, or sixth generations. Moreover, most of them have had public cloud management in mind for a while and are able to bridge the gap between on-premises legacy system management and managing applications and services in the public clouds.They are capable tools for managing and monitoring cloud, multicloud, legacy, and even IoT and edge-based systems. This ability to support complex system heterogeneity is really the true value of the ops tools, and why they are important to those implementing cloud or noncloud systems. Keep up with the latest developments in cloud computing with InfoWorld’s Cloud Computing Report newsletterThe downside seems to be that most users are not taking advantage of the AI subsystems in the tools, so you may be paying for a feature you’re not using. I don’t view this as the fault of the tool provider; for the most part, this relates to how the tools are installed, set up, and used by the traditional and cloudops teams. This is due to a lack of training in some cases, or a lack of valid use cases in the current set of systems—cloud and not cloud—under management. Clearly, AIops is going to be part of most cloud-based deployments. Just as clearly, the more complex those deployments, such as multicloud, the more value they will bring.", "pub_date": "2020-02-14"},
{"title": "RStudio Conference 2020 videos you don’t want to miss", "overview": "You can watch dozens of must-see RStudio Conference videos online. Don't know where to start? Let us help", "image_url": "https://images.idgesg.net/images/article/2019/08/cso_nw_best-conferences-to-attend_conference_convention_audience_applause_clapping_by_django_gettyimages_1200x800-100807805-large.jpg", "url": "https://www.infoworld.com/article/3528303/rstudio-conference-2020-videos-you-dont-want-to-miss.html", "body": "With dozens of RStudio Conference videos now available online, it’s hard to know where to begin. I hope this look at some of my favorites will help get you started!Error messages in RI could probably watch Jenny Bryan teach data  (oh,  looks interesting, maybe  want to try typing in a thousand rows . . . . ). But in this keynote, she tackles a much more compelling topic: dealing with errors in R. There’s a lot of useful advice here, which she shares in an engaging, relatable way. One takeaway: Try the equivalent of a reboot—restart your R session! (I’ve been doing that much more often since returning from the conference.) Video: Object of type ‘closure’ is not subsettable.Also on InfoWorld: Artificial intelligence predictions for 2020New features in RStudioWondering what features are coming to the next version of RStudio desktop? RStudio’s Jonathan McPherson outlined several, including modern-era spell check (at last), better cloud usability on iOS, and more screen-reader accessibility for visually impaired users—something that also improves keyboard navigation for all users. Video: RStudio 1.3 Sneak Preview. State of the tidyverseRStudio Chief Scientist Hadley Wickham reviewed last year’s highlights from the tidyverse and this year’s plans for further development, but he was also fairly forthright in discussing some recent missteps.In particular, he acknowledged that the initial rollout of “tidy evaluation” launched with a somewhat difficult-to-master syntax and an unreasonable expectation that users would want to learn the detailed computing theory behind it. It turned out that many users didn’t care about the mechanics behind incorporating the tidyverse into their own custom functions; they just wanted to write their code. Since then, tidy eval syntax has been changed to more understandable  double braces.Wickham also outlined how tidyverse package authors will help users better understand the lifecycle of older functions and if/how some functions may be deprecated. Video: State of the tidyverse. Styled text with ggtextClaus Wilke gave an overview of the ggtext package in this fast-paced presentation, showing how to customize ggplot visualizations with colored text, images on axes, and more. He also explained the package’s current limits. Video: Spruce up your ggplot2 visualizations with formatted text. Below, you can also watch my Do More With R tutorial on one way to use ggtext: adding color to ggplot text. (Or read the companion article.)What you didn’t know about R’s scales packageI’ve used scales package functions such as  or  to add commas or dollar signs to a vector of numbers, but I never really explored the package further. Turns out that was my loss. At this presentation, data scientist Dana Seidel showed that scales does a lot more than format numbers. One tip: The  function lets you easily see how various colors and palettes look. Video: The little package that could: Taking visualizations to the next level with the scales package.Result from running .Customizing Shiny apps and R MarkdownShiny creator and RStudio CTO Joe Cheng demo’d bootstraplib, a new package for customizing the look of Shiny apps without having to hunt through and tweak complex CSS. The bootstraplib package lets you change Bootstrap defaults within an R script, without having to write HTML and CSS. (Bootstrap is the open source HTML/CSS/JavaScript framework used by Shiny and many other Web projects.) You can also use bootstraplib in non-Shiny R Markdown documents. Video: Styling Shiny apps with Sass and Bootstrap 4. Better spaghetti plots using brolgar in RA spaghetti plot that makes sense, using the brolgar R package. What do you get when you have a load of items plotted over time? That data type is known as longitudinal, and visualizing it can often end up looking like a pile of spaghetti. To help solve this problem, Nicholas Tierney at Monash University created the brolgar package (watch the presentation if you’re wondering why that name) to summarize, visualize, and otherwise understand such data. Video: Making better spaghetti (plots): Exploring the individuals in longitudinal data with the brolgar package. Dataviz best (and worst) practicesThis wasn’t R-specific, but University of Pennsylvania dataviz specialist Will Chase gave an engaging, opinionated talk on how to “take your charts from drab to fab.” One tip: “White space is like garlic — take as much as you think you need and triple it.” Video: The Glamour of Graphics.From Will Chase’s The Glamour of Graphics presentation.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesR Markdown to its limitsThere is a lot more one can do with R Markdown than I thought. And the fun-as-well-as-educational Teacup Giraffe site pushes the limits. In addition to enjoying a look at the Teacup Giraffe website, this presentation by neuroscience Ph.D. student Desiree De Leon includes some basic advice for improving your own R Markdown documents. Video: Of Teacups, Giraffes, & R Markdown.And speaking of getting more out of markdown, RStudio’s Yihui Xie had a separate talk showing how to generate many more file types than just HTML or PDF from an R Markdown document. Video: One R Markdown Document, Fourteen Demos. 3D visualizations in RI’d been resisting razzle-dazzle around the rayshader package for awhile. Did I  need to turn ggplots into 3D visualizations and animate them? But I’m glad I went to author Tyler Morgan-Wall’s presentation, because the package  pretty cool—even if I’m not sure yet how I’d use it in my own work.3D raytracer version of a 2D ggplot.Morgan-Wall showed how to turn a conventional graphic into a 3D visualization and animation with very little code. He also showed some recent package improvements that make some graphics more visually striking. In this case, seeing the animated examples is a lot better than trying to read about them. If you’re at all interested in this package, it’s worth watching the presentation. Video: 3D ggplots with rayshader. Accelerating analytics in RThe Apache Arrow project is a multi-language standard for in-memory data aimed at interoperability and high performance. Arrow has been implemented in R with the arrow package. Ursa Labs Engineering Director Neal Richardson outlined the status of Arrow in R, including the ability to query a directory of files using dplyr syntax without having to load that data into memory, as well as some upcoming features. Video: Accelerating analytics with Apache Arrow. list-columns in data.tableIf you’ve seen the heated discussions on social media, you might think that tidyverse and data.table are in two opposing camps. But while each has its fans, there are an increasing number of people who use both. Utah State Research Assistant Professor Tyson S. Barrett is one, and he brought data.table to RStudio Conference with a talk on using complex list-columns with data.table  tidyverse functions. One interesting tip: If you’re joining a complex data set, nesting all of the columns you’re  joining on can help prevent errors.from juliesquid for openscapes, illustrated by allison_horst(CC BY 4.0)Barrett also mentioned his tidyfast package, which has a streamlined “translation” of data.table code to tidyverse-like functions. (It’s similar to dtplyr but doesn’t use “lazy” data sets.) Unfortunately, this session video occasionally obscures some of the code and graphs being shown. If you watch this one, I suggest looking at the slides separately; they’re available at Barrett’s website. Video: List-columns in data.table: Reducing the cognitive & computational burden of complex data.Not familiar with data.table? Check out my Do More With R 5-minute intro below. Bonus: RStudio Conference 2020 lightning talksThere were a lot of interesting lightning talks, but a few stood out in part for the cool websites and packages being demo’d as well as the presentations themselves.RStudio intern Maya Gans showed a drag-and-drop interface for tidyverse tasks such as transforming, summarizing, and plotting data. It’s an interesting way to teach tidyverse concepts before students have to learn actual code. Video: TidyBlocks: using the language of the tidyverse in a blocks-based interface. Website: TidyBlocks.tech.The still-experimental livecode package lets you live code a demo and have it appear on attendees’ own systems in near real time. University of Edinburgh lecturer Colin Rundel explains why you’d want to do that and how it works. Video: `livecode`: Broadcast your live coding sessions from and to RStudio.Data science for software engineers: Busting software myths with R featured a website designed to teach statistics to software engineering students with relevant problems like: Does test-driven software improve quality? Does sleep deprivation make programmers more or less effective? When will that project be finished? Yim Register showed a bit of the site, but you can also check out all the lessons at Data Science for Software Engineers. Keep up with advances in machine learning, AI, and big data analytics with InfoWorld’s Machine Learning and Analytics Report newsletterBonus: RStudio Conference 2020 keynotesRStudio founder and CEO J.J. Allaire discussed the state of open source software, how it’s possible to fund open source efforts, and the company’s move to become a certified benefit corporation. Video (presentation only, not subsequent Q&A): Open Source Software for Data Science.Every time I see Martin Wattenberg and Fernanda Viegas speak, I leave feeling grateful that I’ve got a job that lets me peek into the work and thoughts of some supersmart people. Co-leaders of Google Brain’s PAIR (People+AI Research), the two discussed a number of their projects on topics like understanding algorithm bias. Several of the projects they discussed are available to the public. Video: Data, visualization, and designing. A final note: With multiple tracks going on at once, I missed a lot of excellent talks. I also attended other good ones that didn’t make the list because I didn’t want this article to get too long. For example, how Associated Press uses R (Larry Fenn), hitting R a million times a day at T-Mobile (Heather Nolis and Jacqueline Nolis), and tuning models with the tune and workflow packages (Max Kuhn). You can find all of the available videos here: https://resources.rstudio.com/rstudio-conf-2020.InfoWorld’s Do More With R video tutorials", "pub_date": "2020-02-21"},
{"title": "2 cloud architecture problems are still unsolved", "overview": "If you think we’ve figured out all cloud design problems, you’d be wrong. Edge devices and multicloud security are still sticking points", "image_url": "https://images.techhive.com/images/article/2016/08/puzzle-pieces-problem-solved-teamwork-collaboration-100678164-large.jpg", "url": "https://www.infoworld.com/article/3570737/2-cloud-architecture-problems-are-still-unsolved.html", "body": "As 2021 begins to appear on the horizon, most people will be happy to see 2020 in the rearview mirror. Despite the pandemic and all its chaos and uncertainties, 2020 was the year of the cloud. Pandemic-weary enterprises rushed to the cloud for safety, scalability, and agility.We still have some architectural obstacles to overcome. There’s no time like the present to review the two architectural problems on the top of my list.Also on InfoWorld: What is CI/CD? Continuous integration and continuous delivery explained. When a tiered architecture includes an edge device, we need best practices and processes to figure out where to place data and knowledge bases and the back-end hyperscalers.This seems like an easy problem to solve: Simply understand the workload requirements and then determine the proper tiering. Then the number of edge devices grows exponentially, the organic growth of data saturates the edge devices, and the requirements become quicksand.We need a dynamic approach that uses automation to migrate the data and the knowledge bases as needed. But even that approach can be problematic through the negative effects of over-engineering. I’m working on this aspect of the solution right now.. You will probably point to IAM (identity and access management) solutions that span clouds, but I find that most are not yet ready for prime time.Cloud security architects are forced to use whatever cloud-native security is available for each cloud brand, which makes this solution more complex and more difficult to operate by the secops team. The outcome is a higher risk of being compromised. Also, the integration of the native directory services of each cloud often leads to manually restarting the integration processes because right now they seem to just jam up.I think IAM providers will solve this security problem now that they recognize that multicloud is their future, and that the market demands the ability to provide all patterns of security to span clouds. We know what the solution to the security problem is; we just need to build one that consistently works.I’m tracking dozens of cloud implementation problems that I see over and over again in my consulting practice. Many people believe there are limitations right now that can’t be overcome. Just watch me.", "pub_date": "2020-08-11"},
{"title": "A brief history of artificial intelligence", "overview": "Despite huge advances in machine learning models, AI challenges remain much the same today as 60 years ago", "image_url": "https://images.idgesg.net/images/article/2019/02/artificial_intelligence_mind-reading_technology_addiction_by_gremlin_gettyimages-1090611158_2400x1600-100788441-large.jpg", "url": "https://www.infoworld.com/article/3527209/a-brief-history-of-artificial-intelligence.html", "body": "In the early days of artificial intelligence, computer scientists attempted to recreate aspects of the human mind in the computer. This is the type of intelligence that is the stuff of science fiction—machines that think, more or less, like us. This type of intelligence is called, unsurprisingly, intelligibility. A computer with intelligibility can be used to explore how we reason, learn, judge, perceive, and execute mental actions.Early research on intelligibility focused on modeling parts of the real world and the mind (from the realm of cognitive scientists) in the computer. It is remarkable when you consider that these experiments took place nearly 60 years ago.Also on InfoWorld: Artificial intelligence predictions for 2020Early models of intelligence focused on deductive reasoning to arrive at conclusions. One of the earliest and best known A.I. programs of this type was the Logic Theorist, written in 1956 to mimic the problem-solving skills of a human being. The Logic Theorist soon proved 38 of the first 52 theorems in chapter two of the , actually improving one theorem in the process. For the first time, it was clearly demonstrated that a machine could perform tasks that, until this point, were considered to require intelligence and creativity.Soon research turned toward a different type of thinking, inductive reasoning. Inductive reasoning is what a scientist uses when examining data and trying to come up with a hypothesis to explain it. To study inductive reasoning, researchers created a cognitive model based on the scientists working in a NASA laboratory, helping them to identify organic molecules using their knowledge of organic chemistry. The Dendral program was the first real example of the second feature of artificial intelligence, , a set of techniques or algorithms to accomplish an inductive reasoning task, in this case molecule identification.Dendral was unique because it also included the first knowledge base, a set of if/then rules that captured the knowledge of the scientists, to use alongside the cognitive model. This form of knowledge would later be called an . Having both kinds of “intelligence” available in a single program allowed computer scientists to ask, “What makes certain scientists so much better than others? Do they have superior cognitive skills, or greater knowledge?”By the late 1960’s the answer was clear. The performance of Dendral was almost completely a function of the amount and quality of knowledge obtained from the experts. The cognitive model was only weakly related to improvements in performance.This realization led to a major paradigm shift in the artificial intelligence community. Knowledge engineering emerged as a discipline to model specific domains of human expertise using expert systems. And the expert systems they created often exceeded the performance of any single human decision maker. This remarkable success sparked great enthusiasm for expert systems within the artificial intelligence community, the military, industry, investors, and the popular press.As expert systems became commercially successful, researchers turned their attention to techniques for modeling these systems and making them more flexible across problem domains. It was during this period that object-oriented design and hierarchical ontologies were developed by the AI community and adopted by other parts of the computer community. Today hierarchical ontologies are at the heart of knowledge graphs, which have seen a resurgence in recent years.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesAs researchers settled on a form of knowledge representation known as “production rules,” a form of first order predicate logic, they discovered that the systems could learn automatically; i.e., the systems coud write or rewrite the rules themselves to improve performance based on additional data. Dendral was modified and given the ability to learn the rules of mass spectrometry based on the empirical data from experiments.As good as these expert systems were, they did have limitations. They were generally restricted to a particular problem domain, and could not distinguish from multiple plausible alternatives or utilize knowledge about structure or statistical correlation. To address some of these issues, researchers added certainty factors—numerical values that indicated how likely a particular fact is true.The start of the second paradigm shift in AI occurred when researchers realized that certainty factors could be wrapped into statistical models. Statistics and Bayesian inference could be used to model domain expertise from the empirical data. From this point forward, artificial intelligence would be increasingly dominated by machine learning.There is a problem, though. Although machine learning techniques such as random forest, neural networks, or GBTs (gradient boosted trees) produce accurate results, they are nearly impenetrable black boxes. Without intelligible output, machine learning models are less useful than traditional models in several respects. For example, with a traditional AI model, a practitioner might ask:Why did the model make this mistake?Is the model biased?Can we demonstrate regulatory compliance?Why does the model disagree with a domain expert?The lack of intelligibility has training implications as well. When a model breaks, and cannot explain why, it makes it more difficult to fix. Add more examples? What kind of examples? Although there are some simple trade-offs we can make in the interim, such as accepting less accurate predictions in exchange for intelligibility, the ability to explain machine learning models has emerged as one of the next big milestones to be achieved in AI.Keep up with advances in machine learning, AI, and big data analytics with InfoWorld’s Machine Learning and Analytics Report newsletterThey say that history repeats itself. Early AI research, like that of today, focused on modeling human reasoning and cognitive models. The three main issues facing early AI researchers—knowledge, explanation, and flexibility—also remain central to contemporary discussions of machine learning systems.Knowledge now takes the form of data, and the need for flexibility can be seen in the brittleness of neural networks, where slight perturbations of data produce dramatically different results. Explainability too has emerged as a top priority for AI researchers. It is somewhat ironic how, 60 years later, we have moved from trying to replicate human thinking to asking the machines how they think.", "pub_date": "2020-02-13"},
{"title": "Boosting AI’s smarts in the absence of training data", "overview": "Zero-shot learning repurposes knowledge through statistical or semantic approaches without needing huge amounts of fresh training data", "image_url": "https://images.idgesg.net/images/article/2017/05/artificial_intelligence_machine_learning_network_thinkstock_671750598-100724432-large.jpg", "url": "https://www.infoworld.com/article/3529871/boosting-ais-smarts-in-the-absence-of-training-data.html", "body": "AI (artificial intelligence) is the most perfect field of dreams in modern culture. If you ask the average person on the street what AI runs on, they probably won’t mention training data. Instead, they might mumble something about computer programs that magically learn how to do useful stuff from thin air.However, some of today’s most sophisticated AI comes close to that naïve dream. I’m referring to a still-developing approach known as “zero-shot learning.” This methodology—which is being explored at Microsoft, Uber, Baidu, Alibaba, and other AI-driven businesses—enables useful pattern recognition with little or no training data.Also on InfoWorld: What AI can really do for your business (and what it can’t)Zero-shot pattern learning will enable intelligent robots to dynamically recognize and respond to unfamiliar objects, behaviors, and environmental patterns that they may never have encountered in training. I predict that zero-shot approaches will increasingly be combined with reinforcement learning in order to enable robots to take the best actions iteratively in environments that are chaotic and one-off.In addition, gaming applications will use zero-shot approaches such as iterative self-play as an alternative to training on voluminous data derived from successful gameplay. This will enable the training of agents to master complex winning strategies in spite of knowing nothing about these games at the outset.Furthermore, zero-shot learning promises to make object recognition applications more versatile, due to its ability to drive:on-the-fly recognition of rare, unfamiliar, and unseen objects that may be substantially missing from training data.recognition of patterns for which it is hard to obtain training data that has been labeled with a sufficiently high degree of expert knowledge.detection of instances of object classes where the proliferation of fine-grained categories has made it difficult or prohibitively expensive to acquire sufficient quantities of statistically diverse, labeled training data.What makes zero-shot learning possible is the existence of prior knowledge that can be discovered and repurposed through statistical or semantic approaches. Zero-shot methods use this knowledge to predict the larger semantic space of features that encompasses both the seen instances (those in the training data) and the unseen instances (those missing from training data). Regarding automated knowledge discovery, some of the most promising technical approaches for zero-shot learning include:Building classification models from statistical knowledge that was gained in prior supervised learning projects that are in distinct but semantically adjacent object-recognition domains (identifying a never-seen class of vertebrate based on features extracted from a related species).Extracting semantic knowledge of target objects from textual descriptions of the targeted classes (crawled Web articles that describe the visual features of the species to be recognized).Using word vectors and other graph approaches to refine inferences of the semantic features of the target classes from those of the source classes, given the availability of textual descriptions of the target classes.Zero-shot learning can’t realize its potential as an AI pipeline accelerator unless data scientists acquire tools that provides simplified access to these techniques. That, in turn, requires deep learning toolkits that support easy visual design of new models from pre-existing functional building blocks under a larger paradigm known as “transfer learning.” This depends on workbenches that provide data scientists with reuseable feature representations, neural-node layerings, weights, training methods, learning rates, and other relevant features of prior models that can be quickly brought into zero-shot AI projects.As zero-shot techniques gain adoption, and the pool of prior knowledge grows, developers of high-quality AI will grow less reliant on training data. During the next few years, we’ll see data scientists build more intelligent robotics, gaming, and pattern-recognition applications by configuring pre-existing statistical and semantic knowledge, without needing to acquire, prepare, and label huge amounts of fresh training data.When that day arrives, more AI-based applications will be able to automate the bootstrapping of their intelligence from a state of pure ignorance to one of deep knowledge through techniques that are ad-hoc, zero-shot, and situationally adaptive.That will mark the true beginning of artificial general intelligence, a dream that has motivated the AI community from the days of Alan Turing all the way to the present.", "pub_date": "2020-02-27"},
{"title": "InfoWorld Technology of the Year Awards promotional information", "overview": "Guidelines for InfoWorld Technology of the Year Award winners", "image_url": "https://images.idgesg.net/images/article/2020/02/toy-2020-intro-fin-100829113-large.jpg", "url": "https://www.infoworld.com/article/2871225/infoworld-technology-of-the-year-awards-promotional-information.html", "body": "Congratulations! To help you promote your win, InfoWorld has supplied the following PR information and usage guidelines.Note that companies named as an InfoWorld Technology of the Year Award winner may be referenced in press outreach to publicize the awards and InfoWorld’s content about the awards. InfoWorld will not disclose proprietary corporate information, but it may highlight information included by your organization in the original InfoWorld Technology of the Year Awards content.If you would like a physical award, or print or electronic reprints, you can place an order through the YGS Group via phone at (800) 290-5460 x129 or via email at infoworld@theYGSgroup.com. Note that YGS solely determines the cost of the awards’ production.InfoWorld logo usage and guidelinesAs an InfoWorld Technology of the Year winner, you have the opportunity to purchase a license for the rights to use the Technology of the Year logo. Please contact the YGS Group via phone at (800) 290-5460 x129 or via email at infoworld@theYGSgroup.com.The InfoWorld Technology of the Year brand is a valuable asset that International Data Group needs to protect. We ask that you help us by properly using the logo in accordance with our guidelines listed below. Accordingly, we ask that your business partners, customers, and other third parties adhere to the guidelines listed below.Parties given permission to use the InfoWorld Technology of the Year logotype must adhere to the following rules:The logotype may not be altered in any manner, including size, proportions, colors, elements, type, or in any other respect. You may not animate, morph, or otherwise distort its perspective or dimensional appearance.The logotype may not be combined with any other graphic or textural elements and may not be used as a design element of any other logo or trademark.The logotype must be separated from your company name and product names by the space of one logo width or one inch, whichever is greatest.If you are unsure if your usage is within these guidelines, please email Stacey Raap in IDG Communications Marketing.PR opportunitiesPlease read the following guidelines carefully before preparing a press release that references InfoWorld and the InfoWorld Technology of the Year rankings.If your company wants to include a quote attributed to an InfoWorld spokesperson, please use the following quote to reinforce InfoWorld Technology of the Year key messages. Modified versions of this quote are subject to approval from InfoWorld. All quotes should be attributed to Doug Dineley, Executive Editor, InfoWorld.“If digital transformation means anything, it means taking advantage of the latest advances in software development, cloud computing, data analytics, and AI to improve your business,” said Doug Dineley, executive editor of InfoWorld. “Our 2020 Technology of the Year Award winners are the platforms and tools that the most innovative companies are using to tap the power of data, streamline business processes, and respond more quickly to customers and new business opportunities.” All communications involving InfoWorld and the InfoWorld Technology of the Year Awards must be consistent with the style and content listed below:The full feature name is the InfoWorld Technology of the Year Awards.InfoWorld is always one word, and the “w” is capitalized.On first reference, list the publication as “IDG’s InfoWorld.” InfoWorld as a stand-alone name may be used after the first reference.If a subsidiary of an organization is included in the rankings, press releases or marketing material should specify that the unit—not the parent organization—received the award. If the parent unit is cited, the name of the subsidiary unit should be more prominent in placement, size, and usage than that of the parent unit.The following approved InfoWorld corporate boilerplate and short InfoWorld Technology of the Year Awards description may be used, where appropriate, in press releases referencing inclusion in the InfoWorld Technology of the Year feature.Selected by InfoWorld editors and reviewers, the annual awards identify the best and most innovative products on the IT landscape. Winners are drawn from products tested during the past year, with the final selections made by InfoWorld’s Reviews staff.   InfoWorld is the leading resource for content and tools on modernizing enterprise IT. Our editors and writers provide first-hand experience from testing, deploying, and managing implementation of emerging enterprise technologies. InfoWorld’s website (InfoWorld.com) and custom solutions provide a deep dive into specific technologies to help IT decision-makers excel in their roles and provide opportunities for IT vendors to reach this audience. InfoWorld is published by IDG Communications, a subsidiary of International Data Group (IDG), the world’s leading media, events, and research company. Company information is available at www.idg.com.IDG Communications, an International Data Group (IDG) company, brings together the leading editorial brands (CIO, Computerworld, CSO, InfoWorld, JavaWorld, and Network World) to serve the information needs of our technology and security-focused audiences. As the premier high-tech B2B media company, we leverage the strengths of our premium owned and operated brands, while simultaneously harnessing their collective reach and audience affinity. We provide market leadership and converged marketing solutions for our customers to engage IT and security decision-makers across our portfolio of award-winning websites, events, magazines, products, and services. Company information is available at www.idg.com.", "pub_date": "2020-02-11"},
{"title": "5 reasons to choose PyTorch for deep learning", "overview": "TensorFlow still has certain advantages, but a stronger case can be made for PyTorch every day", "image_url": "https://images.idgesg.net/images/article/2018/02/number-5-on-fire_top-five_five-tips-100750848-large.jpg", "url": "https://www.infoworld.com/article/3528780/5-reasons-to-choose-pytorch-for-deep-learning.html", "body": "PyTorch is definitely the flavor of the moment, especially with the recent 1.3 and 1.4 releases bringing a host of performance improvements and more developer-friendly support for mobile platforms. But why should you choose to use PyTorch instead of other frameworks like MXNet, Chainer, or TensorFlow? Let’s look into five reasons that add up to a strong case for PyTorch.Before we get started, a plea to TensorFlow users who are already typing furious tweets and emails even before I begin: Yes, there are also plenty of reasons to choose TensorFlow over PyTorch, especially if you’re targeting mobile or web platforms. This isn’t intended to be a list of reasons that “TensorFlow sucks” and “PyTorch is brilliant,” but a set of reasons that together make PyTorch the framework I turn to first. TensorFlow is great in its own ways, I admit, so please hold off on the flames.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesPyTorch is PythonOne of the primary reasons that people choose PyTorch is that the code they look at is fairly simple to understand; the framework is designed and assembled to work with Python instead of often pushing up against it. Your models and layers are simply Python classes, and so is everything else: optimizers, data loaders, loss functions, transformations, and so on.Due to the eager execution mode that PyTorch operates under, rather than the static execution graph of traditional TensorFlow (yes, TensorFlow 2.0 does offer eager execution, but it’s a touch clunky at times) it’s very easy to reason about your custom PyTorch classes, and you can dig into debugging with TensorBoard or standard Python techniques all the way from  statements to generating flame graphs from stack trace samples. This all adds up to a very friendly welcome to those coming into deep learning from other data science frameworks such as Pandas or Scikit-learn.PyTorch also has the plus of a stable API that has only had one major change from the early releases to version 1.3 (that being the change of Variables to Tensors). While this is undoubtedly due to its young age, it does mean that the vast majority of PyTorch code you’ll see in the wild is recognizable and understandable no matter what version it was written for.PyTorch comes ready to use While the “batteries included” philosophy is definitely not exclusive to PyTorch, it’s remarkably easy to get up and running with PyTorch. Using PyTorch Hub, you can get a pre-trained ResNet-50 model with just one line of code:And PyTorch Hub is unified across domains, making it a one-stop shop for architectures for working with text and audio as well as vision.As well as models, PyTorch comes with a long list of, yes, loss functions and optimizers, like you’d expect, but also easy-to-use ways of loading in data and chaining built-in transformations. It’s also rather straightforward to build your own loaders or transforms. Because everything is Python, it’s simply a matter of implementing a standard class interface.One little note of caution is that a  of the batteries that are included with PyTorch have been very biased towards vision problems (found in the torchvision package), with some of the text and audio support being more rudimentary. I’m happy to report that in the post-1.0 era, the torchtext and torchaudio packages are being improved upon considerably.Also on InfoWorld: The best software development, cloud computing, data analytics, and machine learning productsPyTorch rules researchPyTorch is heaven for researchers, and you can see this in its use in papers at all major deep learning conferences. In 2018, PyTorch was growing fast, but in 2019, it has become the framework of choice at CVPR, ICLR, and ICML, among others. The reason for this wholehearted embrace is definitely linked to our first reason above: PyTorch is Python.Experimenting with new concepts is much easier when creating new custom components is a simple, stable subclass of a standard Python class. And the flexibility offered means that if you want to write a layer that sends parameter information to TensorBoard, ElasticSearch, or an Amazon S3 bucket... you can just do it. Want to pull in esoteric libraries and use them inline with network training or an odd new attempt at a training loop? PyTorch is not going to stand in your way.One thing holding PyTorch back a little has been the lack of a clear path from research to production. Indeed, TensorFlow still rules the roost for production usage, no matter how much PyTorch has taken over research. But with PyTorch 1.3 and the expansion of TorchScript, it has become easy to use Python annotations that use the JIT engine to compile research code into a graph representation, with resulting speedups and easy export to a C++ runtime. And these days, integrating PyTorch with Seldon Core and Kubeflow is supported, allowing for production deployments on Kubernetes that are almost (not quite) as simple as with TensorFlow.PyTorch makes learning deep learning easyThere are dozens of deep learning courses out there, but for my money the fast.ai course is the best — and it’s free! While the first year of the course leaned heavily on Keras, the fast.ai team — Jeremy Howard, Rachel Thomas, and Sylvain Gugger — switched to PyTorch in the second iteration of the course and haven’t looked back. (Though to be fair, they are bullish on Swift for TensorFlow.)In the most recent version of the course, you’ll discover how to achieve state-of-the-art results on tasks such as classification, segmentation, and predictions in text and vision domains, along with learning all about GANs and a host of tricks and insights that even hardened experts will find illuminating.While the fast.ai course uses fast.ai’s own library that provides further abstractions on top of PyTorch (making it even easier to get to grips with deep learning), the course also delves deep into the fundamentals, building a PyTorch-like library from scratch, which will give you a thorough understanding of how the internals of PyTorch actually work. The fast.ai team even manages to fix some bugs in mainline PyTorch along the way. Also on InfoWorld: Artificial intelligence predictions for 2020PyTorch has a great communityFinally, the PyTorch community is a wonderful thing. The main website at pytorch.org has both great documentation that is kept in good sync with the PyTorch releases and an excellent set of tutorials that cover everything from an hour blitz of PyTorch’s main features to deeper dives on how to extend the library with custom C++ operators. While the tutorials could use a little more standardization around things like training/validation/test splits and training loops, they are an invaluable resource, especially when a new feature is introduced.Beyond the official documentation, the Discourse-based forum at discuss.pytorch.org is an amazing resource where you can easily find yourself talking to and being helped out by core PyTorch developers. With over fifteen hundred posts a week, it’s a friendly and active community. And while discussion is more focused on fast.ai’s own library, the similar forums over at forums.fast.ai is another great community (with lots of crossover) that is eager to help newcomers in a non-gatekeeping manner, which sadly is a problem in many arenas of deep learning discussion.PyTorch today and tomorrowThere you have it—five reasons to use PyTorch. As I said at the beginning, not all of these are exclusive to PyTorch versus competitors, but the combination of all of these reasons makes PyTorch my deep learning framework of choice. There are definitely areas where PyTorch is currently deficient — e.g., in mobile, with sparse networks, and easy quantizing of models, just to pick three out of the hat. But given the high speed of development, PyTorch will be a much stronger performer in these areas by year’s end.A couple of further examples just to finish us out. First, PyTorch Elastic — introduced as an experimental feature in December — extends PyTorch’s existing distributed training packages to provide for more robust training of large-scale models. As the name suggests, it does so by running on multiple machines with elasticity, allowing nodes to drop in and out of the training job at any time without causing the entire job to come crashing to a halt.Also on InfoWorld: What is CUDA? Parallel programming for GPUsSecond, OpenAI has announced it is adopting PyTorch as its primary development framework. This is a major win for PyTorch, as it indicates that the creators of GPT-2 — a state-of-the-art language model for question answering, machine translation, reading comprehension, and summarization — believe that PyTorch offers them a more productive environment than TensorFlow for iterating over their ideas. Coming in the wake of Preferred Networks putting its deep learning framework Chainer into maintenance mode and moving to PyTorch, OpenAI’s decision highlights how far PyTorch has come in the past two years and strongly suggests that PyTorch will continue to improve and gain users in the years to come. If these big players in the AI world prefer to use PyTorch, then it’s probably good for the rest of us too.", "pub_date": "2020-02-24"},
{"title": "InfoWorld’s 2020 Technology of the Year Award winners", "overview": null, "image_url": null, "url": "https://www.infoworld.com/article/3518995/infoworlds-2020-technology-of-the-year-award-winners.html", "body": "", "pub_date": "2020-02-05"},
{"title": "Microsoft speeds up PyTorch with DeepSpeed", "overview": "A new open source project from Microsoft accelerates machine learning framework PyTorch without needing major code rewrites", "image_url": "https://images.techhive.com/images/article/2017/03/gear-shift-100714533-large.jpg", "url": "https://www.infoworld.com/article/3526449/microsoft-speeds-up-pytorch-with-deepspeed.html", "body": "Microsoft has released DeepSpeed, a new deep learning optimization library for PyTorch, that is designed to reduce memory use and train models with better parallelism on existing hardware.According to a Microsoft Research blog post announcing the new framework, DeepSpeed improves PyTorch model training through a memory optimization technology that increases the number of possible parameters a model can be trained with, makes better use of the memory local to the GPU, and requires only minimal changes to an existing PyTorch application to be useful.Also on InfoWorld: Artificial intelligence predictions for 2020It’s the minimal impact on existing PyTorch code that has the greatest potential impact. As machine learning libraries grow entrenched, and more applications become dependent on them, there is less room for new frameworks, and more incentive to make existing frameworks more performant and scalable.PyTorch is already fast when it comes to both computational and development speed, but there’s always room for improvement. Applications written for PyTorch can make use of DeepSpeed with only minimal changes to the code; there’s no need to start from scratch with another framework.One way DeepSpeed enhances PyTorch is by improving its native parallelism. In one example, provided by Microsoft in the DeepSpeed documentation, attempting to train a model using PyTorch’s Distributed Data Parallel system across Nvidia V100 GPUs with 32GB of device memory “[ran] out of memory with 1.5 billion parameter models,” while DeepSpeed was able to reach 6 billion parameters on the same hardware.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesAnother touted DeepSpeed improvement is more efficient use of GPU memory for training. By partitioning the model training across GPUs, DeepSpeed allows the needed data to be kept close at hand, reduces the memory requirements of each GPU, and reduces the communication overhead between GPUs.A third benefit is allowing for more parameters during model training to improve prediction accuracy. Hyperparameter optimization, which refers to tuning the parameters or variables of the training process itself, can improve the accuracy of a model but typically at the cost of manual effort and expertise.To eliminate the need for expertise and human effort, many machine learning frameworks now support some kind of automated hyperparameter optimization. With DeepSpeed, Microsoft claims that “deep learning models with 100 billion parameters” can be trained on “the current generation of GPU clusters at three to five times the throughput of the current best system.”Also on InfoWorld: Why you should use Python for machine learningDeepSpeed is available as free open source under the MIT License. Tutorials in the official repo work with Microsoft Azure, but Azure is not required to use DeepSpeed.", "pub_date": "2020-02-10"},
{"title": "InfoWorld Technology of the Year Awards", "overview": "About InfoWorld’s annual awards for the best hardware, software, and cloud services of the year", "image_url": "https://images.idgesg.net/images/article/2020/02/toy-2020-intro-fin-100829113-large.jpg", "url": "https://www.infoworld.com/article/2871319/infoworld-technology-of-the-year.html", "body": "InfoWorld’s annual Technology of the Year Awards recognize the best and most innovative products in the areas of software development, cloud computing, big data analytics, and machine learning.Note that we do not have a formal submission process for the Technology of the Year Awards. Winners are chosen based on our coverage of products throughout the year. However, we welcome suggestions regarding products we should consider for coverage and for awards at any time. To have your hardware, software, or cloud service considered for a 2019 Technology of the Year Award, please send the name of the product and a link to product information to Executive Editor Doug Dineley before September 30, 2020.The 2020 Technology of the Year Award winners will be announced February 3, 2021.Previous Technology of the Year Award winners2020 Technology of the Year Awards2019 Technology of the Year Awards2018 Technology of the Year Awards2017 Technology of the Year Awards2016 Technology of the Year Awards2015 Technology of the Year Awards2014 Technology of the Year Awards2013 Technology of the Year Awards2012 Technology of the Year Awards2011 Technology of the Year Awards2010 Technology of the Year AwardsMarketing/PR:  Congratulations on winning a Technology of the Year award! Look here for all the information you’ll need to tell the world.", "pub_date": "2020-02-11"},
{"title": "Swift language targets machine learning", "overview": "Swift 6 roadmap also emphasizes expressive and elegant APIs and ‘fantastic’ development experience", "image_url": "https://images.techhive.com/images/article/2014/04/close-up-of-arrow-in-bulls-eye-of-target-skd283477sdc-100264825-large.jpg", "url": "https://www.infoworld.com/article/3526594/swift-language-targets-machine-learning.html", "body": "Moving toward Swift 6, the core development team behind Apple’s Swift programming language has set priorities including refining the language for use in machine learning.Ambitions in the machine learning space are part of plans to invest in “user-empowering directions” for the language. Apple is not the only company with machine learning ambitions for Swift; Google has integrated Swift with the TensorFlow machine learning library in a project called Swift for TensorFlow. And the Swift community has created Swift Numerics, a library that can be used for machine learning.Also on InfoWorld: Artificial intelligence predictions for 2020In addition to machine learning, directions eyed for Swift include building APIs such as variadic generics and DSL capabilities such as function builders. Solutions for major language features such as memory ownership and concurrency also are part of the plan. Other specific goals for Swift, cited in a January 2020 bulletin, include:Creating a “fantastic development experience,” with developers able to be highly productive and joyful when programming in the language. These investments include faster builds, better diagnostics, responsive code completion, and reliable debugging. Most current engineering work in the project covers these areas.Growing the Swift software ecosystem, including expanding the number of supported platforms and improving how software written in Swift is deployed. Also planned is support for cross-platform tools such as Language Server Protocol, the Swift Package Manager, code formatting, and refactoring. Cultivation of a rich open source library ecosystem also is eyed.Keep up with hot topics in software development with InfoWorld’s App Dev Report newsletterIntroduced in June 2014, Swift has been rising steadily in the Tiobe index of programming language popularity, jumping from 20th place a year ago to 10th place in the February 2020 index. Its predecessor, Objective-C, has done the reverse, dropping from 10th a year ago to 20th this month. The release currently in development is Swift 5.2. A succession of Swift 5.x releases are expected before Swift 6.", "pub_date": "2020-02-11"},
{"title": "Interested in machine learning? Better learn PyTorch", "overview": "Don’t look now, but easy, straightforward PyTorch has become the hottest product in data science", "image_url": "https://images.idgesg.net/images/article/2017/09/data-science-certification7-100734862-large.jpg", "url": "https://www.infoworld.com/article/3518453/interested-in-machine-learning-better-learn-pytorch.html", "body": "Building on the rampant popularity of Python was always going to be a good idea for the Facebook-born PyTorch, an open source machine learning framework. Just how good of an idea, however, few could have guessed. That’s because no matter how many things you get right when launching an open source project (great docs, solid technical foundation, etc.), there is always an element of luck to a project’s success.Well, consider PyTorch lucky, then. Or blessed. Or something. Because it’s booming and, if analyst Thomas Dinsmore is to be believed, “By the end of [2020] PyTorch will have more active contributors than TensorFlow.” More contributors and more adoption? That’s a big jump for a rival to TensorFlow, long considered the industry default since its public release in 2015.Also on InfoWorld: Artificial intelligence predictions for 2020Wild and crazy adoptionAs detailed in OpenHub, TensorFlow and PyTorch are running neck-and-neck in terms of 12-month contributor totals: TensorFlow (906) and PyTorch (900). This represents huge progress by the PyTorch community, given TensorFlow’s head start, and is reflected in the growth in PyTorch’s user community, as reflected in Jeff Hale’s analysis of job posting sites for data scientist roles:To be clear, this analysis reflects  growth or decline over the past year. The TensorFlow user community is still much larger than PyTorch’s, though in academics PyTorch has gone from distant minority to overwhelming majority almost overnight. All things considered, it’s not hard to see PyTorch quickly bridging the gap at this pace.Particularly given PyTorch’s comparative advantages. Did I mention Python?Lowering the bar to data scienceAs Serdar Yegulalp wrote back in 2017 at the launch of PyTorch, “A chief advantage to PyTorch is that it lives in and allows the developer to plug into the vast ecosystem of Python libraries and software. Python programmers are also encouraged to use the styles they’re familiar with, rather than write code specifically meant to be a wrapper for an external C/C++ library.” This means that PyTorch has always had the advantage of approachability. The documentation is excellent and there’s a healthy community of developers happy to help out.This advantage is further accentuated by PyTorch’s computational graph setup. As Savan Visalpara explains:TensorFlow is ‘Define-and-Run,’ whereas PyTorch is ‘Define-by-Run.’ In [a] Define-and-Run framework, one would define conditions and iterations in the graph structure then run it. In [a] Define-by-Run [framework, the] graph structure is defined on-the-fly during forward computation, [which is a more] natural way of coding.Dhiraj Kumar concurs, arguing that such a dynamic model allows data scientists to “fully see each and every computation and know exactly what is going on.”To be sure, with the release of TensorFlow 2.0, Google has made TensorFlow “eager by default.” As Martin Heller explains, “Eager execution means that TensorFlow code runs when it is defined, as opposed to adding nodes and edges to a graph to be run in a session later, which was TensorFlow’s original mode.”Also on InfoWorld: PyTorch vs. TensorFlow: How to chooseWhile this sounds great for TensorFlow because it helps the framework compete better with PyTorch in terms of ease of use, “In enabling Eager mode by default, TensorFlow forces a choice onto their users — use eager execution for ease of use and require a rewrite for deployment, or don’t use eager execution at all.While this is the same situation that PyTorch is in, the opt-in nature of PyTorch’s TorchScript is likely to be more palatable than TensorFlow’s ‘Eager by default,’” warns Horace He. TensorFlow Eager mode also suffers from performance issues, though we’d expect these to improve over time.In sum, while the market still leans heavily on TensorFlow, PyTorch’s easy-to-learn, straightforward-to-use approach that ties into the world’s most popular programming language for data science is proving a winner. Although academia has been fastest to embrace PyTorch, we should expect to see ever-increasing adoption with the enterprise set, too.", "pub_date": "2020-01-30"},
{"title": "Interested in machine learning? Better learn PyTorch", "overview": "Don’t look now, but easy, straightforward PyTorch has become the hottest product in data science", "image_url": "https://images.idgesg.net/images/article/2017/09/data-science-certification7-100734862-large.jpg", "url": "https://www.infoworld.com/article/3519411/interested-in-machine-learning-better-learn-pytorch.html", "body": "Building on the rampant popularity of Python was always going to be a good idea for the Facebook-born PyTorch, an open source machine learning framework. Just how good of an idea, however, few could have guessed. That’s because no matter how many things you get right when launching an open source project (great docs, solid technical foundation, etc.), there is always an element of luck to a project’s success.Machine learning with Python: An introductionWell, consider PyTorch lucky, then. Or blessed. Or something. Because it’s booming and, if analyst Thomas Dinsmore is to be believed, “By the end of [2020] PyTorch will have more active contributors than TensorFlow.” More contributors and more adoption? That’s a big jump for a rival to TensorFlow, long considered the industry default since its public release in 2015.Also on InfoWorld: Artificial intelligence predictions for 2020Wild and crazy adoptionAs detailed in OpenHub, TensorFlow and PyTorch are running neck-and-neck in terms of 12-month contributor totals: TensorFlow (906) and PyTorch (900). This represents huge progress by the PyTorch community, given TensorFlow’s head start, and is reflected in the growth in PyTorch’s user community, as reflected in Jeff Hale’s analysis of job posting sites for data scientist roles:To be clear, this analysis reflects  growth or decline over the past year. The TensorFlow user community is still much larger than PyTorch’s, though in academics PyTorch has gone from distant minority to overwhelming majority almost overnight. All things considered, it’s not hard to see PyTorch quickly bridging the gap at this pace.Particularly given PyTorch’s comparative advantages. Did I mention Python?Lowering the bar to data scienceAs Serdar Yegulalp wrote back in 2017 at the launch of PyTorch, “A chief advantage to PyTorch is that it lives in and allows the developer to plug into the vast ecosystem of Python libraries and software. Python programmers are also encouraged to use the styles they’re familiar with, rather than write code specifically meant to be a wrapper for an external C/C++ library.” This means that PyTorch has always had the advantage of approachability. The documentation is excellent and there’s a healthy community of developers happy to help out.This advantage is further accentuated by PyTorch’s computational graph setup. As Savan Visalpara explains:TensorFlow is ‘Define-and-Run,’ whereas PyTorch is ‘Define-by-Run.’ In [a] Define-and-Run framework, one would define conditions and iterations in the graph structure then run it. In [a] Define-by-Run [framework, the] graph structure is defined on-the-fly during forward computation, [which is a more] natural way of coding.Dhiraj Kumar concurs, arguing that such a dynamic model allows data scientists to “fully see each and every computation and know exactly what is going on.”To be sure, with the release of TensorFlow 2.0, Google has made TensorFlow “eager by default.” As Martin Heller explains, “Eager execution means that TensorFlow code runs when it is defined, as opposed to adding nodes and edges to a graph to be run in a session later, which was TensorFlow’s original mode.”Also on InfoWorld: PyTorch vs. TensorFlow: How to chooseWhile this sounds great for TensorFlow because it helps the framework compete better with PyTorch in terms of ease of use, “In enabling Eager mode by default, TensorFlow forces a choice onto their users — use eager execution for ease of use and require a rewrite for deployment, or don’t use eager execution at all.While this is the same situation that PyTorch is in, the opt-in nature of PyTorch’s TorchScript is likely to be more palatable than TensorFlow’s ‘Eager by default,’” warns Horace He. TensorFlow Eager mode also suffers from performance issues, though we’d expect these to improve over time.In sum, while the market still leans heavily on TensorFlow, PyTorch’s easy-to-learn, straightforward-to-use approach that ties into the world’s most popular programming language for data science is proving a winner. Although academia has been fastest to embrace PyTorch, we should expect to see ever-increasing adoption with the enterprise set, too.", "pub_date": "2020-01-30"},
{"title": "Use Azure Cognitive Services to automate forms processing", "overview": "Form Recognizer brings unsupervised machine learning to paper document processing, and it’s a snap to  build into your applications", "image_url": "https://images.idgesg.net/images/article/2019/11/ai_artificial_intelligence_technology_abstract_layered_background_by_4x-image_gettyimages_1097235688-100817777-large.jpg", "url": "https://www.infoworld.com/article/3528784/use-azure-cognitive-services-to-automate-forms-processing.html", "body": "Microsoft’s Cognitive Services, powered by machine learning, are an easy way to add artificial intelligence to your apps, offering pay-as-you-go access to a selection of useful algorithms. Unlike many other web services, they’re continuously evolving, improving as they ingest more and more labeled data.That’s an important difference between machine learning and other, more familiar, algorithms. As Microsoft improves its training and models, the scope of the services continues to get better, along with responsiveness and accuracy. Some can even take advantage of a process called Transfer Learning, where training a model with one set of data improves its performance with another.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesContinuous improvement isn’t the only benefit of the research work Microsoft puts into its Cognitive Services. Cognitive Services operationalize that research, delivering new tools and services as research moves from the lab into commercial products. What matters here is the transition between preview and general availability, as Azure and Microsoft Research work together to take what was pure research and turn it into tools you can include in your applications.Microsoft has been able to containerize some of its Cognitive Services for use on Azure’s Edge servers and on any other platform that supports Docker. Instead of pushing data to the cloud over low-bandwidth links, you can process data locally, as part of an IoT Hub instance, sending only the information that matters to other applications or to administrators.Introducing Form RecognizerOne of the more interesting new services currently in preview is Form Recognizer. As organizations work their way through digital transformations, it’s important to bring paper documents and forms into new business processes. Traditional scanning and optical character recognition go some way to digitizing documents, but they miss the semistructured nature of forms, scanning all the data on the page.Form Recognizer takes a more nuanced approach to working with form data, using machine learning to parse the structure of a form and then extract the information. By building a model of the structure of a form, you can use that model to build a semantically tagged output, with key/value pairs and tables that can then be used to populate structured stores, either using SQL or NoSQL document databases. All you need are a handful of forms to use as training data, allowing you to build a labeled data set that can be used to tune the Form Recognizer model to work with your files.As Form Recognizer is an API, you can incorporate it in new and existing business processes, replacing manual data capture processes while flagging exceptions that may need human intervention. You can even use Form Recognizer in conjunction with Power Platform tools such as Power BI to deliver business insights from what would have been paper-only data.Also on InfoWorld: Microsoft Azure cloud migration: 3 success storiesTraining a Form Recognizer model One of the interesting aspects of Form Recognizer is that the underlying model uses unsupervised learning. There’s no need to label the training data. The system recognizes the form elements and generates the appropriate data structures for your form data. Although that’s an easier way to train a system, you do have the option of using labeled data to get more accurate and faster results.A key element of the training process is the layout API. This gives the model a structure for the layout of a form, with labels for the various fields. Using the data from this and from labeled training forms, you can quickly define the output data structures and ensure that your code is ready to work with the service.Building labeled samples for training requires a local application, available as a Docker container with a Web UI. You can download it from Microsoft and run the container on Windows, MacOS, or Linux, if you have Docker installed. There’s even the option to run the container with Azure Kubernetes Service (AKS) or on any other Kubernetes infrastructure. Form images are stored in an Azure Blob, and the local recognizer will OCR the forms, making them ready for you to label the various form elements that you want to extract using Form Recognizer. You only need five or six sample forms to train the model.Once trained, you have a custom Form Recognizer model with its own model ID and an accuracy score. If you want to improve the model, add more sample data. The resulting model can be tested using the training tool on documents that haven’t been part of your training set. You’ll be presented with a view of the source document with bounding boxes for recognized data and a confidence level for each element. It’s important to note that Form Recognizer can’t work with all form elements; at the moment there’s no support for check boxes or for complex tables.Also on InfoWorld: Artificial intelligence predictions for 2020Using Form Recognizer in your appsBuilding an application around Form Recognizer is relatively easy. If you’re not using a language with a supported SDK there’s a REST API that can take your form images and extract the data. The service currently supports most common image formats: JPC, PNG, PDF, and TIFF.The API is relatively simple; it uses POST to upload and analyze the form contents, with a GET to bring back the result. Images are sent as part of a JSON object with the POST, or as a standard file stream. Once the job has been loaded, a standard HTTP 2020 response returns the result ID that will hold the analysis results. You can then make a call to the service with the result ID. If the form has been processed, the results will be delivered in a JSON object that can be parsed, delivering the form key/value pairs and any result tables.Like all the Cognitive Services, the results have a confidence level. You can use this to direct some forms for manual checks, otherwise delivering the result data into your line of business applications, either storing the data for future use or using it to drive a business process.One useful feature of Form Recognizer (and one that clearly builds on Microsoft’s own requirements for its expense system) is a prebuilt model that works with common U.S. receipt formats. You can use it to capture and feed receipt data into your own expenses workflow, using a phone camera to capture receipt data on the go. Workers will be able to generate expense reports from their phones without having to spend time entering data into web forms; the Form Recognizer tools will capture the necessary data and, together with user information and device locations, update records automatically.Keep up with the latest developments in software development, cloud computing, data analytics, and machine learning with the InfoWorld Daily newsletterGetting started with Form Recognizer is relatively simple, and with a generous limit of 500 free pages per month, you should be able to see quickly if it works for you. Once up and running, it should provide a useful bridge between pen and paper and the digital world, using photographs or scans to quickly bring form content into your business processes. With the quality of modern phone cameras and their support for computational photography, it’s possible to make form recognition a simple plug-in that takes a photo and uploads it to your recognizer, saving a local copy for your records.Form Recognizer is a tool that quickly shows the benefit of machine learning, with a model that’s designed to work flexibly in a relatively closed domain. Applying Azure’s Cognitive Services to specific business problems makes a lot of sense. Handling a paper-to-digital transition is one of those problems that has long been a blocker to improving business processes. Using machine learning to reduce the cost and time needed to deliver digitization is a win for most businesses, especially if it means that we can use the cameras in our pocket rather than expensive scanners and unreliable OCR software.", "pub_date": "2020-02-25"},
{"title": "How AI will improve API security", "overview": "By using AI models to continuously inspect all API activity, we can fill in the cracks left by policy-based API protections", "image_url": "https://images.idgesg.net/images/article/2019/11/ai_artificial_intelligence_ml_machine_learning_vector_by_kohb_gettyimages_1146634284-100817775-large.jpg", "url": "https://www.infoworld.com/article/3516595/how-ai-will-improve-api-security.html", "body": "APIs have become the crown jewels of organizations’ digital transformation initiatives, empowering employees, partners, customers, and other stakeholders to access applications, data, and business functionality across their digital ecosystem. So, it’s no wonder that hackers have increased their waves of attacks against these critical enterprise assets.Unfortunately, it looks like the problem will only worsen. Gartner has predicted that, “By 2022, API abuses will be the most-frequent attack vector resulting in data breaches for enterprise web applications.”Also on InfoWorld: AI, machine learning, and deep learning: Everything you need to knowMany enterprises have responded by implementing API management solutions that provide mechanisms, such as authentication, authorization, and throttling. These are must-have capabilities for controlling who accesses APIs across the API ecosystem—and how often. However, in building their internal and external API strategies, organizations also need to address the growth of more sophisticated attacks on APIs by implementing dynamic, artificial intelligence (AI) driven security.This article examines API management and security tools that organizations should incorporate to ensure security, integrity, and availability across their API ecosystems.Rule-based and policy-based security measuresRule-based and policy-based security checks, which can be performed in a static or dynamic manner, are mandatory parts of any API management solution. API gateways serve as the main entry point for API access and therefore typically handle policy enforcement by inspecting incoming requests against policies and rules related to security, rate limits, throttling, etc. Let’s look closer at some static and dynamic security checks to see the additional value they bring.Static security checks do not depend on the request volume or any previous request data, since they usually validate message data against a predefined set of rules or policies. Different static security scans are performed in gateways to block SQL injection, cohesive parsing attacks, entity expansion attacks, and schema poisoning, among others.Meanwhile, static policy checks can be applied to payload scanning, header inspection, and access patterns, among others. For example, SQL injection is a common type of attack performed using payloads. If a user sends a JSON (JavaScript Object Notation) payload, the API gateway can validate this particular request against predefined JSON schema. The gateway also can limit the number of elements or other attributes in content as required to protect against harmful data or text patterns within messages.Dynamic security checks, in contrast to static security scans, are always checking against something that varies over time. Usually this involves validating request data with decisions made using existing data. Examples of dynamic checks include access token validation, anomaly detection, and throttling. These dynamic checks depend heavily on the data volume being sent to the gateway. Sometimes these dynamic checks occur outside the API gateway, and then the decisions are communicated to the gateway. Let’s look at a couple examples.Throttling and rate limiting are important for reducing the impact of attacks, because whenever attackers get access to APIs, the first thing they do is read as much data as possible. Throttling API requests — i.e., limiting access to the data — requires that we keep a count of incoming requests within a specific time window. If a request count exceeds the allocated amount at that time, the gateway can block API calls. With rate limiting, we can limit the concurrent access allowed for a given service.Authentication helps API gateways to identify each user who invokes an API uniquely. Available API gateway solutions generally support basic authentication, OAuth 2.0, JWT (JSON Web Token) security, and certificate-based security. Some gateways also provide an authentication layer on top of that for additional fine-grained permission validation, which is usually based on XACML (eXtensible Access Control Markup Language) style policy definition languages. This is important when an API contains multiple resources that need different levels of access control for each resource.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesLimitations of traditional API securityPolicy-based approaches around authentication, authorization, rate limiting, and throttling are effective tools, but they still leave cracks through which hackers can exploit APIs. Notably, API gateways front multiple web services, and the APIs they manage are frequently loaded with a high number of sessions. Even if we analyzed all those sessions using policies and processes, it would be difficult for a gateway to inspect every request without additional computation power.Additionally, each API has its own access pattern. So, a legitimate access pattern for one API could indicate malicious activity for a different API. For example, when someone buys items through an online shopping application, they will conduct multiple searches before making the purchase. So, a single user sending 10 to 20 requests to a search API within a short period of time can be a legitimate access pattern for a search API. However, if the same user sends multiple requests to the buying API, the access pattern could indicate malicious activity, such as a hacker trying to withdraw as much as possible using a stolen credit card. Therefore, each API access pattern needs to be analyzed separately to determine the correct response.Yet another factor is that significant numbers of attacks happen internally. Here, users with valid credentials and access to systems utilize their ability to attack those systems. Policy-based authentication and authorization capabilities are not designed to prevent these kinds of attacks. Even if we could apply more rules and policies to an API gateway to protect against the attacks described here, the additional overhead on the API gateway would be unacceptable. Enterprises cannot afford to frustrate genuine users by asking them to bear the processing delays of their API gateways. Instead, gateways need to process valid requests without blocking or slowing user API calls.The case for adding an AI security layerTo fill the cracks left by policy-based API protections, modern security teams need artificial intelligence-based API security that can detect and respond to dynamic attacks and the unique vulnerabilities of each API. By applying AI models to continuously inspect and report on all API activity, enterprises could automatically discover anomalous API activity and threats across API infrastructures that traditional methods miss.Even in cases where standard security measures are able to detect anomalies and risks, it can take months to make the discoveries. By contrast, using pre-built models based on user access patterns, an AI-driven security layer would make it possible to detect some attacks in near real time.Importantly, AI engines usually run outside of API gateways and communicate their decisions to them. Because the API gateway does not have to expend resources to process these requests, the addition of AI-security typically does not impact runtime performance.Integrating policy-based and AI-driven API securityWhen adding AI-powered security to an API management implementation, there will be a security enforcement point and a decision point. Typically, these units are independent due to the high computational power required, but the latency should not be allowed to affect their efficiency.The API gateway intercepts API requests and applies various policies. Linked to it is the security enforcement point, which describes the attributes of each request (API call) to the decision point, requests a security decision, and then enforces that decision in the gateway. The decision point, powered by AI, continuously learns the behavior of each API access pattern, detects anomalous behaviors, and flags different attributes of the request.Also on InfoWorld: What AI can really do for your business (and what it can’t)There should be an option to add policies to the decision point as needed and invoke these policies—which may vary from API to API—during the learning period. Any policies should be defined by the security team once the potential vulnerabilities of each API they plan to expose are thoroughly understood. However, even without support from external policies, adaptive, AI-powered decision point and enforcement point technology will eventually learn and prevent some of the complex attacks that we cannot detect with policies.Another advantage of having two separate security enforcement point and decision point components is the ability to integrate with existing API management solutions. A simple user interface enhancement and customized extension could integrate the security enforcement point to the API publisher and gateway. From the UI, the API publisher could choose whether to enable AI security for the published API, along with any special policies that needed. The extended security enforcement point would publish the request attributes to the decision point and restrict access to the API according to the decision point’s response.However, publishing events to the decision point and restricting access based on its response will take time and depend heavily on the network. Therefore, it is best implemented asynchronously with the help of a caching mechanism. This will affect the accuracy a bit, but when considering the efficiency of the gateway, adding an AI security layer will minimally contribute to the overall latency.AI-driven security layer challengesOf course, benefits don’t come without costs. While an AI-driven security layer offers an additional level of API protection, it presents some challenges that security teams will need to address.. The additional AI security layer adds some overhead to the message flow. So, mediation solutions should be smart enough to handle information gathering and publishing outside the main mediation flow.. A high volume of false positives will require additional review by security professionals. However, with some advanced AI algorithms, we can reduce the number of false positives triggered.. People feel uncomfortable when they don’t understand how a decision was made. Dashboards and alerts can help users to visualize the factors behind a decision. For example, if an alert clearly states that a user was blocked for accessing the system at an abnormal rate of 1,000-plus times within a minute, people can understand and trust the system’s decision.. Most AI and machine learning solutions rely on massive volumes of data, which is often sensitive and personal. As a result, these solutions could become prone to data breaches and identity theft. Complying with the European Union GDPR (General Data Protection Regulation) helps to mitigate this risk but doesn’t eliminate it entirely.. The most powerful AI systems are trained through supervised learning, which requires labeled data that is organized to make it understandable by machines. But labeled data has limits, and the future automated creation of increasingly difficult algorithms will only exacerbate the problem.. An AI system’s effectiveness depends on the data it is trained on. Too often, bad data is associated with ethnic, communal, gender, or racial biases, which can affect crucial decisions about individual users.Keep up with hot topics in software development with InfoWorld’s App Dev Report newsletterGiven the critical role of APIs in enterprises today, they increasingly are becoming targets for hackers and malicious users. Policy-based mechanisms, such as authentication, authorization, payload scanning, schema validation, throttling, and rate limiting, are baseline requirements for implementing a successful API security strategy. However, only by adding AI models to continuously inspect and report on all API activity will enterprises be protected against the most sophisticated security attacks emerging today.WSO2", "pub_date": "2020-01-29"},
{"title": "JetBrains taps machine learning for full-line code completion", "overview": "IntelliJ IDE's 2020 roadmap also features collaborative editing and lightweight text editing", "image_url": "https://images.techhive.com/images/article/2014/10/artificial_intelligence_virtual_digital_identity_binary_stream_thinkstock-100528010-large.jpg", "url": "https://www.infoworld.com/article/3518452/jetbrains-taps-machine-learning-for-full-line-code-completion.html", "body": "JetBrains has laid out a 2020 roadmap for IntelliJ IDEA and its IntelliJ-based IDEs. The promised new capabilities range from additional machine learning-driven code completion to collaborative editing.The company said the new machine learning-based code completion capabilities would make better use of the context for ranking completion suggestions and generate completion variants that go beyond a single identifier to provide full-line completion. Considered a major area of investment, full-line completion may take a while to appear in the product.Also on InfoWorld: The new features in Java 14Choosing your Java IDEJetBrains already had been exploring the use of machine learning for code completion, and some results of that research have made their way into products. IntelliJ now uses machine learning to improve the ranking of completion variants, and language plug-ins tag each produced completion variant with different attributes. IntelliJ also uses machine learning to determine which attributes contribute to item ranking so the most-relevant items are at the top of the list.In addition to machine learning based code completion, JetBrains cited a multitude of improvements to IntellIj for 2020, subject to change. These include:Collaborative editing support. Users would connect their IDEs to a primary system as “thin clients,” which would not require direct source code access. Each user would have their own state, with a set of open files, caret position, a completion variants list, and other capabilities. Expanded use of the IDE as a lightweight text editor. A dedicated mode for editing non-project files also is being developed.Two modes of integration with Git. Developers would be able to switch between a new UI that supports the staging area but not changelists, and the current UI based on changelists. Combining the two does not seem feasible.Easier onboarding and environment setup and configuration. The system would take care of installing Git, the Java Development Kit, and so forth. Deeper cloud integration.A redesigned project model to remove current limitations such as lack of support for arbitrary mixing of projects of different types. Benefits would include faster project opening and smoother synchronization with Maven and Gradle.Improved indexing performance as well as making indexing less-disruptive. Users also would be notified about indexing anomalies.A redesign of the read/write locks thread model to tackle the issue of UI freezes.More discoverable refactorings during autodetection. One example is adding the possibility to detect changes in the declaration of a method and adjust usage accordingly.Support for loading and unloading most plug-ins without a restart. The intent is to have an IDE that right-sizes itself for each project. Spring projects, for example, would only be loaded with plug-ins that use Spring.The addition of Code Vision capabilities for displaying rich contextual information in the code editor. This capability already has been featured in JetBrain’s Rider IDE for .NET.Localization of IntelliJ-based IDEs in Asian markets, with initial support for Chinese Simplified and support for Korean and Japanese to follow.", "pub_date": "2020-01-29"},
{"title": "Keeping your enterprise AI expertise up to speed", "overview": "AI is such a significant force, you can’t afford not to stay on top of the ever-changing developments", "image_url": "https://images.techhive.com/images/article/2016/10/1_dont_study-100689662-large.jpg", "url": "https://www.infoworld.com/article/3518475/keeping-your-enterprise-ai-expertise-up-to-speed.html", "body": "Artificial intelligence (AI) could not be a more strategic enterprise technology. As we move into the ’20s, the most disruptive business applications will be those that incorporate machine learning, deep learning, and other forms of AI.AI has become the brain driving cloud-native enterprise applications. Developers everywhere are embedding AI microservices to imbue cloud applications with data-driven machine learning intelligence. Increasingly, there is no substitute for the sophisticated AI that performs high-speed inferencing on sensor-sourced data and on data acquired from applications, clouds, hub gateways, and other online resources.How data analysis, AI, and IoT will shape the post-pandemic ‘new normal’Staying abreast of AI trends, technologies, and applications is fundamental to success in modern business, even if you’re not a data scientist or machine learning specialist. Companies that innovate with AI will dominate their industries for decades to come.Build a top-notch enterprise AI competencyConsidering how ubiquitous and dynamic the AI industry has become, keeping your enterprise AI expertise up to snuff can be a challenge. If nothing else, you should make sure that your company implements a far-reaching program of AI skills, processes, tools, platforms, and methodologies that covers the following major points:: Assemble dedicated teams of data scientists and other developers to build, train, deploy, and manage AI applications as a standardized operational process across all business functions.: Deploy an integrated, open, and trusted platform for data science, machine learning, data engineering, and application building across the multicloud.: Combine hybrid data from on-premises platforms and public clouds when building, training, deploying, and managing machine learning, deep learning, and other AI models.: Deploy a fast, scalable, hybrid data environment to manage both data at rest and data in motion for myriad AI workloads.: Adopt cloud-based AI devops tools that incorporate popular modeling frameworks, automate model management and hyperparameter tuning, accelerate AI workloads across distributed GPUs and other compute nodes, and enable developers to access pretrained models from libraries across public clouds, private clouds, and on-premises systems.: Distribute and scale AI inferencing, training, modeling, and data preparation workloads across public clouds, private clouds, and on-premises systems.: Adopt robust tools for data integration, security, governance, lifecycle management, devops, and orchestration across all AI initiatives, projects, applications, and workloads.Also on InfoWorld: Artificial intelligence predictions for 2020Keep up with enterprise AI trendsWrapping your head around AI’s complexities is a never-ending task. As an industry analyst, I do this for a living, and even I often feel like I’m simply treading water.The AI industry is evolving so fast that all of us—enterprises, teams, individuals—need a mental map of trends in both business and technical topics. What follows is a cheat sheet of top AI trends that I’ve compiled from the last two years’ worth of my own year-end predictions, all of which—near as I can tell—are still in full force:: AI regulations are coming fast in most modern countries and every industry. Enterprise chief legal officers are mandating end-to-end AI transparency. AI risk-mitigation controls are becoming standard patterns available in data science pipeline tools. AI data science team workbenches are enabling downstream reproducibility. AI deepfakery is working its magic—benign and otherwise—more deeply into our lives.: GPUs are dominating AI acceleration. GPUs are expanding their footprint in immersive AI applications. AI systems-on-chip are dominating the hardware-accelerator wares.: AI development frameworks are becoming interchangeable within an open industry ecosystem. More data scientists are buying certified high-performance AI algorithms, trained models, and training data from online marketplaces. AI modeling frameworks are converging on a two-horse race.: SaaS-based AI solutions are reducing enterprise demand for data scientists. More labeling of AI training data is being automated through on-demand cloud services. Kubernetes-orchestrated containers are becoming integral to the AI pipeline. Reinforcement learning is becoming a mainstream AI approach. Client-side training is moving toward the AI mainstream. Blockchain is feeling its way into the AI ecosystem.: Dominant AI development frameworks are being re-engineered for superior cloud-to-edge performance. AI benchmarking frameworks are crystallizing and gaining enterprise adoption. Industry-standard AI benchmarks are becoming a competitive battlefront.: AI is automating AI developers’ core modeling functions. Automated end-to-end AI devops pipelines are becoming standard practice. Enterprise AI is shifting toward continual real-world experimentation. AI is becoming an industrialized operational business function. AI is driving closed-loop IT operations management.Also on InfoWorld: Deep learning vs. machine learning: Understand the differencesFor enterprise professionals, your strategy for staying prepared and abreast of all of this should include participation in AI industry and professional confabs taking place this year, such as the Nvidia GPU Technology Conference. Also, going back to school to earn an AI certification would be a good idea. And it couldn’t hurt to engage with friendly AI industry analysts for an expert’s reality check on all this.", "pub_date": "2020-01-30"}
]